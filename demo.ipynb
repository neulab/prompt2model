{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt2Model NoteBook Demo\n",
    "\n",
    "In our `cli_demo.py`, we hard-coded a lot of parameters which is actually configurable. In this jupternote book demo, we use the machine reading quesiton-answering problem, i.e. [squad](https://huggingface.co/datasets/squad), to give a quick guidance of how to configure these parameters for your own setting.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/neulab/prompt2model/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install prompt2model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!export OPENAI_API_KEY=your openai api key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Prompt\n",
    "\n",
    "Use the `OpenAIInstructionParser` to parse the input prompt.\n",
    "from prompt2model.prompt_parser import OpenAIInstructionParser, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt2model.prompt_parser import OpenAIInstructionParser, TaskType\n",
    "\n",
    "prompt = \"\"\"\n",
    "Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. The context is a descriptive passage related to the question and contains the answer. And the question can range from Math, Cultural, Social, Geometry, Biology, History, Sports, Technology, Science, and so on.\n",
    "\n",
    "Here are examples with input questions and context passages, along with their expected outputs:\n",
    "\n",
    "input=\"Question: What city did Super Bowl 50 take place in? Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\"\n",
    "output=\"Santa Clara\"\n",
    "\n",
    "input=\"Question: What river runs through Warsaw? Context: Warsaw (Polish: Warszawa [varˈʂava] ( listen); see also other names) is the capital and largest city of Poland. It stands on the Vistula River in east-central Poland, roughly 260 kilometres (160 mi) from the Baltic Sea and 300 kilometres (190 mi) from the Carpathian Mountains. Its population is estimated at 1.740 million residents within a greater metropolitan area of 2.666 million residents, which makes Warsaw the 9th most-populous capital city in the European Union. The city limits cover 516.9 square kilometres (199.6 sq mi), while the metropolitan area covers 6,100.43 square kilometres (2,355.39 sq mi).\"\n",
    "output=\"Vistula River\"\n",
    "\n",
    "input=\"Question: The Ottoman empire controlled territory on three continents, Africa, Asia and which other? Context: The Ottoman Empire was an imperial state that lasted from 1299 to 1923. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a powerful multinational, multilingual empire controlling much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the empire, while others were granted various types of autonomy during the course of centuries.\"\n",
    "output=\"Europe\"\n",
    "\"\"\"\n",
    "\n",
    "prompt_spec = OpenAIInstructionParser(task_type=TaskType.TEXT_GENERATION)\n",
    "prompt_spec.parse_from_prompt(prompt)\n",
    "print(f\"Instruction: {prompt_spec.instruction}\")\n",
    "print(f\"exmaples: {prompt_spec.examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Dataset\n",
    "\n",
    "Use the `DescriptionDatasetRetriever` to retrieve a dataset.\n",
    "\n",
    "Note that retriving a dataset is an interactive process. Watch the logging of the code block and input your response to in the input block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt2model.dataset_retriever import DescriptionDatasetRetriever\n",
    "\n",
    "retriever = DescriptionDatasetRetriever()\n",
    "retrieved_dataset_dict = retriever.retrieve_dataset_dict(prompt_spec)\n",
    "retrieved_dataset_dict.save_to_disk(\"retrieved_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Model\n",
    "\n",
    "Use the `DescriptionModelRetriever` to retrieve a pretrain model.\n",
    "\n",
    "The `top_model_names` is a list of pretrain model names of HuggingFace model. In our demo, we choose the first one as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt2model.model_retriever import DescriptionModelRetriever\n",
    "\n",
    "retriever = DescriptionModelRetriever(\n",
    "    model_descriptions_index_path=\"huggingface_data/huggingface_models/model_info/\",  # noqa E501\n",
    "    use_bm25=True,\n",
    "    use_HyDE=True,\n",
    ")\n",
    "top_model_names = retriever.retrieve(prompt_spec)\n",
    "pre_train_model_name = top_model_names[0]\n",
    "print(pre_train_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "Use `OpenAIDatasetGenerator` to generte new examples for the machine reading quesiton-answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt2model.dataset_generator import OpenAIDatasetGenerator, DatasetSplit\n",
    "\n",
    "unlimited_dataset_generator = OpenAIDatasetGenerator(\n",
    "    initial_temperature=0.3,\n",
    "    max_temperature=1.4,\n",
    "    responses_per_request=3,\n",
    "    max_api_calls=10000,\n",
    "    requests_per_minute=80,\n",
    ")\n",
    "generated_dataset = unlimited_dataset_generator.generate_dataset_split(\n",
    "    prompt_spec, 5000, split=DatasetSplit.TRAIN\n",
    ")\n",
    "generated_dataset.save_to_disk(\"generated_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset\n",
    "\n",
    "Combine the `generated_dataset` with `retrieved_dataset_dict` and use `TextualizeProcessor` to preprocess the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from prompt2model.dataset_processor import TextualizeProcessor\n",
    "\n",
    "train_generated_dataset = datasets.Dataset.from_dict(generated_dataset[:3000])\n",
    "val_generated_dataset = datasets.Dataset.from_dict(generated_dataset[3000: 4000])\n",
    "test_generated_dataset = datasets.Dataset.from_dict(generated_dataset[4000:])\n",
    "\n",
    "generated_dataset = datasets.DatasetDict(\n",
    "    {\"train\": train_generated_dataset, \"val\": val_generated_dataset, \"test\": test_generated_dataset}\n",
    ")\n",
    "\n",
    "retrieved_dataset = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\": datasets.Dataset.from_dict(retrieved_dataset_dict[\"train\"][:3000]),\n",
    "        \"val\": datasets.Dataset.from_dict(retrieved_dataset_dict[\"train\"][3000:4000]),\n",
    "        \"test\": datasets.Dataset.from_dict(retrieved_dataset_dict[\"train\"][4000:5000]),\n",
    "    }\n",
    ")\n",
    "\n",
    "DATASET_DICTS = [generated_dataset, retrieved_dataset]\n",
    "\n",
    "t5_processor = TextualizeProcessor(has_encoder=True)\n",
    "t5_modified_dataset_dicts = t5_processor.process_dataset_dict(\n",
    "    prompt_spec.instruction, DATASET_DICTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the Model\n",
    "\n",
    "Combine the retrieved dataset with generated dataset and use the `GenerationModelTrainer` to finetune the retrieved model. After the finetuning, we save the model and tokenizer to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt2model.model_trainer import GenerationModelTrainer\n",
    "from prompt2model.utils.logging_utils import get_formatted_logger\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "trainer_logger = get_formatted_logger(\"ModelTrainer\")\n",
    "trainer_logger.setLevel(logging.INFO)\n",
    "train_datasets = [each[\"train\"] for each in t5_modified_dataset_dicts]\n",
    "val_datasets = [each[\"val\"] for each in t5_modified_dataset_dicts]\n",
    "test_datasets = [each[\"test\"] for each in t5_modified_dataset_dicts]\n",
    "\n",
    "trainer = GenerationModelTrainer(\n",
    "    pre_train_model_name,\n",
    "    has_encoder=True,\n",
    "    executor_batch_size=1,\n",
    "    tokenizer_max_length=1024,\n",
    "    sequence_max_length=1280,\n",
    ")\n",
    "\n",
    "args_output_root = Path(\"result/training_output\")\n",
    "args_output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trained_model, trained_tokenizer = trainer.train_model(\n",
    "    hyperparameter_choices={\n",
    "        \"output_dir\": str(args_output_root),\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"per_device_train_batch_size\": 1,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "    },\n",
    "    training_datasets=train_datasets,\n",
    "    validation_datasets=val_datasets,\n",
    ")\n",
    "\n",
    "trained_model.save_pretrained(\"trained_model\")\n",
    "trained_tokenizer.save_pretrained(\"trained_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "After the trainning, we evalaute the trained model on the conbined test set with `ModelEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt2model.model_executor import GenerationModelExecutor\n",
    "from prompt2model.model_evaluator import Seq2SeqEvaluator\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "evaluator_logger = get_formatted_logger(\"ModelEvaluator\")\n",
    "evaluator_logger.setLevel(logging.INFO)\n",
    "\n",
    "trained_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"trained_model\"\n",
    ").to(device)\n",
    "trained_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"trained_tokenizer\"\n",
    ")\n",
    "\n",
    "test_dataset = datasets.concatenate_datasets(test_datasets)\n",
    "\n",
    "model_executor = GenerationModelExecutor(\n",
    "    trained_model,\n",
    "    trained_tokenizer,\n",
    "    1,\n",
    "    tokenizer_max_length=1024,\n",
    "    sequence_max_length=1280,\n",
    ")\n",
    "t5_outputs = model_executor.make_prediction(\n",
    "    test_set=test_dataset, input_column=\"model_input\"\n",
    ")\n",
    "evaluator = Seq2SeqEvaluator()\n",
    "metric_values = evaluator.evaluate_model(\n",
    "    test_dataset,\n",
    "    \"model_output\",\n",
    "    t5_outputs,\n",
    "    encoder_model_name=\"xlm-roberta-base\",\n",
    ")\n",
    "print(metric_values)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# ModelEvaluator Usage

## ModelEvaluator

The `ModelEvaluator` provides a standard interface and defines the necessary
methods for evaluating a model's performance on a given dataset.

To use the `ModelEvaluator`, you need to implement the following method:

- `evaluate_model()`: Evaluates a model on a test set and computes the specified
metrics.

The `ModelEvaluator` class can be subclassed to implement custom evaluation
logic based on different metrics or evaluation criteria.

To see an example of how to use `ModelEvaluator` and its subclasses, you can
refer to the unit tests in the
[model_evaluator_test.py](../tests/model_evaluator_test.py) file.

## Seq2SeqEvaluator

The `Seq2SeqEvaluator` is a concrete implementation of the `ModelEvaluator`
interface that computes the `ChrF++`, `Exact Match`, and `BERTScore` metrics for
sequence-to-sequence generation models.

## Usage

- Import the necessary modules:

```python
from prompt2model.evaluator import Seq2SeqEvaluator
from prompt2model.model_executor import ModelOutput
```

- Create an instance of the `Seq2SeqEvaluator`:

```python
evaluator = Seq2SeqEvaluator()
```

- Prepare the ground truth dataset and model predictions generated by
`ModelExecutor`:

```python
ground_truth = ["The cat is sleeping.", "The dog is playing."]
predictions = [
    ModelOutput("The cat is sleeping.", confidence=0.9, auxiliary_info={}),
    ModelOutput("The dog is barking.", confidence=0.8, auxiliary_info={}),
]
```

- Evaluate the model using the `evaluate_model()` method:

```python
dataset = ... # Prepare the dataset for evaluation gt_column =
"output_col" # Specify the column name for the ground truth metrics = [...] #
(Optional) Specify the metrics to use encoder_model_name = "xlm-roberta-base" #
(Optional) Specify the encoder model for metrics like BertScore

metric_values = evaluator.evaluate_model( dataset, gt_column, predictions,
metrics=metrics, encoder_model_name=encoder_model_name )
```

The `evaluate_model()` method computes the specified metrics for the model's
predictions on the ground truth dataset. The `gt_column` parameter sets the
column name in the dataset to use as the ground truth. The `metrics` parameter
(optional) allows you to specify a list of metrics for evaluation. The
`encoder_model_name` parameter (optional) is used for metrics like BertScore
that require an encoder model.

The method returns a dictionary of metric values, where the keys correspond to
the names of the metrics, and the values represent the computed metric values.

Please ensure that you have the necessary metric libraries installed and adjust
the code and configuration based on your specific evaluation requirements and
the structure of your dataset.

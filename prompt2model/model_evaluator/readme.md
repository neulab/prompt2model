# ModelEvaluator Usage

## ModelEvaluator

The `ModelEvaluator` provides a standard interface and defines the necessary
methods for evaluating a model's performance on a given dataset.

To use the `ModelEvaluator`, you need to implement the following method:

- `evaluate_model()`: Evaluates a model on a dataset set and computes specified
metrics.

The `ModelEvaluator` class can be subclassed to implement custom evaluation
logic based on different metrics or evaluation criteria.

## Seq2SeqEvaluator

The `Seq2SeqEvaluator` is a concrete implementation of the `ModelEvaluator`
interface that computes the `ChrF++`, `Exact Match`, and `BERTScore` metrics for
conditional generation models.

## Usage

- Import the necessary modules:

```python
from prompt2model.evaluator import Seq2SeqEvaluator
from prompt2model.model_executor import ModelOutput
```

- Create an instance of the `Seq2SeqEvaluator`:

```python
evaluator = Seq2SeqEvaluator()
```

- Prepare the validation dataset and model predictions generated by
  `ModelExecutor`. Additionally, some autoregressive
  models' generated output always contains the input. So the optional
  model_input is preferable for evaluating autoregressive models.
  We only support `ChrF++`, `Exact Match`, and `BERTScore`
  metrics by now, and you can select from these three, or the
  evaluation will use them all as default.

```python
GROUND_TRUTH = ["The cat is sleeping.", "The dog is playing."]
MODEL_INPUTS = [
    "Translate Chinese to English: 猫在睡觉。",
    "Translate Chinese to English: 狗在玩耍。",
]
GPT_PREDICTIONS = [
    ModelOutput(
        "Translate Chinese to English: 猫在睡觉。The cat is sleeping.", auxiliary_info={}
    ),
    ModelOutput(
        "Translate Chinese to English: 狗在玩耍。The dog is barking.", auxiliary_info={}
    ),
]
# Note that there is no eos token in the validation dataset.
VALIDATION_DATASET = Dataset.from_dict(
    {"model_ouput": GROUND_TRUTH, "model_input": MODEL_INPUTS}
)
```

- Evaluate the model using the `evaluate_model()` method:

```python
metric_values = evaluator.evaluate_model(
  dataset=VALIDATION_DATASET,
  gt_column="model_ouput",
  model_input_column="model_input",
  predictions=GPT_PREDICTIONS,
  encoder_model_name="xlm-roberta-base",
)
```

The `evaluate_model()` method computes the specified metrics for the model's
predictions on the ground truth dataset. The `gt_column` parameter sets the
column name in the dataset to use as the ground truth. The `metrics` parameter
(optional) allows you to specify a list of metrics for evaluation. The
`encoder_model_name` parameter (optional) is used for `BertScore`
requiring an encoder model. The `model_input_column` parameter (optional)
is used for autoregressive models' evaluation as discussed previously.

`evaluate_model` returns a dictionary of metric values, where
the keys correspond to the names of the metrics, and the values
represent the computed metric values.

Please ensure that you have the necessary metric libraries installed and adjust
the code and configuration based on your specific evaluation requirements and
the structure of your dataset.

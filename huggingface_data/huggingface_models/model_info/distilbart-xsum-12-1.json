{"pretrained_model_name": "sshleifer/distilbart-xsum-12-1", "description": "---\nlanguage: en\ntags:\n- summarization\nlicense: apache-2.0\ndatasets:\n- cnn_dailymail\n- xsum\nthumbnail: https://huggingface.co/front/thumbnails/distilbart_medium.png\n---\n\n### Usage\n\nThis checkpoint should be loaded into `BartForConditionalGeneration.from_pretrained`. See the [BART docs](https://huggingface.co/transformers/model_doc/bart.html?#transformers.BartForConditionalGeneration) for more information.\n\n### Metrics for DistilBART models\n\n| Model Name                 |   MM Params |   Inference Time (MS) |   Speedup |   Rouge 2 |   Rouge-L |\n|:---------------------------|------------:|----------------------:|----------:|----------:|----------:|\n| distilbart-xsum-12-1       |         222 |                    90 |      2.54 |     18.31 |     33.37 |\n| distilbart-xsum-6-6        |         230 |                   132 |      1.73 |     20.92 |     35.73 |\n| distilbart-xsum-12-3       |         255 |                   106 |      2.16 |     21.37 |     36.39 |\n| distilbart-xsum-9-6        |         268 |                   136 |      1.68 |     21.72 |     36.61 |\n| bart-large-xsum (baseline) |         406 |                   229 |      1    |     21.85 |     36.50 |\n| distilbart-xsum-12-6       |         306 |                   137 |      1.68 |     22.12 |     36.99 |\n| bart-large-cnn (baseline)  |         406 |                   381 |      1    |     21.06 |     30.63 |\n| distilbart-12-3-cnn        |         255 |                   214 |      1.78 |     20.57 |     30.00 |\n| distilbart-12-6-cnn        |         306 |                   307 |      1.24 |     21.26 |     30.59 |\n| distilbart-6-6-cnn         |         230 |                   182 |      2.09 |     20.17 |     29.70 |\n", "size_bytes": "443204783", "downloads": 659}
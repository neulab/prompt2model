{"pretrained_model_name": "tau/t5-v1_1-large-rss", "description": "---\nlanguage: en\ndatasets:\n- c4\n- wikipedia\nmetrics:\n- f1\n---\n\n# T5-V1.1-large-rss\nThis model is [T5-v1.1-large](https://huggingface.co/google/t5-v1_1-large) finetuned on RSS dataset. The model was finetuned as part of \n[\"How Optimal is Greedy Decoding for Extractive Question Answering?\"](https://arxiv.org/abs/2108.05857), while the RSS pretraining method was introduced in [this paper](https://arxiv.org/pdf/2101.00438.pdf).\n\n## Model description\nThe original [T5-v1.1-large](https://huggingface.co/google/t5-v1_1-large) was only pre-trained on C4 excluding any supervised training. Our version is further trained on Rucurrent Span Selection scheme (RSS), using a sample from the dataset used to pretrain [Splinter](tau/splinter-large):\n* contexts with a span occurring more than once are detected\n* a single instance of the recurring span is maked\n* the model is trained (teacher forcing) to predict the masked span\nThis training scheme naturally matches the extractive question answering task.\n\nDuring training time, the masked span is replaced with `<extra_id_0>` and the labels are formatted as `<extra_id_0>span<extra_id_0>`. Unlike [Splinter](tau/splinter-large), only one span is mask at a time.\n\n## Intended uses & limitations\nThis model naturally fits tasks where a span from a context is intended to be copied, like extractive question answering.\nThis checkpoint is primarily aimed to be used in zero-shot setting - further fine-tuning it on an annotated dataset gives equal results to those of the original T5-v1.1-large.\n\n### How to use\nYou can use this model directly but it is recommended to format the input to be aligned with that of the training scheme and as a text-question context:\n```python\nfrom transformers import  AutoModelForSeq2SeqLM, AutoTokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained('tau/t5-v1_1-large-rss')\ntokenizer = AutoTokenizer.from_pretrained('tau/t5-v1_1-large-rss')\n\npassage = 'Barack Hussein Obama II is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. '\nquestion = 'When was Obama inaugurated?'\ntext = f'Text: {passage}.\\nQuestion: {question}\\nAnswer:{tokenizer.additional_special_tokens[0]}.'\nencoded_input = tokenizer(text, return_tensors='pt')\noutput_ids = model.generate(input_ids=encoded_input.input_ids, attention_mask=encoded_input.attention_mask,\n               eos_token_id=tokenizer.additional_special_tokens_ids[1], num_beams=1, max_length=512, min_length=3)\ntokenizer.decode(output_ids[0])\n```\nThe generated answer is then `\"<pad><extra_id_0> 2009<extra_id_1>\"`, while the one generated by the original [T5-v1.1-large](https://huggingface.co/google/t5-v1_1-large) is `\"<pad><extra_id_0> On January 20, 2009<extra_id_1>\"` - a correct yet non-extractive answer.\n\n### Limitations and bias\nAlthough using the model with greedy decoding tends toward extracted outputs, is may sometimes produce non-extracted ones - may it be different casing or a whole different string (or substring) that may bear another semantic meaning.\n\n### Pretraining\nThe model was finetuned with 100,000 rss-examples for 3 epochs using Adafactor optimizer with constant learning rate of 5e-5.\n\n## Evaluation results\nEvaluated over few-shot QA in a zero-shot setting (no finetuning on annotated examples):\n\n|Model \\ Dataset| SQuAD |TriviaQA | NaturalQs | NewsQA | SearchQA | HotpotQA | BioASQ | TextbookQA| \n|:-------------:|:-----:|:-------:|:---------:|:------:|:--------:|:--------:|:------:|:---------:| \n|T5             | 50.4  | 61.7    | 42.1      | 19.2   | 24.0     | 43.3     | 55.5   | 17.8      | \n|T5-rss         | 71.4  | 69.3    | 57.2      | 43.2   | 29.7     | 59.0     | 65.5   | 39.0      | \n\nThe gap between the two models diminishes as more training examples are introduced, for additional result see the [paper]((https://arxiv.org/abs/2108.05857).\n\n### BibTeX entry and citation info\n```bibtex\n@inproceedings{ram-etal-2021-shot,\n    title = \"Few-Shot Question Answering by Pretraining Span Selection\",\n    author = \"Ram, Ori  and\n      Kirstain, Yuval  and\n      Berant, Jonathan  and\n      Globerson, Amir  and\n      Levy, Omer\",\n    booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.acl-long.239\",\n    doi = \"10.18653/v1/2021.acl-long.239\",\n    pages = \"3066--3079\",\n},\n@misc{castel2021optimal,\n      title={How Optimal is Greedy Decoding for Extractive Question Answering?}, \n      author={Or Castel and Ori Ram and Avia Efrat and Omer Levy},\n      year={2021},\n      eprint={2108.05857},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n```\n", "size_bytes": "3132858260", "downloads": 328}
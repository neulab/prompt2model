{"pretrained_model_name": "Zekunli/flan-t5-base-SQuAD-qg-ep10", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: flan-t5-base-SQuAD-qg-ep10\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-base-SQuAD-qg-ep10\n\nThis model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.3457\n- Rouge1: 40.1205\n- Rouge2: 18.6383\n- Rougel: 36.6147\n- Rougelsum: 36.6007\n- Gen Len: 13.6362\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 72\n- eval_batch_size: 144\n- seed: 1799\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.6188        | 0.76  | 200  | 1.3784          | 38.118  | 16.6537 | 34.5034 | 34.5096   | 14.0914 |\n| 1.5475        | 1.52  | 400  | 1.3669          | 38.4347 | 16.9434 | 34.832  | 34.8234   | 14.0755 |\n| 1.531         | 2.28  | 600  | 1.3597          | 38.7086 | 17.293  | 35.1629 | 35.1641   | 13.8447 |\n| 1.4996        | 3.04  | 800  | 1.3532          | 39.684  | 18.3222 | 36.162  | 36.1706   | 13.7223 |\n| 1.4857        | 3.8   | 1000 | 1.3521          | 39.518  | 18.1439 | 35.9424 | 35.9616   | 13.7654 |\n| 1.4661        | 4.56  | 1200 | 1.3499          | 39.8722 | 18.5417 | 36.3954 | 36.4133   | 13.6062 |\n| 1.4591        | 5.32  | 1400 | 1.3494          | 40.1145 | 18.6701 | 36.581  | 36.5954   | 13.6018 |\n| 1.4488        | 6.08  | 1600 | 1.3470          | 40.0079 | 18.6909 | 36.5312 | 36.5373   | 13.701  |\n| 1.437         | 6.84  | 1800 | 1.3471          | 40.355  | 18.8814 | 36.8203 | 36.8338   | 13.6488 |\n| 1.4389        | 7.6   | 2000 | 1.3461          | 40.189  | 18.7211 | 36.6556 | 36.6487   | 13.6744 |\n| 1.4146        | 8.37  | 2200 | 1.3464          | 40.3692 | 18.8247 | 36.7903 | 36.7986   | 13.6589 |\n| 1.421         | 9.13  | 2400 | 1.3457          | 40.1205 | 18.6383 | 36.6147 | 36.6007   | 13.6362 |\n| 1.4163        | 9.89  | 2600 | 1.3458          | 40.223  | 18.6957 | 36.6983 | 36.6998   | 13.657  |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "990406605", "downloads": 2}
{"pretrained_model_name": "ml6team/mt5-small-german-query-generation", "description": "---\nlanguage: \n- de\ntags:\n- pytorch\n- query-generation\nwidget:\n- text: \"Das Lama (Lama glama) ist eine Art der Kamele. Es ist in den s\u00fcdamerikanischen Anden verbreitet und eine vom Guanako abstammende Haustierform.\"\n  example_title: \"Article 1\"\nlicense: apache-2.0\nmetrics:\n- Rouge-Score\n---\n# mt5-small-german-query-generation \n\n## Model description:\nThis model was created with the purpose to generate possible queries for a german input article.\n\nFor this model, we finetuned a multilingual T5 model [mt5-small](https://huggingface.co/google/mt5-small) on the [MMARCO dataset](https://huggingface.co/datasets/unicamp-dl/mmarco) the machine translated version of the MS MARCO dataset. \n\n\nThe model was trained for 1 epoch, on 200,000 unique queries of the dataset. We trained the model on one K80 GPU for 25,000 iterations with following parameters:\n  - learning rate: 1e-3\n  - train batch size: 8\n  - max input sequence length: 512\n  - max target sequence length: 64\n\n\n## Model Performance:\n\nModel evaluation was done on 2000 evaluation paragraphs of the dataset. Mean [f1 ROUGE scores](https://github.com/pltrdy/rouge) were calculated for the model. \n\n| Rouge-1 | Rouge-2 | Rouge-L |\n|---|---|---|\n|0.162 | 0.052 | 0.161 |\n", "size_bytes": "1200770885", "downloads": 1705}
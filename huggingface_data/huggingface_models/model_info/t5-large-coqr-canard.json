{"pretrained_model_name": "zhuqi/t5-large-coqr-canard", "description": "---\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- t5-large\n- text2text-generation\n- conversational question rewriting\ndatasets:\n- CANARD\nmetrics:\n- BLEU\n\nmodel-index:\n- name: t5-large-coqr-canard\n  results:\n  - task:\n      type: text2text-generation\n      name: conversational question rewriting\n    dataset:\n      type: CANARD\n      name: CANARD\n      split: test\n    metrics:\n      - type: BLEU\n        value: 77.8\n        name: BLEU\n\nwidget:\n- text: \"Rewrite the question according to the given context to make the dialog fluent using anaphora and ellipsis.\\n\\nquestion: What else happened during 1977-1981 other than Superstar Billy Graham's return?\\n\\ncontext: Superstar Billy Graham\\nReturn to WWWF (1977-1981)\\nWhy did he return to the WWWF?\\nan agreement with promoter Vincent J. McMahon (Senior\\nWhat was his agreement with McMahon?\\nI don't know.\\nHow did people respond to his return?\\nI don't know.\"\n- text: \"Rewrite the question according to the given context to make the dialog fluent using anaphora and ellipsis.\\n\\nquestion: why did Billy Graham personally sued Zahorian and the WWF?\\n\\ncontext: Superstar Billy Graham\\nDisputes with the McMahons\\nwhat disputes did he have?\\nGraham personally sued Zahorian and the WWF,\"\n\ninference:\n  parameters:\n    max_length: 100\n\n---\n\n# t5-large-coqr-canard\n\nThis model is a fine-tuned version of [t5-large](https://huggingface.co/t5-large) on the [CANARD](https://sites.google.com/view/qanta/projects/canard) dataset.\nIt achieves the following results on the test set:\n- Loss: 0.3064\n- Bleu: 77.1979\n- Generation Length: 9.576\n\n## Model description\n\nCANARD dataset rewrites the original questions in conversations to make them context-independent (understandable w/o context).\nOn the contrary, this model is trained to rewrite context-independent questions to conversational questions, aiming to create fluent dialog with anaphora and ellipsis.\n\nInput: \n```\nRewrite the question according to the given context to make the dialog fluent using anaphora and ellipsis.\n\nquestion: How did people respond to Superstar Billy Graham's return?\n\ncontext: Superstar Billy Graham\nReturn to WWWF (1977-1981)\nWhy did he return to the WWWF?\nan agreement with promoter Vincent J. McMahon (Senior\nWhat was his agreement with McMahon?\nI don't know.\n```\n\nTarget:\n```\nHow did people respond to his return?\n```\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.001\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 8\n- total_train_batch_size: 512\n- total_eval_batch_size: 512\n- optimizer: Adafactor\n- lr_scheduler_type: linear\n- num_epochs: 1.0\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|\n| No log        | 1.0   | 62   | 0.2987          | 77.2361 | 9.4534  |\n\n\n### Framework versions\n\n- Transformers 4.20.1\n- Pytorch 1.11.0+cu113\n- Datasets 2.6.1\n- Tokenizers 0.12.1\n", "size_bytes": "2950730119", "downloads": 4}
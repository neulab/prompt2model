{"pretrained_model_name": "mrm8488/t5-small-finetuned-text2log", "description": "---\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- tex2log\n- log2tex\n- foc\n\nwidget:\n- text: \"translate to nl: all x1.(_explanation(x1) -> -_equal(x1))\"\n- text: \"translate to fol: All chains are bad.\"\n\nmodel-index:\n- name: t5-small-text2log\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# T5 (small) fine-tuned on Text2Log\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on an Text2Log dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0313\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 6\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss |\n|:-------------:|:-----:|:------:|:---------------:|\n| 0.0749        | 1.0   | 21661  | 0.0509          |\n| 0.0564        | 2.0   | 43322  | 0.0396          |\n| 0.0494        | 3.0   | 64983  | 0.0353          |\n| 0.0425        | 4.0   | 86644  | 0.0332          |\n| 0.04          | 5.0   | 108305 | 0.0320          |\n| 0.0381        | 6.0   | 129966 | 0.0313          |\n\n### Usage:\n```py\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\nMODEL_CKPT = \"mrm8488/t5-small-finetuned-text2log\"\n\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_CKPT).to(device)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)\n\ndef translate(text):\n    inputs = tokenizer(text, padding=\"longest\", max_length=64, return_tensors=\"pt\")\n    input_ids = inputs.input_ids.to(device)\n    attention_mask = inputs.attention_mask.to(device)\n\n    output = model.generate(input_ids, attention_mask=attention_mask, early_stopping=False, max_length=64)\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\nprompt_nl_to_fol = \"translate to fol: \"\nprompt_fol_to_nl = \"translate to nl: \"\nexample_1 = \"Every killer leaves something.\"\nexample_2 = \"all x1.(_woman(x1) -> exists x2.(_emotion(x2) & _experience(x1,x2)))\"\n\nprint(translate(prompt_nl_to_fol + example_1)) # all x1.(_killer(x1) -> exists x2._leave(x1,x2))\nprint(translate(prompt_fol_to_nl + example_2)) # Every woman experiences emotions.\n```\n\n### Framework versions\n\n- Transformers 4.17.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 1.18.3\n- Tokenizers 0.11.0\n", "size_bytes": "242085627", "downloads": 8}
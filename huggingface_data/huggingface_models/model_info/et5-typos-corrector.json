{"pretrained_model_name": "j5ng/et5-typos-corrector", "description": "---\nlanguage:\n- ko\npipeline_tag: text2text-generation\nlicense: apache-2.0\n---\n\n## \ud55c\uad6d\uc5b4 \ub9de\ucda4\ubc95 \uad50\uc815\uae30(Korean Typos Corrector)\n - ETRI-et5 \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c fine-tuning\ud55c \ud55c\uad6d\uc5b4 \uad6c\uc5b4\uccb4 \uc804\uc6a9 \ub9de\ucda4\ubc95 \uad50\uc815\uae30 \uc785\ub2c8\ub2e4.\n\n## Base on PLM model(ET5)\n - ETRI(https://aiopen.etri.re.kr/et5Model)\n\n## Base on Dataset\n - \ubaa8\ub450\uc758 \ub9d0\ubb49\uce58(https://corpus.korean.go.kr/request/reausetMain.do?lang=ko) \ub9de\ucda4\ubc95 \uad50\uc815 \ub370\uc774\ud130\n\n ## Data Preprocessing\n  - 1. \ud2b9\uc218\ubb38\uc790 \uc81c\uac70 (\uc27c\ud45c) .(\ub9c8\uce68\ud45c) \uc81c\uac70\n  - 2. null \uac12(\"\") \uc81c\uac70\n  - 3. \ub108\ubb34 \uc9e7\uc740 \ubb38\uc7a5 \uc81c\uac70(\uae38\uc774 2 \uc774\ud558) \n  - 4. \ubb38\uc7a5 \ub0b4 &name&, name1 \ub4f1 \uc774\ub984 \ud0dc\uadf8\uac00 \ud3ec\ud568\ub41c \ub2e8\uc5b4 \uc81c\uac70(\ub2e8\uc5b4\ub9cc \uc81c\uac70\ud558\uace0 \ubb38\uc7a5\uc740 \uc0b4\ub9bc)\n  - total : 318,882 \uc30d\n\n***\n\n## How to use\n```python\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\n# T5 \ubaa8\ub378 \ub85c\ub4dc\nmodel = T5ForConditionalGeneration.from_pretrained(\"j5ng/et5-typos-corrector\")\ntokenizer = T5Tokenizer.from_pretrained(\"j5ng/et5-typos-corrector\")\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n# device = \"mps:0\" if torch.cuda.is_available() else \"cpu\" # for mac m1\n\nmodel = model.to(device) \n\n# \uc608\uc2dc \uc785\ub825 \ubb38\uc7a5\ninput_text = \"\uc544\ub2ac \uc9c4\uc9dc \ubb34\u3153\ud558\ub0d0\uace0\"\n\n# \uc785\ub825 \ubb38\uc7a5 \uc778\ucf54\ub529\ninput_encoding = tokenizer(\"\ub9de\ucda4\ubc95\uc744 \uace0\uccd0\uc8fc\uc138\uc694: \" + input_text, return_tensors=\"pt\")\n\ninput_ids = input_encoding.input_ids.to(device)\nattention_mask = input_encoding.attention_mask.to(device)\n\n# T5 \ubaa8\ub378 \ucd9c\ub825 \uc0dd\uc131\noutput_encoding = model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    max_length=128,\n    num_beams=5,\n    early_stopping=True,\n)\n\n# \ucd9c\ub825 \ubb38\uc7a5 \ub514\ucf54\ub529\noutput_text = tokenizer.decode(output_encoding[0], skip_special_tokens=True)\n\n# \uacb0\uacfc \ucd9c\ub825\nprint(output_text) # \uc544\ub2c8 \uc9c4\uc9dc \ubb50 \ud558\ub0d0\uace0.\n```\n\n***\n\n## With Transformer Pipeline\n```python\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline\n\nmodel = T5ForConditionalGeneration.from_pretrained('j5ng/et5-typos-corrector')\ntokenizer = T5Tokenizer.from_pretrained('j5ng/et5-typos-corrector')\n\ntypos_corrector = pipeline(\n    \"text2text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1,\n    framework=\"pt\",\n)\n\ninput_text = \"\uc644\uc8e4 \uc5b4\uc774\uc5c5\u3145\ub124\uc9c4\uca2c\u314b\u314b\u314b\"\noutput_text = typos_corrector(\"\ub9de\ucda4\ubc95\uc744 \uace0\uccd0\uc8fc\uc138\uc694: \" + input_text,\n            max_length=128,\n            num_beams=5,\n            early_stopping=True)[0]['generated_text']\n\nprint(output_text) # \uc644\uc804 \uc5b4\uc774\uc5c6\ub124 \uc9c4\uc9dc \u110f\u110f\u110f\u110f.\n```", "size_bytes": "1296601269", "downloads": 241}
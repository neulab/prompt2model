{"pretrained_model_name": "din0s/t5-small-ro-finetuned-en-to-it", "description": "---\ntags:\n- generated_from_trainer\ndatasets:\n- ccmatrix\nmetrics:\n- bleu\nmodel-index:\n- name: t5-small_ro-finetuned-en-to-it\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: ccmatrix\n      type: ccmatrix\n      config: en-it\n      split: train[3000:12000]\n      args: en-it\n    metrics:\n    - name: Bleu\n      type: bleu\n      value: 7.11\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small_ro-finetuned-en-to-it\n\nThis model is a fine-tuned version of [din0s/t5-small-finetuned-en-to-ro](https://huggingface.co/din0s/t5-small-finetuned-en-to-ro) on the ccmatrix dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.3301\n- Bleu: 7.11\n- Gen Len: 59.538\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 96\n- eval_batch_size: 96\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 40\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Bleu   | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:-------:|\n| No log        | 1.0   | 94   | 3.1873          | 1.8829 | 94.336  |\n| No log        | 2.0   | 188  | 3.0132          | 2.5572 | 83.6413 |\n| No log        | 3.0   | 282  | 2.9226          | 3.0999 | 76.166  |\n| No log        | 4.0   | 376  | 2.8559          | 3.3859 | 73.906  |\n| No log        | 5.0   | 470  | 2.8040          | 3.8793 | 70.5667 |\n| 3.4136        | 6.0   | 564  | 2.7585          | 3.8974 | 69.404  |\n| 3.4136        | 7.0   | 658  | 2.7191          | 4.1236 | 68.6387 |\n| 3.4136        | 8.0   | 752  | 2.6836          | 4.3281 | 67.682  |\n| 3.4136        | 9.0   | 846  | 2.6509          | 4.343  | 67.0113 |\n| 3.4136        | 10.0  | 940  | 2.6231          | 4.561  | 66.094  |\n| 3.0128        | 11.0  | 1034 | 2.5984          | 4.9314 | 64.3033 |\n| 3.0128        | 12.0  | 1128 | 2.5755          | 5.0149 | 64.502  |\n| 3.0128        | 13.0  | 1222 | 2.5529          | 5.0758 | 65.194  |\n| 3.0128        | 14.0  | 1316 | 2.5332          | 5.2774 | 64.0773 |\n| 3.0128        | 15.0  | 1410 | 2.5128          | 5.3165 | 64.5213 |\n| 2.859         | 16.0  | 1504 | 2.4976          | 5.4714 | 62.884  |\n| 2.859         | 17.0  | 1598 | 2.4803          | 5.532  | 63.9593 |\n| 2.859         | 18.0  | 1692 | 2.4662          | 5.6314 | 63.7273 |\n| 2.859         | 19.0  | 1786 | 2.4529          | 5.8855 | 62.8953 |\n| 2.859         | 20.0  | 1880 | 2.4401          | 6.1225 | 61.496  |\n| 2.859         | 21.0  | 1974 | 2.4285          | 6.2697 | 60.034  |\n| 2.7544        | 22.0  | 2068 | 2.4171          | 6.4153 | 60.6027 |\n| 2.7544        | 23.0  | 2162 | 2.4067          | 6.3731 | 61.08   |\n| 2.7544        | 24.0  | 2256 | 2.3974          | 6.5355 | 61.3707 |\n| 2.7544        | 25.0  | 2350 | 2.3882          | 6.5259 | 60.966  |\n| 2.7544        | 26.0  | 2444 | 2.3808          | 6.7178 | 60.1707 |\n| 2.6867        | 27.0  | 2538 | 2.3733          | 6.8196 | 60.398  |\n| 2.6867        | 28.0  | 2632 | 2.3672          | 6.811  | 60.2753 |\n| 2.6867        | 29.0  | 2726 | 2.3609          | 6.8322 | 60.0973 |\n| 2.6867        | 30.0  | 2820 | 2.3551          | 6.8524 | 60.16   |\n| 2.6867        | 31.0  | 2914 | 2.3502          | 6.8363 | 60.2867 |\n| 2.6395        | 32.0  | 3008 | 2.3455          | 6.9495 | 59.8547 |\n| 2.6395        | 33.0  | 3102 | 2.3422          | 6.9939 | 59.4813 |\n| 2.6395        | 34.0  | 3196 | 2.3387          | 6.9953 | 59.812  |\n| 2.6395        | 35.0  | 3290 | 2.3362          | 7.0318 | 59.8813 |\n| 2.6395        | 36.0  | 3384 | 2.3339          | 7.1114 | 59.232  |\n| 2.6395        | 37.0  | 3478 | 2.3322          | 7.1488 | 59.292  |\n| 2.6138        | 38.0  | 3572 | 2.3310          | 7.1334 | 59.6093 |\n| 2.6138        | 39.0  | 3666 | 2.3303          | 7.0984 | 59.6887 |\n| 2.6138        | 40.0  | 3760 | 2.3301          | 7.11   | 59.538  |\n\n\n### Framework versions\n\n- Transformers 4.22.1\n- Pytorch 1.12.1\n- Datasets 2.5.1\n- Tokenizers 0.11.0\n", "size_bytes": "242070267", "downloads": 2}
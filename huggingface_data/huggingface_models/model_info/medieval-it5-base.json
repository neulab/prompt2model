{"pretrained_model_name": "leobertolazzi/medieval-it5-base", "description": "---\nmodel-index:\n- name: medieval-it5-base\n  results: []\nlanguage:\n- it\n---\n\n# medieval-it5-base\n\nThis model is a version of [gsarti/it5-base](https://huggingface.co/gsarti/it5-base) fine-tuned on a dataset called [ita2medieval](https://huggingface.co/datasets/leobertolazzi/ita2medieval). The Dataset contains sentences from medieval italian along with paraphrases in contemporary italian (approximately 6.5k pairs in total).\n\nThe fine-tuning task is text-style-tansfer from contemporary to medieval italian.\n\n\n## Using the model\n\n```\nfrom transformers import AutoTokenzier, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"leobertolazzi/medieval-it5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"leobertolazzi/medieval-it5-base\")\n```\n\nFlax and Tensorflow versions of the model are also available:\n```\nfrom transformers import FlaxT5ForConditionalGeneration, TFT5ForConditionalGeneration\nmodel_flax = FlaxT5ForConditionalGeneration.from_pretrained(\"leobertolazzi/medieval-it5-base\")\nmodel_tf = TFT5ForConditionalGeneration.from_pretrained(\"leobertolazzi/medieval-it5-base\")\n```\n\n## Training procedure\n\nThe code used for the fine-tuning is available in this [repo](https://github.com/leobertolazzi/medievalIT5)\n\n## Intended uses & limitations\n\nThe biggest limitation for this project is the size of the ita2medieval dataset. In fact, it consists only of 6.5K sentence pairs whereas [gsarti/it5-base](https://huggingface.co/gsarti/it5-base) has 220M parameters.\n\nFor this reason the results can be far from perfect, but some nice style translations can also be obtained.\n\nIt would be nice to expand ita2medieval with text and paraphrases from more medieval italian authors!\n\n### Framework versions\n\n- Transformers 4.26.0\n- Tokenizers 0.13.2\n", "size_bytes": "990251317", "downloads": 4}
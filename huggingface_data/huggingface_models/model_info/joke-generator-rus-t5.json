{"pretrained_model_name": "naltukhov/joke-generator-rus-t5", "description": "---\nlicense: afl-3.0\nlanguage:\n- ru\nlibrary_name: transformers\npipeline_tag: text2text-generation\ntags:\n- humor\n- T5\n- jokes-generation\n---\n\n\n## Task\nModel create for jokes generation task on Russian language.\nGenerate jokes from scratch is too difficult task. Too make it easier jokes was splitted into setup and punch pairs.\nEach setup can produce infinite number of punches so inspiration was also introduced,\nwhich means main idea (or main word) of punch for given setup. In the real world, jokes come in different qualities (bad, good, funny, ...).\nTherefore, in order for the models to distinguish them from each other, a mark was introduced. It ranges from 0 (not a joke) to 5 (golden joke).\n\n\n## Info\nModel trained using flax on huge dataset with jokes and anekdots on different tasks:\n1. Span masks (dataset size: 850K)\n2. Conditional generation tasks (simultaneously):  \n  a. Generate inspiration by given setup (dataset size: 230K)  \n  b. Generate punch by given setup and inspiration (dataset size: 240K)  \n  c. Generate mark by given setup and punch (dataset size: 200K)  \n\n## Ethical considerations and risks\nModel is fine-tuned on a large corpus of humorous text data scraped from from websites/telegram channels with anecdotes, shortliners, jokes.\nText was not filtered for explicit content or assessed for existing biases.\nAs a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases\nin the underlying data.\nPlease don't take it seriously.\n", "size_bytes": "2950837703", "downloads": 484}
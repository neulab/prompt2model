{"pretrained_model_name": "MBZUAI/LaMini-Flan-T5-77M", "description": "---\nlicense: cc-by-nc-4.0\ntags:\n- generated_from_trainer\n- instruction fine-tuning\nmodel-index:\n- name: flan-t5-small-distil-v2\n  results: []\nlanguage:\n- en\npipeline_tag: text2text-generation\nwidget:\n  - text: >-\n      how can I become more healthy?\n    example_title: example\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n<p align=\"center\" width=\"100%\">\n    <a><img src=\"https://raw.githubusercontent.com/mbzuai-nlp/lamini-lm/main/images/lamini.png\" alt=\"Title\" style=\"width: 100%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n# LaMini-Flan-T5-77M\n\n[![Model License](https://img.shields.io/badge/Model%20License-CC%20By%20NC%204.0-red.svg)]()\n\nThis model is one of our LaMini-LM model series in paper \"[LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions](https://github.com/mbzuai-nlp/lamini-lm)\". This model is a fine-tuned version of [google/flan-t5-small](https://huggingface.co/google/flan-t5-small) on [LaMini-instruction dataset](https://huggingface.co/datasets/MBZUAI/LaMini-instruction) that contains 2.58M samples for instruction fine-tuning. For more information about our dataset, please refer to our [project repository](https://github.com/mbzuai-nlp/lamini-lm/).  \nYou can view other models of LaMini-LM series as follows. Models with \u2729 are those with the best overall performance given their size/architecture, hence we recommend using them. More details can be seen in our paper. \n\n<table>\n<thead>\n  <tr>\n    <th>Base model</th>\n    <th colspan=\"4\">LaMini-LM series (#parameters)</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>T5</td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-t5-61m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-T5-61M</a></td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-t5-223m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-T5-223M</a></td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-t5-738m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-T5-738M</a></td>\n    <td></td>\n  </tr>\n   <tr>\n        <td>Flan-T5</td>\n        <td><a href=\"https://huggingface.co/MBZUAI/lamini-flan-t5-77m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-Flan-T5-77M</a>\u2729</td>\n        <td><a href=\"https://huggingface.co/MBZUAI/lamini-flan-t5-248m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-Flan-T5-248M</a>\u2729</td>\n        <td><a href=\"https://huggingface.co/MBZUAI/lamini-flan-t5-783m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-Flan-T5-783M</a>\u2729</td>\n    <td></td>\n  </tr>\n    <tr>\n    <td>Cerebras-GPT</td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-cerebras-111m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-Cerebras-111M</a></td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-cerebras-256m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-Cerebras-256M</a></td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-cerebras-590m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-Cerebras-590M</a></td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-cerebras-1.3b\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-Cerebras-1.3B</a></td>\n  </tr>\n  <tr>\n    <td>GPT-2</td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-gpt-124m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-GPT-124M</a>\u2729</td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-gpt-774m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-GPT-774M</a>\u2729</td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-gpt-1.5b\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-GPT-1.5B</a>\u2729</td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>GPT-Neo</td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-neo-125m\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-Neo-125M</a></td>\n    <td><a href=\"https://huggingface.co/MBZUAI/lamini-neo-1.3b\" target=\"_blank\" rel=\"noopener noreferrer\">LaMini-Neo-1.3B</a></td>\n    <td></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>GPT-J</td>\n    <td colspan=\"4\">coming soon</td>\n  </tr>\n  <tr>\n    <td>LLaMA</td>\n    <td colspan=\"4\">coming soon</td>\n  </tr>\n\n  \n</tbody>\n</table>\n\n\n## Use\n\n### Intended use\nWe recommend using the model to response to human instructions written in natural language. \n\nWe now show you how to load and use our model using HuggingFace `pipeline()`.\n\n```python\n# pip install -q transformers\nfrom transformers import pipeline\n\ncheckpoint = \"{model_name}\"\n\nmodel = pipeline('text2text-generation', model = checkpoint)\n\ninput_prompt = 'Please let me know your thoughts on the given place and why you think it deserves to be visited: \\n\"Barcelona, Spain\"'\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\n\nprint(\"Response\", generated_text)\n```\n\n## Training Procedure\n\n<p align=\"center\" width=\"100%\">\n    <a><img src=\"https://raw.githubusercontent.com/mbzuai-nlp/lamini-lm/main/images/lamini-pipeline.drawio.png\" alt=\"Title\" style=\"width: 100%; min-width: 250px; display: block; margin: auto;\"></a>\n</p>\n\nWe initialize with [google/flan-t5-small](https://huggingface.co/google/flan-t5-small) and fine-tune it on our [LaMini-instruction dataset](https://huggingface.co/datasets/MBZUAI/LaMini-instruction). Its total number of parameters is 77M. \n\n### Training Hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 128\n- eval_batch_size: 64\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 512\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n## Evaluation\nWe conducted two sets of evaluations: automatic evaluation on downstream NLP tasks and human evaluation on user-oriented instructions. For more detail, please refer to our [paper](). \n\n## Limitations\n\nMore information needed\n\n\n# Citation\n\n```bibtex\n@article{lamini-lm,\n  author       = {Minghao Wu and\n                  Abdul Waheed and\n                  Chiyu Zhang and\n                  Muhammad Abdul-Mageed and\n                  Alham Fikri Aji\n                  },\n  title        = {LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},\n  journal      = {CoRR},\n  volume       = {abs/2304.14402},\n  year         = {2023},\n  url          = {https://arxiv.org/abs/2304.14402},\n  eprinttype   = {arXiv},\n  eprint       = {2304.14402}\n}\n```", "size_bytes": "307910149", "downloads": 854}
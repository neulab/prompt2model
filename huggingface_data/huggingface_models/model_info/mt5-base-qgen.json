{"pretrained_model_name": "nbroad/mt5-base-qgen", "description": "---\nlanguage:\n- en\n- hi\n- de\n- ar\n- bn\n- fi\n- ja\n- zh\n- id\n- sw\n- ta\n- gr\n- ru\n- es\n- th\n- tr\n- vi\n- multilingual\ndatasets:\n- squad_v2\n- tydiqa\n- mlqa\n- xquad\n- germanquad\nwidget:\n- text: 'Hugging Face has seen rapid growth in its popularity since the get-go. It\n    is definitely doing the right things to attract more and more people to its platform,\n    some of which are on the following lines: Community driven approach through large\n    open source repositories along with paid services. Helps to build a network of\n    like-minded people passionate about open source. Attractive price point. The subscription-based\n    features, e.g.: Inference based API, starts at a price of $9/month.'\n  example_title: English\n- text: 'A un a\u00f1o y tres d\u00edas de que el bal\u00f3n ruede en el Al Bayt Stadium inaugurando\n    el Mundial 2022, ya se han dibujado los primeros bocetos de la pr\u00f3xima Copa del\n    Mundo.13 selecciones est\u00e1n colocadas en el mapa con la etiqueta de clasificadas\n    y tienen asegurado pisar los verdes de Qatar en la primera fase final  oto\u00f1al.\n    Serbia, Dinamarca, Espa\u00f1a, Pa\u00edses Bajos, Suiza, Croacia, Francia, Inglaterra,\n    B\u00e9lgica, Alemania, Brasil, Argentina y Qatar, como anfitriona, entrar\u00e1n en   el\n    sorteo del 1 de abril de 2022 en Doha en el que 32 pa\u00edsses ser\u00e1n repartidos en\n    sus respectivos grupos. '\n  example_title: Spanish\n---\n# Multi-lingual Question Generating Model (mt5-base)\nGive the model a passage and it will generate a question about the passage.  \n\n## Trained on the following datasets:\n\n- [SQuAD (English)](https://rajpurkar.github.io/SQuAD-explorer/)\n- [TyDiQA-GoldP (Arabic, Bengali, Finnish, Japanese, Indonesian, Kiswahili, Korean, Russian, Telugu, Thai)](https://github.com/google-research-datasets/tydiqa)\n- [MLQA (Arabic, Chinese, English, German, Hindi, Spanish, Vietnames)](https://github.com/facebookresearch/MLQA)\n- [XQuAD (Arabic, Chinese, German, Greek, Hindi, Russian, Spanish, Thai, Turkish Vietnamese)](https://github.com/deepmind/xquad)\n- [GermanQuAD (German)](https://huggingface.co/datasets/deepset/germanquad)\n- [Persian QA (Persian)](https://www.kaggle.com/sajjadayobi360/persianqa)\n- [Bengali QA (Bengali)](https://www.kaggle.com/mayeesha/bengali-question-answering-dataset)\n- [chaii (Hindi, Tamil)](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/data)\n\n\n## Training details\nI used [flax summarization script](https://github.com/huggingface/transformers/tree/master/examples/flax/summarization) and a TPU v3-8. Summarization expects a text column and a summary column. For question generation training, use the context column instead of text column and question instead of summary column.\n\n\nThere is no guarantee that it will produce a question in the language of the passage, but it usually does. Lower resource languages will likely have lower quality questions.\n\n\n## Using the model\n\n#### PyTorch version\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n  \ntokenizer = AutoTokenizer.from_pretrained(\"nbroad/mt5-base-qgen\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"nbroad/mt5-base-qgen\")\n\ntext = \"Hugging Face has seen rapid growth in its \\\npopularity since the get-go. It is definitely doing\\\n the right things to attract more and more people to \\\n its platform, some of which are on the following lines:\\\nCommunity driven approach through large open source repositories \\\nalong with paid services. Helps to build a network of like-minded\\\n people passionate about open source. \\\nAttractive price point. The subscription-based features, e.g.: \\\nInference based API, starts at a price of $9/month.\\\n\"\n\ninputs = tokenizer(text, return_tensors=\"pt\")\noutput = model.generate(**inputs, max_length=40)\n\ntokenizer.decode(output[0], skip_special_tokens=True)\n# What is Hugging Face's price point?\n```\n\nModel trained on Cloud TPUs from Google's TPU Research Cloud (TRC)", "size_bytes": "2329696205", "downloads": 83}
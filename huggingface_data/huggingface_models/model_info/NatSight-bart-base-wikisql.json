{"pretrained_model_name": "course5i/NatSight-bart-base-wikisql", "description": "---\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- NatSight-AdpSeq2Seq\n- Text2SQL\ndatasets:\n- wikisql\nwidget:\n- text: \"What was the number of race that Kevin Curtain won? </s> c0 | number <eom>  v4 | Kevin Curtain </s> c0 | No <eom> c1 | Date <eom> c2 | Round <eom> c3 | Circuit <eom> c4 | Pole_Position <eom> c5 | Fastest_Lap <eom> c6 | Race_winner <eom> c7 | Report\"\n---\n\n## Paper\n\n## [NatSight: A framework for building domain agnostic Natural Language Interface to Databases for next-gen Augmented Analytics](https://dcal.iimb.ac.in/baiconf2022/full_papers/2346.pdf)\nAuthors: *Rohit Sroch*, *Dhiraj Patnaik*, *Jayachandran Ramachandran*\n\n## Abstract\n\nIn modern organizations, a large volume of customer, transactional, and operational data is stored in relational database management systems (RDBMS). It provides scalability and agility for various business use cases. However, the interaction between these databases and business users is limited as users often lack the knowledge of query languages such as SQL and they need to rely on technical experts to interact with the database for curating insights \\& analytics. Recent advances in augmented analytics platforms have enabled business users to engage with data in natural language and consume insights in the form of data tables and charts. There are still limitations as the experience is still suboptimal.\n\nThe development of natural language interfaces to databases has long been a challenge as previous approaches rely on a considerable amount of human-labeled data for domain adaptation. Moreover, most interfaces provide a constrained environment and do not allow the users to freely and naturally interact with data.\n\nIn this work, we propose our framework for building domain-agnostic natural language (NL) interfaces to relational databases (NLIDBs) in few-shot and zero-shot scenarios. Also, recent advancements in the area of Transfer learning allowed us to leverage Transformer-based pre-trained language models (PLMs), resulting in various real-world applications in functional areas like CRM, Supply Chain, Ecommerce, Health Care, etc. for getting real-time insights. More specifically, our framework works in the following ways: First, it provides graph focused auto-suggestions to complete the natural language queries based on the graph representation of database schema, and Second, it uses an adaptive sequence-to-sequence translator model that translates natural language queries to corresponding SQL queries. Furthermore, a feedback loop is used to improve the system based on active learning.\n\nExperiment results on benchmark datasets show that our approach achieves a state-of-the-art performance and can be effective in the few-shot and zero-shot scenarios for domain-agnostic applications.\n\n*Sroch, R. & Patnaik, D. & Ramachandran, J. (2022). [NatSight: A framework for building domain agnostic Natural Language Interface to Databases for next-gen Augmented Analytics](https://dcal.iimb.ac.in/baiconf2022/full_papers/2346.pdf). \n        9th International Conference on Business Analytics and Intelligence, IIM Banglore (BAI Conf\u201922).*\n\n## NatSight-bart-base-wikisql\n\n For weights initialization, we used [facebook/bart-base](https://huggingface.co/facebook/bart-base) and fine-tune as sequence-to-sequence task.\n\n\n## Using Transformers\ud83e\udd17\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"course5i/NatSight-bart-base-wikisql\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"course5i/NatSight-bart-base-wikisql\")\n\n# define input\nraw_nat_query = \"What was the number of race that Kevin Curtain won?\"\nquery_mention_schema = \"c0 | number <eom>  v4 | Kevin Curtain\"\ntable_header_schema = \"c0 | No <eom> c1 | Date <eom> c2 | Round <eom> c3 | Circuit <eom> c4 | Pole_Position <eom> c5 | Fastest_Lap <eom> c6 | Race_winner <eom> c7 | Report\"\n\nencoder_input = raw_nat_query + \" </s> \" + query_mention_schema + \" </s> \" + table_header_schema\ninput_ids = tokenizer.encode(encoder_input, return_tensors=\"pt\", add_special_tokens=True)\n\ngenerated_ids = model.generate(input_ids=input_ids, num_beams=5, max_length=128)\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\noutput = preds[0]\n\nprint(\"Output generic SQL query: {}\".format(output))\n\n# output\n\"SELECT COUNT(c0) FROM TABLE WHERE c4 = v4\"\n\n```\n\n## Intended uses & limitations\n\nMore information needed\n\n### Training hyperparameters\n\nPlease take a look at the `training_args.bin` file\n\n```python\n$ import torch\n$ hyperparameters = torch.load(os.path.join('training_args.bin'))\n\n```\n        \n\n### Dev/Test results\n\n|| |NatSight-AdpSeq2Seq (BART-base) | |\n|:----|:----|:----|:----|\n||Dev ||Test |\n|Acc-(Logical form) %|Acc-(Execution) %|Acc-(Logical form) %|Acc-(Execution) %|\n|83.38|87.83|84|86.39|\n\n\n### Framework versions\n\n- Transformers >=4.8.0\n- Pytorch >=1.6.0\n- TensorFlow >=2.5.0\n- Datasets >=1.10.2\n- Tokenizers >=0.11.6\n\nIf you use these models, please cite the following paper:\n\n\n        ```\n        @article{article, \n            author={Sroch, R. & Patnaik, D. & Ramachandran, J}, \n            title={NatSight: A framework for building domain agnostic Natural Language Interface to Databases for next-gen Augmented Analytics}, \n            journal={9th International Conference on Business Analytics and Intelligence, IIM Banglore (BAI Conf\u201922)},\n            day={17},\n            year={2022}, \n            month={Dec},\n            url = {https://dcal.iimb.ac.in/baiconf2022/full_papers/2346.pdf}                                                  \n        } \n        ```", "size_bytes": "557972217", "downloads": 9}
{"pretrained_model_name": "ccdv/lsg-bart-base-16384", "description": "---\ntags:\n- summarization\n- bart\n- long context\nlanguage:\n- en\npipeline_tag: fill-mask\n---\n\n# LSG model \n**Transformers >= 4.23.1**\\\n**This model relies on a custom modeling file, you need to add trust_remote_code=True**\\\n**See [\\#13467](https://github.com/huggingface/transformers/pull/13467)**\n\nLSG ArXiv [paper](https://arxiv.org/abs/2210.15497). \\\nGithub/conversion script is available at this [link](https://github.com/ccdv-ai/convert_checkpoint_to_lsg).\n\n* [Usage](#usage)\n* [Parameters](#parameters)\n* [Sparse selection type](#sparse-selection-type)\n* [Tasks](#tasks)\n\nThis model is adapted from [BART-base](https://huggingface.co/facebook/bart-base) for encoder-decoder tasks without additional pretraining. It uses the same number of parameters/layers and the same tokenizer.\n\n\nThis model can handle long sequences but faster and more efficiently than Longformer (LED) or BigBird (Pegasus) from the hub and relies on Local + Sparse + Global attention (LSG).\n\nThe model requires sequences whose length is a multiple of the block size. The model is \"adaptive\" and automatically pads the sequences if needed (adaptive=True in config). It is however recommended, thanks to the tokenizer, to truncate the inputs (truncation=True) and optionally to pad with a multiple of the block size (pad_to_multiple_of=...). \\\n\nImplemented in PyTorch.\n\n![attn](attn.png)\n\n## Usage\nThe model relies on a custom modeling file, you need to add trust_remote_code=True to use it.\n\n```python: \nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"ccdv/lsg-bart-base-16384\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-16384\")\n``` \n\n## Parameters\nYou can change various parameters like : \n* the number of global tokens (num_global_tokens=1)\n* local block size (block_size=128)\n* sparse block size (sparse_block_size=128)\n* sparsity factor (sparsity_factor=2)\n* mask_first_token (mask first token since it is redundant with the first global token)\n* see config.json file\n\nDefault parameters work well in practice. If you are short on memory, reduce block sizes, increase sparsity factor and remove dropout in the attention score matrix.\n\n```python:\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"ccdv/lsg-bart-base-16384\", \n    trust_remote_code=True, \n    num_global_tokens=16,\n    block_size=64,\n    sparse_block_size=64,\n    attention_probs_dropout_prob=0.0\n    sparsity_factor=4,\n    sparsity_type=\"none\",\n    mask_first_token=True\n)\n``` \n\n## Sparse selection type\n\nThere are 5 different sparse selection patterns. The best type is task dependent. \\\nNote that for sequences with length < 2*block_size, the type has no effect.\n\n* sparsity_type=\"norm\", select highest norm tokens\n    * Works best for a small sparsity_factor (2 to 4)\n    * Additional parameters:\n        * None\n* sparsity_type=\"pooling\", use average pooling to merge tokens\n    * Works best for a small sparsity_factor (2 to 4)\n    * Additional parameters:\n        * None\n* sparsity_type=\"lsh\", use the LSH algorithm to cluster similar tokens\n    * Works best for a large sparsity_factor (4+)\n    * LSH relies on random projections, thus inference may differ slightly with different seeds\n    * Additional parameters:\n        * lsg_num_pre_rounds=1, pre merge tokens n times before computing centroids\n* sparsity_type=\"stride\", use a striding mecanism per head\n    * Each head will use different tokens strided by sparsify_factor\n    * Not recommended if sparsify_factor > num_heads\n* sparsity_type=\"block_stride\", use a striding mecanism per head\n    * Each head will use block of tokens strided by sparsify_factor\n    * Not recommended if sparsify_factor > num_heads\n\n## Tasks\nSeq2Seq example for summarization:\n```python:\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ccdv/lsg-bart-base-16384\", \n    trust_remote_code=True, \n    pass_global_tokens_to_decoder=True, # Pass encoder global tokens to decoder\n)\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-16384\")\n\nSENTENCE = \"This is a test sequence to test the model. \" * 300\ntoken_ids = tokenizer(\n    SENTENCE, \n    return_tensors=\"pt\", \n    padding=\"max_length\", # Optional but recommended\n    truncation=True # Optional but recommended\n    )\noutput = model(**token_ids)\n```\n\n\nClassification example:\n```python:\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ccdv/lsg-bart-base-16384\", \n    trust_remote_code=True, \n    pass_global_tokens_to_decoder=True, # Pass encoder global tokens to decoder\n)\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-16384\")\n\nSENTENCE = \"This is a test sequence to test the model. \" * 300\ntoken_ids = tokenizer(\n    SENTENCE, \n    return_tensors=\"pt\", \n    #pad_to_multiple_of=... # Optional\n    truncation=True\n    )\noutput = model(**token_ids)\n\n> SequenceClassifierOutput(loss=None, logits=tensor([[-0.3051, -0.1762]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n```\n\n**BART**\n```\n@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```", "size_bytes": "653910455", "downloads": 12}
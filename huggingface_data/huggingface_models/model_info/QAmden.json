{"pretrained_model_name": "biu-nlp/QAmden", "description": "---\n\nlanguage: en\n\nlicense: apache-2.0\n\n---\n\n\n# \ud83c\udfecQAmden\ud83c\udfec: Question-Answering-based Multi-DocumENt model\n\nHF-version of the QAmden model: *Peek Across*: Improving Multi-Document Modeling via Cross-Document Question-Answering (ACL 2023). \n\n\nYou can use it by \n\n```python\nfrom transformers import (\n    AutoTokenizer,\n    LEDConfig,\n    LEDForConditionalGeneration,\n)\n# load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('biu-nlp/QAmden')\nconfig=LEDConfig.from_pretrained('biu-nlp/QAmden')\n\nmodel = LEDForConditionalGeneration.from_pretrained('biu-nlp/QAmden')\n```\n\nThe original repo is [here](https://github.com/aviclu/peekacross).\n\nIf you find our work useful, please cite the paper as:\n\n```python\n@article{caciularu2023peekacross,\n  title={Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering},\n  author={Caciularu, Avi and Peters, Matthew E and Goldberger, Jacob and Dagan, Ido and Cohan, Arman},\n  journal={The 61st Annual Meeting of the Association for Computational Linguistics: ACL 2023},\n  year={2023}\n}\n```\n", "size_bytes": "1839608817", "downloads": 2}
{"pretrained_model_name": "fewshot-goes-multilingual/mTk-AdversarialQA_en-SberQuAD_ru-1B", "description": "---\nlicense: mit\ndatasets:\n- sberquad\n- adversarial_qa\nlanguage:\n- en\n- ru\nmetrics:\n- rouge\npipeline_tag: text2text-generation\n---\n\n# Model Card for mTk-AdversarialQA_en-SberQuAD_ru-1B\nThis model is a generative in-context few-shot learner specialized in Russian. It was trained on a combination of English AdversarialQA and Russian SberQuAD datasets.\n\nYou can find detailed information on [Project Github](https://github.com/fewshot-goes-multilingual/slavic-incontext-learning) & the referenced paper.\n\n## Model Details\n### Model Description\n- **Developed by:** Michal Stefanik & Marek Kadlcik, Masaryk University\n- **Model type:** mt5\n- **Language(s) (NLP):** en,ru\n- **License:** MIT\n- **Finetuned from model:** google/mt5-large\n### Model Sources\n- **Repository:** https://github.com/fewshot-goes-multilingual/slavic-incontext-learning\n- **Paper:** https://arxiv.org/abs/2304.01922\n## Uses\nThis model is intended to be used in a few-shot in-context learning format in the target language (Russian), or in the source language (English, see below).\nIt was evaluated for unseen task learning (with k=3 demonstrations) in Russian: see the referenced paper for details.\n### How to Get Started with the Model\nUse the code below to get started with the model.\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"{this model path}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{this model path}\")\n# Instead, use keywords \"\u0412\u043e\u043f\u0440\u043e\u0441\", \"\u041a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\" and \"\u041e\u0442\u0432\u0435\u0447\u0430\u0442\u044c\" for Russian few-shot prompts\ninput_text = \"\"\"\n    Question: What is the customer's name? \n    Context: Origin: Barrack Obama, Customer id: Bill Moe. \n    Answer: Bill Moe, \n    Question: What is the customer's name? \n    Context: Customer id: Barrack Obama, if not deliverable, return to Bill Clinton. \n    Answer:\n\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**inputs)\nprint(\"Answer:\")\nprint(tokenizer.decode(outputs))\n```\n## Training Details\nTraining this model can be reproduced by running `pip install -r requirements.txt && python train_mt5_qa_en_AQA+ru_info.py\n`. \nSee the referenced script for hyperparameters and other training configurations.\n## Citation\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\nIf you use our models or other resources in your research, please cite our work as follows.\n\n\n**BibTeX:**\n\n```bib\n@inproceedings{stefanik2023resources,\n               author = {\\v{S}tef\\'{a}nik, Michal and Kadl\u010d\u00edk, Marek and Gramacki, Piotr and Sojka, Petr},\n               title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n               booktitle = {Proceedings of the 9th Workshop on Slavic Natural Language Processing},\n               publisher = {ACL},\n               numpages = {9},\n               url = {https://arxiv.org/abs/2304.01922},\n}\n```", "size_bytes": "4918511257", "downloads": 42}
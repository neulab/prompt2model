{"pretrained_model_name": "kykim/bertshared-kor-base", "description": "---\nlanguage: ko\n---\n\n# Bert base model for Korean\n\n* 70GB Korean text dataset and 42000 lower-cased subwords are used\n* Check the model performance and other language models for Korean in [github](https://github.com/kiyoungkim1/LM-kor)\n\n```python\n# only for pytorch in transformers\nfrom transformers import BertTokenizerFast, EncoderDecoderModel\n\ntokenizer = BertTokenizerFast.from_pretrained(\"kykim/bertshared-kor-base\")\nmodel = EncoderDecoderModel.from_pretrained(\"kykim/bertshared-kor-base\")\n```", "size_bytes": "589387271", "downloads": 158}
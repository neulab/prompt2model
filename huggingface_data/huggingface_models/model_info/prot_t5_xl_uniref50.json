{"pretrained_model_name": "Rostlab/prot_t5_xl_uniref50", "description": "---\ntags:\n- protein language model\ndatasets:\n- UniRef50\n---\n\n# ProtT5-XL-UniRef50 model\n\nPretrained model on protein sequences using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://doi.org/10.1101/2020.07.12.199554) and first released in\n[this repository](https://github.com/agemagician/ProtTrans). This model is trained on uppercase amino acids: it only works with capital letter amino acids.\n\n\n## Model description\n\nProtT5-XL-UniRef50 is based on the `t5-3b` model and was pretrained on a large corpus of protein sequences in a self-supervised fashion.\nThis means it was pretrained on the raw protein sequences only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those protein sequences.\n\nOne important difference between this T5 model and the original T5 version is the denosing objective.\nThe original T5-3B model was pretrained using a span denosing objective, while this model was pre-trained with a Bart-like MLM denosing objective.\nThe masking probability is consistent with the original T5 training by randomly masking 15% of the amino acids in the input.\n\nIt has been shown that the features extracted from this self-supervised model (LM-embeddings) captured important biophysical properties governing protein shape.\nshape.\nThis implied learning some of the grammar of the language of life realized in protein sequences.\n\n## Intended uses & limitations\n\nThe model could be used for protein feature extraction or to be fine-tuned on downstream tasks.\nWe have noticed in some tasks on can gain more accuracy by fine-tuning the model rather than using it as a feature extractor.\nWe have also noticed that for feature extraction, its better to use the feature extracted from the encoder not from the decoder.\n\n### How to use\n\nHere is how to use this model to extract the features of a given protein sequence in PyTorch:\n\n```python\nsequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n# this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\nsequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n\n# tokenize sequences and pad up to the longest sequence in the batch\nids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\ninput_ids = torch.tensor(ids['input_ids']).to(device)\nattention_mask = torch.tensor(ids['attention_mask']).to(device)\n\n# generate embeddings\nwith torch.no_grad():\n    embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n\n# extract embeddings for the first ([0,:]) sequence in the batch while removing padded & special tokens ([0,:7]) \nemb_0 = embedding_repr.last_hidden_state[0,:7] # shape (7 x 1024)\nprint(f\"Shape of per-residue embedding of first sequences: {emb_0.shape}\")\n# do the same for the second ([1,:]) sequence in the batch while taking into account different sequence lengths ([1,:8])\nemb_1 = embedding_repr.last_hidden_state[1,:8] # shape (8 x 1024)\n\n# if you want to derive a single representation (per-protein embedding) for the whole protein\nemb_0_per_protein = emb_0.mean(dim=0) # shape (1024)\n\nprint(f\"Shape of per-protein embedding of first sequences: {emb_0_per_protein.shape}\")\n```\n\n## Training data\n\nThe ProtT5-XL-UniRef50 model was pretrained on [UniRef50](https://www.uniprot.org/help/uniref), a dataset consisting of 45 million protein sequences.\n\n## Training procedure\n\n### Preprocessing\n\nThe protein sequences are uppercased and tokenized using a single space and a vocabulary size of 21. The rare amino acids \"U,Z,O,B\" were mapped to \"X\".\nThe inputs of the model are then of the form:\n\n```\nProtein Sequence [EOS]\n```\n\nThe preprocessing step was performed on the fly, by cutting and padding the protein sequences up to 512 tokens.\n\nThe details of the masking procedure for each sequence are as follows:\n- 15% of the amino acids are masked.\n- In 90% of the cases, the masked amino acids are replaced by `[MASK]` token.\n- In 10% of the cases, the masked amino acids are replaced by a random amino acid (different) from the one they replace.\n\n### Pretraining\n\nThe model was trained on a single TPU Pod V2-256 for 991.5 thousand steps in total, using sequence length 512 (batch size 2k).\nIt was trained using ProtT5-XL-BFD model as an initial checkpoint, rather than training from scratch.  \nIt has a total of approximately 3B parameters and was trained using the encoder-decoder architecture.\nThe optimizer used is AdaFactor with inverse square root learning rate schedule for pre-training.\n\n\n## Evaluation results\n\nWhen the model is used for feature extraction, this model achieves the following results:\n\nTest results :\n\n| Task/Dataset | secondary structure (3-states) | secondary structure (8-states)  |  Localization | Membrane  |\n|:-----:|:-----:|:-----:|:-----:|:-----:|\n|   CASP12  | 81 | 70 |    |    |\n|   TS115   | 87 | 77 |    |    | \n|   CB513   | 86 | 74 |    |    |\n|  DeepLoc  |    |    | 81 | 91 |\n\n### BibTeX entry and citation info\n\n```bibtex\n@article {Elnaggar2020.07.12.199554,\n\tauthor = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n\ttitle = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n\telocation-id = {2020.07.12.199554},\n\tyear = {2020},\n\tdoi = {10.1101/2020.07.12.199554},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tabstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive language models (Transformer-XL, XLNet) and two auto-encoder models (Bert, Albert) on data from UniRef and BFD containing up to 393 billion amino acids (words) from 2.1 billion protein sequences (22- and 112 times the entire English Wikipedia). The LMs were trained on the Summit supercomputer at Oak Ridge National Laboratory (ORNL), using 936 nodes (total 5616 GPUs) and one TPU Pod (V3-512 or V3-1024). We validated the advantage of up-scaling LMs to larger models supported by bigger data by predicting secondary structure (3-states: Q3=76-84, 8 states: Q8=65-73), sub-cellular localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein shape. This implied learning some of the grammar of the language of life realized in protein sequences. The successful up-scaling of protein LMs through HPC to larger data sets slightly reduced the gap between models trained on evolutionary information and LMs. Availability ProtTrans: \\&lt;a href=\"https://github.com/agemagician/ProtTrans\"\\&gt;https://github.com/agemagician/ProtTrans\\&lt;/a\\&gt;Competing Interest StatementThe authors have declared no competing interest.},\n\tURL = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554},\n\teprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf},\n\tjournal = {bioRxiv}\n}\n```\n\n> Created by [Ahmed Elnaggar/@Elnaggar_AI](https://twitter.com/Elnaggar_AI) | [LinkedIn](https://www.linkedin.com/in/prof-ahmed-elnaggar/)\n", "size_bytes": "11275562724", "downloads": 74085}
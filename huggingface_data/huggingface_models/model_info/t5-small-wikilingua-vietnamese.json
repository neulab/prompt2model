{"pretrained_model_name": "minhtoan/t5-small-wikilingua-vietnamese", "description": "---\nlanguage: vi\ndatasets:\n- Wikilingua \ntags:\n- summarization\nlicense: mit\nwidget:\n- text: \"Hoa qu\u1ea3 v\u00e0 rau th\u01b0\u1eddng r\u1ebb h\u01a1n khi v\u00e0o m\u00f9a. Th\u01b0\u1eddng th\u00ec c\u00e1c c\u1eeda h\u00e0ng th\u1ef1c ph\u1ea9m s\u1ebd gi\u1ea3m gi\u00e1 cho c\u00e1c m\u1eb7t h\u00e0ng n\u00f4ng s\u1ea3n \u0111ang v\u00e0o m\u00f9a v\u00ec h\u1ecd bi\u1ebft kh\u00e1ch h\u00e0ng lu\u00f4n ch\u1edd \u0111\u1ee3i nh\u1eefng lo\u1ea1i n\u00e0o v\u00e0o th\u1eddi gian n\u00e0o trong n\u0103m. H\u01a1n n\u1eefa rau qu\u1ea3 \u0111\u00fang m\u00f9a bao gi\u1edd c\u0169ng ngon h\u01a1n.\"\ninference:\n  parameters:\n    max_length: 256\n---\n\n# Vietnamese pretrain model for Abstractive Text Summarization task\n\n`State-of-the-art lightweights pretrained Transformer-based encoder-decoder model for Vietnamese.`\n\n``Model trained on dataset Wikilingua of vietnamese language with input length = 512, output length = 256``\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"minhtoan/t5-small-wikilingua_vietnamese\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"minhtoan/t5-small-wikilingua_vietnamese\")\nmodel.cuda()\nsrc = 'L\u00e0 Th\u1ee7 t\u01b0\u1edbng khi l\u1ea1m ph\u00e1t g\u1ea7n 70%, ng\u00e2n s\u00e1ch th\u00e2m h\u1ee5t, th\u1ea5t nghi\u1ec7p nhi\u1ec1u, nh\u01b0ng v\u1edbi t\u01b0 duy c\u1edfi m\u1edf, s\u1eb5n s\u00e0ng \"kh\u00f4ng l\u00e0m \u0111\u01b0\u1ee3c th\u00ec t\u1eeb ch\u1ee9c\", \u00f4ng V\u00f5 V\u0103n Ki\u1ec7t \u0111\u00e3 mang \u0111\u1ebfn nhi\u1ec1u c\u1ea3i c\u00e1ch kinh t\u1ebf cho Vi\u1ec7t Nam. Trong tr\u00ed nh\u1edb c\u1ee7a Chuy\u00ean gia kinh t\u1ebf Ph\u1ea1m Chi Lan, c\u1ed1 Th\u1ee7 t\u01b0\u1edbng V\u00f5 V\u0103n Ki\u1ec7t l\u00e0 m\u1ed9t l\u00e3nh \u0111\u1ea1o c\u00f3 t\u01b0 duy kinh t\u1ebf th\u1ecb tr\u01b0\u1eddng, ch\u1ecbu kh\u00f3 h\u1ecdc h\u1ecfi, bi\u1ebft l\u1eafng nghe. \"Nh\u1eefng g\u00ec \u00f4ng th\u1ea5y t\u1ed1t cho ng\u01b0\u1eddi d\u00e2n th\u00ec s\u1ebd ti\u1ebfp thu r\u1ea5t nhanh\", b\u00e0 Lan n\u00f3i v\u1edbi VnExpress. C\u00f2n PGS. TS Tr\u1ea7n Minh Tr\u01b0\u1edfng, Vi\u1ec7n tr\u01b0\u1edfng Vi\u1ec7n H\u1ed3 Ch\u00ed Minh v\u00e0 c\u00e1c l\u00e3nh t\u1ee5 c\u1ee7a \u0110\u1ea3ng (H\u1ecdc vi\u1ec7n Ch\u00ednh tr\u1ecb - H\u00e0nh ch\u00ednh qu\u1ed1c gia) nh\u1eadn \u0111\u1ecbnh v\u1ec1 \u00f4ng V\u00f5 V\u0103n Ki\u1ec7t tr\u00ean B\u00e1o Nh\u00e2n d\u00e2n nh\u01b0 \"m\u1ed9t trong nh\u1eefng ng\u01b0\u1eddi \u0111i \u0111\u1ea7u trong c\u00f4ng cu\u1ed9c \u0111\u1ed5i m\u1edbi \u1edf Vi\u1ec7t Nam k\u1ec3 t\u1eeb n\u0103m 1986\", \"t\u1ed5ng c\u00f4ng tr\u00ecnh s\u01b0 nhi\u1ec1u d\u1ef1 \u00e1n t\u00e1o b\u1ea1o\". Nh\u1eadm ch\u1ee9c Th\u1ee7 t\u01b0\u1edbng gi\u1eefa mu\u00f4n v\u00e0n kh\u00f3 kh\u0103n N\u0103m 1985, sau m\u1ed9t th\u1eadp ni\u00ean theo m\u00f4 h\u00ecnh bao c\u1ea5p, kinh t\u1ebf Vi\u1ec7t Nam ng\u00e0y c\u00e0ng ph\u1ee5 thu\u1ed9c v\u00e0o vi\u1ec7n tr\u1ee3 v\u00e0 vay n\u1ee3 n\u01b0\u1edbc ngo\u00e0i, l\u00ean \u0111\u1ebfn 8,5 t\u1ef7 ruble (kho\u1ea3ng 3.500 t\u1ef7 \u0111\u1ed3ng) v\u00e0 1,9 t\u1ef7 USD (g\u1ea7n 45.600 t\u1ef7 \u0111\u1ed3ng). C\u1ee9 th\u00eam m\u1ed9t n\u0103m, m\u00f4 h\u00ecnh c\u00e0ng tr\u1ee5c tr\u1eb7c v\u00e0 l\u1ed7i th\u1eddi. Ng\u00e2n s\u00e1ch b\u1ecb th\u00e2m h\u1ee5t v\u00e0 ph\u1ea3i b\u00f9 \u0111\u1eafp b\u1eb1ng vi\u1ec7c in ti\u1ec1n \u0111\u1ec3 chi ti\u00eau. Vi\u1ec7c l\u1eb7p l\u1ea1i sai l\u1ea7m \"gi\u00e1 - l\u01b0\u01a1ng - ti\u1ec1n\" khi\u1ebfn l\u1ea1m ph\u00e1t phi m\u00e3 l\u00ean g\u1ea7n 775% v\u00e0o n\u0103m 1986 v\u00e0 v\u1eabn \u1edf m\u1ee9c hai ch\u1eef s\u1ed1 nh\u1eefng n\u0103m 1990, 1991. Trong khi \u0111\u00f3, t\u1ef7 l\u1ec7 th\u1ea5t nghi\u1ec7p l\u00ean \u0111\u1ebfn 12,7%. Nh\u1eefng ch\u1ec9 s\u1ed1 \u0111\u00e1ng b\u00e1o \u0111\u1ed9ng \u0111\u00e3 g\u00f3p ph\u1ea7n th\u1ee9c t\u1ec9nh t\u01b0 duy c\u00e1c nh\u00e0 l\u00e3nh \u0111\u1ea1o, d\u1eabn \u0111\u1ebfn quy\u1ebft \u0111\u1ecbnh \u0111\u1ed5i m\u1edbi t\u1ea1i \u0110\u1ea1i h\u1ed9i VI (th\u00e1ng 12/1986). Trong \u0111\u00f3, \u0111\u1ed5i m\u1edbi v\u1ec1 kinh t\u1ebf l\u00e0 m\u1ed9t trong nh\u1eefng nhi\u1ec7m v\u1ee5 tr\u1ecdng t\u00e2m, nh\u1eb1m \u0111\u01b0a \u0111\u1ea5t n\u01b0\u1edbc tho\u00e1t kh\u1ecfi nguy kh\u00f3. \u00d4ng V\u00f5 V\u0103n Ki\u1ec7t, l\u00fac \u1ea5y \u0111ang l\u00e0 Ph\u00f3 ch\u1ee7 t\u1ecbch H\u1ed9i \u0111\u1ed3ng B\u1ed9 tr\u01b0\u1edfng, tr\u1edf th\u00e0nh m\u1ed9t l\u00e3nh \u0111\u1ea1o ch\u1ee7 ch\u1ed1t ki\u1ebfn t\u1ea1o n\u00ean c\u00f4ng cu\u1ed9c n\u00e0y.'\ntokenized_text = tokenizer.encode(src, return_tensors=\"pt\").cuda()\nmodel.eval()\nsummary_ids = model.generate(\n                    tokenized_text,\n                    max_length=256, \n                    num_beams=5,\n                    repetition_penalty=2.5, \n                    length_penalty=1.0, \n                    early_stopping=True\n                )\noutput = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\noutput\n```\n```python\noutput: \u00d4ng V\u00f5 V\u0103n Ki\u1ec7t, l\u00fac \u1ea5y \u0111ang l\u00e0 Ph\u00f3 ch\u1ee7 t\u1ecbch H\u1ed9i \u0111\u1ed3ng B\u1ed9 tr\u01b0\u1edfng, tr\u1edf th\u00e0nh m\u1ed9t l\u00e3nh \u0111\u1ea1o ch\u1ee7 ch\u1ed1t ki\u1ebfn t\u1ea1o n\u00ean c\u00f4ng cu\u1ed9c \u0111\u1ed5i m\u1edbi \u1edf Vi\u1ec7t Nam k\u1ec3 t\u1eeb n\u0103m 1986. Trong khi \u0111\u00f3, \u00f4ng V\u00f5 V\u0103n Ki\u1ec7t \u0111\u00e3 mang \u0111\u1ebfn nhi\u1ec1u c\u1ea3i c\u00e1ch kinh t\u1ebf cho Vi\u1ec7t Nam.\n```\n\n## Author\n`\nPhan Minh Toan\n`", "size_bytes": "1200768197", "downloads": 48}
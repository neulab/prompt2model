{"pretrained_model_name": "tilomichel/mT5-base-GermanQuAD-e2e-qg", "description": "---\nlicense: mit\nwidget:\n- text: \"generate question: KMI ist eine Variante des allgemeinen Bachelors Informatik und damit zu ca. 80% identisch mit dem allgemeinen Bachelor Informatik, d.h. auch diese Variante ist ein Informatikstudium mit einem hohen Programmieranteil. Der Studienschwerpunkt adressiert insbesondere die heute geforderten Soft-Skills, die f\u00fcr ein Arbeiten im Team unerl\u00e4sslich sind. Des Weiteren lernen Sie das Interaktionsdesign Ihrer Anwendungen kreativ zu optimieren und ihr Auge f\u00fcr eine gelungene Gestaltung zu schulen. In jedem Semester werden Akzente gesetzt: Im ersten und dritten Semester haben Sie beispielsweise ein Projekt anstelle eher technisch ausgerichteter Module. Die H\u00e4lfte Ihrer Wahlpflichtmodule absolvieren Sie am Fachbereich Media. </s>\"\n  example_title: \"Question generation 1\"\n- text: \"generate question: SARS-CoV-2 zirkuliert weiterhin in der Bev\u00f6lkerung und kann sich \u00fcberall dort verbreiten, wo Menschen zusammenkommen. Auch wenn in den Sommermonaten die Fallzahlen saisonbedingt niedriger sind als in der kalten Jahreszeit, empfiehlt das RKI nach wie vor, die AHA+A+L-Regeln einzuhalten (Abstand halten, Hygieneregeln beachten, Alltag mit Maske, Coronawarnapp nutzen, L\u00fcften), bei Atemwegssymptomen zu Hause zu bleiben und sich testen zu lassen, und auf einen vollst\u00e4ndigen Impfschutz gegen COVID-19 zu achten. </s>\"\n  example_title: \"Question generation 2\"\n- text: \"generate question: Ballaststoffe haben eine Reihe von Wirkungen auf den K\u00f6rper, vor allem auf die Verdauung, z. B. Einfluss auf die Transitzeit der Nahrung in Magen und Darm, Masse und Konsistenz des Stuhls sowie H\u00e4ufigkeit der Darmentleerung, S\u00e4ttigungswirkung, ver\u00e4nderte N\u00e4hrstoff\u00adabsorption und pr\u00e4biotische Wirkung. Je nach Art der Ballaststoffe und nach Abschnitt im Verdauungstrakt kann es zu unterschiedlichen Effekten kommen. Bei der Fermentation von Ballaststoffen entstehen zudem verschiedene kurzkettige Fetts\u00e4uren, die dem K\u00f6rper teilweise als Energiequelle zur Verf\u00fcgung stehen. Sch\u00e4tzungsweise liefern die kurzkettigen Fetts\u00e4uren 8,4 kJ (2,0 kcal) pro g Ballaststoff. </s>\"\n  example_title: \"Question generation 3\"\ninference:\n    parameters:\n        max_length: 128\n        num_beams: 4\n        length_penalty: 1.5\n        no_repeat_ngram_size: 3\n        early_stopping: True\nlanguage:\n- de\ntags:\n- question generation\ndatasets:\n- deepset/germanquad\nmetrics:\n- sacrebleu\n- bleu\n- rouge-l\n- meteor\n- bertscore\nmodel-index:\n- name: tilomichel/mT5-base-GermanQuAD-e2e-qg\n  results:\n  - task:\n      type: question-generation\n      name: Question generation\n    dataset:\n      type: xquad\n      name: XQuAD (de)\n      split: de\n    metrics:\n      - type: sacrebleu\n        value: 1.72837804716791\n        name: BLEU Score\n        args:\n          lowercase: true\n        verified: false\n      - type: sacrebleu\n        value: 49.210584834334\n        name: BLEU-1\n        args:\n          lowercase: true\n        verified: false\n      - type: sacrebleu\n        value: 16.960300681230915\n        name: BLEU-2\n        args:\n          lowercase: true\n        verified: false\n      - type: sacrebleu\n        value: 7.144635299975106\n        name: BLEU-3\n        args:\n          lowercase: true\n        verified: false\n      - type: sacrebleu\n        value: 3.230076780513635\n        name: BLEU-4\n        args:\n          lowercase: true\n        verified: false\n      - type: rouge\n        name: ROUGE-L (f-measure)\n        value: 0.171130005590873\n        args:\n          use_aggregator: true\n          use_stemmer: false\n        verified: false\n      - type: meteor\n        value: 0.0835049103331918\n        name: METEOR\n        args:\n          language: de\n        verified: false\n      - type: bertscore\n        value: 0.331940584507538\n        name: BERTScore (F1)\n        args:\n          rescale_with_baseline: true\n        verified: false\n---\n\n# mT5-base finetuned on the GermanQuAD dataset for answer-agnostic question generation\n\nThis model is a finetuned [mT5-base](https://arxiv.org/abs/2010.11934) model for the task of answer-agnostic (or end-to-end) question generation. The approach from [Lopez et al.](https://arxiv.org/abs/2005.01107) was used called *All questions per line (AQPL)*. This means a paragraph is provided as input and multiple questions are generated from it. Other models already tested this approach with the T5 model for [English](https://huggingface.co/valhalla/t5-base-e2e-qg) and [German](https://huggingface.co/dehio/german-qg-t5-e2e-quad).\n\nFor finetuning this model only used the [GermanQuAD dataset from deepset](https://www.deepset.ai/germanquad) was used. The dataset was modified and filtered with scripts that can be found in [another repository](https://github.com/TiloMichel/textgen-for-chatbot-training-german/tree/main/1_data_preparation_and_exploration).\n\n## Training, test and evaluation data\nFor training and test the original split from GermanQuAD was used. As evaluation dataset the German split of the [XQuAD](https://github.com/deepmind/xquad) dataset was used.\n\n## Training hyperparameters\nThe training parameters are provided in JSON and can be used with a training script provided in a [repository](https://github.com/TiloMichel/textgen-for-chatbot-training-german/tree/main/2_training)\n\n```JS\n{\n    \"model_name_or_path\": \"google/mt5-base\",\n    \"output_dir\": \"mt5-base-germanquad-e2e-qg\",\n    \"overwrite_output_dir\": true,\n    \"cache_dir\": \"model-cache\",\n    \"dataset_dir\": \"e2e-qg-germanquad\",\n    \"preprocessing_num_workers\": 20,\n    \"max_source_length\": 1024,\n    \"max_target_length\": 128,\n    \"val_max_target_length\": 128,\n    \"pad_to_max_length\": true,\n    \"seed\": 42,\n    \"do_train\": true,\n    \"gradient_accumulation_steps\": 64,\n    \"per_device_train_batch_size\": 1,\n    \"per_device_eval_batch_size\": 1,\n    \"learning_rate\": 1e-4,\n    \"num_train_epochs\": 10,\n    \"evaluation_strategy\": \"epoch\",\n    \"logging_strategy\": \"epoch\",\n    \"save_strategy\": \"epoch\",\n    \"save_total_limit\": 3,\n    \"dataloader_num_workers\": 8,\n    \"ddp_find_unused_parameters\": false\n}\n```\n\n## Training results\nThe evaluation is reported on XQuAD. The implementations and configurations can be found in [another repository](https://github.com/TiloMichel/textgen-for-chatbot-training-german/tree/main/3_evaluation).", "size_bytes": "2329733709", "downloads": 23}
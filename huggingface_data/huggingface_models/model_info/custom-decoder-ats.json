{"pretrained_model_name": "josh-oo/custom-decoder-ats", "description": "---\nlicense: mit\nlanguage:\n- de\n---\n# German text simplification with custom decoder\nThis model was initialized from an mBART model and the decoder was replaced by a GPT2 language model pre-trained for German Easy Language. For more details, visit our [Github repository](https://github.com/MiriUll/Language-Models-German-Simplification).\n\n## Usage\n```python\nimport torch\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"josh-oo/custom-decoder-ats\")\n\n##gerpt\n\n#model = AutoModelForSeq2SeqLM.from_pretrained(\"josh-oo/custom-decoder-ats\", trust_remote_code=True, revision=\"35197269f0235992fcc6b8363ca4f48558b624ff\")\n#decoder_tokenizer = AutoTokenizer.from_pretrained(\"josh-oo/gerpt2\")\n\n##dbmdz\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"josh-oo/custom-decoder-ats\", trust_remote_code=True, revision=\"4accedbe0b57d342d95ff546b6bbd3321451d504\")\ndecoder_tokenizer = AutoTokenizer.from_pretrained(\"josh-oo/german-gpt2-easy\")\ndecoder_tokenizer.add_tokens(['<</s>>','<<s>>','<<pad>>'])\n\n##\n\nexample_text = \"In tausenden Schweizer Privathaushalten k\u00fcmmern sich Haushaltsangestellte um die W\u00e4sche, betreuen die Kinder und sorgen f\u00fcr Sauberkeit. Durchschnittlich bekommen sie f\u00fcr die Arbeit rund 30 Franken pro Stunde Bruttolohn. Der gr\u00f6sste Teil von ihnen erh\u00e4lt aber 28 Franken.\"\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\ntest_input = tokenizer([example_text], return_tensors=\"pt\", padding=True, pad_to_multiple_of=1024)\nfor key, value in test_input.items():\n  test_input[key] = value.to(device)\n\noutputs = model.generate(**test_input, num_beams=3, max_length=1024)\ndecoder_tokenizer.batch_decode(outputs)\n```\n\n## Citation\nIf you use our mode, please cite:  \n@misc{ansch\u00fctz2023language,  \n&emsp;        title={Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training},   \n&emsp;       author={Miriam Ansch\u00fctz and Joshua Oehms and Thomas Wimmer and Bart\u0142omiej Jezierski and Georg Groh},  \n&emsp;       year={2023},  \n&emsp;       eprint={2305.12908},  \n&emsp;       archivePrefix={arXiv},  \n&emsp;       primaryClass={cs.CL}  \n}", "size_bytes": "1649008805", "downloads": 9}
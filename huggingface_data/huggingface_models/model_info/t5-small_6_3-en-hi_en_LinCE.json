{"pretrained_model_name": "sayanmandal/t5-small_6_3-en-hi_en_LinCE", "description": "---\ntags:\n- translation\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: t5-small_6_3-en-hi_en_LinCE\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small_6_3-en-hi_en_LinCE\n\nThis model was trained from scratch on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.2034\n- Bleu: 7.8135\n- Gen Len: 39.5564\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Bleu   | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:-------:|\n| No log        | 0.99  | 94   | 3.5424          | 0.9187 | 16.7437 |\n| No log        | 1.99  | 188  | 3.1434          | 1.2886 | 16.8158 |\n| No log        | 2.99  | 282  | 2.9494          | 1.4577 | 16.7824 |\n| No log        | 3.99  | 376  | 2.8233          | 1.4745 | 16.8879 |\n| No log        | 4.99  | 470  | 2.7300          | 1.7116 | 16.6636 |\n| 3.6303        | 5.99  | 564  | 2.6589          | 1.7857 | 16.6302 |\n| 3.6303        | 6.99  | 658  | 2.6005          | 1.8572 | 16.4553 |\n| 3.6303        | 7.99  | 752  | 2.5456          | 2.139  | 16.3925 |\n| 3.6303        | 8.99  | 846  | 2.5023          | 2.3835 | 16.2911 |\n| 3.6303        | 9.99  | 940  | 2.4725          | 2.5607 | 16.3271 |\n| 2.9087        | 10.99 | 1034 | 2.4272          | 2.6614 | 16.3138 |\n| 2.9087        | 11.99 | 1128 | 2.3977          | 2.9623 | 16.3338 |\n| 2.9087        | 12.99 | 1222 | 2.3686          | 3.1248 | 16.2443 |\n| 2.9087        | 13.99 | 1316 | 2.3438          | 3.3294 | 16.3458 |\n| 2.9087        | 14.99 | 1410 | 2.3253          | 3.3885 | 16.3591 |\n| 2.6588        | 15.99 | 1504 | 2.3028          | 3.3985 | 16.3124 |\n| 2.6588        | 16.99 | 1598 | 2.2839          | 3.3772 | 16.3858 |\n| 2.6588        | 17.99 | 1692 | 2.2704          | 3.5804 | 16.3872 |\n| 2.6588        | 18.99 | 1786 | 2.2533          | 3.8751 | 16.2697 |\n| 2.6588        | 19.99 | 1880 | 2.2378          | 4.0003 | 16.3271 |\n| 2.6588        | 20.99 | 1974 | 2.2233          | 4.0271 | 16.3031 |\n| 2.5079        | 21.99 | 2068 | 2.2160          | 4.1898 | 16.3057 |\n| 2.5079        | 22.99 | 2162 | 2.2010          | 4.1216 | 16.3031 |\n| 2.5079        | 23.99 | 2256 | 2.1935          | 4.1311 | 16.2644 |\n| 2.5079        | 24.99 | 2350 | 2.1833          | 4.1373 | 16.3138 |\n| 2.5079        | 25.99 | 2444 | 2.1725          | 4.3471 | 16.3057 |\n| 2.4027        | 26.99 | 2538 | 2.1657          | 4.183  | 16.3298 |\n| 2.4027        | 27.99 | 2632 | 2.1611          | 4.2867 | 16.3351 |\n| 2.4027        | 28.99 | 2726 | 2.1531          | 4.2689 | 16.2737 |\n| 2.4027        | 29.99 | 2820 | 2.1482          | 4.4802 | 16.2644 |\n| 2.4027        | 30.99 | 2914 | 2.1443          | 4.469  | 16.231  |\n| 2.3251        | 31.99 | 3008 | 2.1375          | 4.5295 | 16.227  |\n| 2.3251        | 32.99 | 3102 | 2.1330          | 4.4799 | 16.2243 |\n| 2.3251        | 33.99 | 3196 | 2.1307          | 4.7124 | 16.2417 |\n| 2.3251        | 34.99 | 3290 | 2.1248          | 4.5954 | 16.3004 |\n| 2.3251        | 35.99 | 3384 | 2.1215          | 4.7455 | 16.215  |\n| 2.3251        | 36.99 | 3478 | 2.1166          | 4.6233 | 16.2016 |\n| 2.2818        | 37.99 | 3572 | 2.1147          | 4.6843 | 16.219  |\n| 2.2818        | 38.99 | 3666 | 2.1112          | 4.7068 | 16.2163 |\n| 2.2818        | 39.99 | 3760 | 2.1071          | 4.684  | 16.223  |\n| 2.2818        | 40.99 | 3854 | 2.1034          | 4.7323 | 16.2523 |\n| 2.2818        | 41.99 | 3948 | 2.0998          | 4.6406 | 16.2016 |\n| 2.2392        | 42.99 | 4042 | 2.1017          | 4.7609 | 16.1976 |\n| 2.2392        | 43.99 | 4136 | 2.1021          | 4.7634 | 16.2069 |\n| 2.2392        | 44.99 | 4230 | 2.0994          | 4.7854 | 16.1976 |\n| 2.2392        | 45.99 | 4324 | 2.0980          | 4.7562 | 16.2243 |\n| 2.2392        | 46.99 | 4418 | 2.0964          | 4.7921 | 16.219  |\n| 2.2192        | 47.99 | 4512 | 2.0970          | 4.8029 | 16.2377 |\n| 2.2192        | 48.99 | 4606 | 2.0967          | 4.7953 | 16.2176 |\n| 2.2192        | 49.99 | 4700 | 2.0968          | 4.819  | 16.2457 |\n\n\n### Framework versions\n\n- Transformers 4.20.0.dev0\n- Pytorch 1.8.0\n- Datasets 2.1.0\n- Tokenizers 0.12.1\n", "size_bytes": "191651552", "downloads": 11}
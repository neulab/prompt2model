{"pretrained_model_name": "BramVanroy/ul2-large-dutch-simplification-mai-2023", "description": "---\nlicense: cc-by-nc-sa-4.0\ntags:\n  - generated_from_trainer\n  - simplification\ntask_categories:\n  - text2text-generation\ntask_ids:\n  - text-simplification\nlanguage:\n  - nl\ndatasets:\n  - BramVanroy/chatgpt-dutch-simplification\nmetrics:\n  - rouge\n  - sari\nmodel-index:\n- name: BramVanroy/ul2-large-dutch-simplification-mai-2023\n  results:\n  - task:\n      type: text-simplification\n      name: Text Simplification\n    dataset:\n      type: BramVanroy/chatgpt-dutch-simplification\n      name: ChatGPT Dutch Simplification\n    metrics:\n      - type: rouge\n        value: 41.3871\n        name: Eval Rouge-1\n      - type: rouge\n        value: 19.6751\n        name: Eval Rouge-2\n      - type: rouge\n        value: 36.0469\n        name: Eval RougeL\n      - type: rouge\n        value: 36.1178\n        name: Eval RougeLsum\n      - type: sari\n        value: 54.3588\n        name: Eval SARI\n      - type: rouge\n        value: 43.8191\n        name: Test Rouge-1\n      - type: rouge\n        value: 21.7783\n        name: Test Rouge-2\n      - type: rouge\n        value: 39.3657\n        name: Test RougeL\n      - type: rouge\n        value: 39.3751\n        name: Test RougeLsum\n      - type: sari\n        value: 52.3752\n        name: Test SARI\nwidget:\n- example_title: \"Cooking\"\n  text: \"Op bepaalde tijdstippen verlang ik naar de smaakvolle culinaire creaties welke door de ambachtelijke expertise van mijn grootmoeder zijn vervaardigd.\"\n\n---\n\n# ul2-large-dutch-simplification-mai-2023\n\nThis model is intended to simplify Dutch sentences.\n\nThis model is a fine-tuned version of [yhavinga/ul2-large-dutch](https://huggingface.co/yhavinga/ul2-large-dutch) on\nthe [BramVanroy/chatgpt-dutch-simplification](https://huggingface.co/datasets/BramVanroy/chatgpt-dutch-simplification)\ndataset. \n\nThe model was created in light of the master thesis of Charlotte Van de Velde in the Master of Science in Artificial\nIntelligence (MAI) at KU Leuven in 2023. Charlotte is supervised by Vincent Vandeghinste and Bram Vanroy. \nDataset creation by Charlotte, model training by Bram.\n\n## Quick links\n\n- [Repository](https://github.com/BramVanroy/mai-simplification-nl-2023#22-hyperparameter-sweep): includes training code and model creation log\n- [Dataset](https://huggingface.co/datasets/BramVanroy/chatgpt-dutch-simplification): `BramVanroy/chatgpt-dutch-simplification`\n- [Parent model](https://huggingface.co/yhavinga/ul2-large-dutch): this model was finetuned on `yhavinga/ul2-large-dutch`\n- [Demo](https://huggingface.co/spaces/BramVanroy/mai-simplification-nl-2023-demo): shows the \"base\" model in action (don't rely on the \"Hosted inference API\" widget on this page, it does not work very well)\n\n## Intended uses & limitations, and dataset\n\nThe model is intended for sentence-level simplification of Dutch. It might extend to document-level simplification\nbut most of the dataset is limited to sentences so document-level performance is not guaranteed.\n\nThe dataset has been generated automatically (cf.\n[dataset description](https://huggingface.co/datasets/BramVanroy/chatgpt-dutch-simplification)) and has not been\nmanually verified. On top of that, this model has been fine-tuned and we did not scrutinize the parent model or its\ntraining data. Output of the current model is therefore subject to unexpected results (as most if not all neural\nnetworks).\n\nBecause the dataset was generated with ChatGPT, this model cannot be used for commercial purposes.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0002927210895006501\n- train_batch_size: 32\n- optimizer: Adafactor\n- num_epochs: 27\n\nThese hyperarameters were found through Bayesian hyperparameter search with `wandb`. This is described in the\n[repository](https://github.com/BramVanroy/mai-simplification-nl-2023#22-hyperparameter-sweep).\n\n### Training results\n\n`eval` results are on the evaluation set, `predict` results are on the test set. These were achieved with\nbeam search (num_beams=3).\n\n```json\n{\n    \"eval_gen_len\": 21.404761904761905,\n    \"eval_loss\": 3.0882697105407715,\n    \"eval_rouge1\": 41.3871,\n    \"eval_rouge2\": 19.6751,\n    \"eval_rougeL\": 36.0469,\n    \"eval_rougeLsum\": 36.1178,\n    \"eval_sari\": 54.3588,\n  \n    \"predict_gen_len\": 22.1484375,\n    \"predict_loss\": 2.7822625637054443,\n    \"predict_rouge1\": 43.8191,\n    \"predict_rouge2\": 21.7783,\n    \"predict_rougeL\": 39.3657,\n    \"predict_rougeLsum\": 39.3751,\n    \"predict_sari\": 52.3752\n}\n```\n\nNote: the model seems to underperform compared to the\n[base variant](https://huggingface.co/BramVanroy/ul2-small-dutch-simplification-mai-2023) of the model, achieving only\nsimilar results with a much larger size. The reason for this may be found in the hyperparameters, where\nthis large model may have benefitted from a smaller learning rate in the optimisation space. In the hyperparameter \nsearch, the learning rate spectrum was set to 1e-03 to 1e-04 but this might be too large for this model and size. \n\n### Framework versions\n\n- Transformers 4.29.2\n- Pytorch 2.0.1+cu117\n- Datasets 2.12.0\n- Tokenizers 0.13.3\n", "size_bytes": "3132793669", "downloads": 10}
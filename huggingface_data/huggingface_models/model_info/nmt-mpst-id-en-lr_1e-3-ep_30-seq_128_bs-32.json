{"pretrained_model_name": "meongracun/nmt-mpst-id-en-lr_1e-3-ep_30-seq_128_bs-32", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: nmt-mpst-id-en-lr_1e-3-ep_30-seq_128_bs-32\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# nmt-mpst-id-en-lr_1e-3-ep_30-seq_128_bs-32\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.1003\n- Bleu: 20.4006\n- Meteor: 0.3752\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.001\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Bleu    | Meteor |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:------:|\n| No log        | 1.0   | 202  | 2.1601          | 8.8793  | 0.2344 |\n| No log        | 2.0   | 404  | 1.8368          | 12.6317 | 0.2818 |\n| 2.2884        | 3.0   | 606  | 1.7154          | 15.2843 | 0.3142 |\n| 2.2884        | 4.0   | 808  | 1.6613          | 16.6089 | 0.332  |\n| 1.4437        | 5.0   | 1010 | 1.6141          | 17.6498 | 0.3421 |\n| 1.4437        | 6.0   | 1212 | 1.5957          | 18.0781 | 0.3491 |\n| 1.4437        | 7.0   | 1414 | 1.5860          | 18.5857 | 0.3532 |\n| 1.0727        | 8.0   | 1616 | 1.5875          | 19.1539 | 0.3605 |\n| 1.0727        | 9.0   | 1818 | 1.6126          | 19.0598 | 0.3599 |\n| 0.8623        | 10.0  | 2020 | 1.6371          | 19.2492 | 0.3608 |\n| 0.8623        | 11.0  | 2222 | 1.6553          | 19.455  | 0.3626 |\n| 0.8623        | 12.0  | 2424 | 1.6946          | 19.5787 | 0.365  |\n| 0.6908        | 13.0  | 2626 | 1.7131          | 19.7264 | 0.3666 |\n| 0.6908        | 14.0  | 2828 | 1.7351          | 19.9509 | 0.3695 |\n| 0.5725        | 15.0  | 3030 | 1.7576          | 19.9999 | 0.369  |\n| 0.5725        | 16.0  | 3232 | 1.7984          | 20.079  | 0.3707 |\n| 0.5725        | 17.0  | 3434 | 1.8397          | 20.0174 | 0.3717 |\n| 0.4773        | 18.0  | 3636 | 1.8763          | 19.7486 | 0.3683 |\n| 0.4773        | 19.0  | 3838 | 1.9018          | 20.0435 | 0.3716 |\n| 0.3998        | 20.0  | 4040 | 1.9268          | 20.2496 | 0.3735 |\n| 0.3998        | 21.0  | 4242 | 1.9699          | 20.2014 | 0.3726 |\n| 0.3998        | 22.0  | 4444 | 1.9935          | 20.0657 | 0.372  |\n| 0.3426        | 23.0  | 4646 | 1.9934          | 20.1145 | 0.3734 |\n| 0.3426        | 24.0  | 4848 | 2.0242          | 20.1118 | 0.3727 |\n| 0.2999        | 25.0  | 5050 | 2.0496          | 20.1956 | 0.3734 |\n| 0.2999        | 26.0  | 5252 | 2.0789          | 20.2959 | 0.3747 |\n| 0.2999        | 27.0  | 5454 | 2.0850          | 20.2194 | 0.3738 |\n| 0.2663        | 28.0  | 5656 | 2.0943          | 20.2931 | 0.3754 |\n| 0.2663        | 29.0  | 5858 | 2.0941          | 20.3255 | 0.3745 |\n| 0.2483        | 30.0  | 6060 | 2.1003          | 20.4006 | 0.3752 |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.12.1+cu113\n- Datasets 2.6.1\n- Tokenizers 0.13.2\n", "size_bytes": "242070267", "downloads": 2}
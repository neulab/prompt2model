{"pretrained_model_name": "Seungjun/textSummary", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: t5-finetuned-epoch80\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-finetuned-epoch80\n\nThis model is a fine-tuned version of [Seungjun/t5-small-finetuned-epoch15-finetuned-epoch30](https://huggingface.co/Seungjun/t5-small-finetuned-epoch15-finetuned-epoch30) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.3251\n- Rouge1: 31.3079\n- Rouge2: 19.3788\n- Rougel: 27.8831\n- Rougelsum: 29.9289\n- Gen Len: 18.9869\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.5167        | 1.0   | 765   | 1.3357          | 30.7806 | 18.8561 | 27.322  | 29.3203   | 18.9915 |\n| 1.5114        | 2.0   | 1530  | 1.3331          | 30.8821 | 18.8886 | 27.4069 | 29.427    | 18.9921 |\n| 1.5027        | 3.0   | 2295  | 1.3318          | 31.0168 | 19.0051 | 27.4976 | 29.5617   | 18.9921 |\n| 1.4929        | 4.0   | 3060  | 1.3310          | 30.9304 | 18.9689 | 27.4371 | 29.4587   | 18.9921 |\n| 1.4753        | 5.0   | 3825  | 1.3288          | 31.1043 | 19.0365 | 27.5671 | 29.626    | 18.9921 |\n| 1.4741        | 6.0   | 4590  | 1.3300          | 31.0773 | 19.0409 | 27.5637 | 29.6434   | 18.9921 |\n| 1.4648        | 7.0   | 5355  | 1.3288          | 31.0566 | 19.0196 | 27.5198 | 29.6072   | 18.9921 |\n| 1.459         | 8.0   | 6120  | 1.3289          | 30.9956 | 18.9737 | 27.5183 | 29.5477   | 18.9921 |\n| 1.4498        | 9.0   | 6885  | 1.3285          | 31.0239 | 18.9609 | 27.5007 | 29.5565   | 18.9921 |\n| 1.4374        | 10.0  | 7650  | 1.3273          | 31.0677 | 19.0404 | 27.57   | 29.6191   | 18.9921 |\n| 1.4494        | 11.0  | 8415  | 1.3268          | 31.0141 | 18.9744 | 27.5356 | 29.6089   | 18.9921 |\n| 1.4315        | 12.0  | 9180  | 1.3278          | 31.134  | 19.0892 | 27.5964 | 29.6855   | 18.9921 |\n| 1.4296        | 13.0  | 9945  | 1.3273          | 31.0735 | 18.9957 | 27.5448 | 29.6213   | 18.9921 |\n| 1.4194        | 14.0  | 10710 | 1.3280          | 31.1302 | 19.0632 | 27.5609 | 29.699    | 18.9921 |\n| 1.4124        | 15.0  | 11475 | 1.3250          | 31.1165 | 19.0928 | 27.5799 | 29.6889   | 18.9921 |\n| 1.4084        | 16.0  | 12240 | 1.3254          | 31.1263 | 19.0793 | 27.646  | 29.6891   | 18.9921 |\n| 1.407         | 17.0  | 13005 | 1.3253          | 31.1609 | 19.1378 | 27.6904 | 29.757    | 18.9921 |\n| 1.4023        | 18.0  | 13770 | 1.3273          | 31.1342 | 19.0976 | 27.6493 | 29.7176   | 18.9869 |\n| 1.4003        | 19.0  | 14535 | 1.3243          | 31.1514 | 19.0548 | 27.6337 | 29.7256   | 18.9869 |\n| 1.3812        | 20.0  | 15300 | 1.3241          | 31.141  | 19.0963 | 27.6509 | 29.7211   | 18.9869 |\n| 1.384         | 21.0  | 16065 | 1.3254          | 31.1899 | 19.1287 | 27.6734 | 29.7467   | 18.9869 |\n| 1.3845        | 22.0  | 16830 | 1.3240          | 31.2449 | 19.128  | 27.7214 | 29.8514   | 18.9869 |\n| 1.3828        | 23.0  | 17595 | 1.3235          | 31.2479 | 19.2076 | 27.7455 | 29.8529   | 18.9869 |\n| 1.3704        | 24.0  | 18360 | 1.3240          | 31.2648 | 19.281  | 27.8002 | 29.9013   | 18.9869 |\n| 1.3696        | 25.0  | 19125 | 1.3246          | 31.2989 | 19.2466 | 27.8263 | 29.8893   | 18.9869 |\n| 1.3655        | 26.0  | 19890 | 1.3241          | 31.2432 | 19.2747 | 27.7854 | 29.8454   | 18.9869 |\n| 1.3642        | 27.0  | 20655 | 1.3243          | 31.2356 | 19.3381 | 27.8251 | 29.8522   | 18.9869 |\n| 1.3591        | 28.0  | 21420 | 1.3258          | 31.2872 | 19.3189 | 27.8329 | 29.8712   | 18.9869 |\n| 1.3617        | 29.0  | 22185 | 1.3253          | 31.2717 | 19.3092 | 27.8307 | 29.8619   | 18.9869 |\n| 1.3528        | 30.0  | 22950 | 1.3243          | 31.2964 | 19.2643 | 27.8174 | 29.8636   | 18.9869 |\n| 1.3507        | 31.0  | 23715 | 1.3242          | 31.3035 | 19.2633 | 27.8234 | 29.9001   | 18.9869 |\n| 1.3555        | 32.0  | 24480 | 1.3249          | 31.1853 | 19.2155 | 27.7712 | 29.8318   | 18.9869 |\n| 1.3519        | 33.0  | 25245 | 1.3237          | 31.2987 | 19.2769 | 27.8254 | 29.9139   | 18.9869 |\n| 1.3485        | 34.0  | 26010 | 1.3251          | 31.2792 | 19.3177 | 27.8478 | 29.9052   | 18.9869 |\n| 1.3431        | 35.0  | 26775 | 1.3249          | 31.3273 | 19.3137 | 27.8582 | 29.9152   | 18.9869 |\n| 1.3441        | 36.0  | 27540 | 1.3243          | 31.2821 | 19.2692 | 27.8287 | 29.8906   | 18.9869 |\n| 1.3394        | 37.0  | 28305 | 1.3244          | 31.2441 | 19.2968 | 27.8135 | 29.8674   | 18.9869 |\n| 1.3417        | 38.0  | 29070 | 1.3245          | 31.2828 | 19.3228 | 27.8211 | 29.8829   | 18.9869 |\n| 1.3387        | 39.0  | 29835 | 1.3249          | 31.2479 | 19.2455 | 27.8016 | 29.8241   | 18.9869 |\n| 1.3382        | 40.0  | 30600 | 1.3250          | 31.2938 | 19.3179 | 27.8289 | 29.8908   | 18.9869 |\n| 1.3314        | 41.0  | 31365 | 1.3251          | 31.3194 | 19.3474 | 27.8534 | 29.9201   | 18.9869 |\n| 1.3305        | 42.0  | 32130 | 1.3255          | 31.3226 | 19.3875 | 27.8795 | 29.9215   | 18.9869 |\n| 1.3291        | 43.0  | 32895 | 1.3253          | 31.3393 | 19.3496 | 27.8896 | 29.9458   | 18.9869 |\n| 1.3297        | 44.0  | 33660 | 1.3252          | 31.3158 | 19.3493 | 27.8796 | 29.941    | 18.9869 |\n| 1.3305        | 45.0  | 34425 | 1.3247          | 31.3665 | 19.3972 | 27.9329 | 29.9925   | 18.9869 |\n| 1.3315        | 46.0  | 35190 | 1.3248          | 31.3447 | 19.3805 | 27.8995 | 29.971    | 18.9869 |\n| 1.3274        | 47.0  | 35955 | 1.3251          | 31.3327 | 19.402  | 27.9104 | 29.965    | 18.9869 |\n| 1.3286        | 48.0  | 36720 | 1.3252          | 31.316  | 19.3811 | 27.8851 | 29.9374   | 18.9869 |\n| 1.3246        | 49.0  | 37485 | 1.3250          | 31.3013 | 19.3684 | 27.8701 | 29.9252   | 18.9869 |\n| 1.3269        | 50.0  | 38250 | 1.3251          | 31.3079 | 19.3788 | 27.8831 | 29.9289   | 18.9869 |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.13.1+cu116\n- Tokenizers 0.13.2\n", "size_bytes": "242071641", "downloads": 2}
{"pretrained_model_name": "trojblue/blip2-opt-6.7b-coco-fp16", "description": "---\nlanguage: en\nlicense: mit\ntags:\n- vision\n- image-to-text\n- image-captioning\n- visual-question-answering\npipeline_tag: image-to-text\n---\n\n# BLIP-2, OPT-6.7b, Fine-tuned on COCO - Unofficial FP16 Version\n\nThis repository contains an unofficial version of the BLIP-2 model, leveraging [OPT-6.7b](https://huggingface.co/facebook/opt-6.7b), which has been fine-tuned on COCO and converted to FP16 for reduced model size and memory footprint.\n\nThe original model, BLIP-2, was introduced in the paper [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by Li et al. and first released in [this repository](https://github.com/salesforce/LAVIS/tree/main/projects/blip2).\n\nFor a comprehensive understanding of the model, its description, intended uses, limitations, and instructions on usage with different hardware and precision settings, please refer to the [official model card](https://huggingface.co/Salesforce/blip2-opt-6.7b-coco).\n\n## Unofficial FP16 Version\n\nThis version of the BLIP-2 model has been converted to use FP16 precision, which effectively reduces the model size and memory requirements. The conversion to FP16 can potentially accelerate the model's computation time on hardware with FP16 support, although it might slightly affect the model's performance due to reduced numerical precision.\n\nThis unofficial FP16 version is ideal for situations where storage, memory, or computational resources are limited.\n\nPlease note, this is an **unofficial** repository and not maintained or endorsed by the original authors of the model. The FP16 conversion was conducted independently and any potential issues, limitations or discrepancies with the original model are not the responsibility of the original authors.\n\n### How to use\n\nThe usage of this FP16 version of the model is similar to the original model. For specific code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration.forward.example).\n\nPlease ensure to test the performance and accuracy of this FP16 model thoroughly in your specific use-case to confirm it meets your needs.\n\nThis version can be used for tasks like:\n\n- image captioning\n- visual question answering (VQA)\n- chat-like conversations by feeding the image and the previous conversation as a prompt to the model\n\n*Disclaimer: This is an unofficial version of the model and any potential issues or discrepancies from the official model are not the responsibility of the original authors.*", "size_bytes": 15918487552, "downloads": 11}
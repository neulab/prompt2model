{"pretrained_model_name": "sahithyaravi/comet-m", "description": "---\n# For reference on model card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1\n# Doc / guide: https://huggingface.co/docs/hub/model-cards\n{}\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n\n\n- **Developed by:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** Commonsense generation\n- **Language(s) (NLP):** English\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** comet-atomic-2020-BART\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\nWe propose COMET-M (Multi-event), an event-centric commonsense model that is capable of generating commonsense inferences for multiple target events within a complex sentence. For example, given that \"John, who was shopping in the mall, was shot and spent weeks at a hospital\", COMET-M may predict what happened before each event \u2013 he reached the mall before shopping, a gun was loaded before he was shot, and he got admitted to the hospital before spending time there. We trained COMET-M on human-written inferences and also created baselines using automatically labeled examples. Our experiments demonstrate that COMET-M performs better than the baselines and successfully produces distinct inferences for each target event in the sentence while taking the complete context into consideration. COMET-M may be used in the future in downstream tasks that involve natural texts, such as coreference resolution, dialogue, and story understanding.\n\n### Direct Use\nThis model supports 6 relations.\n- HasPrerequisite- What are typically the prerequisites for the event?\n- isBefore- What typically happens immediately before the event?\n- isAfter- What typically happens immediately after the event?\n- xReason- What can cause the event?\n- Causes- What could be the effect of the event?\n- HinderedBy- What can hinder the event?\n\nLets look at how to derive inferences for the XReason relation for the target event \"have to say\" in the below sentence:\n\n\n<code>\nI &lt;TGT&gt; have to say &lt;TGT&gt; that my initial aversion to her, and I think a lot of people who don't like her probably don't for this same reason, was established before I even got a feel for how layered and subtle the \"Game\" and the political intrigue really is xReason [GEN]\n\n</code>\n", "size_bytes": "1625751501", "downloads": 2}
{"pretrained_model_name": "RUCAIBox/Erya", "description": "---\nlicense: apache-2.0\npipeline_tag: translation\nlanguage:\n- zh\n---\n\n# Model Description\n\nErya is a pretrained model specifically designed for translating Ancient Chinese into Modern Chinese. It utilizes an Encoder-Decoder architecture and has been trained using a combination of DMLM (Dual Masked Language Model) and DAS (Disyllabic Aligned Substitution) techniques on datasets comprising both Ancient Chinese and Modern Chinese texts. The detailed information of our work can be found here: [RUCAIBox/Erya (github.com)](https://github.com/RUCAIBox/Erya) \n\nMore information about Erya dataset can be found here: [RUCAIBox/Erya-dataset \u00b7 Datasets at Hugging Face](https://huggingface.co/datasets/RUCAIBox/Erya-dataset), which can be used to tune the Erya model further for a better translation performance.\n\n\n\n# Example\n\n```python\n>>> from transformers import BertTokenizer, CPTForConditionalGeneration\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"RUCAIBox/Erya\")\n>>> model = CPTForConditionalGeneration.from_pretrained(\"RUCAIBox/Erya\")\n\n>>> input_ids = tokenizer(\"\u5b89\u4e16\u5b57\u5b50\u5b7a\uff0c\u5c11\u4ee5\u7236\u4efb\u4e3a\u90ce\u3002\", return_tensors='pt')\n>>> input_ids.pop(\"token_type_ids\")\n\n>>> pred_ids = model.generate(max_new_tokens=256, **input_ids)\n>>> print(tokenizer.batch_decode(pred_ids, skip_special_tokens=True))\n    ['\u5b89 \u4e16 \u5b57 \u5b50 \u5b7a \uff0c \u5e74 \u8f7b \u65f6 \u56e0 \u7236 \u4efb \u90ce \u5b98 \u3002']\n```", "size_bytes": "1052463039", "downloads": 41}
{"pretrained_model_name": "domenicrosati/QA2D-t5-small", "description": "---\nlicense: apache-2.0\ntags:\n- text2text-generation\n- generated_from_trainer\nmetrics:\n- rouge\n- bleu\ndatasets:\n- domenicrosati/QA2D\nmodel-index:\n- name: QA2D-t5-small\n  results:\n  - task:\n      name: Question to Declarative Sentence\n      type: text2text-generation\n    dataset:\n      name: domenicrosati/QA2D\n      type: domenicrosati/QA2D\n      args: plain_text\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 89.8753\n    - name: Rouge2\n      type: rouge\n      value: 81.8104\n    - name: Rougel\n      type: rouge\n      value: 85.4253\n    - name: Rougelsum\n      type: rouge\n      value: 85.4236\n    - name: Bleu\n      type: bleu\n      value: 72.1080\nwidget:\n- text: \"where in the world is carmen sandiego. she is in abruzzo\"\n  example_title: \"Where is Carmen Sandiego?\"\n- text: \"which province is halifax in. nova scotia\"\n  example_title: \"A Halifact\"\n---\n\n# QA2D-t5-small\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on [QA2D](https://huggingface.co/datasets/domenicrosati/QA2D).\nIt achieves the following results on the evaluation set:\n- Loss: 0.3236\n- Rouge1: 89.8753\n- Rouge2: 81.8104\n- Rougel: 85.4253\n- Rougelsum: 85.4236\n- Bleu: 72.1080\n\nSee: [https://wandb.ai/domenicrosati/huggingface/runs/n1yallpe](https://wandb.ai/domenicrosati/huggingface/runs/n1yallpe) for training and eval stats and [https://github.com/domenicrosati/qa2d-models](https://github.com/domenicrosati/qa2d-models) for the code!\n\n## Model description\n\nA t5-model model to convert questions, answer pairs into statements.\n\nDue to the way it's been trained the input should be all lower case and punctuation removed.\nUse with `. ` as the seperator between question and answer.\n> \"where in the world is carmen. abruzzo\"\n> Output: \"carmen is in abruzzo\"\n\nThought punctation and upper case works.\n\n```\nfrom transformers import AutoTokenizer,  AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained('domenicrosati/QA2D-t5-small')\nmodel = AutoModelForSeq2SeqLM.from_pretrained('domenicrosati/QA2D-t5-small')\n\nquestion = \"where in the world is carmen sandiego\"\nanswer = \"she is in abruzzo\"\nSEP = \". \"\n\nprompt = f'{question}{SEP}{answer}'\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids\noutput_ids = model.generate(input_ids)\nresponses = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n# ['carmen sandiego is in abruzzo']\n```\n\n## Intended uses & limitations\n\nTo convert questions, answer pairs into statements.\n\n## Training and evaluation data\n\nUses [QA2D](https://huggingface.co/datasets/domenicrosati/QA2D).\n\nSee [https://github.com/domenicrosati/qa2d-models](https://github.com/domenicrosati/qa2d-models)\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 12\n- eval_batch_size: 12\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 20\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Bleu    |\n|:-------------:|:-----:|:------:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 0.3177        | 1.0   | 5060   | 0.3144          | 89.6379 | 81.3168 | 85.2036 | 85.1904   | 71.4255 |\n| 0.2479        | 2.0   | 10120  | 0.3035          | 89.7816 | 81.6556 | 85.3541 | 85.3406   | 71.7248 |\n| 0.2268        | 3.0   | 15180  | 0.3015          | 89.8287 | 81.698  | 85.3434 | 85.3387   | 71.8344 |\n| 0.2111        | 4.0   | 20240  | 0.3014          | 89.8082 | 81.7192 | 85.4094 | 85.406    | 71.9172 |\n| 0.1991        | 5.0   | 25300  | 0.3023          | 89.8776 | 81.7607 | 85.3912 | 85.3842   | 71.9417 |\n| 0.1886        | 6.0   | 30360  | 0.3012          | 89.901  | 81.7614 | 85.3345 | 85.3315   | 72.0218 |\n| 0.1803        | 7.0   | 35420  | 0.3010          | 89.8776 | 81.8189 | 85.4154 | 85.4097   | 72.0533 |\n| 0.1724        | 8.0   | 40480  | 0.3041          | 89.9168 | 81.8663 | 85.4457 | 85.4447   | 72.1470 |\n| 0.1654        | 9.0   | 45540  | 0.3076          | 89.8901 | 81.8536 | 85.4857 | 85.4863   | 72.0830 |\n| 0.1601        | 10.0  | 50600  | 0.3083          | 89.9186 | 81.881  | 85.4653 | 85.4594   | 72.1048 |\n| 0.1546        | 11.0  | 55660  | 0.3136          | 89.8958 | 81.8533 | 85.4217 | 85.4238   | 72.0752 |\n| 0.1502        | 12.0  | 60720  | 0.3138          | 89.903  | 81.8604 | 85.4301 | 85.4267   | 72.1373 |\n| 0.1461        | 13.0  | 65780  | 0.3140          | 89.8867 | 81.7945 | 85.3698 | 85.3662   | 72.0718 |\n| 0.1423        | 14.0  | 70840  | 0.3171          | 89.8985 | 81.8221 | 85.4348 | 85.4331   | 72.1168 |\n| 0.1392        | 15.0  | 75900  | 0.3186          | 89.8938 | 81.8246 | 85.402  | 85.3991   | 72.0858 |\n| 0.1366        | 16.0  | 80960  | 0.3208          | 89.859  | 81.8133 | 85.4194 | 85.4182   | 72.1014 |\n| 0.1344        | 17.0  | 86020  | 0.3222          | 89.8909 | 81.828  | 85.4392 | 85.435    | 72.1380 |\n| 0.1324        | 18.0  | 91080  | 0.3226          | 89.8906 | 81.8351 | 85.4506 | 85.4441   | 72.1622 |\n| 0.1309        | 19.0  | 96140  | 0.3231          | 89.8925 | 81.8369 | 85.4375 | 85.4366   | 72.1552 |\n| 0.1305        | 20.0  | 101200 | 0.3236          | 89.8753 | 81.8104 | 85.4253 | 85.4236   | 72.1080 |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.11.0+cu113\n- Datasets 2.1.0\n- Tokenizers 0.12.1\n", "size_bytes": "242070267", "downloads": 2}
{"pretrained_model_name": "santiviquez/bart-base-finetuned-samsum-en", "description": "---\nlicense: apache-2.0\ntags:\n- summarization\n- generated_from_trainer\ndatasets:\n- samsum\nmetrics:\n- rouge\nmodel-index:\n- name: bart-base-finetuned-samsum-en\n  results:\n  - task:\n      type: text2text-generation\n      name: Sequence-to-sequence Language Modeling\n    dataset:\n      name: samsum\n      type: samsum\n      args: samsum\n    metrics:\n    - type: rouge\n      value: 46.8825\n      name: Rouge1\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: samsum\n      type: samsum\n      config: samsum\n      split: test\n    metrics:\n    - type: rouge\n      value: 45.0692\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmJmZGIzMjdlODZhOThjMTFhYzM3NWJjOTNjMjkyN2U4NmE2YjViZmM0ZTBkNTU0NTc0YmZlMGY4NDI1NmM0YyIsInZlcnNpb24iOjF9.4p6xyMhFMia_Ms0bohyUchjrHhYvz7tH_O9LdeEK5FYF6pWEWs3Aw3i6LCc2HPnsge2ZKG-pzsRdbMJwO5w4Bg\n    - type: rouge\n      value: 20.9049\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTY3OTVmM2E5ZmRjYzYzZmE3MGQwODNmMTk0MGJjMTRkMDQ2NGEwNGY0ZTA3OWJjODIzOWVkYTgwZjRjNzhkYiIsInZlcnNpb24iOjF9.YKFCR8fT38vuj9TA9wZHUNX6hmV0_m4yErKuuVvEBtWSwOmrzIrdSHVk40wSQomojTFFA9HRAZ_Ssf0rke_SAw\n    - type: rouge\n      value: 37.3128\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDZhN2I5NDdjNTEwYzg2YTZlMWJkZDFmYmE0NDkzYmI4Mjg2YjVkYTQwMmEyMTEzZmI4YzBmMDg3OWY5MDQ0MiIsInZlcnNpb24iOjF9.RFrkndxIOt61Vs59zCdI09IXh349inPilhD-bXvpqfUKa92PTUNBqsumL8lzS4N7uEcfca1QYG4STEy_Efc9Dw\n    - type: rouge\n      value: 40.662\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIxMjlkZjY3ZDNiOGQwM2FlNTExYjI5OGIwOGUxYTU4ZGFiNGRjOTU5MTFjMDQ2YWU2MzU3M2E0NTQ3Yjk4MiIsInZlcnNpb24iOjF9.AEJgh8jnUs0nyIsUj6mNxyqIzfFRVwqEowIMnK-xnSRbuMkCMEREeao4FSK2a0Cx5rNkXuEW4fBIRDrhyDKIBA\n    - type: loss\n      value: 5.763935565948486\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzc2Yzc5YjRjOGFjZmIwYTFiYzNiN2VmNzJiNjdlZmRhNjZlNzU4ZDc1OWE2MTk1MjQ5MGM0NDNmMDY3NTZmYiIsInZlcnNpb24iOjF9.r-Z0kcXf81IlDxT0yYoLjya7Uc3q6sJT0LxzhIdKVYc3mOoxPLMX1_vLQPX8fsIHIqPHPtKczfZ_z6mB0yptBg\n    - type: gen_len\n      value: 18.4921\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2NkYTg0MGFiYmJiZWI1ZjYzMGEzOGM1MGM2NjgyNzE2MTk2ZTc3MTQ2Mzg4Y2U5NTM3MDBiNTQ5YzUwNzAzYyIsInZlcnNpb24iOjF9.fn2kkheKjJJio6pcI0X0OWMEE08CuO35VrebfTNqTGJDV3U5O2f6Vf6RMo0d5b4N5X-E5-Ju-7k2CDxNi8CAAw\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bart-base-finetuned-samsum-en\n\nThis model is a fine-tuned version of [facebook/bart-base](https://huggingface.co/facebook/bart-base) on the samsum dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.3676\n- Rouge1: 46.8825\n- Rouge2: 22.0923\n- Rougel: 39.7249\n- Rougelsum: 42.9187\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 10\n- eval_batch_size: 10\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|\n| 0.5172        | 1.0   | 300  | 2.1613          | 47.4152 | 22.8106 | 39.93   | 43.3639   |\n| 0.3627        | 2.0   | 600  | 2.2771          | 47.2676 | 22.6325 | 40.1345 | 43.19     |\n| 0.2466        | 3.0   | 900  | 2.3676          | 46.8825 | 22.0923 | 39.7249 | 42.9187   |\n\n\n### Framework versions\n\n- Transformers 4.19.2\n- Pytorch 1.11.0+cu113\n- Datasets 2.2.2\n- Tokenizers 0.12.1\n", "size_bytes": "557969145", "downloads": 8}
{"pretrained_model_name": "research-backup/mbart-large-cc25-trimmed-es", "description": "# Vocabulary Trimmed [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25): `asahi417/mbart-large-cc25-trimmed-es` \nThis model is a trimmed version of [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25) by [`vocabtrimmer`](https://github.com/asahi417/lm-vocab-trimmer), a tool for trimming vocabulary of language models to compress the model size.\nFollowing table shows a summary of the trimming process.\n\n|                            | facebook/mbart-large-cc25   | asahi417/mbart-large-cc25-trimmed-es   |\n|:---------------------------|:----------------------------|:---------------------------------------|\n| parameter_size_full        | 610,851,840                 | 476,043,264                            |\n| parameter_size_embedding   | 512,055,296                 | 242,438,144                            |\n| vocab_size                 | 250,027                     | 118,378                                |\n| compression_rate_full      | 100.0                       | 77.93                                  |\n| compression_rate_embedding | 100.0                       | 47.35                                  |", "size_bytes": "1904815133", "downloads": 2}
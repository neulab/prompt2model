{"pretrained_model_name": "wangqi777/chinese-base-model-13b-gptq-4bit-128g", "description": "---\nlicense: apache-2.0\ndatasets:\n- Chinese-Vicuna/guanaco_belle_merge_v1.0\n- Chinese-Vicuna/instruct_chat_50k.jsonl\nlanguage:\n- zh\n- en\npipeline_tag: text2text-generation\n---\n\n\u672c\u9879\u76ee\u81f4\u529b\u4e8e\u63d0\u4f9b\u4e2d\u6587\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4f5c\u4e3a\u5176\u4ed6\u4e2a\u6027\u5316\u8bad\u7ec3\u7684\u57fa\u7840\u3002\n\u672c\u6a21\u578b\u7684\u57fa\u7840\u662f[openaccess-ai-collective/wizard-mega-13b](https://huggingface.co/openaccess-ai-collective/wizard-mega-13b)\uff0c\u6b64\u6a21\u578b\u5bf9\u82f1\u6587\u7684\u591a\u8f6e\u5bf9\u8bdd\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u9002\u4e8e\u521b\u5efa\u89d2\u8272\u626e\u6f14\u548c\u79c1\u4eba\u52a9\u7406\u7b49\u670d\u52a1\u3002\n\n\u672c\u6a21\u578b\u91c7\u7528\u4e86\u201cChinese-LLaMA-Alpaca\u201d\u6574\u5408\u7684\u652f\u6301\u4e2d\u6587\u7684tokenizer\uff0c\u53c2\u89c1[\u8bad\u7ec3\u7ec6\u8282](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82)\n\n\u5728\u6b64\u57fa\u7840\u4e0a\u96c6\u6210\u4e86\u5982\u4e0b\u4e2d\u6587LoRA\n1. [ziqingyang/chinese-llama-plus-lora-13b](https://huggingface.co/ziqingyang/chinese-llama-plus-lora-13b)\n2. [ziqingyang/chinese-alpaca-plus-lora-13b](https://huggingface.co/ziqingyang/chinese-alpaca-plus-lora-13b)\n3. [Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco](https://huggingface.co/Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco)\n\n\u6a21\u578b\u63d0\u4f9b\u4ee5\u4e0b\u7248\u672c\uff1a\n1. HuggingFace FP16\u7248\u672c\n2. ggml-q5_0\u7248\u672c\n3. gptq-4bit-128g\u7248\u672c\n\n\u6a21\u578b\u6d4b\u8bd5\n```bash\nhf-causal-experimental (pretrained=../game-npc-vicuna/models/game_npc_vicuna_base,load_in_8bit=True,trust_remote_code=True,device_map_option=auto,dtype=auto),\nlimit: None, provide_description: False, num_fewshot: 0, batch_size: None\n```\n\n|     Task      |Version| Metric |Value |   |Stderr|\n|---------------|------:|--------|-----:|---|-----:|\n|arc_challenge  |      0|acc     |0.3618|\u00b1  |0.0140|\n|               |       |acc_norm|0.3575|\u00b1  |0.0140|\n|hellaswag      |      0|acc     |0.4609|\u00b1  |0.0050|\n|               |       |acc_norm|0.5945|\u00b1  |0.0049|\n|truthfulqa_mc  |      1|mc1     |0.3586|\u00b1  |0.0168|\n|               |       |mc2     |0.5574|\u00b1  |0.0161|\n|xcopa_zh       |      0|acc     |0.5360|\u00b1  |0.0223|\n|xstory_cloze_zh|      0|acc     |0.5116|\u00b1  |0.0129|\n|xwinograd_zh   |      0|acc     |0.6190|\u00b1  |0.0217|\n|xnli_zh        |      0|acc     |0.3425|\u00b1  |0.0067|", "size_bytes": 26399436800, "downloads": 5}
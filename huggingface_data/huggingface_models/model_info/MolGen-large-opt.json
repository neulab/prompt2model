{"pretrained_model_name": "zjunlp/MolGen-large-opt", "description": "---\ntags:\n- molecular language model\n- SELFIES\n- molecule optimization\ninference: false\n---\n\n# MolGen-large-opt\nMolGen-large-opt was introduced in the paper [\"Domain-Agnostic Molecular Generation with Self-feedback\"](https://arxiv.org/pdf/2301.11259.pdf) and first released in [this repository](https://github.com/zjunlp/MolGen). \n\n## Model description\nMolGen-large-opt is the fine-tuned version of [MolGen-large](https://huggingface.co/zjunlp/MolGen-large). MolGen-large is the first pre-trained model that only produces chemically valid molecules. \nWith a training corpus of over 100 million molecules in SELFIES representation, MolGen-large learns the intrinsic structural patterns of molecules by mapping corrupted SELFIES to their original forms.\nSpecifically, MolGen-large employs a bidirectional Transformer as its encoder and an autoregressive Transformer as its decoder.\nThrough its carefully designed multi-task molecular prefix tuning (MPT), MolGen-large-opt can generate molecules with desired properties, making it a valuable tool for molecular optimization.\n\n![image.png](./molgen.png)\n\n## Intended uses\nYou can use the fine-tuned model for molecule optimization for downstream tasks. See the [repository](https://github.com/zjunlp/MolGen) to look for fine-tune details on a task that interests you.\n\n### How to use\nMolecule optimization example:\n```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"zjunlp/MolGen-large-opt\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"zjunlp/MolGen-large-opt\")\n\n>>> sf_input = tokenizer(\"[N][#C][C][C][C@@H1][C][C][C][C][C][C][C][C][C][C][C][Ring1][N][=O]\", return_tensors=\"pt\")\n>>> # beam search\n>>> molecules = model.generate(input_ids=sf_input[\"input_ids\"],\n                              attention_mask=sf_input[\"attention_mask\"],\n                              max_length=35,\n                              min_length=5,\n                              num_return_sequences=5,\n                              num_beams=5)\n>>> sf_output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True).replace(\" \",\"\") for g in molecules]\n['[N][#C][C][C][C@@H1][C][C][C][C][C][C][C][C][C][C][C][C][Ring1][N][=O]',\n'[N][#C][C][C][C@@H1][C][C][C][C][C][C][C][C][C][C][C][Ring1][N][=O]',\n'[N][#C][C][C][C@@H1][C][C][C][C][C][C][C][C][C][C][C][C][C][Ring1][N][=O]',\n'[N][#C][C][C][C@@H1][C][C][C][C][C][C][C][C][C][C][Ring1][N][=O]',\n'[N][#C][C][C][C@@H1][C][C][C][C][C][C][C][C][C][C][C][C][C][C][Ring1][N][=O]']\n```\n\n\n### BibTeX entry and citation info\n```bibtex\n@article{fang2023molecular,\n  title={Molecular Language Model as Multi-task Generator},\n  author={Fang, Yin and Zhang, Ningyu and Chen, Zhuo and Fan, Xiaohui and Chen, Huajun},\n  journal={arXiv preprint arXiv:2301.11259},\n  year={2023}\n}\n```", "size_bytes": "1420206221", "downloads": 29}
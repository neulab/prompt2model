{"pretrained_model_name": "theojolliffe/bart-large-cnn-finetuned-roundup-64", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: bart-large-cnn-finetuned-roundup-64\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bart-large-cnn-finetuned-roundup-64\n\nThis model is a fine-tuned version of [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.4772\n- Rouge1: 46.5444\n- Rouge2: 27.4056\n- Rougel: 29.6779\n- Rougelsum: 44.0905\n- Gen Len: 142.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 64\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| No log        | 1.0   | 132  | 1.3213          | 48.3389 | 28.6641 | 31.4086 | 45.6679   | 142.0   |\n| No log        | 2.0   | 264  | 1.2325          | 48.798  | 29.3068 | 31.4329 | 45.7945   | 142.0   |\n| No log        | 3.0   | 396  | 1.2791          | 47.1449 | 27.3965 | 30.56   | 44.4704   | 142.0   |\n| 0.9574        | 4.0   | 528  | 1.3134          | 46.2319 | 25.6249 | 28.7673 | 43.7555   | 140.3   |\n| 0.9574        | 5.0   | 660  | 1.3187          | 46.7313 | 25.3467 | 29.3873 | 43.9495   | 142.0   |\n| 0.9574        | 6.0   | 792  | 1.4271          | 48.1638 | 27.8874 | 30.5334 | 45.9944   | 142.0   |\n| 0.9574        | 7.0   | 924  | 1.4876          | 46.7481 | 25.7259 | 29.7214 | 43.7042   | 140.5   |\n| 0.3303        | 8.0   | 1056 | 1.5259          | 46.7075 | 26.0716 | 29.5521 | 43.7312   | 142.0   |\n| 0.3303        | 9.0   | 1188 | 1.6223          | 48.012  | 27.2795 | 30.4989 | 45.4644   | 142.0   |\n| 0.3303        | 10.0  | 1320 | 1.6842          | 48.0074 | 26.8831 | 29.3396 | 45.1937   | 142.0   |\n| 0.3303        | 11.0  | 1452 | 1.7317          | 46.52   | 26.5152 | 29.5124 | 43.8797   | 142.0   |\n| 0.1478        | 12.0  | 1584 | 1.8087          | 47.5887 | 27.0488 | 29.8569 | 44.7318   | 140.8   |\n| 0.1478        | 13.0  | 1716 | 1.8263          | 46.1251 | 25.8576 | 30.1698 | 42.7228   | 142.0   |\n| 0.1478        | 14.0  | 1848 | 1.9459          | 46.4034 | 25.7039 | 28.2542 | 43.7254   | 142.0   |\n| 0.1478        | 15.0  | 1980 | 1.9539          | 44.4666 | 24.5827 | 27.7147 | 41.9769   | 142.0   |\n| 0.0779        | 16.0  | 2112 | 1.9654          | 47.2267 | 26.4562 | 29.7352 | 44.0823   | 142.0   |\n| 0.0779        | 17.0  | 2244 | 1.9580          | 48.5086 | 28.0294 | 30.8311 | 45.6336   | 142.0   |\n| 0.0779        | 18.0  | 2376 | 2.0065          | 48.293  | 28.5678 | 30.0243 | 45.1384   | 142.0   |\n| 0.0499        | 19.0  | 2508 | 1.9313          | 49.0549 | 28.9695 | 32.0711 | 46.3834   | 142.0   |\n| 0.0499        | 20.0  | 2640 | 2.0176          | 47.0121 | 25.1606 | 29.0108 | 44.1556   | 142.0   |\n| 0.0499        | 21.0  | 2772 | 2.0711          | 48.3754 | 28.2221 | 30.772  | 45.8547   | 140.95  |\n| 0.0499        | 22.0  | 2904 | 2.0848          | 45.7392 | 25.254  | 29.0833 | 43.0381   | 142.0   |\n| 0.0335        | 23.0  | 3036 | 2.0711          | 47.2931 | 27.4573 | 30.718  | 44.5932   | 142.0   |\n| 0.0335        | 24.0  | 3168 | 2.1200          | 50.515  | 30.4253 | 33.7045 | 47.6158   | 142.0   |\n| 0.0335        | 25.0  | 3300 | 2.1097          | 46.4737 | 26.3055 | 29.0148 | 43.2135   | 142.0   |\n| 0.0335        | 26.0  | 3432 | 2.1695          | 46.9099 | 26.5227 | 29.7757 | 44.0613   | 142.0   |\n| 0.0249        | 27.0  | 3564 | 2.1494          | 47.8319 | 27.6364 | 31.3593 | 45.065    | 141.95  |\n| 0.0249        | 28.0  | 3696 | 2.1510          | 47.504  | 26.8971 | 31.7196 | 45.0328   | 142.0   |\n| 0.0249        | 29.0  | 3828 | 2.1612          | 46.8789 | 27.266  | 30.1009 | 43.8248   | 142.0   |\n| 0.0249        | 30.0  | 3960 | 2.1579          | 47.7012 | 27.7761 | 30.935  | 44.3686   | 142.0   |\n| 0.018         | 31.0  | 4092 | 2.1981          | 48.4703 | 29.167  | 31.9815 | 45.8005   | 142.0   |\n| 0.018         | 32.0  | 4224 | 2.2332          | 45.9512 | 25.8111 | 29.2467 | 42.9234   | 142.0   |\n| 0.018         | 33.0  | 4356 | 2.1944          | 47.7189 | 28.1413 | 30.9692 | 44.9361   | 142.0   |\n| 0.018         | 34.0  | 4488 | 2.2589          | 50.9687 | 32.3987 | 36.5644 | 48.3938   | 142.0   |\n| 0.0132        | 35.0  | 4620 | 2.2269          | 47.8241 | 28.0442 | 31.5535 | 44.9394   | 142.0   |\n| 0.0132        | 36.0  | 4752 | 2.2865          | 47.4383 | 27.0825 | 30.4109 | 44.194    | 142.0   |\n| 0.0132        | 37.0  | 4884 | 2.3267          | 49.1786 | 29.6416 | 32.875  | 46.8821   | 142.0   |\n| 0.0095        | 38.0  | 5016 | 2.2872          | 48.2085 | 28.3304 | 32.1473 | 45.3571   | 142.0   |\n| 0.0095        | 39.0  | 5148 | 2.3340          | 46.6762 | 26.1637 | 29.0149 | 43.5923   | 142.0   |\n| 0.0095        | 40.0  | 5280 | 2.3425          | 46.7561 | 26.1645 | 29.6337 | 43.6188   | 142.0   |\n| 0.0095        | 41.0  | 5412 | 2.3111          | 49.4118 | 29.9761 | 33.4765 | 46.601    | 142.0   |\n| 0.0076        | 42.0  | 5544 | 2.3892          | 45.3335 | 25.0161 | 28.4124 | 41.9873   | 142.0   |\n| 0.0076        | 43.0  | 5676 | 2.3808          | 46.2506 | 26.4283 | 29.3841 | 42.7488   | 142.0   |\n| 0.0076        | 44.0  | 5808 | 2.3825          | 45.6823 | 26.0048 | 29.5501 | 42.6475   | 142.0   |\n| 0.0076        | 45.0  | 5940 | 2.3592          | 47.9127 | 26.7924 | 30.2353 | 44.791    | 142.0   |\n| 0.0051        | 46.0  | 6072 | 2.4206          | 46.0415 | 27.0681 | 29.9602 | 43.1225   | 142.0   |\n| 0.0051        | 47.0  | 6204 | 2.4214          | 48.1229 | 29.0913 | 31.1828 | 45.0022   | 142.0   |\n| 0.0051        | 48.0  | 6336 | 2.4176          | 47.3825 | 27.7622 | 30.4138 | 43.9047   | 142.0   |\n| 0.0051        | 49.0  | 6468 | 2.4137          | 48.2544 | 28.277  | 31.5548 | 45.6053   | 142.0   |\n| 0.0041        | 50.0  | 6600 | 2.4384          | 49.6459 | 30.186  | 33.0059 | 47.0483   | 142.0   |\n| 0.0041        | 51.0  | 6732 | 2.4433          | 47.7279 | 27.7857 | 30.2982 | 45.0842   | 142.0   |\n| 0.0041        | 52.0  | 6864 | 2.4068          | 48.6047 | 28.1758 | 31.2744 | 45.8336   | 142.0   |\n| 0.0041        | 53.0  | 6996 | 2.4362          | 48.7095 | 29.3335 | 31.9509 | 46.4161   | 142.0   |\n| 0.003         | 54.0  | 7128 | 2.4307          | 48.836  | 29.6069 | 32.4004 | 46.1986   | 142.0   |\n| 0.003         | 55.0  | 7260 | 2.4292          | 47.2945 | 26.7577 | 28.9719 | 43.8988   | 142.0   |\n| 0.003         | 56.0  | 7392 | 2.4425          | 45.2261 | 25.6879 | 28.8129 | 42.6474   | 142.0   |\n| 0.0024        | 57.0  | 7524 | 2.4386          | 47.967  | 28.5415 | 32.2049 | 45.5111   | 142.0   |\n| 0.0024        | 58.0  | 7656 | 2.4528          | 47.5552 | 27.6397 | 30.9151 | 44.2627   | 142.0   |\n| 0.0024        | 59.0  | 7788 | 2.4574          | 46.7821 | 27.3368 | 30.6334 | 44.0533   | 142.0   |\n| 0.0024        | 60.0  | 7920 | 2.4659          | 47.3507 | 26.8371 | 30.4566 | 44.4452   | 142.0   |\n| 0.0018        | 61.0  | 8052 | 2.4766          | 47.9847 | 28.2678 | 30.0664 | 45.0071   | 142.0   |\n| 0.0018        | 62.0  | 8184 | 2.4682          | 46.8392 | 27.1275 | 30.144  | 43.6379   | 142.0   |\n| 0.0018        | 63.0  | 8316 | 2.4754          | 45.6338 | 26.2812 | 29.4831 | 42.8744   | 142.0   |\n| 0.0018        | 64.0  | 8448 | 2.4772          | 46.5444 | 27.4056 | 29.6779 | 44.0905   | 142.0   |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.11.0+cu113\n- Datasets 2.1.0\n- Tokenizers 0.12.1\n", "size_bytes": "1625533697", "downloads": 2}
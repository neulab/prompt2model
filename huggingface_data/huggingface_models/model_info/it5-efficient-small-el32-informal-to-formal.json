{"pretrained_model_name": "it5/it5-efficient-small-el32-informal-to-formal", "description": "---\nlanguage:\n- it\nlicense: apache-2.0\ntags:\n- italian\n- sequence-to-sequence\n- style-transfer\n- efficient\n- formality-style-transfer\ndatasets:\n- yahoo/xformal_it\nwidget:\n- text: \"maronn qualcuno mi spieg' CHECCOSA SUCCEDE?!?!\"\n- text: \"wellaaaaaaa, ma frat\u00e9 sei proprio troppo simpatiko, grazieeee!!\"\n- text: \"nn capisco xke tt i ragazzi lo fanno\"\n- text: \"IT5 \u00e8 SUPERMEGA BRAVISSIMO a capire tt il vernacolo italiano!!!\"\nmetrics:\n- rouge\n- bertscore\nmodel-index:\n- name: it5-efficient-small-el32-informal-to-formal\n  results:\n  - task: \n      type: formality-style-transfer\n      name: \"Informal-to-formal Style Transfer\"\n    dataset:\n      type: xformal_it\n      name: \"XFORMAL (Italian Subset)\"\n    metrics:\n      - type: rouge1\n        value: 0.430\n        name: \"Avg. Test Rouge1\"\n      - type: rouge2\n        value: 0.221\n        name: \"Avg. Test Rouge2\"\n      - type: rougeL\n        value: 0.408\n        name: \"Avg. Test RougeL\"\n      - type: bertscore\n        value: 0.630\n        name: \"Avg. Test BERTScore\"\n---\n\n# IT5 Cased Small Efficient EL32 for Informal-to-formal Style Transfer \ud83e\uddd0\n\n*Shout-out to [Stefan Schweter](https://github.com/stefan-it) for contributing the pre-trained efficient model!*\n\nThis repository contains the checkpoint for the [IT5 Cased Small Efficient EL32](https://huggingface.co/it5/it5-efficient-small-el32) model fine-tuned on Informal-to-formal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). \n\nEfficient IT5 models differ from the standard ones by adopting a different vocabulary that enables cased text generation and an [optimized model architecture](https://arxiv.org/abs/2109.10686) to improve performances while reducing parameter count. The Small-EL32 replaces the original encoder from the T5 Small architecture with a 32-layer deep encoder, showing improved performances over the base model.\n\nA comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.\n\n## Using the model\n\nModel checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:\n\n```python\nfrom transformers import pipelines\n\ni2f = pipeline(\"text2text-generation\", model='it5/it5-efficient-small-el32-informal-to-formal')\ni2f(\"nn capisco xke tt i ragazzi lo fanno\")\n>>> [{\"generated_text\": \"non comprendo perch\u00e9 tutti i ragazzi agiscono cos\u00ec\"}]\n```\n\nor loaded using autoclasses:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"it5/it5-efficient-small-el32-informal-to-formal\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"it5/it5-efficient-small-el32-informal-to-formal\")\n```\n\nIf you use this model in your research, please cite our work as:\n\n```bibtex\n@article{sarti-nissim-2022-it5,\n    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},\n    author={Sarti, Gabriele and Nissim, Malvina},\n    journal={ArXiv preprint 2203.03759},\n    url={https://arxiv.org/abs/2203.03759},\n    year={2022},\n\tmonth={mar}\n}\n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10.0\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu102\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "size_bytes": "569387035", "downloads": 10}
{"pretrained_model_name": "retrieva-jp/t5-base-short", "description": "---\nlicense: cc-by-sa-4.0\nlanguage: \n- ja\n---\n# Model card for model ID\n\nThis is a T5 v1.1 model, pre-trained on a Japanese corpus.\n\n## Model details\n\nT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.\n- GEGLU activation in feed-forward hidden layer, rather than ReLU - see https://arxiv.org/abs/2002.05202 .\n- Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.\n- no parameter sharing between embedding and classifier layer\n- \"xl\" and \"xxl\" replace \"3B\" and \"11B\". The model shapes are a bit different - larger d_model and smaller num_heads and d_ff.\n\nThis model is based on T5 v1.1. It was pre-trained on a Japanese corpus. For the Japanese corpus, Japanese Wikipedia and mC4/ja were used.\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n- **Developed by:** Retrieva, Inc.\n- **Model type:** T5 v1.1\n- **Language(s) (NLP):** Japanese\n- **License:** CC-BY-SA 4.0 Although commercial use is permitted, we kindly request that you contact us beforehand.\n\n\n## Training Details\n\nWe use T5X (https://github.com/google-research/t5x) for the training of this model, and it has been converted to the Huggingface transformer format.\n\n## Training Data\n\nThe training data used is\n- The Japanese part of the multilingual C4(mC4/ja).\n- Japanese Wikipedia(20220920).\n  \n#### Preprocessing\nThe following filtering is done\n- Remove documents that do not use a single hiragana character. This removes English-only documents and documents in Chinese.\n- Whitelist-style filtering using the top level domain of URL to remove affiliate sites.\n\n#### Training Hyperparameters\n\n- dropout rate: 0.0\n- batch size: 256\n- fp32\n- input length: 512\n- output length: 114\n\n- Otherwise, the default value of T5X (https://github.com/google-research/t5x/blob/main/t5x/examples/t5/t5_1_1/base.gin) is followed, including the following.\n  - optimizer: Adafactor\n  - base_learning_rate: 1.0\n  - warmup steps: 10000\n\n#### Speeds, Sizes, Times\n\nWe trained 524288 steps.\n\n## Technical Specifications\n\n### Model Architecture and Objective\nModel architecture.\n- T5 v1.1(https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)\n- Size: Base(~220 million parameters)\n\n### Compute Infrastructure\n\nGoogle Cloud TPU v4-8.\n\n#### Software\n\n- T5X(https://github.com/google-research/t5x).\n\n## More Information\n\nhttps://note.com/retrieva/n/n7b4186dc5ada (in Japanese)\n\n## Model Card Authors\n\nJiro Nishitoba\n\n## Model Card Contact\n\npr@retrieva.jp\n", "size_bytes": "990404917", "downloads": 13}
{"pretrained_model_name": "summervent/speller-t5-90", "description": "---\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: speller-t5-90\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# speller-t5-90\n\nThis model is a fine-tuned version of [sberbank-ai/ruT5-base](https://huggingface.co/sberbank-ai/ruT5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1486\n- Rouge1: 19.3503\n- Rouge2: 8.3898\n- Rougel: 19.4209\n- Rougelsum: 19.4915\n- Gen Len: 41.3136\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2 | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:------:|:-------:|:---------:|:-------:|\n| 0.3435        | 0.03  | 500   | 0.2100          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.4492 |\n| 0.3245        | 0.07  | 1000  | 0.2102          | 19.5975 | 8.7571 | 19.7034 | 19.774    | 41.1949 |\n| 0.3777        | 0.1   | 1500  | 0.2010          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.0    |\n| 0.3643        | 0.14  | 2000  | 0.1980          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.0593 |\n| 0.3212        | 0.17  | 2500  | 0.1986          | 19.209  | 8.2062 | 19.2797 | 19.2797   | 41.1525 |\n| 0.4181        | 0.2   | 3000  | 0.1896          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 42.2373 |\n| 0.3175        | 0.24  | 3500  | 0.1879          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.4576 |\n| 0.3399        | 0.27  | 4000  | 0.1838          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.1102 |\n| 0.314         | 0.31  | 4500  | 0.1837          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.0339 |\n| 0.3063        | 0.34  | 5000  | 0.1796          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 40.9407 |\n| 0.3434        | 0.38  | 5500  | 0.1769          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 40.8814 |\n| 0.376         | 0.41  | 6000  | 0.1790          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.0593 |\n| 0.3355        | 0.44  | 6500  | 0.1735          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.4153 |\n| 0.3181        | 0.48  | 7000  | 0.1665          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.0508 |\n| 0.3017        | 0.51  | 7500  | 0.1701          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.2881 |\n| 0.2953        | 0.55  | 8000  | 0.1664          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.2458 |\n| 0.2711        | 0.58  | 8500  | 0.1664          | 19.5975 | 8.7571 | 19.7034 | 19.774    | 41.4068 |\n| 0.3661        | 0.61  | 9000  | 0.1626          | 19.5975 | 8.7571 | 19.7034 | 19.774    | 41.2797 |\n| 0.273         | 0.65  | 9500  | 0.1585          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.3051 |\n| 0.3346        | 0.68  | 10000 | 0.1627          | 19.5975 | 8.7571 | 19.7034 | 19.774    | 41.2797 |\n| 0.2529        | 0.72  | 10500 | 0.1590          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.2627 |\n| 0.2926        | 0.75  | 11000 | 0.1601          | 19.5975 | 8.7571 | 19.7034 | 19.774    | 41.2712 |\n| 0.2677        | 0.78  | 11500 | 0.1551          | 19.5975 | 8.7571 | 19.7034 | 19.774    | 41.2797 |\n| 0.2746        | 0.82  | 12000 | 0.1570          | 19.5975 | 8.7571 | 19.7034 | 19.774    | 41.1186 |\n| 0.2494        | 0.85  | 12500 | 0.1513          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.2373 |\n| 0.2834        | 0.89  | 13000 | 0.1506          | 19.5975 | 8.7571 | 19.7034 | 19.774    | 41.2458 |\n| 0.2646        | 0.92  | 13500 | 0.1512          | 19.5975 | 8.7571 | 19.7034 | 19.774    | 41.3729 |\n| 0.2782        | 0.95  | 14000 | 0.1528          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.3644 |\n| 0.2954        | 0.99  | 14500 | 0.1486          | 19.3503 | 8.3898 | 19.4209 | 19.4915   | 41.3136 |\n\n\n### Framework versions\n\n- Transformers 4.26.0\n- Pytorch 1.7.1+cu110\n- Datasets 2.9.0\n- Tokenizers 0.13.2\n", "size_bytes": "891704497", "downloads": 2}
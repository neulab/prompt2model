{"pretrained_model_name": "Helsinki-NLP/opus-mt-alv-en", "description": "---\nlanguage: \n- sn\n- rw\n- wo\n- ig\n- sg\n- ee\n- zu\n- lg\n- ts\n- ln\n- ny\n- yo\n- rn\n- xh\n- alv\n- en\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### alv-eng\n\n* source group: Atlantic-Congo languages \n* target group: English \n*  OPUS readme: [alv-eng](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/alv-eng/README.md)\n\n*  model: transformer\n* source language(s): ewe fuc fuv ibo kin lin lug nya run sag sna swh toi_Latn tso umb wol xho yor zul\n* target language(s): eng\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* download original weights: [opus2m-2020-07-31.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/alv-eng/opus2m-2020-07-31.zip)\n* test set translations: [opus2m-2020-07-31.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/alv-eng/opus2m-2020-07-31.test.txt)\n* test set scores: [opus2m-2020-07-31.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/alv-eng/opus2m-2020-07-31.eval.txt)\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| Tatoeba-test.ewe-eng.ewe.eng \t| 6.3 \t| 0.328 |\n| Tatoeba-test.ful-eng.ful.eng \t| 0.4 \t| 0.108 |\n| Tatoeba-test.ibo-eng.ibo.eng \t| 4.5 \t| 0.196 |\n| Tatoeba-test.kin-eng.kin.eng \t| 30.7 \t| 0.511 |\n| Tatoeba-test.lin-eng.lin.eng \t| 2.8 \t| 0.213 |\n| Tatoeba-test.lug-eng.lug.eng \t| 3.4 \t| 0.140 |\n| Tatoeba-test.multi.eng \t| 20.9 \t| 0.376 |\n| Tatoeba-test.nya-eng.nya.eng \t| 38.7 \t| 0.492 |\n| Tatoeba-test.run-eng.run.eng \t| 24.5 \t| 0.417 |\n| Tatoeba-test.sag-eng.sag.eng \t| 5.5 \t| 0.177 |\n| Tatoeba-test.sna-eng.sna.eng \t| 26.9 \t| 0.412 |\n| Tatoeba-test.swa-eng.swa.eng \t| 4.9 \t| 0.196 |\n| Tatoeba-test.toi-eng.toi.eng \t| 3.9 \t| 0.147 |\n| Tatoeba-test.tso-eng.tso.eng \t| 76.7 \t| 0.957 |\n| Tatoeba-test.umb-eng.umb.eng \t| 4.0 \t| 0.195 |\n| Tatoeba-test.wol-eng.wol.eng \t| 3.7 \t| 0.170 |\n| Tatoeba-test.xho-eng.xho.eng \t| 38.9 \t| 0.556 |\n| Tatoeba-test.yor-eng.yor.eng \t| 25.1 \t| 0.412 |\n| Tatoeba-test.zul-eng.zul.eng \t| 46.1 \t| 0.623 |\n\n\n### System Info: \n- hf_name: alv-eng\n\n- source_languages: alv\n\n- target_languages: eng\n\n- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/alv-eng/README.md\n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['sn', 'rw', 'wo', 'ig', 'sg', 'ee', 'zu', 'lg', 'ts', 'ln', 'ny', 'yo', 'rn', 'xh', 'alv', 'en']\n\n- src_constituents: {'sna', 'kin', 'wol', 'ibo', 'swh', 'sag', 'ewe', 'zul', 'fuc', 'lug', 'tso', 'lin', 'nya', 'yor', 'run', 'xho', 'fuv', 'toi_Latn', 'umb'}\n\n- tgt_constituents: {'eng'}\n\n- src_multilingual: True\n\n- tgt_multilingual: False\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/alv-eng/opus2m-2020-07-31.zip\n\n- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/alv-eng/opus2m-2020-07-31.test.txt\n\n- src_alpha3: alv\n\n- tgt_alpha3: eng\n\n- short_pair: alv-en\n\n- chrF2_score: 0.376\n\n- bleu: 20.9\n\n- brevity_penalty: 1.0\n\n- ref_len: 15208.0\n\n- src_name: Atlantic-Congo languages\n\n- tgt_name: English\n\n- train_date: 2020-07-31\n\n- src_alpha2: alv\n\n- tgt_alpha2: en\n\n- prefer_old: False\n\n- long_pair: alv-eng\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "size_bytes": "305003505", "downloads": 25}
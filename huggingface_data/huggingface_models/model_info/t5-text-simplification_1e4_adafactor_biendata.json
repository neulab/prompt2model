{"pretrained_model_name": "Smaraa/t5-text-simplification_1e4_adafactor_biendata", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: t5-text-simplification_1e4_adafactor_biendata\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-text-simplification_1e4_adafactor_biendata\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.7562\n- Rouge1: 10.4603\n- Rouge2: 2.642\n- Rougel: 9.6362\n- Rougelsum: 9.6589\n- Gen Len: 13.2838\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 20\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| No log        | 1.0   | 464  | 0.5489          | 29.7693 | 11.1997 | 25.6091 | 25.5979   | 14.7281 |\n| 0.9314        | 2.0   | 928  | 0.5392          | 29.9099 | 10.9645 | 25.334  | 25.3259   | 14.7188 |\n| 0.5594        | 3.0   | 1392 | 0.5342          | 30.3194 | 11.4204 | 25.8248 | 25.8255   | 14.7666 |\n| 0.5333        | 4.0   | 1856 | 0.5376          | 30.8368 | 11.6152 | 26.3172 | 26.3583   | 14.1578 |\n| 0.5192        | 5.0   | 2320 | 0.8890          | 7.5517  | 1.4313  | 7.0971  | 7.1064    | 9.9191  |\n| 0.8897        | 6.0   | 2784 | 0.8252          | 6.9283  | 1.3484  | 6.5916  | 6.5877    | 10.9894 |\n| 0.9385        | 7.0   | 3248 | 0.7971          | 8.2401  | 1.9957  | 7.7693  | 7.7675    | 10.7732 |\n| 0.9089        | 8.0   | 3712 | 0.7725          | 9.7559  | 2.2249  | 9.0272  | 9.0098    | 10.7175 |\n| 0.8824        | 9.0   | 4176 | 0.7552          | 12.006  | 2.8041  | 11.0115 | 10.992    | 10.7838 |\n| 0.8658        | 10.0  | 4640 | 0.7490          | 13.311  | 3.4159  | 12.1933 | 12.1551   | 10.6499 |\n| 0.864         | 11.0  | 5104 | 0.7448          | 13.9983 | 3.6176  | 12.7712 | 12.7347   | 10.752  |\n| 0.868         | 12.0  | 5568 | 0.7495          | 12.318  | 3.2975  | 11.3451 | 11.3218   | 12.0252 |\n| 0.8844        | 13.0  | 6032 | 0.7552          | 10.6154 | 2.7347  | 9.8228  | 9.8116    | 13.191  |\n| 0.8844        | 14.0  | 6496 | 0.7562          | 10.4603 | 2.642   | 9.6362  | 9.6589    | 13.2838 |\n| 0.8971        | 15.0  | 6960 | 0.7562          | 10.4603 | 2.642   | 9.6362  | 9.6589    | 13.2838 |\n| 0.8981        | 16.0  | 7424 | 0.7562          | 10.4603 | 2.642   | 9.6362  | 9.6589    | 13.2838 |\n| 0.8956        | 17.0  | 7888 | 0.7562          | 10.4603 | 2.642   | 9.6362  | 9.6589    | 13.2838 |\n| 0.8984        | 18.0  | 8352 | 0.7562          | 10.4603 | 2.642   | 9.6362  | 9.6589    | 13.2838 |\n| 0.8959        | 19.0  | 8816 | 0.7562          | 10.4603 | 2.642   | 9.6362  | 9.6589    | 13.2838 |\n| 0.8977        | 20.0  | 9280 | 0.7562          | 10.4603 | 2.642   | 9.6362  | 9.6589    | 13.2838 |\n\n\n### Framework versions\n\n- Transformers 4.30.2\n- Pytorch 2.0.1+cu118\n- Datasets 2.13.1\n- Tokenizers 0.13.3\n", "size_bytes": "891702929", "downloads": 3}
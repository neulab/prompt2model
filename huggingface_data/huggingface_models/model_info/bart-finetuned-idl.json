{"pretrained_model_name": "ehuang2/bart-finetuned-idl", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: bart-finetuned-idl\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bart-finetuned-idl\n\nThis model is a fine-tuned version of [facebook/bart-base](https://huggingface.co/facebook/bart-base) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0031\n- Bleu: 0.0\n- Gen Len: 4.9917\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 35\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss | Bleu | Gen Len |\n|:-------------:|:-----:|:------:|:---------------:|:----:|:-------:|\n| 0.2005        | 1.0   | 13874  | 0.1589          | 0.0  | 5.0002  |\n| 0.1182        | 2.0   | 27748  | 0.0949          | 0.0  | 4.9924  |\n| 0.0983        | 3.0   | 41622  | 0.0778          | 0.0  | 4.9924  |\n| 0.0724        | 4.0   | 55496  | 0.0724          | 0.0  | 4.9903  |\n| 0.0532        | 5.0   | 69370  | 0.0549          | 0.0  | 4.9928  |\n| 0.0458        | 6.0   | 83244  | 0.0463          | 0.0  | 4.9861  |\n| 0.0435        | 7.0   | 97118  | 0.0548          | 0.0  | 4.9923  |\n| 0.0464        | 8.0   | 110992 | 0.0847          | 0.0  | 4.9899  |\n| 0.0317        | 9.0   | 124866 | 0.0303          | 0.0  | 4.9922  |\n| 0.0302        | 10.0  | 138740 | 0.0284          | 0.0  | 4.9919  |\n| 0.0306        | 11.0  | 152614 | 0.0120          | 0.0  | 4.9919  |\n| 0.0224        | 12.0  | 166488 | 0.0462          | 0.0  | 4.9917  |\n| 0.0184        | 13.0  | 180362 | 0.0138          | 0.0  | 4.9924  |\n| 0.0208        | 14.0  | 194236 | 0.0730          | 0.0  | 4.9919  |\n| 0.0149        | 15.0  | 208110 | 0.0126          | 0.0  | 4.992   |\n| 0.0161        | 16.0  | 221984 | 0.0100          | 0.0  | 4.9915  |\n| 0.0178        | 17.0  | 235858 | 0.0106          | 0.0  | 4.992   |\n| 0.0116        | 18.0  | 249732 | 0.0149          | 0.0  | 4.9921  |\n| 0.0096        | 19.0  | 263606 | 0.0085          | 0.0  | 4.9918  |\n| 0.0094        | 20.0  | 277480 | 0.0101          | 0.0  | 4.9916  |\n| 0.0084        | 21.0  | 291354 | 0.0093          | 0.0  | 4.9918  |\n| 0.0077        | 22.0  | 305228 | 0.0138          | 0.0  | 4.992   |\n| 0.0094        | 23.0  | 319102 | 0.0084          | 0.0  | 4.9918  |\n| 0.0079        | 24.0  | 332976 | 0.0058          | 0.0  | 4.9917  |\n| 0.006         | 25.0  | 346850 | 0.0067          | 0.0  | 4.9918  |\n| 0.0046        | 26.0  | 360724 | 0.0041          | 0.0  | 4.9918  |\n| 0.0049        | 27.0  | 374598 | 0.0061          | 0.0  | 4.9919  |\n| 0.002         | 28.0  | 388472 | 0.0035          | 0.0  | 4.9918  |\n| 0.003         | 29.0  | 402346 | 0.0038          | 0.0  | 4.9917  |\n| 0.0027        | 30.0  | 416220 | 0.0050          | 0.0  | 4.9917  |\n| 0.001         | 31.0  | 430094 | 0.0063          | 0.0  | 4.9918  |\n| 0.0017        | 32.0  | 443968 | 0.0042          | 0.0  | 4.992   |\n| 0.0013        | 33.0  | 457842 | 0.0032          | 0.0  | 4.9917  |\n| 0.0005        | 34.0  | 471716 | 0.0031          | 0.0  | 4.9917  |\n| 0.0003        | 35.0  | 485590 | 0.0031          | 0.0  | 4.9917  |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "557969145", "downloads": 2}
{"pretrained_model_name": "din0s/t5-small-de-finetuned-en-to-it", "description": "---\ntags:\n- generated_from_trainer\ndatasets:\n- ccmatrix\nmetrics:\n- bleu\nmodel-index:\n- name: t5-small_de-finetuned-en-to-it\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: ccmatrix\n      type: ccmatrix\n      config: en-it\n      split: train[3000:12000]\n      args: en-it\n    metrics:\n    - name: Bleu\n      type: bleu\n      value: 6.7338\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small_de-finetuned-en-to-it\n\nThis model is a fine-tuned version of [din0s/t5-small-finetuned-en-to-de](https://huggingface.co/din0s/t5-small-finetuned-en-to-de) on the ccmatrix dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.3480\n- Bleu: 6.7338\n- Gen Len: 61.3273\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 96\n- eval_batch_size: 96\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 40\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Bleu   | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:-------:|\n| No log        | 1.0   | 94   | 3.1064          | 2.9057 | 47.5067 |\n| No log        | 2.0   | 188  | 2.9769          | 2.7484 | 76.9273 |\n| No log        | 3.0   | 282  | 2.9015          | 3.0624 | 79.8873 |\n| No log        | 4.0   | 376  | 2.8444          | 3.2959 | 78.276  |\n| No log        | 5.0   | 470  | 2.7989          | 3.6694 | 74.6013 |\n| 3.3505        | 6.0   | 564  | 2.7564          | 3.8098 | 74.3247 |\n| 3.3505        | 7.0   | 658  | 2.7212          | 3.9596 | 72.554  |\n| 3.3505        | 8.0   | 752  | 2.6886          | 4.2231 | 70.7673 |\n| 3.3505        | 9.0   | 846  | 2.6572          | 4.1466 | 72.0113 |\n| 3.3505        | 10.0  | 940  | 2.6294          | 4.2696 | 71.1647 |\n| 3.0254        | 11.0  | 1034 | 2.6064          | 4.6375 | 67.7707 |\n| 3.0254        | 12.0  | 1128 | 2.5838          | 4.7208 | 68.6707 |\n| 3.0254        | 13.0  | 1222 | 2.5614          | 4.9191 | 68.5767 |\n| 3.0254        | 14.0  | 1316 | 2.5427          | 4.9837 | 66.3867 |\n| 3.0254        | 15.0  | 1410 | 2.5241          | 5.1011 | 66.7667 |\n| 2.8789        | 16.0  | 1504 | 2.5093          | 5.283  | 64.944  |\n| 2.8789        | 17.0  | 1598 | 2.4919          | 5.3205 | 65.738  |\n| 2.8789        | 18.0  | 1692 | 2.4788          | 5.3046 | 65.3207 |\n| 2.8789        | 19.0  | 1786 | 2.4651          | 5.5282 | 64.9407 |\n| 2.8789        | 20.0  | 1880 | 2.4532          | 5.6745 | 63.0873 |\n| 2.8789        | 21.0  | 1974 | 2.4419          | 5.7073 | 63.4973 |\n| 2.7782        | 22.0  | 2068 | 2.4308          | 5.8513 | 62.8813 |\n| 2.7782        | 23.0  | 2162 | 2.4209          | 5.8267 | 64.1033 |\n| 2.7782        | 24.0  | 2256 | 2.4124          | 5.8534 | 64.2993 |\n| 2.7782        | 25.0  | 2350 | 2.4037          | 6.0406 | 63.8313 |\n| 2.7782        | 26.0  | 2444 | 2.3964          | 6.1517 | 63.4213 |\n| 2.7116        | 27.0  | 2538 | 2.3897          | 6.2175 | 63.0573 |\n| 2.7116        | 28.0  | 2632 | 2.3836          | 6.2551 | 62.876  |\n| 2.7116        | 29.0  | 2726 | 2.3777          | 6.4412 | 62.4167 |\n| 2.7116        | 30.0  | 2820 | 2.3717          | 6.4604 | 62.1087 |\n| 2.7116        | 31.0  | 2914 | 2.3673          | 6.5471 | 62.1373 |\n| 2.6662        | 32.0  | 3008 | 2.3634          | 6.5296 | 62.2533 |\n| 2.6662        | 33.0  | 3102 | 2.3596          | 6.6623 | 61.276  |\n| 2.6662        | 34.0  | 3196 | 2.3564          | 6.6591 | 61.392  |\n| 2.6662        | 35.0  | 3290 | 2.3539          | 6.7201 | 61.0827 |\n| 2.6662        | 36.0  | 3384 | 2.3516          | 6.675  | 61.3173 |\n| 2.6662        | 37.0  | 3478 | 2.3500          | 6.6894 | 61.3507 |\n| 2.6411        | 38.0  | 3572 | 2.3488          | 6.6539 | 61.5253 |\n| 2.6411        | 39.0  | 3666 | 2.3482          | 6.7135 | 61.3733 |\n| 2.6411        | 40.0  | 3760 | 2.3480          | 6.7338 | 61.3273 |\n\n\n### Framework versions\n\n- Transformers 4.22.1\n- Pytorch 1.12.1\n- Datasets 2.5.1\n- Tokenizers 0.11.0\n", "size_bytes": "242070267", "downloads": 2}
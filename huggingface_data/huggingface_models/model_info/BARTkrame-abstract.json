{"pretrained_model_name": "krm/BARTkrame-abstract", "description": "---\nlicense: apache-2.0\ntags:\n- summarization\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: BARTkrame-abstract\n  results: []\nwidget:\n- text: \"Nous voulons que tous les \u00eatres humains, ensemble ou pris isol\u00e9ment, jeunes ou vieux, riches ou pauvres, nobles ou roturiers, hommes ou femmes, puissent pleinement s'instruire et devenir des \u00eatres achev\u00e9s. Nous voulons qu'ils soient instruits parfaitement et form\u00e9s non seulement sur tel ou tel point, mais \u00e9galement sur tout ce qui permet \u00e0 l'homme de r\u00e9aliser int\u00e9gralement son essence\"\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# BARTkrame-abstract\n\nThis model is a fine-tuned version of [krm/BARTkrame-abstract](https://huggingface.co/krm/BARTkrame-abstract) on the [krm/for-ULPGL-Dissertation](https://huggingface.co/datasets/krm/for-ULPGL-Dissertation) dataset.\nIt achieves (15/10/2022) the following results on the evaluation set:\n- Loss: 2.4196\n- Rouge1: 0.2703\n- Rouge2: 0.1334\n- Rougel: 0.2392\n- Rougelsum: 0.2419\n\n## Model description\n\nThis model is primarly a finetuned version of [moussaKam/mbarthez](https://huggingface.co/moussaKam/mbarthez).\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\nWe have used the [krm/for-ULPGL-Dissertation](https://huggingface.co/datasets/krm/for-ULPGL-Dissertation) dataset reduced to :\n\n> **Training data :** **5000** samples taken at random with *seed=42*.\n\n> **Validation data :** **100** samples taken at random with *seed=42*.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9, 0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 12\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1 | Rouge2 | Rougel | Rougelsum |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|:---------:|\n| 0.1316        | 9.0   | 1250 | 2.3251          | 0.2505 | 0.1158 | 0.2150 | 0.2184    |\n| 0.0894        | 10.0   | 2500 | 2.3467          | 0.2526 | 0.1073 | 0.2067 | 0.2124    |\n| 0.045         | 11.0   | 3750 | 2.3742          | 0.2593 | 0.1211 | 0.2281 | 0.2308    |\n| 0.0242        | 12.0   | 5000 | 2.4196          | 0.2703 | 0.1334 | 0.2392 | 0.2419    |\n\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.12.1+cu113\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n", "size_bytes": "1834069241", "downloads": 4}
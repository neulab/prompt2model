{"pretrained_model_name": "noahkim/KoBigBird-KoBart-News-Summarization", "description": "---\nlanguage: ko\ntags:\n- summarization\n- news\ninference: false\nmodel-index:\n- name: KoBigBird-KoBart-News-Summarization\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# KoBigBird-KoBart-News-Summarization\n\nThis model is a fine-tuned version of [noahkim/KoBigBird-KoBart-News-Summarization](https://huggingface.co/noahkim/KoBigBird-KoBart-News-Summarization) on the [daekeun-ml/naver-news-summarization-ko](https://huggingface.co/datasets/daekeun-ml/naver-news-summarization-ko)\n\n\n## Model description\n\n<<20221110  Commit>>\n\n<<KoBigBird-KoBart-News-Summarization \ubaa8\ub378 \uc124\uba85>>\n\n\ub2e4\uc911\ubb38\uc11c\uc694\uc57d(Multi-Document-Summarization) Task\ub97c \uc704\ud574\uc11c KoBigBird \ubaa8\ub378\uc744 Encoder-Decoder\ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\uc11c \ud559\uc2b5\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4. KoBigBird\ub97c Decoder\ub85c \uc4f0\ub824\uace0 \ud588\uc73c\ub098 \uc624\ub958\uac00 \uc0dd\uaca8\uc11c \uc694\uc57d\uc5d0 \ud2b9\ud654\ub41c KoBART\uc758 Decoder\ub97c \ud65c\uc6a9\ud574\uc11c \ubaa8\ub378\uc744 \uc0dd\uc131\ud588\uc2b5\ub2c8\ub2e4.\n\n\ud504\ub85c\uc81d\ud2b8\uc6a9\uc73c\ub85c \ub274\uc2a4 \uc694\uc57d \ubaa8\ub378 \ud2b9\ud654\ub41c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574 \uae30\uc874\uc5d0 \ub9cc\ub4e4\uc5c8\ub358 KoBigBird-KoBart-News-Summarization \ubaa8\ub378\uc5d0 \ucd94\uac00\uc801\uc73c\ub85c daekeun-ml\ub2d8\uc774 \uc81c\uacf5\ud574\uc8fc\uc2e0 naver-news-summarization-ko \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud30c\uc778\ud29c\ub2dd \ud588\uc2b5\ub2c8\ub2e4.\n\n\ud604\uc7ac AI-HUB\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \uc694\uc57d \ub370\uc774\ud130\ub97c \ucd94\uac00 \ud559\uc2b5 \uc9c4\ud589 \uc608\uc815\uc785\ub2c8\ub2e4.\n\uc9c0\uc18d\uc801\uc73c\ub85c \ubc1c\uc804\uc2dc\ucf1c \uc88b\uc740 \uc131\ub2a5\uc758 \ubaa8\ub378\uc744 \uad6c\ud604\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\uac10\uc0ac\ud569\ub2c8\ub2e4.\n\n\uc2e4\ud589\ud658\uacbd\n- Google Colab Pro\n- CPU : Intel(R) Xeon(R) CPU @ 2.20GHz\n- GPU : A100-SXM4-40GB\n\n<pre><code>\n# Python Code\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"noahkim/KoT5_news_summarization\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"noahkim/KoT5_news_summarization\")\n</pre></code> \n\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 4.0748        | 1.0   | 1388 | 4.3067          |\n| 3.8457        | 2.0   | 2776 | 4.2039          |\n| 3.7459        | 3.0   | 4164 | 4.1433          |\n| 3.6773        | 4.0   | 5552 | 4.1236          |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.12.1+cu113\n- Datasets 2.6.1\n- Tokenizers 0.13.2\n", "size_bytes": "599515441", "downloads": 18}
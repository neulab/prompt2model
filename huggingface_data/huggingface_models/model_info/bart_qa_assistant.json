{"pretrained_model_name": "MrVicente/bart_qa_assistant", "description": "---\nlanguage: en\ntags:\n- generative qa\ndatasets:\n- eli5\n- stackexchange(pets, cooking, gardening, diy, crafts)\n---\n\nWork by [Frederico Vicente](https://huggingface.co/mrvicente) & [Diogo Tavares](https://huggingface.co/d-c-t). We finetuned BART Large for the task of generative question answering. It was trained on eli5, askScience and stackexchange using the following forums: pets, cooking, gardening, diy, crafts. \n\nCheck demo: https://huggingface.co/spaces/unlisboa/bart_qa_assistant\n\n### Usage\n\n```python\n\nfrom transformers import (\n      BartForConditionalGeneration,\n      BartTokenizer\n)\nimport torch\nimport json\n\ndef read_json_file_2_dict(filename, store_dir='.'):\n    with open(f'{store_dir}/{filename}', 'r', encoding='utf-8') as file:\n        return json.load(file)\n\ndef get_device():\n    # If there's a GPU available...\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        n_gpus = torch.cuda.device_count()\n        first_gpu = torch.cuda.get_device_name(0)\n\n        print(f'There are {n_gpus} GPU(s) available.')\n        print(f'GPU gonna be used: {first_gpu}')\n    else:\n        print('No GPU available, using the CPU instead.')\n        device = torch.device(\"cpu\")\n    return device\n\nmodel_name = 'unlisboa/bart_qa_assistant'\ntokenizer = BartTokenizer.from_pretrained(model_name)\ndevice = get_device()\nmodel = BartForConditionalGeneration.from_pretrained(model_name).to(device)\nmodel.eval()\n                                                                                                                                                          \nmodel_input = tokenizer(question, truncation=True, padding=True, return_tensors=\"pt\")\ngenerated_answers_encoded = model.generate(input_ids=model_input[\"input_ids\"].to(device),attention_mask=model_input[\"attention_mask\"].to(device),\n                                                                                      force_words_ids=None,\n                                                                                      min_length=1,\n                                                                                      max_length=100,\n                                                                                      do_sample=True,\n                                                                                      early_stopping=True,\n                                                                                      num_beams=4,\n                                                                                      temperature=1.0,\n                                                                                      top_k=None,\n                                                                                      top_p=None,\n                                                                                      # eos_token_id=tokenizer.eos_token_id,\n                                                                                      no_repeat_ngram_size=2,\n                                                                                      num_return_sequences=1,\n                                                                                      return_dict_in_generate=True,\n                                                                                      output_scores=True)\nresponse = tokenizer.batch_decode(generated_answers_encoded['sequences'], skip_special_tokens=True,clean_up_tokenization_spaces=True)    \nprint(response)\n\n```\n\nHave fun!", "size_bytes": "1625530625", "downloads": 28}
{"pretrained_model_name": "ybagoury/flan-t5-base-tldr_news", "description": "---\ndatasets:\n- JulesBelveze/tldr_news\nmetrics:\n- rouge\npipeline_tag: summarization\nlanguage:\n- en\ntags:\n- tldr\n---\n\n# flan-t5-base-tldr_news\n\nA fine-tuned T5 model for text summarization and title generation on TLDR (Too Long; Didn't Read) news articles.\n\n## Introduction\nflan-t5-base-tldr_news is a deep learning model that has been fine-tuned on a dataset of TLDR news articles. The model is specifically designed to perform the tasks of text summarization and title generation. \n\nThe T5 architecture is a transformer-based neural network architecture that has been used to achieve state-of-the-art results on a variety of NLP tasks. By fine-tuning the T5 architecture on a dataset of TLDR news articles, we aim to create a model that is capable of generating concise and informative summaries and titles for news articles.\n\n## Task\nThe main goal of this model is to perform two NLP tasks: text summarization and title generation. Text summarization involves generating a shortened version of a longer text that retains the most important information and ideas. Title generation, on the other hand, involves generating a headline or title for a given text that accurately and concisely captures the main theme or idea of the text. \n\n## Architecture\nflan-t5-base-tldr_news uses the T5 architecture, which has been shown to be effective for a variety of NLP tasks. The T5 architecture consists of an encoder and a decoder, which are trained to generate a summary or title given an input text.\n\n## Model Size\nThe model has 247,577,856 parameters, which represents the number of tunable weights in the model. The size of the model can impact the speed and memory requirements during training and inference, as well as the performance of the model on specific tasks.\n\n## Training Data\nThe model was fine-tuned on a dataset of TLDR news articles. This dataset was selected because it contains a large number of news articles that have been condensed into short summaries, making it a good choice for training a model for text summarization. The training data was preprocessed to perform all types of standard preprocessing steps, including tokenization, to prepare the data for input into the model.\n\n## Evaluation Metrics\nTo evaluate the performance of the model on the tasks of text summarization and title generation, we used the ROUGE metric. ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, measures the overlap between the generated text and the reference text, which in this case is the original news article or its summary. The ROUGE metric is commonly used in NLP evaluations and provides a good way to measure the quality of the generated summaries and titles. \n\nThe following table shows the ROUGE scores for the model on the test set, which provides a good indication of its overall performance on the text summarization and title generation tasks:\n\n| Metric | Score |\n| ------ | ------|\n| Rouge1 | 45.04 |\n| Rouge2 | 25.24 |\n| RougeL | 41.89 |\n| RougeIsum | 41.84 |\n\nIt's important to note that these scores are just a snapshot of the model's performance on a specific test set, and the performance of the model may vary depending on the input text, the quality of the training data, and the specific application for which the model is being used.\n\n## How to use via API\n\n```python\nfrom transformers import pipeline\n\nsummarizer = pipeline(\n              'summarization',\n              'ybagoury/flan-t5-base-tldr_news',\n              )\nraw_text = \"\"\" your text here... \"\"\"\nresults = summarizer(raw_text)\nprint(results)\n```", "size_bytes": "990406605", "downloads": 102}
{"pretrained_model_name": "nijatzeynalov/mT5-based-azerbaijani-summarize", "description": "---\nlicense: creativeml-openrail-m\nwidget:\n- text: >-\n    \u00d6t\u0259n il Az\u0259rbaycana 74 577 avtomobil idxal edilib. Bu da 2021-ci ill\u0259\n    m\u00fcqayis\u0259d\u0259 16 617 \u0259d\u0259d v\u0259 ya 18,2% azd\u0131r.\n\n\n    Xezerxeber.az-\u0131n m\u0259lumat\u0131na g\u00f6r\u0259, avtomobil bazar\u0131 \u00fczr\u0259 qiym\u0259tl\u0259ndirici\n    S\u0259rxan Q\u0259dirov deyib ki, \u0259vv\u0259l ay \u0259rzind\u0259 5-10 avtomobil g\u0259tir\u0259n \u015f\u0259xsl\u0259r\n    haz\u0131rda bu say\u0131 2-3 \u0259d\u0259d\u0259 endiribl\u0259r. H\u0259tta \u00f6lk\u0259y\u0259 n\u0259qliyyat vasit\u0259l\u0259rinin\n    g\u0259tirilm\u0259si i\u015fini dayand\u0131ranlar da var.\n\n\n    N\u0259qliyyat m\u0259s\u0259l\u0259l\u0259ri \u00fczr\u0259 ekspert Eld\u0259niz C\u0259f\u0259rov is\u0259 bildirib ki,\n    g\u00f6zl\u0259nil\u0259nd\u0259n f\u0259rqli olaraq, \u00f6lk\u0259y\u0259 idxal olunan ki\u00e7ik m\u00fch\u0259rrikli\n    avtomobill\u0259rin say\u0131nda da azalma var. Bunun ba\u015fl\u0131ca s\u0259b\u0259bi Rusiyada\n    istehsal\u0131n dayand\u0131r\u0131lmas\u0131d\u0131r.\n\n\n    Ekspertin s\u00f6zl\u0259rin\u0259 g\u00f6r\u0259, \u0259vv\u0259ll\u0259r Az\u0259rbaycan bazar\u0131nda Rusiya istehsal\u0131\n    olan n\u0259qliyyat vasit\u0259l\u0259ri geni\u015f yer tuturdu. Haz\u0131rda is\u0259 h\u0259min \u00f6lk\u0259d\u0259n idxal\n    tam dayan\u0131b.\ndatasets:\n- nijatzeynalov/azerbaijani-multi-news\nlanguage:\n- az\nmetrics:\n- rouge\npipeline_tag: summarization\n---\n\n# mT5-small based Azerbaijani Summarization\n\nIn this model, [Google's Multilingual T5-small](https://github.com/google-research/multilingual-t5) is fine-tuned on [Azerbaijani News Summary Dataset](https://huggingface.co/datasets/nijatzeynalov/azerbaijani-multi-news) for **Summarization** downstream task. The model is trained with 3 epochs, 64 batch size and 10e-4 learning rate. It took almost 12 hours on GPU instance with Ubuntu Server 20.04 LTS image in Microsoft Azure. The max news length is kept as 2048 and max summary length is determined as 128.\n\n\nmT5 is a multilingual variant of __T5__ and only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual)\nexcluding any supervised training. Therefore, the mT5 model has to be fine-tuned before it is useable on a downstream task.\n\n### Text-to-Text Transfer Transformer (T5)\n\nThe paper [\u201cExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\u201d](https://arxiv.org/pdf/1910.10683.pdf) presents a large-scale empirical survey to determine which transfer learning techniques work best and apply these insights at scale to create a new model called the Text-To-Text Transfer Transformer.\n\n![Alt Text](https://miro.medium.com/max/1280/0*xfXDPjASztwmJlOa.gif)\n\n\n\nT5, or Text-to-Text Transfer Transformer, is a Transformer based architecture that uses a text-to-text approach. Every task \u2013 including translation, question answering, and classification \u2013 is cast as feeding the model text as input and training it to generate some target text. This allows for the use of the same model, loss function, hyperparameters, etc. across our diverse set of tasks. \n\nThe changes compared to BERT include:\n\n- adding a causal decoder to the bidirectional architecture.\n- replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.\n\nThe model was trained on a cleaned version of Common Crawl that is two orders of magnitude larger than Wikipedia. \n\nThe T5 model, pre-trained on C4, achieves state-of-the-art results on many NLP benchmarks while being flexible enough to be fine-tuned to several downstream tasks. The pre-trained T5 in Hugging Face is also trained on the mixture of unsupervised training (which is trained by reconstructing the masked sentence) and task-specific training.\n\n### Multilingual t5\n\n[\"mt5\"](https://arxiv.org/pdf/2010.11934v3.pdf) is a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering \n101 languages. \n\nmT5 is pre-trained only by unsupervised manner with multiple languages, and it\u2019s not trained for specific downstream tasks. To dare say, this pre-trained model has ability to build correct text in Azerbaijani, but it doesn\u2019t have any ability for specific tasks, such as, summarization, correction, machine translation, etc.\n\nIn HuggingFace, several sizes of mT5 models are available, and here I used small one (google/mt5-small). Therefore I trained (fine-tune) this model for summarization in Azerbaijani using [Azerbaijani News Summary Dataset](https://huggingface.co/datasets/nijatzeynalov/azerbaijani-multi-news).\n\n\n## Training hyperparameters\n\n__mT5-based-azerbaijani-summarize__ model training took almost 12 hours on GPU instance with Ubuntu Server 20.04 LTS image in Microsoft Azure. The following hyperparameters were used during training:\n\n- learning_rate: 0.0005\n- train_batch_size: 2\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 16\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 90\n- num_epochs: 10\n\n## Dataset\n\nModel was trained on [__az-news-summary__ dataset](https://huggingface.co/datasets/nijatzeynalov/azerbaijani-multi-news), a comprehensive and diverse dataset comprising 143k (143,448) Azerbaijani news articles extracted using a set of carefully designed heuristics. \n \nThe dataset covers common topics for news reports include war, government, politics, education, health, the environment, economy, business, fashion, entertainment, and sport, as well as quirky or unusual events.\n\nThis dataset has 3 splits: _train_, _validation_, and _test_. \\\nToken counts are white space based.\n\n| Dataset Split | Number of Instances |     Size (MB)         |\n| ------------- | --------------------|:----------------------|\n| Train         | 100,413             |      150              |\n| Validation    | 14,344              |      21.3             |\n| Test          | 28,691              |      42.8             |\n\n\n## Training results with comparison\n\n__mT5-based-azerbaijani-summarize__ model rouge scores on the test set:\n\n- Rouge1: 39.4222\n- Rouge2: 24.8624\n- Rougel: 32.2487\n\nFor __Azerbaijani text summarization downstream task__, mT5-multilingual-XLSum has also been developed on the 45 languages of [XL-Sum](https://huggingface.co/datasets/csebuetnlp/xlsum) dataset. For finetuning details and scripts,\nsee the [paper](https://aclanthology.org/2021.findings-acl.413/) and the [official repository](https://github.com/csebuetnlp/xl-sum). .\n\n__mT5_multilingual_XLSum__ modelrouge scores on the XL-Sum test set (only for Azerbaijani):\n\n- Rouge1: 21.4227\n- Rouge2: 9.5214\n- Rougel: 19.3331\n\nAs seen from the numbers, our model __mT5-based-azerbaijani-summarize__  achieves dramatically better performance than __mT5_multilingual_XLSum__.\n\n## Using this model in transformers\n\n```python\n!pip install sentencepiece\n!pip install transformers\n```\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\narticle_text = \"\"\"\u00d6t\u0259n il Az\u0259rbaycana 74 577 avtomobil idxal edilib. Bu da 2021-ci ill\u0259 m\u00fcqayis\u0259d\u0259 16 617 \u0259d\u0259d v\u0259 ya 18,2% azd\u0131r.\nXezerxeber.az-\u0131n m\u0259lumat\u0131na g\u00f6r\u0259, avtomobil bazar\u0131 \u00fczr\u0259 qiym\u0259tl\u0259ndirici S\u0259rxan Q\u0259dirov deyib ki, \u0259vv\u0259l ay \u0259rzind\u0259 5-10 avtomobil g\u0259tir\u0259n \u015f\u0259xsl\u0259r haz\u0131rda bu say\u0131 2-3 \u0259d\u0259d\u0259 endiribl\u0259r. H\u0259tta \u00f6lk\u0259y\u0259 n\u0259qliyyat vasit\u0259l\u0259rinin g\u0259tirilm\u0259si i\u015fini dayand\u0131ranlar da var.\nN\u0259qliyyat m\u0259s\u0259l\u0259l\u0259ri \u00fczr\u0259 ekspert Eld\u0259niz C\u0259f\u0259rov is\u0259 bildirib ki, g\u00f6zl\u0259nil\u0259nd\u0259n f\u0259rqli olaraq, \u00f6lk\u0259y\u0259 idxal olunan ki\u00e7ik m\u00fch\u0259rrikli avtomobill\u0259rin say\u0131nda da azalma var. Bunun ba\u015fl\u0131ca s\u0259b\u0259bi Rusiyada istehsal\u0131n dayand\u0131r\u0131lmas\u0131d\u0131r.\nEkspertin s\u00f6zl\u0259rin\u0259 g\u00f6r\u0259, \u0259vv\u0259ll\u0259r Az\u0259rbaycan bazar\u0131nda Rusiya istehsal\u0131 olan n\u0259qliyyat vasit\u0259l\u0259ri geni\u015f yer tuturdu. Haz\u0131rda is\u0259 h\u0259min \u00f6lk\u0259d\u0259n idxal tam dayan\u0131b.\"\"\"\n\nmodel_name = \"nijatzeynalov/mT5-based-azerbaijani-summarize\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n```\n\n```python\ninput_ids = tokenizer(\n    article_text,\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    truncation=True,\n    max_length=2048\n)[\"input_ids\"]\n\noutput_ids = model.generate(\n    input_ids=input_ids,\n    max_length=128,\n    no_repeat_ngram_size=2,\n    num_beams=4\n)[0]\n\nsummary = tokenizer.decode(\n    output_ids,\n    skip_special_tokens=True,\n    clean_up_tokenization_spaces=False\n)\n\nprint(summary)\n```\n\nResult:\n\n```python\nAz\u0259rbaycana idxal olunan avtomobill\u0259rin say\u0131 a\u00e7\u0131qlan\u0131b\n```\n\n## Citation\n\nIf you use this model, please cite:\n\n```\n@misc {nijatzeynalov_2023,\n\tauthor       = { {NijatZeynalov} },\n\ttitle        = { mT5-based-azerbaijani-summarize (Revision 19930ab) },\n\tyear         = 2023,\n\turl          = { https://huggingface.co/nijatzeynalov/mT5-based-azerbaijani-summarize },\n\tdoi          = { 10.57967/hf/0316 },\n\tpublisher    = { Hugging Face }\n}\n```", "size_bytes": "1200772485", "downloads": 9}
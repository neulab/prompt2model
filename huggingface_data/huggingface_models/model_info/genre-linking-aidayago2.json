{"pretrained_model_name": "facebook/genre-linking-aidayago2", "description": "---\n\nlanguage:\n- en\n\ntags:\n- retrieval\n- entity-retrieval\n- named-entity-disambiguation\n- entity-disambiguation\n- named-entity-linking\n- entity-linking\n- text2text-generation\n---\n\n\n# GENRE\n\n\nThe GENRE (Generative ENtity REtrieval) system as presented in [Autoregressive Entity Retrieval](https://arxiv.org/abs/2010.00904) implemented in pytorch.\n\nIn a nutshell, GENRE uses a sequence-to-sequence approach to entity retrieval (e.g., linking), based on fine-tuned [BART](https://arxiv.org/abs/1910.13461) architecture. GENRE performs retrieval generating the unique entity name conditioned on the input text using constrained beam search to only generate valid identifiers. The model was first released in the [facebookresearch/GENRE](https://github.com/facebookresearch/GENRE) repository using `fairseq` (the `transformers` models are obtained with a conversion script similar to [this](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py).\n\nThis model was trained on the full training set of [BLINK](https://arxiv.org/abs/1911.03814) (i.e., 9M datapoints for entity-disambiguation grounded on Wikipedia) and then fine-tuned on [AIDA-YAGO2](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads).\n\n## BibTeX entry and citation info\n\n**Please consider citing our works if you use code from this repository.**\n\n```bibtex\n@inproceedings{decao2020autoregressive,\n  title={Autoregressive Entity Retrieval},\n  author={Nicola {De Cao} and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\n  booktitle={International Conference on Learning Representations},\n  url={https://openreview.net/forum?id=5k8F6UU39V},\n  year={2021}\n}\n```\n\n## Usage\n\nHere is an example of generation for Wikipedia page disambiguation:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# OPTIONAL: load the prefix tree (trie), you need to additionally download\n# https://huggingface.co/facebook/genre-linking-aidayago2/blob/main/trie.py and \n# https://huggingface.co/facebook/genre-linking-aidayago2/blob/main/kilt_titles_trie_dict.pkl\n# import pickle\n# from trie import Trie\n# with open(\"kilt_titles_trie_dict.pkl\", \"rb\") as f:\n#     trie = Trie.load_from_dict(pickle.load(f))\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-linking-aidayago2\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-linking-aidayago2\").eval()\n\nsentences = [\"Einstein was a [START_ENT] German [END_ENT] physicist.\"]\n\noutputs = model.generate(\n    **tokenizer(sentences, return_tensors=\"pt\"),\n    num_beams=5,\n    num_return_sequences=5,\n    # OPTIONAL: use constrained beam search\n    # prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n)\n\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n```\nwhich outputs the following top-5 predictions (using constrained beam search)\n```\n['Germany',\n 'German Empire',\n 'Nazi Germany',\n 'German language',\n 'France']\n```\n", "size_bytes": "1625526529", "downloads": 9}
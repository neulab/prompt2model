{"pretrained_model_name": "ihgn/similar-questions", "description": "---\ndatasets:\n- tapaco\nmetrics:\n- bleu\n- rouge\n- ter\npipeline_tag: text2text-generation\n---\nfrom transformers import pipeline\n\n# Load the model from the Hugging Face Model Hub\n    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n    tokenizer = AutoTokenizer.from_pretrained(\"ihgn/similar-questions\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(\"ihgn/similar-questions\")\n    model = pipeline(\"text2text-generation\", model=model_name)\n\n# Configure the generation parameters\n    generation_config = {\n        \"max_length\": 512,\n        \"num_beams\": 1,\n        \"top_k\": 50,\n        \"top_p\": 0.92,\n        \"do_sample\": True,\n        \"num_return_sequences\": 1\n    }\n\n# Generate text using the configured parameters\n    input_text= \"Your input text goes here.\"\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n    generated_ids = model(input_ids, **generation_config)\n    generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n# Print the generated text\n    print(generated_text)", "size_bytes": "242071641", "downloads": 17}
{"pretrained_model_name": "facebook/mgenre-wiki", "description": "---\n\nlanguage:\n- multilingual\n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bm\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- ff\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gn\n- gu\n- ha\n- he\n- hi\n- hr\n- ht\n- hu\n- hy\n- id\n- ig\n- is\n- it\n- ja\n- jv\n- ka\n- kg\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lg\n- ln\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- no\n- om\n- or\n- pa\n- pl\n- ps\n- pt\n- qu\n- ro\n- ru\n- sa\n- sd\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- ss\n- su\n- sv\n- sw\n- ta\n- te\n- th\n- ti\n- tl\n- tn\n- tr\n- uk\n- ur\n- uz\n- vi\n- wo\n- xh\n- yo\n- zh\n\n\ntags:\n- retrieval\n- entity-retrieval\n- named-entity-disambiguation\n- entity-disambiguation\n- named-entity-linking\n- entity-linking\n- text2text-generation\n---\n\n\n# mGENRE \n\n\nThe mGENRE (multilingual Generative ENtity REtrieval) system as presented in [Multilingual Autoregressive Entity Linking](https://arxiv.org/abs/2103.12528) implemented in pytorch.\n\nIn a nutshell, mGENRE uses a sequence-to-sequence approach to entity retrieval (e.g., linking), based on fine-tuned [mBART](https://arxiv.org/abs/2001.08210) architecture. GENRE performs retrieval generating the unique entity name conditioned on the input text using constrained beam search to only generate valid identifiers. The model was first released in the [facebookresearch/GENRE](https://github.com/facebookresearch/GENRE) repository using `fairseq` (the `transformers` models are obtained with a conversion script similar to [this](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py).\n\nThis model was trained on 105 languages from Wikipedia.\n\n## BibTeX entry and citation info\n\n**Please consider citing our works if you use code from this repository.**\n\n```bibtex\n@article{decao2020multilingual,\n    author = {De Cao, Nicola and Wu, Ledell and Popat, Kashyap and Artetxe, Mikel \n    and Goyal, Naman and Plekhanov, Mikhail and Zettlemoyer, Luke \n    and Cancedda, Nicola and Riedel, Sebastian and Petroni, Fabio},\n    title = \"{Multilingual Autoregressive Entity Linking}\",\n    journal = {Transactions of the Association for Computational Linguistics},\n    volume = {10},\n    pages = {274-290},\n    year = {2022},\n    month = {03},\n    issn = {2307-387X},\n    doi = {10.1162/tacl_a_00460},\n    url = {https://doi.org/10.1162/tacl\\_a\\_00460},\n    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00460/2004070/tacl\\_a\\_00460.pdf},\n}\n```\n\n## Usage\n\nHere is an example of generation for Wikipedia page disambiguation:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# OPTIONAL: load the prefix tree (trie), you need to additionally download\n# https://huggingface.co/facebook/mgenre-wiki/blob/main/trie.py and \n# https://huggingface.co/facebook/mgenre-wiki/blob/main/titles_lang_all105_trie_with_redirect.pkl\n# that is fast but memory inefficient prefix tree (trie) -- it is implemented with nested python `dict`\n# NOTE: loading this map may take up to 10 minutes and occupy a lot of RAM!\n# import pickle\n# from trie import Trie\n# with open(\"titles_lang_all105_marisa_trie_with_redirect.pkl\", \"rb\") as f:\n#     trie = Trie.load_from_dict(pickle.load(f))\n\n# or a memory efficient but a bit slower prefix tree (trie) -- it is implemented with `marisa_trie` from\n# https://huggingface.co/facebook/mgenre-wiki/blob/main/titles_lang_all105_marisa_trie_with_redirect.pkl\n# from genre.trie import MarisaTrie\n# with open(\"titles_lang_all105_marisa_trie_with_redirect.pkl\", \"rb\") as f:\n#     trie = pickle.load(f)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/mgenre-wiki\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mgenre-wiki\").eval()\n\nsentences = [\"[START] Einstein [END] era un fisico tedesco.\"]\n# Italian for \"[START] Einstein [END] was a German physicist.\"\n\noutputs = model.generate(\n    **tokenizer(sentences, return_tensors=\"pt\"),\n    num_beams=5,\n    num_return_sequences=5,\n    # OPTIONAL: use constrained beam search\n    # prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n)\n\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n```\nwhich outputs the following top-5 predictions (using constrained beam search)\n```\n['Albert Einstein >> it',\n 'Albert Einstein (disambiguation) >> en',\n 'Alfred Einstein >> it',\n 'Alberto Einstein >> it',\n 'Einstein >> it']\n```", "size_bytes": "2469065849", "downloads": 639}
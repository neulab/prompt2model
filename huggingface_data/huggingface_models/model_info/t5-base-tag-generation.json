{"pretrained_model_name": "fabiochiu/t5-base-tag-generation", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: t5-base-tag-generation\n  results: []\nwidget:\n- text: \"Python is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.\"\n  example_title: \"Programming\"\n---\n\n# Model description\n\nThis model is [t5-base](https://huggingface.co/t5-base) fine-tuned on the [190k Medium Articles](https://www.kaggle.com/datasets/fabiochiusano/medium-articles) dataset for predicting article tags using the article textual content as input. While usually formulated as a multi-label classification problem, this model deals with _tag generation_ as a text2text generation task (inspiration from [text2tags](https://huggingface.co/efederici/text2tags)).\n# How to use the model\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport nltk\nnltk.download('punkt')\n\ntokenizer = AutoTokenizer.from_pretrained(\"fabiochiu/t5-base-tag-generation\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"fabiochiu/t5-base-tag-generation\")\n\ntext = \"\"\"\nPython is a high-level, interpreted, general-purpose programming language. Its\ndesign philosophy emphasizes code readability with the use of significant\nindentation. Python is dynamically-typed and garbage-collected.\n\"\"\"\n\ninputs = tokenizer([text], max_length=512, truncation=True, return_tensors=\"pt\")\noutput = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10,\n                        max_length=64)\ndecoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\ntags = list(set(decoded_output.strip().split(\", \")))\n\nprint(tags)\n# ['Programming', 'Code', 'Software Development', 'Programming Languages',\n#  'Software', 'Developer', 'Python', 'Software Engineering', 'Science',\n#  'Engineering', 'Technology', 'Computer Science', 'Coding', 'Digital', 'Tech',\n#  'Python Programming']\n```\n\n## Data cleaning\n\nThe dataset is composed of Medium articles and their tags. However, each Medium article can have at most five tags, therefore the author needs to choose what he/she believes are the best tags (mainly for SEO-related purposes). This means that an article with the \"Python\" tag may have not the \"Programming Languages\" tag, even though the first implies the latter.\n\nTo clean the dataset accounting for this problem, a hand-made taxonomy of about 1000 tags was built. Using the taxonomy, the tags of each articles have been augmented (e.g. an article with the \"Python\" tag will have the \"Programming Languages\" tag as well, as the taxonomy says that \"Python\" is part of \"Programming Languages\"). The taxonomy is not public, if you are interested in it please send an email at chiusanofabio94@gmail.com.\n\n## Training and evaluation data\n\nThe model has been trained on a single epoch spanning about 50000 articles, evaluating on 1000 random articles not used during training.\n\n## Evaluation results\n\n- eval_loss: 0.8474\n- eval_rouge1: 38.6033\n- eval_rouge2: 20.5952\n- eval_rougeL: 36.4458\n- eval_rougeLsum: 36.3202\n- eval_gen_len: 15.257 # average number of generated tokens\n\n## Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 4e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Framework versions\n\n- Transformers 4.19.2\n- Pytorch 1.11.0+cu113\n- Datasets 2.2.2\n- Tokenizers 0.12.1\n", "size_bytes": "891700799", "downloads": 7245}
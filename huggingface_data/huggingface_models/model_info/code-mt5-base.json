{"pretrained_model_name": "flax-community/code-mt5-base", "description": "# Tokenizer\n\nWe trained our tokenizer using [sentencepiece](https://github.com/google/sentencepiece)'s unigram tokenizer. Then loaded the tokenizer as MT5TokenizerFast.\n\n## Model\n\nWe used [MT5-base](https://huggingface.co/google/mt5-base) model.\n\n## Datasets\n\nWe used [Code Search Net](https://huggingface.co/datasets/code_search_net)'s dataset and some scrapped data from internet to train the model. We maintained a list of datasets where each dataset had codes of same language.\n\n## Plots\n\n### Train loss\n\n![train loss](https://i.ibb.co/x53Wm8n/train-loss.png)\n\n### Evaluation loss\n\n![eval loss](https://i.ibb.co/McB2jnf/eval-loss.png)\n\n### Evaluation accuracy\n\n![eval accuracy](https://i.ibb.co/YDGhLdn/eval-accuracy.png)\n\n### Learning rate\n\n![learning rate](https://i.ibb.co/CMStzWv/learning-rate.png)\n\n## Fine tuning (WIP)\n\nWe fine tuned the model with [CodeXGLUE code-to-code-trans dataset](https://huggingface.co/datasets/code_x_glue_cc_code_to_code_trans), and scrapper data.\n", "size_bytes": "966054989", "downloads": 9}
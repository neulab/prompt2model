{"pretrained_model_name": "Vamsi/T5_Paraphrase_Paws", "description": "---\nlanguage: \"en\"\ntags:\n- paraphrase-generation\n- text-generation\n- Conditional Generation\ninference: false\n---\n\n# Paraphrase-Generation\n\u200b\n## Model description\n\u200b\nT5 Model for generating paraphrases of english sentences. Trained on the [Google PAWS](https://github.com/google-research-datasets/paws) dataset.\n\u200b\n## How to use\n\u200b## Requires sentencepiece: # !pip install sentencepiece\nPyTorch and TF models available\n\u200b\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\").to('cuda')\n\u200b\nsentence = \"This is something which i cannot understand at all\"\n\ntext =  \"paraphrase: \" + sentence + \" </s>\"\n\nencoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n\n\noutputs = model.generate(\n    input_ids=input_ids, attention_mask=attention_masks,\n    max_length=256,\n    do_sample=True,\n    top_k=120,\n    top_p=0.95,\n    early_stopping=True,\n    num_return_sequences=5\n)\n\nfor output in outputs:\n    line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    print(line)\n\u200b\n\n```\n\nFor more reference on training your own T5 model or using this model, do check out [Paraphrase Generation](https://github.com/Vamsi995/Paraphrase-Generator).\n", "size_bytes": "891689022", "downloads": 156335}
{"pretrained_model_name": "UNIST-Eunchan/Pegasus-x-base-govreport-12288-1024-numepoch-5-lr-0.002", "description": "---\ntags:\n- generated_from_trainer\ndatasets:\n- govreport-summarization\nmodel-index:\n- name: Pegasus-x-base-govreport-12288-1024-numepoch-5-lr-0.002\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# Pegasus-x-base-govreport-12288-1024-numepoch-5-lr-0.002\n\nThis model is a fine-tuned version of [google/pegasus-x-base](https://huggingface.co/google/pegasus-x-base) on the govreport-summarization dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.5935\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.002\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 64\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 2.953         | 0.07  | 20   | 2.3208          |\n| 2.2294        | 0.15  | 40   | 1.9354          |\n| 2.0682        | 0.22  | 60   | 1.8995          |\n| 2.0892        | 0.29  | 80   | 1.8481          |\n| 2.0304        | 0.37  | 100  | 1.8098          |\n| 2.014         | 0.44  | 120  | 1.8216          |\n| 1.9686        | 0.51  | 140  | 1.7909          |\n| 1.9704        | 0.58  | 160  | 1.8038          |\n| 1.9512        | 0.66  | 180  | 1.7712          |\n| 1.9458        | 0.73  | 200  | 1.7784          |\n| 1.9429        | 0.8   | 220  | 1.7472          |\n| 1.9543        | 0.88  | 240  | 1.7330          |\n| 1.9124        | 0.95  | 260  | 1.7163          |\n| 1.8301        | 1.02  | 280  | 1.7130          |\n| 1.7666        | 1.1   | 300  | 1.7080          |\n| 1.7531        | 1.17  | 320  | 1.7076          |\n| 1.7344        | 1.24  | 340  | 1.7031          |\n| 1.7538        | 1.32  | 360  | 1.7103          |\n| 1.7472        | 1.39  | 380  | 1.6936          |\n| 1.7573        | 1.46  | 400  | 1.6819          |\n| 1.7675        | 1.53  | 420  | 1.6885          |\n| 1.7501        | 1.61  | 440  | 1.6722          |\n| 1.7503        | 1.68  | 460  | 1.6620          |\n| 1.7245        | 1.75  | 480  | 1.6629          |\n| 1.7448        | 1.83  | 500  | 1.6502          |\n| 1.7479        | 1.9   | 520  | 1.6579          |\n| 1.6973        | 1.97  | 540  | 1.6514          |\n| 1.5818        | 2.05  | 560  | 1.6610          |\n| 1.5976        | 2.12  | 580  | 1.6420          |\n| 1.5851        | 2.19  | 600  | 1.6471          |\n| 1.6306        | 2.27  | 620  | 1.6471          |\n| 1.5761        | 2.34  | 640  | 1.6452          |\n| 1.5824        | 2.41  | 660  | 1.6378          |\n| 1.5651        | 2.48  | 680  | 1.6300          |\n| 1.6094        | 2.56  | 700  | 1.6325          |\n| 1.5866        | 2.63  | 720  | 1.6294          |\n| 1.5573        | 2.7   | 740  | 1.6274          |\n| 1.5799        | 2.78  | 760  | 1.6255          |\n| 1.5801        | 2.85  | 780  | 1.6230          |\n| 1.5655        | 2.92  | 800  | 1.6107          |\n| 1.5453        | 3.0   | 820  | 1.6126          |\n| 1.5005        | 3.07  | 840  | 1.6199          |\n| 1.4822        | 3.14  | 860  | 1.6238          |\n| 1.4579        | 3.22  | 880  | 1.6168          |\n| 1.4897        | 3.29  | 900  | 1.6134          |\n| 1.4453        | 3.36  | 920  | 1.6213          |\n| 1.4738        | 3.43  | 940  | 1.6111          |\n| 1.4912        | 3.51  | 960  | 1.6118          |\n| 1.4461        | 3.58  | 980  | 1.6071          |\n| 1.4365        | 3.65  | 1000 | 1.6034          |\n| 1.4244        | 3.73  | 1020 | 1.6008          |\n| 1.4682        | 3.8   | 1040 | 1.5991          |\n| 1.4714        | 3.87  | 1060 | 1.5963          |\n| 1.4586        | 3.95  | 1080 | 1.5987          |\n| 1.4434        | 4.02  | 1100 | 1.6166          |\n| 1.3898        | 4.09  | 1120 | 1.6080          |\n| 1.4035        | 4.17  | 1140 | 1.6099          |\n| 1.4145        | 4.24  | 1160 | 1.6064          |\n| 1.3806        | 4.31  | 1180 | 1.6003          |\n| 1.3645        | 4.38  | 1200 | 1.6004          |\n| 1.381         | 4.46  | 1220 | 1.6053          |\n| 1.3606        | 4.53  | 1240 | 1.6004          |\n| 1.3788        | 4.6   | 1260 | 1.5990          |\n| 1.3526        | 4.68  | 1280 | 1.6029          |\n| 1.3618        | 4.75  | 1300 | 1.5941          |\n| 1.3684        | 4.82  | 1320 | 1.5949          |\n| 1.3721        | 4.9   | 1340 | 1.5948          |\n| 1.3695        | 4.97  | 1360 | 1.5935          |\n\n\n### Framework versions\n\n- Transformers 4.30.2\n- Pytorch 2.0.1+cu117\n- Datasets 2.13.1\n- Tokenizers 0.13.3\n", "size_bytes": "1089301733", "downloads": 0}
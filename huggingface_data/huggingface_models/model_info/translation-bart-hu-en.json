{"pretrained_model_name": "NYTK/translation-bart-hu-en", "description": "---\nlanguage:\n- hu\n- en\ntags:\n- translation\nlicense: apache-2.0\nmetrics:\n- sacrebleu\n- chrf\nwidget:\n- text: >-\n    Szeretn\u00e9m megragadni az alkalmat uram, hogy az enged\u00e9ly\u00e9t k\u00e9rjem, hogy\n    tal\u00e1lkozhassak a l\u00e1ny\u00e1val.\n---\n\n# BART Translation model\n\nFor further models, scripts and details, see [our repository](https://github.com/nytud/machine-translation) or [our demo site](https://juniper.nytud.hu/demo/nlp).\n\n- Source language: Hungarian\n- Target language: English\n\n- Pretrained on English WikiText-103 and Hungarian Wikipedia\n- Finetuned on subcorpora from OPUS\n\t- Segments: 56.837.602\n\n## Limitations\n\n- tokenized input text (tokenizer: [HuSpaCy](https://huggingface.co/huspacy))\n\n## Results\n\n| Model | BLEU | chrF-3 |\n| ------------- | ------------- | ------------- |\n| Google en-hu  | 25.30  | 54.08 |\n| **BART-base-enhu** | **34.38**  | **58.88** |\n| Google hu-en| 34.48  | 59.59 |\n| **BART-base-huen** | **38.03** | **61,37** |\n\n## Citation\nIf you use this model, please cite the following paper:\n```\n@inproceedings {yang-bart,\n    title = {{BARTerezz\u00fcnk! - Messze, messze, messze a vil\u00e1gt\u00f3l, - BART k\u00eds\u00e9rleti modellek magyar nyelvre}},\n\tbooktitle = {XVIII. Magyar Sz\u00e1m\u00edt\u00f3g\u00e9pes Nyelv\u00e9szeti Konferencia},\n\tyear = {2022},\n\tpublisher = {Szegedi Tudom\u00e1nyegyetem, Informatikai Int\u00e9zet},\n\taddress = {Szeged, Magyarorsz\u00e1g},\n\tauthor = {Yang, Zijian Gy\u0151z\u0151},\n\tpages = {15--29}\n}\n\n```", "size_bytes": "526410579", "downloads": 7}
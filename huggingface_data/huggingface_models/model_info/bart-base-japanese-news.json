{"pretrained_model_name": "stockmark/bart-base-japanese-news", "description": "---\nlanguage: ja\ntags:\n- ja\n- japanese\n- bart\n- lm\n- nlp\nlicense: mit\n---\n\n# bart-base-japanese-news(base-sized model)\nThis repository provides a Japanese BART model. The model was trained by [Stockmark Inc.](https://stockmark.co.jp)\n\nAn introductory article on the model can be found at the following URL.\n\n[https://tech.stockmark.co.jp/blog/bart-japanese-base-news/](https://tech.stockmark.co.jp/blog/bart-japanese-base-news/)\n\n## Model description\n\nBART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n\nBART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).\n\n## Intended uses & limitations\n\nYou can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset.\n\n# How to use the model\n\n*NOTE:* Since we are using a custom tokenizer, please use `trust_remote_code=True` to initialize the tokenizer.\n\n## Simple use\n\n```python\nfrom transformers import AutoTokenizer, BartModel\n\nmodel_name = \"stockmark/bart-base-japanese-news\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = BartModel.from_pretrained(model_name)\n\ninputs = tokenizer(\"\u4eca\u65e5\u306f\u826f\u3044\u5929\u6c17\u3067\u3059\u3002\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n```\n\n## Sentence Permutation\n```python\nimport torch\nfrom transformers import AutoTokenizer, BartForConditionalGeneration\n\nmodel_name = \"stockmark/bart-base-japanese-news\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\nif torch.cuda.is_available():\n    model = model.to(\"cuda\")\n\n# correct order text is \"\u660e\u65e5\u306f\u5927\u96e8\u3067\u3059\u3002\u96fb\u8eca\u306f\u6b62\u307e\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3067\u3059\u304b\u3089\u3001\u81ea\u5b85\u304b\u3089\u50cd\u304d\u307e\u3059\u3002\"\ntext = \"\u96fb\u8eca\u306f\u6b62\u307e\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3067\u3059\u304b\u3089\u3001\u81ea\u5b85\u304b\u3089\u50cd\u304d\u307e\u3059\u3002\u660e\u65e5\u306f\u5927\u96e8\u3067\u3059\u3002\"\n\ninputs = tokenizer([text], max_length=128, return_tensors=\"pt\", truncation=True)\ntext_ids = model.generate(inputs[\"input_ids\"].to(model.device), num_beams=3, max_length=128)\noutput = tokenizer.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\nprint(output)\n# sample output: \u660e\u65e5\u306f\u5927\u96e8\u3067\u3059\u3002\u96fb\u8eca\u306f\u6b62\u307e\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3067\u3059\u304b\u3089\u3001\u81ea\u5b85\u304b\u3089\u50cd\u304d\u307e\u3059\u3002\n```\n## Mask filling\n```python\nimport torch\nfrom transformers import AutoTokenizer, BartForConditionalGeneration\n\nmodel_name = \"stockmark/bart-base-japanese-news\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\nif torch.cuda.is_available():\n    model = model.to(\"cuda\")\n\ntext = \"\u4eca\u65e5\u306e\u5929\u6c17\u306f<mask>\u306e\u305f\u3081\u3001\u5098\u304c\u5fc5\u8981\u3067\u3057\u3087\u3046\u3002\"\n\ninputs = tokenizer([text], max_length=128, return_tensors=\"pt\", truncation=True)\ntext_ids = model.generate(inputs[\"input_ids\"].to(model.device), num_beams=3, max_length=128)\noutput = tokenizer.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\nprint(output)\n# sample output: \u4eca\u65e5\u306e\u5929\u6c17\u306f\u3001\u96e8\u306e\u305f\u3081\u3001\u5098\u304c\u5fc5\u8981\u3067\u3057\u3087\u3046\u3002\n```\n\n## Text generation\n\n*NOTE:* You can use the raw model for text generation. However, the model is mostly meant to be fine-tuned on a supervised dataset.\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, BartForConditionalGeneration\n\nmodel_name = \"stockmark/bart-base-japanese-news\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\nif torch.cuda.is_available():\n   model = model.to(\"cuda\")\n\ntext = \"\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\uff08\u3057\u305c\u3093\u3052\u3093\u3054\u3057\u3087\u308a\u3001\u7565\u79f0\uff1aNLP\uff09\u306f\u3001\u4eba\u9593\u304c\u65e5\u5e38\u7684\u306b\u4f7f\u3063\u3066\u3044\u308b\u81ea\u7136\u8a00\u8a9e\u3092\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u306b\u51e6\u7406\u3055\u305b\u308b\u4e00\u9023\u306e\u6280\u8853\u3067\u3042\u308a\u3001\u4eba\u5de5\u77e5\u80fd\u3068\u8a00\u8a9e\u5b66\u306e\u4e00\u5206\u91ce\u3067\u3042\u308b\u3002\u300c\u8a08\u7b97\u8a00\u8a9e\u5b66\u300d\uff08computational linguistics\uff09\u3068\u306e\u985e\u4f3c\u3082\u3042\u308b\u304c\u3001\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306f\u5de5\u5b66\u7684\u306a\u8996\u70b9\u304b\u3089\u306e\u8a00\u8a9e\u51e6\u7406\u3092\u3055\u3059\u306e\u306b\u5bfe\u3057\u3066\u3001\u8a08\u7b97\u8a00\u8a9e\u5b66\u306f\u8a00\u8a9e\u5b66\u7684\u8996\u70b9\u3092\u91cd\u8996\u3059\u308b\u624b\u6cd5\u3092\u3055\u3059\u4e8b\u304c\u591a\u3044\u3002\"\n\ninputs = tokenizer([text], max_length=512, return_tensors=\"pt\", truncation=True)\ntext_ids = model.generate(inputs[\"input_ids\"].to(model.device), num_beams=3, min_length=0, max_length=40)\noutput = tokenizer.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\nprint(output)\n# sample output: \u81ea\u7136\u8a00\u8a9e\u51e6\u7406(\u3057\u305c\u3093\u3052\u3093\u3054\u3057\u3087\u308a\u3001\u7565\u79f0:NLP)\u306f\u3001\u4eba\u9593\u304c\u65e5\u5e38\u7684\u306b\u4f7f\u3063\u3066\u3044\u308b\u81ea\u7136\u8a00\u8a9e\u3092\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u306b\u51e6\u7406\u3055\u305b\u308b\u4e00\u9023\u306e\u6280\u8853\u3067\u3042\u308a\u3001\u8a00\u8a9e\u5b66\u306e\u4e00\u5206\u91ce\u3067\u3042\u308b\u3002\n```\n\n# Training\nThe model was trained on Japanese News Articles.\n\n# Tokenization\nThe model uses a [sentencepiece](https://github.com/google/sentencepiece)-based tokenizer. The vocabulary was first trained on a selected subset from the training data using the official sentencepiece training script.\n\n# Licenses\nThe pretrained models are distributed under the terms of the [MIT License](https://opensource.org/licenses/mit-license.php).\n\n*NOTE:*  Only tokenization_bart_japanese_news.py is [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0). Please see tokenization_bart_japanese_news.py for license details.\n\n# Contact\nIf you have any questions, please contact us using [our contact form](https://stockmark.co.jp/contact).\n\n# Acknowledgement\nThis comparison study supported with Cloud TPUs from Google\u2019s TensorFlow Research Cloud (TFRC).\n", "size_bytes": "498636537", "downloads": 255}
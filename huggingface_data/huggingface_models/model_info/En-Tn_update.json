{"pretrained_model_name": "kabelomalapane/En-Tn_update", "description": "---\nlicense: apache-2.0\ntags:\n- translation\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: En-Tn_update\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# En-Tn_update\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-en-tn](https://huggingface.co/Helsinki-NLP/opus-mt-en-tn) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.13002\n- Bleu: 39.1470\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n|Epoch| \tTraining Loss |\t Validation Loss|  \tBleu |\n|:---:|:---------------:|:----------------:|:-------:|\n| 1   | 1.929300        |\t1.884056        | 29.762382|\n| 2   | 1.637300        |\t1.605588        | 32.846868|\n| 3   | 1.500000        |\t1.457442        | 34.307484|\n| 4   | 1.402400        |\t1.356578        | 35.423774|\n| 5 \t| 1.324000        |\t1.276492        | 36.553368|\n| 6 \t| 1.251300        |\t1.221768        | 37.464270|\n| 7 \t| 1.224700        |\t1.181320        | 38.157490|\n| 8 \t| 1.193200        |\t1.152997        | 38.800566|\n| 9 \t| 1.166700        |\t1.136147      \t| 38.985707|\n| 10  | 1.142500        |\t1.130020      \t| 39.209327|\n\n### Framework versions\n\n- Transformers 4.21.0\n- Pytorch 1.12.0+cu113\n- Datasets 2.4.0\n- Tokenizers 0.12.1\n", "size_bytes": "299358277", "downloads": 6}
{"pretrained_model_name": "KETI-AIR-Downstream/long-ke-t5-base-translation-aihub-bidirection", "description": "---\nlanguage:\n- ko\n- en\ntags:\n- generated_from_trainer\ndatasets:\n- >-\n  KETI-AIR/aihub_koenzh_food_translation,KETI-AIR/aihub_scitech_translation,KETI-AIR/aihub_scitech20_translation,KETI-AIR/aihub_socialtech20_translation,KETI-AIR/aihub_spoken_language_translation\nmetrics:\n- bleu\nmodel-index:\n- name: ko2en_bidirection2\n  results:\n  - task:\n      name: Translation\n      type: translation\n    dataset:\n      name: >-\n        KETI-AIR/aihub_koenzh_food_translation,KETI-AIR/aihub_scitech_translation,KETI-AIR/aihub_scitech20_translation,KETI-AIR/aihub_socialtech20_translation,KETI-AIR/aihub_spoken_language_translation\n        koen,none,none,none,none\n      type: >-\n        KETI-AIR/aihub_koenzh_food_translation,KETI-AIR/aihub_scitech_translation,KETI-AIR/aihub_scitech20_translation,KETI-AIR/aihub_socialtech20_translation,KETI-AIR/aihub_spoken_language_translation\n      args: koen,none,none,none,none\n    metrics:\n    - name: Bleu\n      type: bleu\n      value: 51.5949\nlicense: apache-2.0\npipeline_tag: translation\nwidget:\n- text: \"translate_ko2en: IBM \uc653\uc2a8X\ub294 AI \ubc0f \ub370\uc774\ud130 \ud50c\ub7ab\ud3fc\uc774\ub2e4. \uc2e0\ub8b0\ud560 \uc218 \uc788\ub294 \ub370\uc774\ud130, \uc18d\ub3c4, \uac70\ubc84\ub10c\uc2a4\ub97c \uac16\uace0 \ud30c\uc6b4\ub370\uc774\uc158 \ubaa8\ub378 \ubc0f \uba38\uc2e0 \ub7ec\ub2dd \uae30\ub2a5\uc744 \ud3ec\ud568\ud55c AI \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\uace0, \uc870\uc815\ud574, \uc870\uc9c1 \uc804\uccb4\uc5d0\uc11c \ud65c\uc6a9\ud558\uae30 \uc704\ud55c \uc804 \uacfc\uc815\uc744 \uc544\uc6b0\ub974\ub294 \uae30\uc220\uacfc \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud55c\ub2e4.\"\n  example_title: \"KO2EN 1\"\n- text: \"translate_ko2en: \uc774\uc6a9\uc790\ub294 \uc2e0\ub8b0\ud560 \uc218 \uc788\uace0 \uac1c\ubc29\ub41c \ud658\uacbd\uc5d0\uc11c \uc790\uc2e0\uc758 \ub370\uc774\ud130\uc5d0 \ub300\ud574 \uc790\uccb4\uc801\uc778 AI\ub97c \uad6c\ucd95\ud558\uac70\ub098, \uc2dc\uc7a5\uc5d0 \ucd9c\uc2dc\ub41c AI \ubaa8\ub378\uc744 \uc815\uad50\ud558\uac8c \uc870\uc815\ud560 \uc218 \uc788\ub2e4. \ub300\uaddc\ubaa8\ub85c \ud65c\uc6a9\ud558\uae30 \uc704\ud55c \ub3c4\uad6c \uc138\ud2b8, \uae30\uc220, \uc778\ud504\ub77c \ubc0f \uc804\ubb38 \ucee8\uc124\ud305 \uc11c\ube44\uc2a4\ub97c \ud65c\uc6a9\ud560 \uc218 \uc788\ub2e4.\"\n  example_title: \"KO2EN 2\"\n- text: \"translate_en2ko: The Seoul Metropolitan Government said Wednesday that it would develop an AI-based congestion monitoring system to provide better information to passengers about crowd density at each subway station.\"\n  example_title: \"EN2KO 1\"\n- text: \"translate_en2ko: According to Seoul Metro, the operator of the subway service in Seoul, the new service will help analyze the real-time flow of passengers and crowd levels in subway compartments, improving operational efficiency.\"\n  example_title: \"EN2KO 2\"\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# ko2en_bidirection2\n\nThis model is a fine-tuned version of [KETI-AIR/long-ke-t5-base](https://huggingface.co/KETI-AIR/long-ke-t5-base) on the KETI-AIR/aihub_koenzh_food_translation,KETI-AIR/aihub_scitech_translation,KETI-AIR/aihub_scitech20_translation,KETI-AIR/aihub_socialtech20_translation,KETI-AIR/aihub_spoken_language_translation koen,none,none,none,none dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5716\n- Bleu: 51.5949\n- Gen Len: 28.8348\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.001\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 8\n- total_train_batch_size: 128\n- total_eval_batch_size: 128\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:------:|:---------------:|:-------:|:-------:|\n| 0.7004        | 1.0   | 187524 | 0.6461          | 28.0622 | 17.8368 |\n| 0.6176        | 2.0   | 375048 | 0.5967          | 29.3033 | 17.8281 |\n| 0.5642        | 3.0   | 562572 | 0.5716          | 30.0045 | 17.8366 |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.12.0\n- Datasets 2.8.0\n- Tokenizers 0.13.2", "size_bytes": "1186882355", "downloads": 1378}
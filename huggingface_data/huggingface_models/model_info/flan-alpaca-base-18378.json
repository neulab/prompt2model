{"pretrained_model_name": "IAJw/flan-alpaca-base-18378", "description": "---\nlicense: bsd\npipeline_tag: text2text-generation\n---\nUsed scraped data as input, ChatGPT result as output and limited types of instruction to fine-tune flan-t5-base.\nUsed declare-lab repo. Refer to https://github.com/declare-lab/flan-alpaca\nEpoch set to 1\nInput token max 512\nOutput token max 512\nTrained on Free Google Colab T4\nTraining time ~ 40 mins", "size_bytes": "990408885", "downloads": 2}
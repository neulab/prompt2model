{"pretrained_model_name": "BramVanroy/flan-t5-small-amr-en", "description": "---\nlicense: apache-2.0\nbase_model: google/flan-t5-small\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- bleu\nmodel-index:\n- name: 6e-5lr+30ep+128tbs\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 6e-5lr+30ep+128tbs\n\nThis model is a fine-tuned version of [google/flan-t5-small](https://huggingface.co/google/flan-t5-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3294\n- Accuracy: 0.9117\n- Bleu: 79.9229\n- Smatch Precision: 0.5734\n- Smatch Recall: 0.5923\n- Smatch Fscore: 0.5827\n- Ratio Invalid Amrs: 82.3424\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 6e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 64\n- total_train_batch_size: 128\n- optimizer: Adam with betas=(0.9,0.95) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Accuracy | Bleu    | Smatch Precision | Smatch Recall | Smatch Fscore | Ratio Invalid Amrs |\n|:-------------:|:-----:|:-----:|:---------------:|:--------:|:-------:|:----------------:|:-------------:|:-------------:|:------------------:|\n| 2.0053        | 0.71  | 300   | 1.6165          | 0.6926   | 39.7789 | 0.1608           | 0.4950        | 0.2427        | 94.0547            |\n| 1.4988        | 1.42  | 600   | 1.1002          | 0.7477   | 47.7867 | 0.2012           | 0.5317        | 0.2919        | 92.9251            |\n| 1.0641        | 2.13  | 900   | 0.8784          | 0.7984   | 57.9803 | 0.1063           | 0.5310        | 0.1771        | 87.1581            |\n| 0.8601        | 2.84  | 1200  | 0.7774          | 0.8142   | 61.4242 | 0.1100           | 0.5267        | 0.1820        | 86.1474            |\n| 0.7629        | 3.55  | 1500  | 0.7029          | 0.8312   | 65.2120 | 0.1228           | 0.5341        | 0.1997        | 85.9096            |\n| 0.8427        | 4.27  | 1800  | 0.6425          | 0.8459   | 67.5972 | 0.1437           | 0.5263        | 0.2258        | 84.9584            |\n| 0.8156        | 4.98  | 2100  | 0.6030          | 0.8510   | 68.8724 | 0.2090           | 0.5084        | 0.2962        | 86.8014            |\n| 0.6933        | 5.69  | 2400  | 0.5681          | 0.8583   | 69.4943 | 0.2606           | 0.4990        | 0.3424        | 86.9203            |\n| 0.6716        | 6.4   | 2700  | 0.5488          | 0.8608   | 70.4497 | 0.3436           | 0.5070        | 0.4096        | 88.8228            |\n| 0.6311        | 7.11  | 3000  | 0.5148          | 0.8701   | 71.7897 | 0.3317           | 0.5061        | 0.4007        | 88.7634            |\n| 0.6261        | 7.82  | 3300  | 0.4952          | 0.8748   | 72.6531 | 0.3715           | 0.5096        | 0.4297        | 88.2283            |\n| 0.5591        | 8.53  | 3600  | 0.4901          | 0.8756   | 73.2086 | 0.3885           | 0.4968        | 0.4360        | 86.8609            |\n| 0.5694        | 9.24  | 3900  | 0.4656          | 0.8810   | 73.9410 | 0.3853           | 0.5086        | 0.4385        | 85.3746            |\n| 0.6155        | 9.95  | 4200  | 0.4523          | 0.8837   | 74.3200 | 0.4016           | 0.4938        | 0.4430        | 84.4828            |\n| 0.5763        | 10.66 | 4500  | 0.4463          | 0.8846   | 74.8242 | 0.4262           | 0.5020        | 0.4610        | 84.7800            |\n| 0.6237        | 11.37 | 4800  | 0.4274          | 0.8893   | 75.3448 | 0.4219           | 0.5152        | 0.4639        | 84.1260            |\n| 0.5704        | 12.09 | 5100  | 0.4232          | 0.8900   | 76.0293 | 0.4386           | 0.5142        | 0.4734        | 84.0666            |\n| 0.4967        | 12.8  | 5400  | 0.4125          | 0.8928   | 76.4527 | 0.4500           | 0.5180        | 0.4817        | 83.6504            |\n| 0.5481        | 13.51 | 5700  | 0.4029          | 0.8938   | 76.7124 | 0.4640           | 0.5274        | 0.4937        | 83.6504            |\n| 0.5311        | 14.22 | 6000  | 0.3937          | 0.8969   | 76.9762 | 0.4752           | 0.5372        | 0.5043        | 83.1153            |\n| 0.5363        | 14.93 | 6300  | 0.3877          | 0.8978   | 77.3301 | 0.5020           | 0.5409        | 0.5207        | 83.7099            |\n| 0.4552        | 15.64 | 6600  | 0.3785          | 0.9003   | 77.5798 | 0.5137           | 0.5534        | 0.5328        | 83.2342            |\n| 0.6432        | 16.35 | 6900  | 0.3721          | 0.9014   | 77.7707 | 0.5162           | 0.5562        | 0.5354        | 83.2342            |\n| 0.552         | 17.06 | 7200  | 0.3673          | 0.9026   | 77.9649 | 0.5253           | 0.5593        | 0.5418        | 82.9370            |\n| 0.4323        | 17.77 | 7500  | 0.3643          | 0.9031   | 78.2248 | 0.5340           | 0.5643        | 0.5487        | 83.1153            |\n| 0.3873        | 18.48 | 7800  | 0.3608          | 0.9042   | 78.4648 | 0.5345           | 0.5655        | 0.5496        | 82.9370            |\n| 0.4171        | 19.19 | 8100  | 0.3556          | 0.9051   | 78.5759 | 0.5390           | 0.5692        | 0.5537        | 82.8775            |\n| 0.3746        | 19.9  | 8400  | 0.3507          | 0.9064   | 78.7091 | 0.5426           | 0.5747        | 0.5582        | 83.2342            |\n| 0.512         | 20.62 | 8700  | 0.3493          | 0.9069   | 78.9185 | 0.5510           | 0.5785        | 0.5644        | 83.1748            |\n| 0.6123        | 21.33 | 9000  | 0.3458          | 0.9076   | 79.1584 | 0.5560           | 0.5781        | 0.5669        | 82.8181            |\n| 0.5856        | 22.04 | 9300  | 0.3440          | 0.9080   | 79.3017 | 0.5614           | 0.5814        | 0.5712        | 83.0559            |\n| 0.5037        | 22.75 | 9600  | 0.3386          | 0.9095   | 79.4649 | 0.5550           | 0.5818        | 0.5681        | 82.2235            |\n| 0.4633        | 23.46 | 9900  | 0.3385          | 0.9096   | 79.6088 | 0.5634           | 0.5848        | 0.5739        | 82.5208            |\n| 0.452         | 24.17 | 10200 | 0.3394          | 0.9089   | 79.6218 | 0.5650           | 0.5875        | 0.5761        | 82.7586            |\n| 0.5063        | 24.88 | 10500 | 0.3342          | 0.9103   | 79.6980 | 0.5720           | 0.5917        | 0.5817        | 82.4614            |\n| 0.467         | 25.59 | 10800 | 0.3329          | 0.9110   | 79.7955 | 0.5717           | 0.5906        | 0.5810        | 82.0452            |\n| 0.5166        | 26.3  | 11100 | 0.3325          | 0.9109   | 79.8235 | 0.5706           | 0.5891        | 0.5797        | 82.3424            |\n| 0.4275        | 27.01 | 11400 | 0.3322          | 0.9111   | 79.8273 | 0.5713           | 0.5897        | 0.5803        | 82.2235            |\n| 0.4595        | 27.72 | 11700 | 0.3303          | 0.9116   | 79.9065 | 0.5760           | 0.5945        | 0.5851        | 82.1641            |\n| 0.4709        | 28.44 | 12000 | 0.3295          | 0.9117   | 79.9299 | 0.5733           | 0.5917        | 0.5824        | 82.1046            |\n| 0.4448        | 29.15 | 12300 | 0.3290          | 0.9118   | 79.9404 | 0.5768           | 0.5956        | 0.5861        | 82.3424            |\n| 0.4631        | 29.86 | 12600 | 0.3294          | 0.9117   | 79.9229 | 0.5734           | 0.5923        | 0.5827        | 82.3424            |\n\n\n### Framework versions\n\n- Transformers 4.31.0\n- Pytorch 2.0.1+cu118\n- Datasets 2.14.0\n- Tokenizers 0.13.3\n", "size_bytes": "308250117", "downloads": 20}
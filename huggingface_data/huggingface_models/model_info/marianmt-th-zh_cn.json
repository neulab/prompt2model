{"pretrained_model_name": "Lalita/marianmt-th-zh_cn", "description": "---\ntags:\n- translation\n- torch==1.8.0\nwidget:\n- text: \"Inference Unavailable\"\n---\n### marianmt-th-zh_cn\n* source languages: th\n* target languages: zh_cn\n* dataset: \n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* test set scores: 15.53\n\n## Training\n\nTraining scripts from [LalitaDeelert/NLP-ZH_TH-Project](https://github.com/LalitaDeelert/NLP-ZH_TH-Project). Experiments tracked at [cstorm125/marianmt-th-zh_cn](https://wandb.ai/cstorm125/marianmt-th-zh_cn).\n\n```\nexport WANDB_PROJECT=marianmt-th-zh_cn\npython train_model.py --input_fname ../data/v1/Train.csv \\\\\\\\\\\\\\\\\n\\\\\\\\t--output_dir ../models/marianmt-th-zh_cn \\\\\\\\\\\\\\\\\n\\\\\\\\t--source_lang th --target_lang zh \\\\\\\\\\\\\\\\\n\\\\\\\\t--metric_tokenize zh --fp16\n```\n\n## Usage\n\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n \ntokenizer = AutoTokenizer.from_pretrained(\"Lalita/marianmt-zh_cn-th\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Lalita/marianmt-zh_cn-th\").cpu()\n\nsrc_text = [\n    '\u0e09\u0e31\u0e19\u0e23\u0e31\u0e01\u0e04\u0e38\u0e13',\n    '\u0e09\u0e31\u0e19\u0e2d\u0e22\u0e32\u0e01\u0e01\u0e34\u0e19\u0e02\u0e49\u0e32\u0e27',\n]\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\nprint([tokenizer.decode(t, skip_special_tokens=True) for t in translated])\n\n> ['\u6211\u7231\u4f60', '\u6211\u60f3\u5403\u996d\u3002']\n```\n\n## Requirements\n```\ntransformers==4.6.0\ntorch==1.8.0\n```", "size_bytes": "242307293", "downloads": 564}
{"pretrained_model_name": "gaussalgo/T5-LM-Large_Canard-HotpotQA-rephrase", "description": "---\ndatasets:\n- hotpot_qa\n- gaussalgo/Canard_Wiki-augmented\n---\n\n# Model Card for T5-LM-Large_Canard-HotpotQA-rephrase \n\nThis model is trained on three objectives: (1) Generating answers for Canard dataset, (2) Generating answers for HotpotQA, (3) Rephrasing questions by the previous conversations of Canard.\n\n## Training\n\nThe model was trained using the following script, exported from the corresponding Jupyter notebook. All details, including the request format, can be inferred without errors from the code.\nThe best checkpoint was picked by a minimal loss on all (3) training objectives.\n\n```python\nimport datasets\ncanard_train_augm = datasets.load_dataset(\"gaussalgo/Canard_Wiki-augmented\", split=\"train\")  # see the dataset card for details\ncanard_test_augm = datasets.load_dataset(\"gaussalgo/Canard_Wiki-augmented\", split=\"test\")\n\ncanard_df = canard_train_augm.to_pandas()\ncanard_test_df = canard_train_augm.to_pandas()\n\n### Curation of seq2seq input contexts and labels\nimport random\n\ndef input_context_from_sample(row: dict, max_length=5) -> str:\n    context = \"Previous conversation:\"\n    context += \"\\nQuestion: \"\n    context += \", \".join(row[\"History\"][:3])\n    for i in range(3, len(row[\"History\"]), 2):\n        context += \"\\nAnswer: \"\n        context += row[\"History\"][i]\n        if i+1 < len(row[\"History\"]):\n            context += \"\\nQuestion: \"\n            context += row[\"History\"][i+1]\n\n    context += \"\\n\\nCurrent Question: \"\n    context += row[\"Question\"]\n\n    context += \"\\nSearch results:\"\n    all_contexts = row[\"retrieved_contexts\"].tolist()[:max_length-1] + [row[\"true_contexts\"]]\n    random.shuffle(all_contexts)\n\n    for i, search_result in enumerate(all_contexts):\n        context += \"\\n[%s]: \" % (i+1)\n        context += search_result.replace(\"CANNOTANSWER\", \"\")\n\n    context += \"\\nCurrent Answer: \"\n    return context\n\n\ndef rephrasing_context_from_sample(row: dict) -> str:\n    context = \"Previous conversation:\"\n    context += \"\\nQuestion: \"\n    context += \", \".join(row[\"History\"][:3])\n    for i in range(3, len(row[\"History\"]), 2):\n        context += \"\\nAnswer: \"\n        context += row[\"History\"][i]\n        if i+1 < len(row[\"History\"]):\n            context += \"\\nQuestion: \"\n            context += row[\"History\"][i+1]\n    \n    context += \"\\n\\nCurrent Question: \"\n    context += row[\"Question\"]\n\n    context += \"\\nMore specific question: \"\n    return context\n\n\ndef hotpotqa_context(row: dict) -> str:\n    context = \"Current Question: \"\n    context += row[\"question\"]\n\n    context += \"\\nSearch results:\"\n    all_contexts = [\" \".join(context) for context in row[\"context\"][\"sentences\"]]\n\n    for i, search_result in enumerate(all_contexts):\n        context += \"\\n[%s]: \" % (i+1)\n        # context += search_result.replace(\"CANNOTANSWER\", \"\")\n\n    context += \"\\nCurrent Answer: \"\n    return context\n\n\ninput_texts = canard_df.apply(lambda row: input_context_from_sample(row), axis=1).values\ninput_val_texts = canard_test_df.iloc[:200].apply(lambda row: input_context_from_sample(row), axis=1).values\n\ntoo_long_index = [len(t) > 20000 for t in input_texts]\ninput_texts = [t for i, t in enumerate(input_texts) if not too_long_index[i]]\nprint(\"training on %s samples\" % len(input_texts))\n\nlabels = canard_df.answer.apply(lambda ans: \"No answer\" if ans == \"CANNOTANSWER\" else ans).values\nlabels = [l for i, l in enumerate(labels)  if not too_long_index[i]]\n\nval_labels = canard_test_df.answer.apply(lambda ans: \"No answer\" if ans == \"CANNOTANSWER\" else ans).values\n\nrephrasing_inputs = canard_df.apply(lambda row: rephrasing_context_from_sample(row), axis=1).values\nprint(rephrasing_inputs[0])\n\nrephrasing_val_inputs = canard_test_df.apply(lambda row: rephrasing_context_from_sample(row), axis=1).values\n\nrephrasing_labels = canard_df.Rewrite.values\nrephrasing_val_labels = canard_test_df.Rewrite.values\nprint(rephrasing_labels[0])\n\n# Training\n# see Adaptor's homepage for details:\n# https://github.com/gaussalgo/adaptor\n\nfrom adaptor.lang_module import LangModule\n\nlang_module = LangModule(\"google/t5-large-lm-adapt\")\n\nfrom adaptor.evaluators.generative import ROUGE, BLEU\n\nevaluators = [BLEU(), ROUGE()]\n\nfrom adaptor.objectives.seq2seq import Sequence2Sequence\n\nseq_qa = Sequence2Sequence(lang_module,\n                           texts_or_path=input_texts,\n                           labels_or_path=labels,\n                           val_texts_or_path=input_val_texts,\n                           val_labels_or_path=val_labels,\n                           batch_size=4,\n                           val_evaluators=evaluators,\n                           objective_id=\"Canard\")\n\nhotpot_train = datasets.load_dataset(\"hotpot_qa\", \"distractor\")[\"train\"]\nhotpot_val = datasets.load_dataset(\"hotpot_qa\", \"distractor\")[\"validation\"]\n\nhotpot_inputs = hotpot_train.to_pandas().apply(hotpotqa_context, axis=1)\nhotpot_val_inputs = hotpot_val.to_pandas().apply(hotpotqa_context, axis=1)\n\ntoo_long_index = [len(t) > 20000 for t in hotpot_inputs]\n\nhotpot_inputs = [t for i, t in enumerate(hotpot_inputs) if not too_long_index[i]]\nhotpot_answers = [t for i, t in enumerate(hotpot_train[\"answer\"]) if not too_long_index[i]]\n\nseq_additional_qa = Sequence2Sequence(lang_module,\n                                      texts_or_path=hotpot_inputs,\n                                      labels_or_path=hotpot_answers,\n                                      val_texts_or_path=hotpot_val_inputs[:200],\n                                      val_labels_or_path=hotpot_val[\"answer\"][:200],\n                                      batch_size=4,\n                                      val_evaluators=evaluators,\n                                      objective_id=\"HotpotQA\",\n                                      share_other_objective_head=seq_qa)\n\n\nseq_rephrasing = Sequence2Sequence(lang_module,\n                                   texts_or_path=rephrasing_inputs,\n                                   labels_or_path=rephrasing_labels,\n                                   val_texts_or_path=rephrasing_val_inputs[:200],\n                                   val_labels_or_path=rephrasing_val_labels[:200],\n                                   batch_size=4,\n                                   val_evaluators=evaluators,\n                                   objective_id=\"rephrasing\",\n                                   share_other_objective_head=seq_qa)\n\nfrom adaptor.utils import AdaptationArguments, StoppingStrategy\n\ntraining_arguments = AdaptationArguments(output_dir=\"checkpoints-chatbot\",\n                                         learning_rate=5e-5,\n                                         stopping_strategy=StoppingStrategy.ALL_OBJECTIVES_CONVERGED,\n                                         stopping_patience=8,\n                                         save_total_limit=8,\n                                         do_train=True,\n                                         do_eval=True,\n                                         bf16=True,\n                                         warmup_steps=1000,\n                                         gradient_accumulation_steps=8,\n                                         logging_steps=10,\n                                         eval_steps=200,\n                                         save_steps=1000,\n                                         num_train_epochs=10,\n                                         evaluation_strategy=\"steps\")\n\nfrom adaptor.schedules import ParallelSchedule\nfrom adaptor.adapter import Adapter\n\nschedule = ParallelSchedule(objectives=[seq_qa, seq_additional_qa, seq_rephrasing],\n                            args=training_arguments)\nadapter = Adapter(lang_module, schedule, args=training_arguments)\n\nadapter.train()\n```\n\n\n## Usage\n\nSee the prompting templates used in training to infer the optimal prompting format.\n\n#### Contact\n\nFeel free to ask questions here, or at stefanik{at} gaussalgo.com", "size_bytes": "3132785797", "downloads": 25}
{"pretrained_model_name": "iliemihai/mt5-base-romanian-diacritics", "description": "---\nlanguage: ro\ninference: true\nlicense: apache-2.0\n\ntags:\n- romanian\n- seq2seq\n- t5\n---\n\nThis is the fine-tuned [mt5-base-romanian](https://huggingface.co/dumitrescustefan/mt5-base-romanian) base model (**390M** parameters).\n\nThe model was fine-tuned on the [romanian diacritics dataset](https://huggingface.co/datasets/dumitrescustefan/diacritic) for 150k steps with a batch of size 8. The encoder sequence length is 256 and the decoder sequence length is also 256. It was trained with the following [scripts](https://github.com/iliemihai/t5x_diacritics).\n\n### How to load the fine-tuned mt5x model\n\n```python\nfrom transformers import MT5ForConditionalGeneration, T5Tokenizer\nmodel = MT5ForConditionalGeneration.from_pretrained('iliemihai/mt5-base-romanian-diacritics')\ntokenizer = T5Tokenizer.from_pretrained('iliemihai/mt5-base-romanian-diacritics')\ninput_text = \"A inceput sa ii taie un fir de par, iar fata sta in fata, tine camasa de in in mana si canta nota SI.\"\ninputs = tokenizer(input_text, max_length=256, truncation=True, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\noutput = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(output)  # this will print \"A \u00eenceput s\u0103 \u00eei taie un fir de p\u0103r, iar fata st\u0103 \u00een fa\u021b\u0103, \u021bine c\u0103ma\u0219a de in \u00een m\u00e2n\u0103 \u0219i c\u00e2nt\u0103 nota SI\" \n```\n\n### Evaluation\n\nEvaluation will be done soon [here]()\n\n### Acknowledgements\n\nWe'd like to thank [TPU Research Cloud](https://sites.research.google/trc/about/) for providing the TPUv3 cores we used to train these models!\n\n### Authors\n\nYours truly,  \n\n_[Stefan Dumitrescu](https://github.com/dumitrescustefan), [Mihai Ilie](https://github.com/iliemihai) and [Per Egil Kummervold](https://huggingface.co/north)_\n\n", "size_bytes": "2329696205", "downloads": 44}
{"pretrained_model_name": "learningmachineaz/mt5-enaz-10m", "description": "---\nlanguage:\n- en\n- az\ntags:\n- machine-translation\n- mt5\n- english\n- azerbaijani\nlicense: cc-by-nc-sa-4.0\nwidget:\n- text: >-\n    Artificial intelligence is already superior to human learning in numerous\n    domains.\n- text: Learn as if you will live forever, live like you will die tomorrow.\n- text: When you change your thoughts, remember to also change your world.\npipeline_tag: translation\ninference:\n  parameters:\n    max_length: 128\n    num_return_sequences: 1\n    do_sample: false\ndatasets:\n- learningmachineaz/translate_enaz_10m\n---\n\n# Machine Translation (Ma\u015f\u0131n t\u0259rc\u00fcm\u0259si)\n\nThis is the most advanced and accurate mT5 based model for machine translation available as for Azerbaijani language.\\\nThe model was trained on 10 million sentences extracted from various text sources of Azerbaijan National Library.\\\nQuality of translation is very close to Google Translate as it was used for English translations.\n\n## Text above translated using this model\n```\nBu, Az\u0259rbaycan dilin\u0259 oldu\u011fu kimi, ma\u015f\u0131n t\u0259rc\u00fcm\u0259si \u00fc\u00e7\u00fcn \u0259n qabaqc\u0131l v\u0259 d\u0259qiq mT5 \u0259sasl\u0131 modeldir.\nModel Az\u0259rbaycan Milli Kitabxanas\u0131n\u0131n m\u00fcxt\u0259lif m\u0259tn m\u0259nb\u0259l\u0259rind\u0259n \u00e7\u0131xar\u0131lan 10 milyon c\u00fcml\u0259 \u00fczr\u0259 t\u0259lim ke\u00e7ib.\nT\u0259rc\u00fcm\u0259 keyfiyy\u0259ti ingilis dilin\u0259 t\u0259rc\u00fcm\u0259l\u0259r \u00fc\u00e7\u00fcn istifad\u0259 olundu\u011fundan Google T\u0259rc\u00fcm\u0259 il\u0259 \u00e7ox yax\u0131nd\u0131r.\n```\n\n## Training\n\n| Key point               | Info |\n|-------------------------|---------|\n| Base model | mT5-base    |\n| Batch size | 16    |\n| Epochs | 10    |\n| Steps | 620k    |\n| Training Loss |  0.56    |\n| Eval Loss | 0.53    |\n| Training Duration | 2 days   |\n\n\n## Here is an example of how you can run inference:\n\n```python \nfrom transformers import MT5Tokenizer, MT5ForConditionalGeneration\n\nmodel_name = 'learningmachineaz/mt5-enaz-10m'\nmax_length = 128\n\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\n\ntext = \"Artificial intelligence is already superior to human learning in numerous domains.\"\ninput_ids = tokenizer(f'translate English to Azerbaijani: {text}', return_tensors=\"pt\").input_ids\n\n# OPTION 1 - SINGLE TRANSLATION\noutputs = model.generate(input_ids, max_length=max_length, do_sample=False, num_return_sequences=1)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n# OPTION 2 - MULTIPLE VARIATIONS\noutputs = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=10, num_return_sequences=3)\nfor i, output in enumerate(outputs):\n    print(tokenizer.decode(output, skip_special_tokens=True))\n```\n\nOPTION 1 - OUTPUT:\n```\nS\u00fcni intellekt art\u0131q \u00e7oxsayl\u0131 domenl\u0259rd\u0259 insan\u0131n \u00f6yr\u0259nilm\u0259sind\u0259n \u00fcst\u00fcnl\u00fck t\u0259\u015fkil edir.\n```\n\nOPTION 2 - OUTPUT:\n```\nArt\u0131q \u00e7oxsayl\u0131 domenl\u0259rd\u0259 s\u00fcni z\u0259ka insan\u0131n \u00f6yr\u0259nilm\u0259sind\u0259n daha \u00fcst\u00fcn olmas\u0131 \u015f\u0259rti il\u0259 m\u00fc\u015fahid\u0259 edilir.\nS\u00fcni intellekt art\u0131q \u00e7oxsayl\u0131 oblastlarda insan\u0131n t\u0259limind\u0259n y\u00fcks\u0259kdir.\nS\u00fcni intellekt art\u0131q \u00e7oxsayl\u0131 domenl\u0259rd\u0259 insan \u00f6yr\u0259nm\u0259sind\u0259n daha \u00fcst\u00fcn g\u0259lir.\n```\n\n## Author\n\nTrained and evaluated by [Renat Kalimulin](https://www.linkedin.com/in/rinat-kalimulin-16853358/)", "size_bytes": "2329700173", "downloads": 12}
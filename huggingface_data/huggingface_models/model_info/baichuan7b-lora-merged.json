{"pretrained_model_name": "Laurie/baichuan7b-lora-merged", "description": "---\nlicense: apache-2.0\ndatasets:\n- Laurie/alpaca_chinese_dataset\nlanguage:\n- zh\n- en\nmetrics:\n- accuracy\nlibrary_name: transformers\npipeline_tag: text2text-generation\n---\n## Usage:\n\n    from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n    import torch\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"baichuan7b-lora-merged\", trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\"baichuan7b-lora-merged\", device_map=\"auto\", trust_remote_code=True,torch_dtype=torch.float16)\n    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n    while True:\n        query = input('\u8bf7\u8f93\u5165\u95ee\u9898:')\n        if len(query.strip()) == 0:\n            break\n        inputs = tokenizer([\"<human>:{}\\n<bot>:\".format(query)], return_tensors=\"pt\")\n        inputs = inputs.to(\"cuda\")\n        generate_ids = model.generate(**inputs, max_new_tokens=256, streamer=streamer)\n", "size_bytes": 14001127424, "downloads": 4}
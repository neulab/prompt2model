{"pretrained_model_name": "TeraSpace/dialofrednocontext", "description": "---\nlicense: mit\nwidget:\n- text: |-\n    <SC1>- \u043a\u0430\u043a \u0442\u044b?\n    - <extra_id_0>\n  example_title: how r u\nlanguage:\n- ru\ntags:\n- conversational\ndatasets:\n- Den4ikAI/russian_dialogues\n---\nBetter version: [TeraSpace/dialofred](https://huggingface.co/TeraSpace/dialofred).\nHow to use:\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ndevice='cuda'\ntokenizer = AutoTokenizer.from_pretrained('TeraSpace/dialofrednocontext')\nmodel = AutoModelForSeq2SeqLM.from_pretrained('TeraSpace/dialofrednocontext').to(device)\nwhile True:\n    text_inp = input(\"=>\")\n    lm_text=f'<SC1>- {text_inp}\\n- <extra_id_0>'\n    input_ids=torch.tensor([tokenizer.encode(lm_text)]).to(device)\n    # outputs=model.generate(input_ids=input_ids,\n    #                                 max_length=200,\n    #                                 eos_token_id=tokenizer.eos_token_id,\n    #                                 early_stopping=True,\n    #                                 do_sample=True,\n    #                                 temperature=1.0,\n    #                                 top_k=0,\n    #                                 top_p=0.85)\n    # outputs=model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,early_stopping=True)\n    outputs=model.generate(input_ids=input_ids,\n                                    max_length=200,\n                                    eos_token_id=tokenizer.eos_token_id,\n                                    early_stopping=True,\n                                    do_sample=True,\n                                    temperature=0.7,\n                                    top_k=0,\n                                    top_p=0.8)\n    \n    print(tokenizer.decode(outputs[0][1:]))\n```", "size_bytes": "6961608417", "downloads": 66}
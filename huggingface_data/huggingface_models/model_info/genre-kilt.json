{"pretrained_model_name": "facebook/genre-kilt", "description": "---\n\nlanguage:\n- en\n\ntags:\n- retrieval\n- entity-retrieval\n- named-entity-disambiguation\n- entity-disambiguation\n- named-entity-linking\n- entity-linking\n- text2text-generation\n---\n\n\n# GENRE\n\n\nThe GENRE (Generative ENtity REtrieval) system as presented in [Autoregressive Entity Retrieval](https://arxiv.org/abs/2010.00904) implemented in pytorch.\n\nIn a nutshell, GENRE uses a sequence-to-sequence approach to entity retrieval (e.g., linking), based on fine-tuned [BART](https://arxiv.org/abs/1910.13461) architecture. GENRE performs retrieval generating the unique entity name conditioned on the input text using constrained beam search to only generate valid identifiers. The model was first released in the [facebookresearch/GENRE](https://github.com/facebookresearch/GENRE) repository using `fairseq` (the `transformers` models are obtained with a conversion script similar to [this](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py).\n\nThis model was trained on the full training set of [KILT](https://arxiv.org/abs/2009.02252) (i.e., 11 datasets for fact-checking, entity-linking, slot filling, dialogue, open-domain extractive and abstractive QA).\n\n## BibTeX entry and citation info\n\n**Please consider citing our works if you use code from this repository.**\n\n```bibtex\n@inproceedings{decao2020autoregressive,\n  title={Autoregressive Entity Retrieval},\n  author={Nicola {De Cao} and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\n  booktitle={International Conference on Learning Representations},\n  url={https://openreview.net/forum?id=5k8F6UU39V},\n  year={2021}\n}\n```\n\n## Usage\n\nHere is an example of generation for Wikipedia page retrieval for open-domain fact-checking:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# OPTIONAL: load the prefix tree (trie), you need to additionally download\n# https://huggingface.co/facebook/genre-kilt/blob/main/trie.py and \n# https://huggingface.co/facebook/genre-kilt/blob/main/kilt_titles_trie_dict.pkl\n# import pickle\n# from trie import Trie\n# with open(\"kilt_titles_trie_dict.pkl\", \"rb\") as f:\n#     trie = Trie.load_from_dict(pickle.load(f))\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-kilt\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-kilt\").eval()\n\nsentences = [\"Einstein was a German physicist.\"]\n\noutputs = model.generate(\n    **tokenizer(sentences, return_tensors=\"pt\"),\n    num_beams=5,\n    num_return_sequences=5,\n    # OPTIONAL: use constrained beam search\n    # prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n)\n\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n```\nwhich outputs the following top-5 predictions (using constrained beam search)\n```\n['Albert Einstein',\n 'Erwin Schr\u00f6dinger',\n 'Werner Bruschke',\n 'Werner von Habsburg',\n 'Werner von Moltke']\n```", "size_bytes": "1625526529", "downloads": 75}
{"pretrained_model_name": "CadenzaBaron/M2M100-418M-for-GameTranslation-Finetuned-Zh-En", "description": "---\nlanguage:\n- zh\n- en\ntags:\n- translation\n- game\n- cultivation\nlicense: cc-by-nc-4.0\ndatasets:\n- Custom\nmetrics:\n- BLEU\npipeline_tag: translation\ninference:\n  parameters:\n    src_lang : \"zh\"\n    tgt_lang : \"en\"\nwidget:\n- text: \"\u5730\u9636\u4e0a\u54c1\u9041\u672f\uff0c\u65bd\u5c55\u540e\u4fbf\u53ef\u7acb\u4e8e\u6240\u6301\u4e4b\u5251\u4e0a\uff0c\u4ee5\u6781\u5feb\u7684\u901f\u5ea6\u81ea\u7531\u98de\u884c\u3002\"\n---\n\n# Note : Model version has been bumped to V2, providing seemingly better translations. Legacy model still available on the 'V1' branch\n\nThis is a finetuned version of Facebook/M2M100. \nIt's a project born from the activity of [Amateur Modding Avenue](discord.gg/agFA6xa6un), a Discord based modding community.\nSpecial thanks to the Path of Wuxia modding team for kindly sharing their translations to help build the dataset.\n\nIt has been trained on a 46k lines parallel corpus on several Chinese video games translations. All of them are from human/fan translations.\n\nIt's not perfect but it's the best I could do. \nIt should be sitting somewhere between Google Translate and DeepL, I guess.\nSo... Before you go any further, lower your expectations.\nNo, lower.\nJust a bit lower... and.. here we are. \n\nThat being said, it has upsides for first MT pass in a game translation context :\n\n1) It should not mess up tags\n2) It has basic cultivation/martial arts vocabulary\n3) Nothing is locked behind a paywall \\o/\n\nNote : Considering the dataset is built from the work of from modding groups (AMA and PoW Translation Team), who may not want their work to be reused for further AI training, it will not be made public nor shared. \n\nSample generation script : \n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, M2M100Tokenizer\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ntokenizer = transformers.M2M100Tokenizer.from_pretrained(\"CadenzaBaron/M2M100-418M-for-GameTranslation-Finetuned-Zh-En\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"CadenzaBaron/M2M100-418M-for-GameTranslation-Finetuned-Zh-En\")\nmodel.to(device)\ntokenizer.src_lang = \"zh\"\ntokenizer.tgt_lang = \"en\"\ntest_string = \"\u5730\u9636\u4e0a\u54c1\u9041\u672f\uff0c\u65bd\u5c55\u540e\u4fbf\u53ef\u7acb\u4e8e\u6240\u6301\u4e4b\u5251\u4e0a\uff0c\u4ee5\u6781\u5feb\u7684\u901f\u5ea6\u81ea\u7531\u98de\u884c\u3002\"\n\ninputs = tokenizer(test_string, return_tensors=\"pt\").to(device)\ntranslated_tokens = model.generate(**inputs, num_beams=10, do_sample=True)\ntranslation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n\nprint(\"CH : \", test_string , \" // EN : \", translation)\n```\n\nTranslation sample and comparison with Google Translate and DeepL : [Link to Spreadsheet](https://docs.google.com/spreadsheets/d/1J1i9P0nyI9q5-m2iZGSUatt3ZdHSxU8NOp9tJH7wxsk/edit?usp=sharing)", "size_bytes": "1935792071", "downloads": 57}
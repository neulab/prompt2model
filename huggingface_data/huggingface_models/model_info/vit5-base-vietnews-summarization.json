{"pretrained_model_name": "VietAI/vit5-base-vietnews-summarization", "description": "---\nlanguage: vi\ndatasets:\n- cc100\ntags:\n- summarization\n\nlicense: mit\n\nwidget:\n- text: \"VietAI l\u00e0 t\u1ed5 ch\u1ee9c phi l\u1ee3i nhu\u1eadn v\u1edbi s\u1ee9 m\u1ec7nh \u01b0\u01a1m m\u1ea7m t\u00e0i n\u0103ng v\u1ec1 tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o v\u00e0 x\u00e2y d\u1ef1ng m\u1ed9t c\u1ed9ng \u0111\u1ed3ng c\u00e1c chuy\u00ean gia trong l\u0129nh v\u1ef1c tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o \u0111\u1eb3ng c\u1ea5p qu\u1ed1c t\u1ebf t\u1ea1i Vi\u1ec7t Nam.\"\n---\n\n# ViT5-Base Finetuned on `vietnews` Abstractive Summarization (No prefix needed)\n\n\nState-of-the-art pretrained Transformer-based encoder-decoder model for Vietnamese.\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vit5-pretrained-text-to-text-transformer-for/abstractive-text-summarization-on-vietnews)](https://paperswithcode.com/sota/abstractive-text-summarization-on-vietnews?p=vit5-pretrained-text-to-text-transformer-for)\n\n\n## How to use\nFor more details, do check out [our Github repo](https://github.com/vietai/ViT5) and [eval script](https://github.com/vietai/ViT5/blob/main/eval/Eval_vietnews_sum.ipynb). \n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base-vietnews-summarization\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-base-vietnews-summarization\")\nmodel.cuda()\n\u200b\nsentence = \"VietAI l\u00e0 t\u1ed5 ch\u1ee9c phi l\u1ee3i nhu\u1eadn v\u1edbi s\u1ee9 m\u1ec7nh \u01b0\u01a1m m\u1ea7m t\u00e0i n\u0103ng v\u1ec1 tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o v\u00e0 x\u00e2y d\u1ef1ng m\u1ed9t c\u1ed9ng \u0111\u1ed3ng c\u00e1c chuy\u00ean gia trong l\u0129nh v\u1ef1c tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o \u0111\u1eb3ng c\u1ea5p qu\u1ed1c t\u1ebf t\u1ea1i Vi\u1ec7t Nam.\"\nsentence = sentence + \"</s>\"\nencoding = tokenizer(sentence, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\noutputs = model.generate(\n    input_ids=input_ids, attention_mask=attention_masks,\n    max_length=256,\n    early_stopping=True\n)\nfor output in outputs:\n    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    print(line)\n```\n\n## Citation\n```\n@inproceedings{phan-etal-2022-vit5,\n    title = \"{V}i{T}5: Pretrained Text-to-Text Transformer for {V}ietnamese Language Generation\",\n    author = \"Phan, Long and Tran, Hieu and Nguyen, Hieu and Trinh, Trieu H.\",\n    booktitle = \"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop\",\n    year = \"2022\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.naacl-srw.18\",\n    pages = \"136--142\",\n}\n```", "size_bytes": "903886847", "downloads": 166}
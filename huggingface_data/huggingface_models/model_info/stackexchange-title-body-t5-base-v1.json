{"pretrained_model_name": "doc2query/stackexchange-title-body-t5-base-v1", "description": "---\nlanguage: en\ndatasets:\n- flax-sentence-embeddings/stackexchange_title_body_jsonl\nwidget:\n- text: \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n\nlicense: apache-2.0\n---\n\n# doc2query/stackexchange-title-body-t5-base-v1\n\nThis is a [doc2query](https://arxiv.org/abs/1904.08375) model based on T5 (also known as [docT5query](https://cs.uwaterloo.ca/~jimmylin/publications/Nogueira_Lin_2019_docTTTTTquery-v2.pdf)).\n\nIt can be used for:\n- **Document expansion**: You generate for your paragraphs 20-40 queries and index the paragraphs and the generates queries in a standard BM25 index like Elasticsearch, OpenSearch, or Lucene. The generated queries help to close the lexical gap of lexical search, as the generate queries contain synonyms. Further, it re-weights words giving important words a higher weight even if they appear seldomn in a paragraph. In our [BEIR](https://arxiv.org/abs/2104.08663) paper we showed that BM25+docT5query is a powerful search engine. In the [BEIR repository](https://github.com/UKPLab/beir) we have an example how to use docT5query with Pyserini.\n- **Domain Specific Training Data Generation**: It can be used to generate training data to learn an embedding model. On [SBERT.net](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html) we have an example how to use the model to generate (query, text) pairs for a given collection of unlabeled texts. These pairs can then be used to train powerful dense embedding models.\n\n## Usage\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\nmodel_name = 'doc2query/stackexchange-title-body-t5-base-v1'\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ntext = \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n\n\ninput_ids = tokenizer.encode(text, max_length=320, truncation=True, return_tensors='pt')\noutputs = model.generate(\n    input_ids=input_ids,\n    max_length=64,\n    do_sample=True,\n    top_p=0.95,\n    num_return_sequences=5)\n\nprint(\"Text:\")\nprint(text)\n\nprint(\"\\nGenerated Queries:\")\nfor i in range(len(outputs)):\n    query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n    print(f'{i + 1}: {query}')\n```\n\n**Note:** `model.generate()` is non-deterministic. It produces different queries each time you run it.\n\n## Training\nThis model fine-tuned [google/t5-v1_1-base](https://huggingface.co/google/t5-v1_1-base) for 550k training steps. For the training script, see the `train_script.py` in this repository.\n\nThe input-text was truncated to 320 word pieces. Output text was generated up to 64 word pieces. \n\nThis model was trained on a (title, question_body) from StackExchange.\n\n\n\n", "size_bytes": "990445401", "downloads": 5}
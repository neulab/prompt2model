{"pretrained_model_name": "CLAck/en-vi", "description": "---\nlanguage:\n- en\n- vi\ntags:\n- translation\nlicense: apache-2.0\ndatasets:\n- ALT\nmetrics:\n- sacrebleu\n---\n\nThis is a finetuning of a MarianMT pretrained on English-Chinese. The target language pair is English-Vietnamese.\nThe first phase of training (mixed) is performed on a dataset containing both English-Chinese and English-Vietnamese sentences.\nThe second phase of training (pure) is performed on a dataset containing only English-Vietnamese sentences.\n\n### Example\n```\n%%capture\n!pip install transformers transformers[sentencepiece]\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n# Download the pretrained model for English-Vietnamese available on the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"CLAck/en-vi\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"CLAck/en-vi\")\n# Download a tokenizer that can tokenize English since the model Tokenizer doesn't know anymore how to do it\n# We used the one coming from the initial model\n# This tokenizer is used to tokenize the input sentence\ntokenizer_en = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-zh')\n# These special tokens are needed to reproduce the original tokenizer\ntokenizer_en.add_tokens([\"<2zh>\", \"<2vi>\"], special_tokens=True)\n\nsentence = \"The cat is on the table\"\n# This token is needed to identify the target language\ninput_sentence = \"<2vi> \" + sentence \ntranslated = model.generate(**tokenizer_en(input_sentence, return_tensors=\"pt\", padding=True))\noutput_sentence = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n```\n\n### Training results\n\nMIXED\n\n| Epoch | Bleu    |\n|:-----:|:-------:|\n| 1.0   | 26.2407 |\n| 2.0   | 32.6016 |\n| 3.0   | 35.4060 |\n| 4.0   | 36.6737 |\n| 5.0   | 37.3774 |\n\n\nPURE\n\n| Epoch | Bleu    |\n|:-----:|:-------:|\n| 1.0   | 37.3169 |\n| 2.0   | 37.4407 |\n| 3.0   | 37.6696 |\n| 4.0   | 37.8765 |\n| 5.0   | 38.0105 |\n\n", "size_bytes": "328974469", "downloads": 25}
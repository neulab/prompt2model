{"pretrained_model_name": "ltg/nort5-base", "description": "---\nlanguage:\n- 'no'\n- nb\n- nn\ninference: false\ntags:\n- T5\n- NorT5\n- Norwegian\n- encoder-decoder\nlicense: cc-by-4.0\npipeline_tag: text2text-generation\n---\n\n# NorT5 base\n\n<img src=\"https://huggingface.co/ltg/norbert3-base/resolve/main/norbert.png\" width=12.5%>\n\nThe official release of a new generation of NorT5 language models described in paper [**NorBench \u2014 A Benchmark for Norwegian Language Models**](https://arxiv.org/abs/2305.03880). Plese read the paper to learn more details about the model.\n\n\n## Other sizes:\n- [NorT5 xs (32M)](https://huggingface.co/ltg/nort5-xs)\n- [NorT5 small (88M)](https://huggingface.co/ltg/nort5-small)\n- [NorT5 base (228M)](https://huggingface.co/ltg/nort5-base)\n- [NorT5 large (808M)](https://huggingface.co/ltg/nort5-large)\n\n\n## Encoder-only NorBERT siblings:\n- [NorBERT 3 xs (15M)](https://huggingface.co/ltg/norbert3-xs)\n- [NorBERT 3 small (40M)](https://huggingface.co/ltg/norbert3-small)\n- [NorBERT 3 base (123M)](https://huggingface.co/ltg/norbert3-base)\n- [NorBERT 3 large (323M)](https://huggingface.co/ltg/norbert3-large)\n\n\n## Example usage\n\nThis model currently needs a custom wrapper from `modeling_nort5.py`, you should therefore load the model with `trust_remote_code=True`.\n\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"ltg/nort5-base\", trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ltg/nort5-base\", trust_remote_code=True)\n\n\n# MASKED LANGUAGE MODELING\n\nsentence = \"Brukseksempel: Elektrisk oppvarming. Definisjonen p\u00e5 ordet oppvarming er: \u00e5[MASK_0].\"\nencoding = tokenizer(sentence)\n\ninput_tensor = torch.tensor([encoding.input_ids])\noutput_tensor = model.generate(input_tensor, decoder_start_token_id=7, eos_token_id=8)\ntokenizer.decode(output_tensor.squeeze(), skip_special_tokens=True)\n\n# should output: ' varme opp et rom.'\n\n\n# PREFIX LANGUAGE MODELING\n# you need to finetune this model or use `nort5-{size}-lm` model, which is finetuned on prefix language modeling\n\nsentence = \"Brukseksempel: Elektrisk oppvarming. Definisjonen p\u00e5 ordet oppvarming er (Wikipedia) \"\nencoding = tokenizer(sentence)\n\ninput_tensor = torch.tensor([encoding.input_ids])\noutput_tensor = model.generate(input_tensor, max_new_tokens=50, num_beams=4, do_sample=False)\ntokenizer.decode(output_tensor.squeeze())\n\n# should output: [BOS]\u02c8oppvarming, det vil si at det skjer en endring i temperaturen i et medium, f.eks. en ovn eller en radiator, slik at den blir varmere eller kaldere, eller at den blir varmere eller kaldere, eller at den blir\n```\n\n\nThe following classes are currently implemented: `AutoModel`, `AutoModelForSeq2SeqLM`.\n\n## Cite us\n\n```bibtex\n@inproceedings{samuel-etal-2023-norbench,\n    title = \"{N}or{B}ench {--} A Benchmark for {N}orwegian Language Models\",\n    author = \"Samuel, David  and\n      Kutuzov, Andrey  and\n      Touileb, Samia  and\n      Velldal, Erik  and\n      {\\O}vrelid, Lilja  and\n      R{\\o}nningstad, Egil  and\n      Sigdel, Elina  and\n      Palatkina, Anna\",\n    booktitle = \"Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)\",\n    month = may,\n    year = \"2023\",\n    address = \"T{\\'o}rshavn, Faroe Islands\",\n    publisher = \"University of Tartu Library\",\n    url = \"https://aclanthology.org/2023.nodalida-1.61\",\n    pages = \"618--633\",\n    abstract = \"We present NorBench: a streamlined suite of NLP tasks and probes for evaluating Norwegian language models (LMs) on standardized data splits and evaluation metrics. We also introduce a range of new Norwegian language models (both encoder and encoder-decoder based). Finally, we compare and analyze their performance, along with other existing LMs, across the different benchmark tests of NorBench.\",\n}\n\n```", "size_bytes": "1062120285", "downloads": 359}
{"pretrained_model_name": "Roy029/mpyt5_e20", "description": "---\nlicense: openrail\n---\n# Model Card for mpyt5_e15\n\n<!-- Provide a quick summary of what the model is/does. [Optional] -->\n\u4e8b\u524d\u306b\u81ea\u7136\u8a00\u8a9e\u3060\u3051\u3067\u306a\u304fPython\u3092\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nPython Code (1.05GB)\n\n\n## Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n- MLM\n- python vocab (https://huggingface.co/kkuramitsu/mt5-pytoken)\n\n### Preprocessing\n\nmT5 + Python\n\n### Speeds, Sizes, Times\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n- mT5-small(300M Paramators)\n- max_length = 128\n\n# Model Version\n\n- *epoch5\uff1a https://huggingface.co/Roy029/mpyt5_e5\n- *epoch10\uff1a https://huggingface.co/Roy029/mpyt5_e10\n- *epoch15\uff1a https://huggingface.co/Roy029/mpyt5_e15\n- *epoch20\uff1a This Model", "size_bytes": "1200770885", "downloads": 7}
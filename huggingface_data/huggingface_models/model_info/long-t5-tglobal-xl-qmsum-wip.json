{"pretrained_model_name": "pszemraj/long-t5-tglobal-xl-qmsum-wip", "description": "---\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nlicense: apache-2.0\ndatasets:\n- pszemraj/qmsum-cleaned\nlanguage:\n- en\npipeline_tag: summarization\ninference: false\n---\n\n# long-t5-tglobal-xl-qmsum-wip\n\n> \u26a0\ufe0f warning - this is a work in progress \u26a0\ufe0f\n\n<a href=\"https://colab.research.google.com/gist/pszemraj/ea0ac20dae4ad84bea4ea64543f84a85/long-t5-tglobal-xl-qmsum-wip.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nThis model is a fine-tuned version of [google/long-t5-tglobal-xl](https://huggingface.co/google/long-t5-tglobal-xl) on the `pszemraj/qmsum-cleaned` dataset. \n- Refer to the [dataset card](https://huggingface.co/datasets/pszemraj/qmsum-cleaned) for details but this model was trained **with the task/prompt prefixes at the start of `input`** which means that **inference should be run in a similar fashion**.\n- an example of how to run inference is in the Colab notebook linked above.\n\nIt achieves the following results on the evaluation set:\n- Loss: 2.0505\n- Rouge1: 35.3881\n- Rouge2: 11.509\n- Rougel: 23.1543\n- Rougelsum: 31.3295\n- Gen Len: 80.8\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 7e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 2526\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.03\n- num_epochs: 3.0\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.5376        | 1.0   | 99   | 2.0104          | 35.8802 | 11.4595 | 23.6656 | 31.49     | 77.77   |\n| 1.499         | 2.0   | 198  | 2.0358          | 35.1265 | 11.549  | 23.1062 | 30.8815   | 88.88   |\n| 1.5034        | 3.0   | 297  | 2.0505          | 35.3881 | 11.509  | 23.1543 | 31.3295   | 80.8    |", "size_bytes": 11925614592, "downloads": 7}
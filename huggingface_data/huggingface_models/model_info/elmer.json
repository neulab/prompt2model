{"pretrained_model_name": "RUCAIBox/elmer", "description": "---\nlicense: apache-2.0\nlanguage:\n- en\ntags:\n- text-generation\n- non-autoregressive-generation\n- early-exit\n---\n\n# ELMER\nThe ELMER model was proposed in [**ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation**](https://arxiv.org/abs/2210.13304) by Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie and Ji-Rong Wen.\n\nThe detailed information and instructions can be found [https://github.com/RUCAIBox/ELMER](https://github.com/RUCAIBox/ELMER).\n\n## Model Description\nELMER is an efficient and effective PLM for NAR text generation, which generates tokens at different layers by leveraging the early exit technique.\n\nThe architecture of ELMER is a variant of the standard Transformer encoder-decoder and poses three technical contributions:\n\n1. For decoder, we replace the original masked multi-head attention with bi-directional multi-head attention akin to the encoder. Therefore, ELMER dynamically adjusts the output length by emitting an end token \"[EOS]\" at any position.\n2. Leveraging early exit, ELMER injects \"off-ramps\" at each decoder layer, which make predictions with intermediate hidden states. If ELMER exits at the $l$-th layer, we copy the $l$-th hidden states to the subsequent layers.\n3. ELMER utilizes a novel pre-training objective, layer permutation language modeling (LPLM), to pre-train on the large-scale corpus. LPLM permutes the exit layer for each token from 1 to the maximum layer $L$.\n\n## Examples\nTo fine-tune ELMER on non-autoregressive text generation:\n```python\n>>> from transformers import BartTokenizer as ElmerTokenizer\n>>> from transformers import BartForConditionalGeneration as ElmerForConditionalGeneration\n\n>>> tokenizer = ElmerTokenizer.from_pretrained(\"RUCAIBox/elmer\")\n>>> model = ElmerForConditionalGeneration.from_pretrained(\"RUCAIBox/elmer\")\n```\n\n## Citation\n```bibtex\n@article{lijunyi2022elmer,\n  title={ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation},\n  author={Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},\n  booktitle={EMNLP 2022},\n  year={2022}\n}\n```", "size_bytes": "557969145", "downloads": 332}
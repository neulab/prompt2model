{"pretrained_model_name": "leadawon/jeju-ko-nmt-v8", "description": "# ALERT!!!\n\n[leadawon/jeju-ko-nmt-v6](https://huggingface.co/leadawon/jeju-ko-nmt-v6) is better than leadawon/jeju-ko-nmt-v8\n\n6\ubc84\uc804 \uc131\ub2a5\uc774 \ub354 \uc88b\uc2b5\ub2c8\ub2e4!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n\n# tag\n\n---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: jeju-ko-nmt-v8\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# jeju-ko-nmt-v8\n\nThis model is a fine-tuned version of [leadawon/jeju-ko-nmt-v7](https://huggingface.co/leadawon/jeju-ko-nmt-v7) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2448\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-06\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 96\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss |\n|:-------------:|:-----:|:-----:|:---------------:|\n| 0.2684        | 0.04  | 500   | 0.2568          |\n| 0.2468        | 0.08  | 1000  | 0.2547          |\n| 0.2167        | 0.12  | 1500  | 0.2540          |\n| 0.1966        | 0.16  | 2000  | 0.2535          |\n| 0.1846        | 0.2   | 2500  | 0.2533          |\n| 0.1727        | 0.24  | 3000  | 0.2535          |\n| 0.1746        | 0.28  | 3500  | 0.2522          |\n| 0.1726        | 0.32  | 4000  | 0.2521          |\n| 0.1722        | 0.36  | 4500  | 0.2519          |\n| 0.1731        | 0.4   | 5000  | 0.2515          |\n| 0.1701        | 0.44  | 5500  | 0.2518          |\n| 0.168         | 0.48  | 6000  | 0.2515          |\n| 0.1706        | 0.52  | 6500  | 0.2509          |\n| 0.1659        | 0.56  | 7000  | 0.2514          |\n| 0.1702        | 0.6   | 7500  | 0.2509          |\n| 0.1667        | 0.64  | 8000  | 0.2510          |\n| 0.1661        | 0.68  | 8500  | 0.2508          |\n| 0.1647        | 0.72  | 9000  | 0.2510          |\n| 0.1632        | 0.76  | 9500  | 0.2510          |\n| 0.1655        | 0.8   | 10000 | 0.2506          |\n| 0.1645        | 0.84  | 10500 | 0.2508          |\n| 0.1617        | 0.88  | 11000 | 0.2508          |\n| 0.1627        | 0.91  | 11500 | 0.2511          |\n| 0.2764        | 0.95  | 12000 | 0.2478          |\n| 0.2755        | 0.99  | 12500 | 0.2462          |\n| 0.2275        | 1.03  | 13000 | 0.2464          |\n| 0.2201        | 1.07  | 13500 | 0.2463          |\n| 0.2207        | 1.11  | 14000 | 0.2463          |\n| 0.2202        | 1.15  | 14500 | 0.2462          |\n| 0.2194        | 1.19  | 15000 | 0.2460          |\n| 0.2177        | 1.23  | 15500 | 0.2461          |\n| 0.2187        | 1.27  | 16000 | 0.2460          |\n| 0.2184        | 1.31  | 16500 | 0.2459          |\n| 0.2182        | 1.35  | 17000 | 0.2457          |\n| 0.219         | 1.39  | 17500 | 0.2458          |\n| 0.2206        | 1.43  | 18000 | 0.2455          |\n| 0.2211        | 1.47  | 18500 | 0.2455          |\n| 0.2164        | 1.51  | 19000 | 0.2455          |\n| 0.2202        | 1.55  | 19500 | 0.2454          |\n| 0.2208        | 1.59  | 20000 | 0.2452          |\n| 0.2208        | 1.63  | 20500 | 0.2450          |\n| 0.2204        | 1.67  | 21000 | 0.2450          |\n| 0.2193        | 1.71  | 21500 | 0.2450          |\n| 0.221         | 1.75  | 22000 | 0.2451          |\n| 0.2168        | 1.79  | 22500 | 0.2450          |\n| 0.2169        | 1.83  | 23000 | 0.2449          |\n| 0.218         | 1.87  | 23500 | 0.2449          |\n| 0.2196        | 1.91  | 24000 | 0.2449          |\n| 0.2218        | 1.95  | 24500 | 0.2448          |\n| 0.2199        | 1.99  | 25000 | 0.2448          |\n\n\n### Framework versions\n\n- Transformers 4.26.0\n- Pytorch 1.13.1+cu116\n- Tokenizers 0.13.2\n", "size_bytes": "750621677", "downloads": 0}
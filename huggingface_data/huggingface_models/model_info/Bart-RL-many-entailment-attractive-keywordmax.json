{"pretrained_model_name": "Evan-Lin/Bart-RL-many-entailment-attractive-keywordmax", "description": "---\nlicense: apache-2.0\ntags:\n- trl\n- transformers\n- reinforcement-learning\n---\n\n# TRL Model\n\nThis is a [TRL language model](https://github.com/lvwerra/trl) that has been fine-tuned with reinforcement learning to\n guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.\n\n## Usage\n\nTo use this model for inference, first install the TRL library:\n\n```bash\npython -m pip install trl\n```\n\nYou can then generate text as follows:\n\n```python\nfrom transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=\"Evan-Lin//tmp/tmp0uc94902/Evan-Lin/Bart-RL-many-entailment-attractive\")\noutputs = generator(\"Hello, my llama is cute\")\n```\n\nIf you want to use the model for training or to obtain the outputs from the value head, load the model as follows:\n\n```python\nfrom transformers import AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"Evan-Lin//tmp/tmp0uc94902/Evan-Lin/Bart-RL-many-entailment-attractive\")\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"Evan-Lin//tmp/tmp0uc94902/Evan-Lin/Bart-RL-many-entailment-attractive\")\n\ninputs = tokenizer(\"Hello, my llama is cute\", return_tensors=\"pt\")\noutputs = model(**inputs, labels=inputs[\"input_ids\"])\n```\n", "size_bytes": "557974885", "downloads": 9}
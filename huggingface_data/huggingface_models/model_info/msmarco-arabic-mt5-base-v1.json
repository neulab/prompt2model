{"pretrained_model_name": "doc2query/msmarco-arabic-mt5-base-v1", "description": "---\nlanguage: ar\ndatasets:\n- unicamp-dl/mmarco\nwidget:\n- text: \"\u0628\u0627\u064a\u062b\u0648\u0646 (\u0628\u0627\u0644\u0625\u0646\u062c\u0644\u064a\u0632\u064a\u0629: Python)\u200f \u0647\u064a \u0644\u063a\u0629 \u0628\u0631\u0645\u062c\u0629\u060c \u0639\u0627\u0644\u064a\u0629 \u0627\u0644\u0645\u0633\u062a\u0648\u0649 \u0633\u0647\u0644\u0629 \u0627\u0644\u062a\u0639\u0644\u0645 \u0645\u0641\u062a\u0648\u062d\u0629 \u0627\u0644\u0645\u0635\u062f\u0631 \u0642\u0627\u0628\u0644\u0629 \u0644\u0644\u062a\u0648\u0633\u064a\u0639\u060c \u062a\u0639\u062a\u0645\u062f \u0623\u0633\u0644\u0648\u0628 \u0627\u0644\u0628\u0631\u0645\u062c\u0629 \u0627\u0644\u0643\u0627\u0626\u0646\u064a\u0629 (OOP). \u0644\u063a\u0629 \u0628\u0627\u064a\u062b\u0648\u0646 \u0647\u064a \u0644\u063a\u0629 \u0645\u064f\u0641\u0633\u064e\u0651\u0631\u0629\u060c \u0648\u0645\u064f\u062a\u0639\u062f\u0650\u062f\u0629 \u0627\u0644\u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0627\u062a\u060c \u0648\u062a\u0633\u062a\u062e\u062f\u0645 \u0628\u0634\u0643\u0644 \u0648\u0627\u0633\u0639 \u0641\u064a \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u0645\u062c\u0627\u0644\u0627\u062a\u060c \u0643\u0628\u0646\u0627\u0621 \u0627\u0644\u0628\u0631\u0627\u0645\u062c \u0627\u0644\u0645\u0633\u062a\u0642\u0644\u0629 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0648\u0627\u062c\u0647\u0627\u062a \u0627\u0644\u0631\u0633\u0648\u0645\u064a\u0629 \u0648\u0641\u064a \u062a\u0637\u0628\u064a\u0642\u0627\u062a \u0627\u0644\u0648\u064a\u0628\u060c \u0648\u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647\u0627 \u0643\u0644\u063a\u0629 \u0628\u0631\u0645\u062c\u0629 \u0646\u0635\u064a\u0629 \u0644\u0644\u062a\u062d\u0643\u0645 \u0641\u064a \u0623\u062f\u0627\u0621 \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u0628\u0631\u0645\u062c\u064a\u0627\u062a \u0645\u062b\u0644 \u0628\u0644\u0646\u062f\u0631. \u0628\u0634\u0643\u0644 \u0639\u0627\u0645\u060c \u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0628\u0627\u064a\u062b\u0648\u0646 \u0644\u0639\u0645\u0644 \u0627\u0644\u0628\u0631\u0627\u0645\u062c \u0627\u0644\u0628\u0633\u064a\u0637\u0629 \u0644\u0644\u0645\u0628\u062a\u062f\u0626\u064a\u0646\u060c \u0648\u0644\u0625\u0646\u062c\u0627\u0632 \u0627\u0644\u0645\u0634\u0627\u0631\u064a\u0639 \u0627\u0644\u0636\u062e\u0645\u0629 \u0641\u064a \u0627\u0644\u0648\u0642\u062a \u0646\u0641\u0633\u0647. \u063a\u0627\u0644\u0628\u0627\u064b \u0645\u0627 \u064a\u064f\u0646\u0635\u062d \u0627\u0644\u0645\u0628\u062a\u062f\u0624\u0648\u0646 \u0641\u064a \u0645\u064a\u062f\u0627\u0646 \u0627\u0644\u0628\u0631\u0645\u062c\u0629 \u0628\u062a\u0639\u0644\u0645 \u0647\u0630\u0647 \u0627\u0644\u0644\u063a\u0629 \u0644\u0623\u0646\u0647\u0627 \u0645\u0646 \u0628\u064a\u0646 \u0623\u0633\u0631\u0639 \u0627\u0644\u0644\u063a\u0627\u062a \u0627\u0644\u0628\u0631\u0645\u062c\u064a\u0629 \u062a\u0639\u0644\u0645\u0627\u064b.\"\n\nlicense: apache-2.0\n---\n\n# doc2query/msmarco-arabic-mt5-base-v1\n\nThis is a [doc2query](https://arxiv.org/abs/1904.08375) model based on mT5 (also known as [docT5query](https://cs.uwaterloo.ca/~jimmylin/publications/Nogueira_Lin_2019_docTTTTTquery-v2.pdf)).\n\nIt can be used for:\n- **Document expansion**: You generate for your paragraphs 20-40 queries and index the paragraphs and the generates queries in a standard BM25 index like Elasticsearch, OpenSearch, or Lucene. The generated queries help to close the lexical gap of lexical search, as the generate queries contain synonyms. Further, it re-weights words giving important words a higher weight even if they appear seldomn in a paragraph. In our [BEIR](https://arxiv.org/abs/2104.08663) paper we showed that BM25+docT5query is a powerful search engine. In the [BEIR repository](https://github.com/beir-cellar/beir) we have an example how to use docT5query with Pyserini.\n- **Domain Specific Training Data Generation**: It can be used to generate training data to learn an embedding model. In our [GPL-Paper](https://arxiv.org/abs/2112.07577) / [GPL Example on SBERT.net](https://www.sbert.net/examples/domain_adaptation/README.html#gpl-generative-pseudo-labeling) we have an example how to use the model to generate (query, text) pairs for a given collection of unlabeled texts. These pairs can then be used to train powerful dense embedding models.\n\n## Usage\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nmodel_name = 'doc2query/msmarco-arabic-mt5-base-v1'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ntext = \"\u0628\u0627\u064a\u062b\u0648\u0646 (\u0628\u0627\u0644\u0625\u0646\u062c\u0644\u064a\u0632\u064a\u0629: Python)\u200f \u0647\u064a \u0644\u063a\u0629 \u0628\u0631\u0645\u062c\u0629\u060c \u0639\u0627\u0644\u064a\u0629 \u0627\u0644\u0645\u0633\u062a\u0648\u0649 \u0633\u0647\u0644\u0629 \u0627\u0644\u062a\u0639\u0644\u0645 \u0645\u0641\u062a\u0648\u062d\u0629 \u0627\u0644\u0645\u0635\u062f\u0631 \u0642\u0627\u0628\u0644\u0629 \u0644\u0644\u062a\u0648\u0633\u064a\u0639\u060c \u062a\u0639\u062a\u0645\u062f \u0623\u0633\u0644\u0648\u0628 \u0627\u0644\u0628\u0631\u0645\u062c\u0629 \u0627\u0644\u0643\u0627\u0626\u0646\u064a\u0629 (OOP). \u0644\u063a\u0629 \u0628\u0627\u064a\u062b\u0648\u0646 \u0647\u064a \u0644\u063a\u0629 \u0645\u064f\u0641\u0633\u064e\u0651\u0631\u0629\u060c \u0648\u0645\u064f\u062a\u0639\u062f\u0650\u062f\u0629 \u0627\u0644\u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0627\u062a\u060c \u0648\u062a\u0633\u062a\u062e\u062f\u0645 \u0628\u0634\u0643\u0644 \u0648\u0627\u0633\u0639 \u0641\u064a \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u0645\u062c\u0627\u0644\u0627\u062a\u060c \u0643\u0628\u0646\u0627\u0621 \u0627\u0644\u0628\u0631\u0627\u0645\u062c \u0627\u0644\u0645\u0633\u062a\u0642\u0644\u0629 \u0628\u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0627\u0644\u0648\u0627\u062c\u0647\u0627\u062a \u0627\u0644\u0631\u0633\u0648\u0645\u064a\u0629 \u0648\u0641\u064a \u062a\u0637\u0628\u064a\u0642\u0627\u062a \u0627\u0644\u0648\u064a\u0628\u060c \u0648\u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645\u0647\u0627 \u0643\u0644\u063a\u0629 \u0628\u0631\u0645\u062c\u0629 \u0646\u0635\u064a\u0629 \u0644\u0644\u062a\u062d\u0643\u0645 \u0641\u064a \u0623\u062f\u0627\u0621 \u0627\u0644\u0639\u062f\u064a\u062f \u0645\u0646 \u0627\u0644\u0628\u0631\u0645\u062c\u064a\u0627\u062a \u0645\u062b\u0644 \u0628\u0644\u0646\u062f\u0631. \u0628\u0634\u0643\u0644 \u0639\u0627\u0645\u060c \u064a\u0645\u0643\u0646 \u0627\u0633\u062a\u062e\u062f\u0627\u0645 \u0628\u0627\u064a\u062b\u0648\u0646 \u0644\u0639\u0645\u0644 \u0627\u0644\u0628\u0631\u0627\u0645\u062c \u0627\u0644\u0628\u0633\u064a\u0637\u0629 \u0644\u0644\u0645\u0628\u062a\u062f\u0626\u064a\u0646\u060c \u0648\u0644\u0625\u0646\u062c\u0627\u0632 \u0627\u0644\u0645\u0634\u0627\u0631\u064a\u0639 \u0627\u0644\u0636\u062e\u0645\u0629 \u0641\u064a \u0627\u0644\u0648\u0642\u062a \u0646\u0641\u0633\u0647. \u063a\u0627\u0644\u0628\u0627\u064b \u0645\u0627 \u064a\u064f\u0646\u0635\u062d \u0627\u0644\u0645\u0628\u062a\u062f\u0624\u0648\u0646 \u0641\u064a \u0645\u064a\u062f\u0627\u0646 \u0627\u0644\u0628\u0631\u0645\u062c\u0629 \u0628\u062a\u0639\u0644\u0645 \u0647\u0630\u0647 \u0627\u0644\u0644\u063a\u0629 \u0644\u0623\u0646\u0647\u0627 \u0645\u0646 \u0628\u064a\u0646 \u0623\u0633\u0631\u0639 \u0627\u0644\u0644\u063a\u0627\u062a \u0627\u0644\u0628\u0631\u0645\u062c\u064a\u0629 \u062a\u0639\u0644\u0645\u0627\u064b.\"\n\n\ndef create_queries(para):\n    input_ids = tokenizer.encode(para, return_tensors='pt')\n    with torch.no_grad():\n        # Here we use top_k / top_k random sampling. It generates more diverse queries, but of lower quality\n        sampling_outputs = model.generate(\n            input_ids=input_ids,\n            max_length=64,\n            do_sample=True,\n            top_p=0.95,\n            top_k=10, \n            num_return_sequences=5\n            )\n        \n        # Here we use Beam-search. It generates better quality queries, but with less diversity\n        beam_outputs = model.generate(\n            input_ids=input_ids, \n            max_length=64, \n            num_beams=5, \n            no_repeat_ngram_size=2, \n            num_return_sequences=5, \n            early_stopping=True\n        )\n\n\n    print(\"Paragraph:\")\n    print(para)\n    \n    print(\"\\nBeam Outputs:\")\n    for i in range(len(beam_outputs)):\n        query = tokenizer.decode(beam_outputs[i], skip_special_tokens=True)\n        print(f'{i + 1}: {query}')\n\n    print(\"\\nSampling Outputs:\")\n    for i in range(len(sampling_outputs)):\n        query = tokenizer.decode(sampling_outputs[i], skip_special_tokens=True)\n        print(f'{i + 1}: {query}')\n\ncreate_queries(text)\n\n```\n\n**Note:** `model.generate()` is non-deterministic for top_k/top_n sampling. It produces different queries each time you run it.\n\n## Training\nThis model fine-tuned [google/mt5-base](https://huggingface.co/google/mt5-base) for 66k training steps (4 epochs on the 500k training pairs from MS MARCO). For the  training script, see the `train_script.py` in this repository.\n\nThe input-text was truncated to 320 word pieces. Output text was generated up to 64 word pieces. \n\nThis model was trained on a (query, passage) from the [mMARCO dataset](https://github.com/unicamp-dl/mMARCO).\n\n\n\n", "size_bytes": "2329700301", "downloads": 50}
{"pretrained_model_name": "din0s/t5-small-finetuned-en-to-it-hrs", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: t5-small-finetuned-en-to-it-hrs\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-finetuned-en-to-it-hrs\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.1558\n- Bleu: 9.8991\n- Gen Len: 51.8287\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 40\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu   | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:------:|:-------:|\n| 2.0084        | 1.0   | 1125  | 2.8804          | 4.4102 | 67.6067 |\n| 1.7918        | 2.0   | 2250  | 2.7757          | 6.1959 | 58.0313 |\n| 1.6944        | 3.0   | 3375  | 2.6845          | 6.9152 | 55.6953 |\n| 1.5955        | 4.0   | 4500  | 2.6219          | 7.3056 | 54.8213 |\n| 1.5304        | 5.0   | 5625  | 2.5659          | 7.9427 | 53.4173 |\n| 1.52          | 6.0   | 6750  | 2.5249          | 8.2049 | 53.678  |\n| 1.4934        | 7.0   | 7875  | 2.4853          | 8.6612 | 52.304  |\n| 1.4518        | 8.0   | 9000  | 2.4522          | 8.7991 | 52.6467 |\n| 1.4393        | 9.0   | 10125 | 2.4353          | 8.8251 | 52.7047 |\n| 1.4196        | 10.0  | 11250 | 2.4027          | 9.01   | 52.5387 |\n| 1.405         | 11.0  | 12375 | 2.3797          | 9.1513 | 52.0273 |\n| 1.3741        | 12.0  | 13500 | 2.3590          | 9.2401 | 52.3373 |\n| 1.3693        | 13.0  | 14625 | 2.3378          | 9.3611 | 52.1507 |\n| 1.3638        | 14.0  | 15750 | 2.3226          | 9.4213 | 52.2813 |\n| 1.3366        | 15.0  | 16875 | 2.3071          | 9.5199 | 52.1507 |\n| 1.3294        | 16.0  | 18000 | 2.2943          | 9.5296 | 51.9587 |\n| 1.3258        | 17.0  | 19125 | 2.2788          | 9.6231 | 51.5807 |\n| 1.3152        | 18.0  | 20250 | 2.2693          | 9.6586 | 51.8933 |\n| 1.3023        | 19.0  | 21375 | 2.2543          | 9.6762 | 51.5733 |\n| 1.3061        | 20.0  | 22500 | 2.2451          | 9.6926 | 51.6727 |\n| 1.3004        | 21.0  | 23625 | 2.2344          | 9.773  | 51.6527 |\n| 1.2839        | 22.0  | 24750 | 2.2242          | 9.7973 | 51.8113 |\n| 1.2869        | 23.0  | 25875 | 2.2161          | 9.8177 | 51.9073 |\n| 1.2819        | 24.0  | 27000 | 2.2115          | 9.8183 | 51.6707 |\n| 1.2642        | 25.0  | 28125 | 2.2037          | 9.7645 | 52.0853 |\n| 1.2685        | 26.0  | 29250 | 2.1984          | 9.7764 | 51.6927 |\n| 1.2609        | 27.0  | 30375 | 2.1934          | 9.7205 | 51.9647 |\n| 1.2585        | 28.0  | 31500 | 2.1834          | 9.8116 | 51.7373 |\n| 1.2564        | 29.0  | 32625 | 2.1811          | 9.8547 | 51.8553 |\n| 1.2563        | 30.0  | 33750 | 2.1766          | 9.8346 | 51.7293 |\n| 1.258         | 31.0  | 34875 | 2.1748          | 9.8204 | 51.6747 |\n| 1.2391        | 32.0  | 36000 | 2.1708          | 9.8485 | 51.7647 |\n| 1.2364        | 33.0  | 37125 | 2.1644          | 9.8503 | 51.6713 |\n| 1.2436        | 34.0  | 38250 | 2.1629          | 9.8457 | 51.76   |\n| 1.2408        | 35.0  | 39375 | 2.1614          | 9.8899 | 51.6893 |\n| 1.2564        | 36.0  | 40500 | 2.1591          | 9.8867 | 51.706  |\n| 1.2318        | 37.0  | 41625 | 2.1575          | 9.866  | 51.782  |\n| 1.2423        | 38.0  | 42750 | 2.1570          | 9.8756 | 51.8933 |\n| 1.2399        | 39.0  | 43875 | 2.1558          | 9.8871 | 51.7967 |\n| 1.2339        | 40.0  | 45000 | 2.1558          | 9.8991 | 51.8287 |\n\n\n### Framework versions\n\n- Transformers 4.22.1\n- Pytorch 1.12.1\n- Datasets 2.5.1\n- Tokenizers 0.11.0\n", "size_bytes": "242070267", "downloads": 2}
{"pretrained_model_name": "Mauregato/qqq-finetuned-on-calls", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: qqq-finetuned-on-calls\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# qqq-finetuned-on-calls\n\nThis model is a fine-tuned version of [bragovo/qqq](https://huggingface.co/bragovo/qqq) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0021\n- Rouge-1: 1.0\n- Rouge-2: 1.0\n- Rouge-l: 1.0\n- Gen Len: 11.0\n- Avg Rouge F: 1.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 3\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 50\n- num_epochs: 100\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge-1 | Rouge-2 | Rouge-l | Gen Len | Avg Rouge F |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:-------:|:-----------:|\n| 2.1855        | 3.12  | 25   | 1.4282          | 0.0     | 0.0     | 0.0     | 15.0    | 0.0         |\n| 1.5665        | 6.25  | 50   | 0.6420          | 0.1818  | 0.0     | 0.1818  | 12.0    | 0.1212      |\n| 1.1046        | 9.38  | 75   | 0.2184          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.8218        | 12.5  | 100  | 0.1098          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.606         | 15.62 | 125  | 0.0749          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.5488        | 18.75 | 150  | 0.0577          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.4161        | 21.88 | 175  | 0.0684          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.3196        | 25.0  | 200  | 0.0570          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.2929        | 28.12 | 225  | 0.0416          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.2519        | 31.25 | 250  | 0.0247          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.178         | 34.38 | 275  | 0.0118          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.1603        | 37.5  | 300  | 0.0064          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.1684        | 40.62 | 325  | 0.0051          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.1326        | 43.75 | 350  | 0.0051          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.1349        | 46.88 | 375  | 0.0064          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.1105        | 50.0  | 400  | 0.0061          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.1026        | 53.12 | 425  | 0.0049          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.0936        | 56.25 | 450  | 0.0030          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.0704        | 59.38 | 475  | 0.0025          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.0699        | 62.5  | 500  | 0.0021          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.0863        | 65.62 | 525  | 0.0020          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.0595        | 68.75 | 550  | 0.0024          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.0594        | 71.88 | 575  | 0.0028          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.0683        | 75.0  | 600  | 0.0026          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n| 0.074         | 78.12 | 625  | 0.0025          | 1.0     | 1.0     | 1.0     | 11.0    | 1.0         |\n\n\n### Framework versions\n\n- Transformers 4.29.0\n- Pytorch 2.0.0+cu118\n- Datasets 2.12.0\n- Tokenizers 0.13.3\n", "size_bytes": "977334453", "downloads": 2}
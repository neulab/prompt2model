{"pretrained_model_name": "pvduy/SteamSHP-flan-t5-xl-finetuned-summarize-tldr", "description": "### Training Code\n```python\nfrom torch.utils.data import dataset\nfrom datasets import load_dataset, load_from_disk\nfrom tqdm import tqdm\nfrom datasets import load_metric\nfrom transformers import (\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq\n)\nimport evaluate\nimport os\nfrom datasets import load_dataset\nimport numpy as np\n\nMAX_LENGTH_INPUT = 512+128\nMAX_LENGTH_OUTPUT = 2\n\nfrom datasets import load_dataset\n\nclass Seq2SeqDataset(dataset.Dataset):\n\n    def __init__(self, tokenizer, type_data='train'):\n        \n        # Set up the datasets\n        data_path = \"CarperAI/openai_summarize_comparisons\"\n        if type_data == 'train':\n            dataset = load_dataset(\"CarperAI/openai_summarize_comparisons\", split=\"train\")\n        else:\n            dataset = load_dataset(\"CarperAI/openai_summarize_comparisons\", split=\"test\").select(range(20000))\n        self.prompts = []\n        self.outputs = []\n        inputs = dataset[\"prompt\"]\n        choosen = dataset[\"chosen\"]\n        rejected = dataset[\"rejected\"]\n        for i, (inp, ch, re) in enumerate(zip(inputs, choosen, rejected)):\n            choice_first = np.random.choice([ch, re])\n            res = \"A\" if choice_first == ch else \"B\"\n            choice_second = ch if choice_first == re else re\n            prompt = f\"\"\"POST: {inp}\\n\\nRESPONSE A: {choice_first}\\n\\nRESPONSE B: {choice_second}\\n\\nWhich response is better? RESPONSE\"\"\"\n            output = f\"{res}\"\n            self.prompts.append(prompt)\n            self.outputs.append(output)\n        print(\"Example prompt: \", self.prompts[0])\n        print(\"Example output: \", self.outputs[0])\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.prompts)\n    \n    def __getitem__(self, idx):\n        input_text = self.prompts[idx]\n        output_text = self.outputs[idx]\n\n        model_input = self.tokenizer(\n            input_text,\n            max_length=MAX_LENGTH_INPUT, \n            padding='max_length',\n            truncation=True\n        )\n        with self.tokenizer.as_target_tokenizer():\n            labels = self.tokenizer(\n                output_text,\n                max_length=MAX_LENGTH_OUTPUT,\n                padding='max_length',\n                truncation=True\n            )[\"input_ids\"]\n            model_input['labels'] = labels\n            model_input['labels'] = [-100 if token == self.tokenizer.pad_token_id else token for token in model_input['labels']]\n        return model_input\n    \nimport wandb \nwandb.init(name=\"stanfordnlp/SteamSHP-flan-t5-xl\", project=\"trlx\", entity=\"pvduy\")\n\n\nif __name__==\"__main__\":\n    config = {\n        \"logging_steps\": 100,\n        \"eval_steps\": 100,\n        \"save_steps\": 500,\n        \"batch_size\": 4,\n        \"batch_size_val\": 4,\n        \"warmup_steps\": 100,\n        \"accum_steps\": 2,\n        \"num_beams\": 3,\n        \"output_dir\": \"flan-t5-rm\",\n    }\n    \n    accuracy_metric = evaluate.load(\"accuracy\")\n    def compute_metrics(pred):\n        labels_ids = pred.label_ids\n        pred_ids = pred.predictions\n        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n        labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n        acc = sum(np.array(labels_str) == np.array(pred_str)) / len(labels_str)\n        return {\"accuracy\": acc}\n\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=config[\"output_dir\"],\n        do_train=True,\n        num_train_epochs=5,\n        do_eval=False,\n        predict_with_generate=True,\n        adam_beta1=0.9,\n        adam_beta2=0.999,\n        learning_rate=5e-5,\n        half_precision_backend=True,\n        bf16=True,\n        per_device_train_batch_size=config[\"batch_size\"],\n        per_device_eval_batch_size=config[\"batch_size_val\"],\n        logging_steps=config[\"logging_steps\"],\n        evaluation_strategy=\"epoch\",\n        warmup_steps=config[\"warmup_steps\"],\n        eval_accumulation_steps=1,\n        lr_scheduler_type=\"linear\",\n        save_strategy=\"epoch\",\n        gradient_accumulation_steps=config[\"accum_steps\"],\n        deepspeed='configs/ds_configs/ds_config_gpt_2.json',\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"stanfordnlp/SteamSHP-flan-t5-xl\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(\"stanfordnlp/SteamSHP-flan-t5-xl\")\n    \n    train_dataset = Seq2SeqDataset(tokenizer, type_data='train')\n    val_dataset = Seq2SeqDataset(tokenizer, type_data='val')\n    print(\"Train dataset size: \", len(train_dataset))\n    print(\"Val dataset size: \", len(val_dataset))\n\n    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Number of trainable parameters: {params}\")\n\n    trainer = Seq2SeqTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n```\n\n### Inference Code\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom datasets import load_dataset\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\ndataset = load_dataset(\"CarperAI/openai_summarize_comparisons\", split=\"test\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"flan-t5-rm/checkpoint-4338/\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"flan-t5-rm/checkpoint-4338/\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\ndf = dataset.to_pandas()\npredictions = []\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    prompt = f\"\"\"POST: {row[\"prompt\"]}\\n\\nRESPONSE A: {row[\"chosen\"]}\\n\\nRESPONSE B: {row[\"rejected\"]}\\n\\nWhich response is better? RESPONSE\"\"\"\n    x = tokenizer([prompt], return_tensors='pt').input_ids.to(device)\n    y = model.generate(x, max_new_tokens=1)\n    predictions.append(tokenizer.batch_decode(y, skip_special_tokens=True)[0])\n\nprint(\"Accuracy: \", sum(np.array(predictions) == 'A') / len(predictions))\n```", "size_bytes": 11925413888, "downloads": 4}
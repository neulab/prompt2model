{"pretrained_model_name": "Salesforce/codet5p-6b", "description": "---\nlicense: bsd-3-clause\n---\n\n# CodeT5+ 6B\n\n## Model description\n\n[CodeT5+](https://github.com/salesforce/CodeT5/tree/main/CodeT5+) is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. _encoder-only_, _decoder-only_, and _encoder-decoder_) to support a wide range of code understanding and generation tasks. \nIt is introduced in the paper:\n\n[CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/pdf/2305.07922.pdf)\nby [Yue Wang](https://yuewang-cuhk.github.io/)\\*, [Hung Le](https://sites.google.com/view/henryle2018/home?pli=1)\\*, [Akhilesh Deepak Gotmare](https://akhileshgotmare.github.io/), [Nghi D.Q. Bui](https://bdqnghi.github.io/), [Junnan Li](https://sites.google.com/site/junnanlics), [Steven C.H. Hoi](https://sites.google.com/view/stevenhoi/home) (* indicates equal contribution).\n\nCompared to the original CodeT5 family (base: `220M`, large: `770M`), CodeT5+ is pretrained with a diverse set of pretraining tasks including _span denoising_, _causal language modeling_, _contrastive learning_, and _text-code matching_ to learn rich representations from both unimodal code data and bimodal code-text data. \nAdditionally, it employs a simple yet effective _compute-efficient pretraining_ method to initialize the model components with frozen off-the-shelf LLMs such as [CodeGen](https://github.com/salesforce/CodeGen) to efficiently scale up the model (i.e. `2B`, `6B`, `16B`), and adopts a \"shallow encoder and deep decoder\" architecture. \nFurthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B) following [Code Alpaca](https://github.com/sahil280114/codealpaca).  \n\n## How to use\n\nThis model can be easily loaded using the `AutoModelForSeq2SeqLM` functionality and employs the same tokenizer as [CodeGen](https://github.com/salesforce/CodeGen).\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ncheckpoint = \"Salesforce/codet5p-6b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint,\n                                              torch_dtype=torch.float16,\n                                              trust_remote_code=True).to(device)\n\nencoding = tokenizer(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\nencoding['decoder_input_ids'] = encoding['input_ids'].clone()\noutputs = model.generate(**encoding, max_length=15)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n## Pretraining data\n\nThis checkpoint is trained on the stricter permissive subset of the deduplicated version of the [github-code dataset](https://huggingface.co/datasets/codeparrot/github-code).\nThe data is preprocessed by reserving only permissively licensed code (\"mit\" \u201capache-2\u201d, \u201cbsd-3-clause\u201d, \u201cbsd-2-clause\u201d, \u201ccc0-1.0\u201d, \u201cunlicense\u201d, \u201cisc\u201d).\nSupported languages (9 in total) are as follows:\n`c`, `c++`, `c-sharp`,  `go`, `java`, `javascript`,  `php`, `python`, `ruby.`\n\n## Training procedure\n\nThis checkpoint is initialized from off-the-shelf LLMs, i.e. its encoder is initialized from [CodeGen-350M-mono](https://huggingface.co/Salesforce/codegen-350M-mono) and its decoder is initialized from [CodeGen-6B-mono](https://huggingface.co/Salesforce/codegen-6B-mono).\nIt is trained on the unimodal code data at the first-stage pretraining, which includes a diverse set of pretraining tasks including _span denoising_ and two variants of _causal language modeling_.\nAfter that, it is further trained on the Python subset with the causal language modeling objective for another epochs to better adapt for Python code generation. \nPlease refer to the paper for more details.\n\n## Evaluation results\n\nCodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_.\nSpecifically, CodeT5+ yields substantial performance gains on many downstream tasks compared to their SoTA baselines, e.g.,\n8 text-to-code retrieval tasks (+3.2 avg. MRR), 2 line-level code completion tasks (+2.1 avg. Exact Match), and 2 retrieval-augmented code generation tasks (+5.8 avg. BLEU-4). \nIn 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters. \nParticularly, in the zero-shot text-to-code generation task on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode\nPlease refer to the [paper](https://arxiv.org/pdf/2305.07922.pdf) for more details.\n\n\n## BibTeX entry and citation info\n\n```bibtex\n@article{wang2023codet5plus,\n  title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation},\n  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi D.Q. and Li, Junnan and Hoi, Steven C. H.},\n  journal={arXiv preprint},\n  year={2023}\n}\n```", "size_bytes": 15105998848, "downloads": 1074}
{"pretrained_model_name": "xfbai/AMRBART-large-finetuned-AMR3.0-AMR2Text", "description": "---\nlanguage: en\ntags:\n- AMRBART\nlicense: mit\n---\n\n## AMRBART-large-finetuned-AMR3.0-AMR2Text\n\nThis model is a fine-tuned version of [AMRBART-large](https://huggingface.co/xfbai/AMRBART-large) on an AMR3.0 dataset. It achieves a sacre-bleu score of 45.0 on the evaluation set: More details are introduced in the paper: [Graph Pre-training for AMR Parsing and Generation](https://arxiv.org/pdf/2203.07836.pdf) by bai et al. in ACL 2022.\n\n## Model description\nSame with AMRBART.\n\n## Training data\n\nThe model is finetuned on [AMR2.0](https://catalog.ldc.upenn.edu/LDC2020T02), a dataset consisting of 55,635\ntraining instances, 1,722 validation instances, and 1,898 test instances.\n\n## Intended uses & limitations\n\nYou can use the model for AMR-to-text generation, but it's mostly intended to be used in the domain of News. \n\n## How to use\nHere is how to initialize this model in PyTorch:\n\n```python\nfrom transformers import BartForConditionalGeneration\nmodel = BartForConditionalGeneration.from_pretrained(\"xfbai/AMRBART-large-finetuned-AMR3.0-AMR2Text\")\n```\nPlease refer to [this repository](https://github.com/muyeby/AMRBART) for tokenizer initialization and data preprocessing.\n\n\n## BibTeX entry and citation info\nPlease cite this paper if you find this model helpful\n\n```bibtex\n@inproceedings{bai-etal-2022-graph,\n    title = \"Graph Pre-training for {AMR} Parsing and Generation\",\n    author = \"Bai, Xuefeng  and\n      Chen, Yulong and\n      Zhang, Yue\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"todo\",\n    doi = \"todo\",\n    pages = \"todo\"\n}\n```", "size_bytes": "1640243311", "downloads": 4}
{"pretrained_model_name": "EColi/sponsorblock-base-v1", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: out\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# out\n\nThis model is a fine-tuned version of [/1TB_SSD/SB_AI/out_epoch1/out/checkpoint-1115000/](https://huggingface.co//1TB_SSD/SB_AI/out_epoch1/out/checkpoint-1115000/) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0645\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 2518227880\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2.0\n\n### Training results\n\n| Training Loss | Epoch | Step    | Validation Loss |\n|:-------------:|:-----:|:-------:|:---------------:|\n| 0.0867        | 0.07  | 75000   | 0.0742          |\n| 0.0783        | 0.13  | 150000  | 0.0695          |\n| 0.0719        | 0.2   | 225000  | 0.0732          |\n| 0.0743        | 0.27  | 300000  | 0.0663          |\n| 0.0659        | 0.34  | 375000  | 0.0686          |\n| 0.0664        | 0.4   | 450000  | 0.0683          |\n| 0.0637        | 0.47  | 525000  | 0.0680          |\n| 0.0655        | 0.54  | 600000  | 0.0641          |\n| 0.0676        | 0.6   | 675000  | 0.0644          |\n| 0.0704        | 0.67  | 750000  | 0.0645          |\n| 0.0687        | 0.74  | 825000  | 0.0610          |\n| 0.059         | 0.81  | 900000  | 0.0652          |\n| 0.0666        | 0.87  | 975000  | 0.0619          |\n| 0.0624        | 0.94  | 1050000 | 0.0619          |\n| 0.0625        | 1.01  | 1125000 | 0.0667          |\n| 0.0614        | 1.03  | 1150000 | 0.0658          |\n| 0.0597        | 1.05  | 1175000 | 0.0683          |\n| 0.0629        | 1.07  | 1200000 | 0.0691          |\n| 0.0603        | 1.1   | 1225000 | 0.0678          |\n| 0.0601        | 1.12  | 1250000 | 0.0746          |\n| 0.0606        | 1.14  | 1275000 | 0.0691          |\n| 0.0671        | 1.16  | 1300000 | 0.0702          |\n| 0.0625        | 1.19  | 1325000 | 0.0661          |\n| 0.0617        | 1.21  | 1350000 | 0.0688          |\n| 0.0579        | 1.23  | 1375000 | 0.0679          |\n| 0.0663        | 1.25  | 1400000 | 0.0634          |\n| 0.0583        | 1.28  | 1425000 | 0.0638          |\n| 0.0623        | 1.3   | 1450000 | 0.0681          |\n| 0.0615        | 1.32  | 1475000 | 0.0670          |\n| 0.0592        | 1.34  | 1500000 | 0.0666          |\n| 0.0626        | 1.37  | 1525000 | 0.0666          |\n| 0.063         | 1.39  | 1550000 | 0.0647          |\n| 0.0648        | 1.41  | 1575000 | 0.0653          |\n| 0.0611        | 1.43  | 1600000 | 0.0700          |\n| 0.0622        | 1.46  | 1625000 | 0.0634          |\n| 0.0617        | 1.48  | 1650000 | 0.0651          |\n| 0.0613        | 1.5   | 1675000 | 0.0634          |\n| 0.0639        | 1.52  | 1700000 | 0.0661          |\n| 0.0615        | 1.54  | 1725000 | 0.0644          |\n| 0.0605        | 1.57  | 1750000 | 0.0662          |\n| 0.0622        | 1.59  | 1775000 | 0.0656          |\n| 0.0585        | 1.61  | 1800000 | 0.0633          |\n| 0.0628        | 1.63  | 1825000 | 0.0625          |\n| 0.0638        | 1.66  | 1850000 | 0.0662          |\n| 0.0599        | 1.68  | 1875000 | 0.0664          |\n| 0.0583        | 1.7   | 1900000 | 0.0668          |\n| 0.0543        | 1.72  | 1925000 | 0.0631          |\n| 0.06          | 1.75  | 1950000 | 0.0629          |\n| 0.0615        | 1.77  | 1975000 | 0.0644          |\n| 0.0587        | 1.79  | 2000000 | 0.0663          |\n| 0.0647        | 1.81  | 2025000 | 0.0654          |\n| 0.0604        | 1.84  | 2050000 | 0.0639          |\n| 0.0641        | 1.86  | 2075000 | 0.0636          |\n| 0.0604        | 1.88  | 2100000 | 0.0636          |\n| 0.0654        | 1.9   | 2125000 | 0.0652          |\n| 0.0588        | 1.93  | 2150000 | 0.0638          |\n| 0.0616        | 1.95  | 2175000 | 0.0657          |\n| 0.0598        | 1.97  | 2200000 | 0.0646          |\n| 0.0633        | 1.99  | 2225000 | 0.0645          |\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.1+cu113\n- Datasets 1.17.0\n- Tokenizers 0.10.3\n", "size_bytes": "891703231", "downloads": 9}
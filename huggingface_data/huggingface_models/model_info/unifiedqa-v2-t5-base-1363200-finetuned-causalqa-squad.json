{"pretrained_model_name": "andreaschandra/unifiedqa-v2-t5-base-1363200-finetuned-causalqa-squad", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: unifiedqa-v2-t5-base-1363200-finetuned-causalqa-squad\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# unifiedqa-v2-t5-base-1363200-finetuned-causalqa-squad\n\nThis model is a fine-tuned version of [allenai/unifiedqa-v2-t5-base-1363200](https://huggingface.co/allenai/unifiedqa-v2-t5-base-1363200) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.2574\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 0.7378        | 0.05  | 73   | 1.1837          |\n| 0.6984        | 0.1   | 146  | 0.8918          |\n| 0.4511        | 0.15  | 219  | 0.8342          |\n| 0.4696        | 0.2   | 292  | 0.7642          |\n| 0.295         | 0.25  | 365  | 0.7996          |\n| 0.266         | 0.3   | 438  | 0.7773          |\n| 0.2372        | 0.35  | 511  | 0.8592          |\n| 0.2881        | 0.39  | 584  | 0.8440          |\n| 0.2578        | 0.44  | 657  | 0.8306          |\n| 0.2733        | 0.49  | 730  | 0.8228          |\n| 0.2073        | 0.54  | 803  | 0.8419          |\n| 0.2683        | 0.59  | 876  | 0.8241          |\n| 0.2693        | 0.64  | 949  | 0.8573          |\n| 0.355         | 0.69  | 1022 | 0.8204          |\n| 0.2246        | 0.74  | 1095 | 0.8530          |\n| 0.2468        | 0.79  | 1168 | 0.8410          |\n| 0.3102        | 0.84  | 1241 | 0.8035          |\n| 0.2115        | 0.89  | 1314 | 0.8262          |\n| 0.1855        | 0.94  | 1387 | 0.8560          |\n| 0.1772        | 0.99  | 1460 | 0.8747          |\n| 0.1509        | 1.04  | 1533 | 0.9132          |\n| 0.1871        | 1.09  | 1606 | 0.8920          |\n| 0.1624        | 1.14  | 1679 | 0.9085          |\n| 0.1404        | 1.18  | 1752 | 0.9460          |\n| 0.1639        | 1.23  | 1825 | 0.9812          |\n| 0.0983        | 1.28  | 1898 | 0.9790          |\n| 0.1395        | 1.33  | 1971 | 0.9843          |\n| 0.1439        | 1.38  | 2044 | 0.9877          |\n| 0.1397        | 1.43  | 2117 | 1.0338          |\n| 0.1095        | 1.48  | 2190 | 1.0589          |\n| 0.1228        | 1.53  | 2263 | 1.0498          |\n| 0.1246        | 1.58  | 2336 | 1.0923          |\n| 0.1438        | 1.63  | 2409 | 1.0995          |\n| 0.1305        | 1.68  | 2482 | 1.0867          |\n| 0.1077        | 1.73  | 2555 | 1.1013          |\n| 0.2104        | 1.78  | 2628 | 1.0765          |\n| 0.1633        | 1.83  | 2701 | 1.0796          |\n| 0.1658        | 1.88  | 2774 | 1.0314          |\n| 0.1358        | 1.92  | 2847 | 0.9823          |\n| 0.1571        | 1.97  | 2920 | 0.9826          |\n| 0.1127        | 2.02  | 2993 | 1.0324          |\n| 0.0927        | 2.07  | 3066 | 1.0679          |\n| 0.0549        | 2.12  | 3139 | 1.1069          |\n| 0.0683        | 2.17  | 3212 | 1.1624          |\n| 0.0677        | 2.22  | 3285 | 1.1174          |\n| 0.0615        | 2.27  | 3358 | 1.1431          |\n| 0.0881        | 2.32  | 3431 | 1.1721          |\n| 0.0807        | 2.37  | 3504 | 1.1885          |\n| 0.0955        | 2.42  | 3577 | 1.1991          |\n| 0.0779        | 2.47  | 3650 | 1.1999          |\n| 0.11          | 2.52  | 3723 | 1.1774          |\n| 0.0852        | 2.57  | 3796 | 1.2095          |\n| 0.0616        | 2.62  | 3869 | 1.1824          |\n| 0.072         | 2.67  | 3942 | 1.2397          |\n| 0.1055        | 2.71  | 4015 | 1.2181          |\n| 0.0806        | 2.76  | 4088 | 1.2159          |\n| 0.0684        | 2.81  | 4161 | 1.1864          |\n| 0.0869        | 2.86  | 4234 | 1.1816          |\n| 0.1023        | 2.91  | 4307 | 1.1717          |\n| 0.0583        | 2.96  | 4380 | 1.1477          |\n| 0.0684        | 3.01  | 4453 | 1.1662          |\n| 0.0319        | 3.06  | 4526 | 1.2174          |\n| 0.0609        | 3.11  | 4599 | 1.1947          |\n| 0.0435        | 3.16  | 4672 | 1.1821          |\n| 0.0417        | 3.21  | 4745 | 1.1964          |\n| 0.0502        | 3.26  | 4818 | 1.2140          |\n| 0.0844        | 3.31  | 4891 | 1.2028          |\n| 0.0692        | 3.36  | 4964 | 1.2215          |\n| 0.0366        | 3.41  | 5037 | 1.2136          |\n| 0.0615        | 3.46  | 5110 | 1.2224          |\n| 0.0656        | 3.5   | 5183 | 1.2468          |\n| 0.0469        | 3.55  | 5256 | 1.2554          |\n| 0.0475        | 3.6   | 5329 | 1.2804          |\n| 0.0998        | 3.65  | 5402 | 1.2035          |\n| 0.0505        | 3.7   | 5475 | 1.2095          |\n| 0.0459        | 3.75  | 5548 | 1.2064          |\n| 0.0256        | 3.8   | 5621 | 1.2164          |\n| 0.0831        | 3.85  | 5694 | 1.2154          |\n| 0.0397        | 3.9   | 5767 | 1.2126          |\n| 0.0449        | 3.95  | 5840 | 1.2174          |\n| 0.0322        | 4.0   | 5913 | 1.2288          |\n| 0.059         | 4.05  | 5986 | 1.2274          |\n| 0.0382        | 4.1   | 6059 | 1.2228          |\n| 0.0202        | 4.15  | 6132 | 1.2177          |\n| 0.0328        | 4.2   | 6205 | 1.2305          |\n| 0.0407        | 4.24  | 6278 | 1.2342          |\n| 0.0356        | 4.29  | 6351 | 1.2448          |\n| 0.0414        | 4.34  | 6424 | 1.2537          |\n| 0.0448        | 4.39  | 6497 | 1.2540          |\n| 0.0545        | 4.44  | 6570 | 1.2552          |\n| 0.0492        | 4.49  | 6643 | 1.2570          |\n| 0.0293        | 4.54  | 6716 | 1.2594          |\n| 0.0498        | 4.59  | 6789 | 1.2562          |\n| 0.0349        | 4.64  | 6862 | 1.2567          |\n| 0.0497        | 4.69  | 6935 | 1.2550          |\n| 0.0194        | 4.74  | 7008 | 1.2605          |\n| 0.0255        | 4.79  | 7081 | 1.2590          |\n| 0.0212        | 4.84  | 7154 | 1.2571          |\n| 0.0231        | 4.89  | 7227 | 1.2583          |\n| 0.0399        | 4.94  | 7300 | 1.2580          |\n| 0.0719        | 4.99  | 7373 | 1.2574          |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.12.1+cu113\n- Datasets 2.7.0\n- Tokenizers 0.13.2\n", "size_bytes": "891700799", "downloads": 2}
{"pretrained_model_name": "spybot/Timpi_Wilson", "description": "---\nlicense: apache-2.0\ninference: false\n---\n\n# TimpiWilson Model Card\n\n## Model details\n\n**Model type:**\nTimpiWilson is an open-source chatbot trained by fine-tuning Flan-t5-xl (3B parameters) on user-shared conversations collected from ShareGPT and conversation collection created from TImpi's index.\nIt is based on an encoder-decoder transformer architecture, and can autoregressively generate responses to users' inputs. \n\n**Model date:**\nTimpiWilson was trained on June 2023.\n\n**Organizations developing the model:**\nThe Timpi developers.\n\n\n**License:**\nApache License 2.0\n\n\n## Intended use\n**Primary intended uses:**\nThis version ot TimpiWilson is to showcase the abilities of the model.\n\n**Primary intended users:**\nThe primary intended users of the model are entrepreneurs and researchers in natural language processing, machine learning, and artificial intelligence.\n\n## Training dataset\n70K conversations collected from ShareGPT.com.\n40K conversations collected from Timpi's index\n\n## Training details\nIt processes the data in the form of question answering. Each Wilson response is processed as an answer, and previous conversations between the user and the Wilson are processed as the question.\nThe encoder bi-directionally encodes a question into a hidden representation. The decoder uses cross-attention to attend to this representation while generating an answer uni-directionally from a start token.\nThis model is fine-tuned for 3 epochs, with a max learning rate 2e-5, warmup ratio 0.03, and a cosine learning rate schedule. \n\n## Evaluation dataset\nA preliminary evaluation of the model quality is conducted by creating a set of 80 diverse questions and utilizing GPT-4 to judge the model outputs.\n", "size_bytes": "6706187737", "downloads": 3}
{"pretrained_model_name": "dehio/german-qg-t5-drink600", "description": "---\nlicense: mit\nwidget:\n- text: \"generate question: Der Monk Sour Drink ist ein somit eine aromatische \u00dcberraschung, die sowohl <hl>im Sommer wie auch zu Silvester<hl> funktioniert.\"\nlanguage:\n- de\ntags:\n- question generation\ndatasets:\n- deepset/germanquad\nmodel-index:\n- name: german-qg-t5-drink600\n  results: []\n---\n\n# german-qg-t5-drink600\n\nThis model is fine-tuned in question generation in German. The expected answer must be highlighted with &lt;hl> token. It is based on [german-qg-t5-quad](https://huggingface.co/dehio/german-qg-t5-quad) and further pre-trained on drink related questions.\n\n## Task example\n\n#### Input\n\ngenerate question: Der Monk Sour Drink ist ein somit eine aromatische \u00dcberraschung, \ndie sowohl &lt;hl>im Sommer wie auch zu Silvester&lt;hl> funktioniert.\n\n#### Expected Question\nZu welchen Gelegenheiten passt der Monk Sour gut?\n\n## Model description\n\nThe model is based on [german-qg-t5-quad](https://huggingface.co/dehio/german-qg-t5-quad), which was pre-trained on [GermanQUAD](https://www.deepset.ai/germanquad). We further pre-trained it on questions annotated on drink receipts from [Mixology](https://mixology.eu/) (\"drink600\"). \nWe have not yet open sourced the dataset, since we do not own copyright on the source material.\n\n## Training and evaluation data\n\nThe training script can be accessed [here](https://github.com/d-e-h-i-o/german-qg).\n\n## Evaluation\n\nIt achieves a **BLEU-4 score of 29.80** on the drink600 test set (n=120) and **11.30** on the GermanQUAD test set. \nThus, fine-tuning on drink600 did not affect performance on GermanQuAD.\n\nIn comparison, *german-qg-t5-quad* achieves a BLEU-4 score of **10.76** on the drink600 test set.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 100\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Framework versions\n\n- Transformers 4.13.0.dev0\n- Pytorch 1.10.0+cu102\n- Datasets 1.16.1\n- Tokenizers 0.10.3\n", "size_bytes": "891647935", "downloads": 6}
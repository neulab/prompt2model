{"pretrained_model_name": "stanford-oval/paraphraser-bart-large", "description": "---\nlicense: apache-2.0\n---\n# Introduction\nThe automatic paraphrasing model described and used in the paper\n\"[AutoQA: From Databases to QA Semantic Parsers with Only Synthetic Training Data](https://arxiv.org/abs/2010.04806)\" (EMNLP 2020).\n\n# Training data \nA cleaned version of the ParaBank 2 dataset introduced in \"[Large-Scale, Diverse, Paraphrastic Bitexts via Sampling and Clustering](https://aclanthology.org/K19-1005/)\".\nParaBank 2 is a paraphrasing dataset constructed by back-translating the Czech portion of an English-Czech parallel corpus.\nWe use a subset of 5 million sentence pairs with the highest dual conditional cross-entropy score (which corresponds to the highest paraphrasing quality), and use only one of the five paraphrases provided for each sentence.\nThe cleaning process involved removing sentences that do not look like normal English sentences, e.g. contain URLs, contain too many special characters, etc. \n\n# Training Procedure\nThe model is fine-tuned for 4 epochs on the above-mentioned dataset, starting from `facebook/bart-large` checkpoint.\nWe use token-level cross-entropy loss calculated using the gold paraphrase sentence. To ensure the output of the model is grammatical, during training, we use the back-translated Czech sentence as the input and the human-written English sentence as the output. Training is done with mini-batches of 1280 examples. For higher training efficiency, each mini-batch is constructed by grouping sentences of similar length together.\n\n # How to use\n Using `top_p=0.9` and `temperature` between `0` and `1` usually results in good generated paraphrases. Higher temperatures make paraphrases more diverse and more different from the input, but might slightly change the meaning of the original sentence.\n Note that this is a sentence-level paraphraser. If you want to paraphrase longer inputs (like paragraphs) with this model, make sure to first break the input into individual sentences.\n \n \n# Citation\nIf you are using this model in your work, please use this citation:\n\n```\n@inproceedings{xu-etal-2020-autoqa,\n    title = \"{A}uto{QA}: From Databases to {QA} Semantic Parsers with Only Synthetic Training Data\",\n    author = \"Xu, Silei  and Semnani, Sina  and Campagna, Giovanni  and Lam, Monica\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.31\",\n    pages = \"422--434\",\n}\n```", "size_bytes": "1625577455", "downloads": 4145}
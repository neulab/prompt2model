{"pretrained_model_name": "eunyounglee/mBART_translator_json_2", "description": "---\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: mBART_translator_json_2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mBART_translator_json_2\n\nThis model is a fine-tuned version of [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1203\n- Bleu: 77.8658\n- Gen Len: 36.1527\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 1.7858        | 1.0   | 1912  | 0.6568          | 55.2937 | 75.6389 |\n| 0.994         | 2.0   | 3824  | 0.4015          | 71.3655 | 35.744  |\n| 0.7267        | 3.0   | 5736  | 0.2971          | 66.7522 | 34.5473 |\n| 0.5916        | 4.0   | 7648  | 0.2437          | 80.0233 | 37.4331 |\n| 0.502         | 5.0   | 9560  | 0.2072          | 80.9632 | 36.9833 |\n| 0.433         | 6.0   | 11472 | 0.1767          | 69.9384 | 36.6381 |\n| 0.3581        | 7.0   | 13384 | 0.1566          | 64.615  | 34.8954 |\n| 0.3244        | 8.0   | 15296 | 0.1382          | 77.5563 | 36.1736 |\n| 0.2815        | 9.0   | 17208 | 0.1259          | 76.1662 | 36.1548 |\n| 0.2555        | 10.0  | 19120 | 0.1203          | 77.8658 | 36.1527 |\n\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.12.1+cu113\n- Datasets 2.5.2\n- Tokenizers 0.13.1\n", "size_bytes": "2444579705", "downloads": 4}
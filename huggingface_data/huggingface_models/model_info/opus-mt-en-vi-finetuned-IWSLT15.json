{"pretrained_model_name": "tdobrxl/opus-mt-en-vi-finetuned-IWSLT15", "description": "# Overview\nThis is a fine-tuned version of the model [Helsinki-NLP/opus-mt-en-vi](https://huggingface.co/Helsinki-NLP/opus-mt-en-vi?text=My+name+is+Sarah+and+I+live+in+London) on the dataset [IWSLT'15 English-Vietnamese](https://huggingface.co/datasets/mt_eng_vietnamese). \nPerformance in terms of [sacrebleu](https://huggingface.co/docs/datasets/v1.5.0/using_metrics.html) on the test set is as follows:\n\n* Original  opus-mt-en-vi: 29.83\n* Fine-tuned opus-mt-en-vi: 37.35\n\n# Parameters\n* learning_rate=2e-5\n* batch_size: 32\n* weight_decay=0.01\n* num_train_epochs=1\n\n# Thoughts\n* Model `Helsinki-NLP/opus-mt-en-vi` is small (around 260MB), and can be easily deployed to a cheap server (e.g., EC2 t2.medium) without a GPU\n* Easier and much faster to train compared to t5 or byt5. ", "size_bytes": "286796485", "downloads": 2}
{"pretrained_model_name": "Iulian277/ro-bart-1024", "description": "---\ntags:\n- summarization\n- bart\nlanguage:\n- ro\ninference: false\n---\n\nThis is a pretrained-from-scratch **BART base** model (**140M** parameters).\n\nTraining was performed on a clean **50GB Romanian** text corpus for 3M steps with these [scripts](https://github.com/cosmoquester/transformers-bart-pretrain). The model was trained with a maximum sequence length of **1024**. \n\n**!! IMPORTANT !!** This model was pretrained on the text corruption task, meaning this model is **not usable** in any downstream task **without finetuning** first!", "size_bytes": "563443869", "downloads": 25}
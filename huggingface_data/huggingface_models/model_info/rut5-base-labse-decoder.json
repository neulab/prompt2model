{"pretrained_model_name": "cointegrated/rut5-base-labse-decoder", "description": "---\nlanguage: [\"ru\"]\ntags:\n- russian\nlicense: mit\n---\n\nThis is the [rut5-base](https://huggingface.co/cointegrated/rut5-base) model, with the decoder fine-tuned to recover (approximately) Russian sentences from their [LaBSE](https://huggingface.co/sentence-transformers/LaBSE) embeddings. Details are [here](https://habr.com/ru/post/677618/) (in Russian).\n\nIt can be used, for example, for:\n- Paraphrasing Russian sentences;\n- Translating from the 109 LaBSE languages to Russian;\n- Summarizing a collection of sentences with a single sentence;\n- Interpolating between sentences;\n- Few-shot text style transfer (including cross-lingual).\n\nExample code:\n```python\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModel\nfrom transformers.modeling_outputs import BaseModelOutput\n\nenc_tokenizer = AutoTokenizer.from_pretrained('cointegrated/LaBSE-en-ru')\nencoder = AutoModel.from_pretrained('cointegrated/LaBSE-en-ru')\n\ndec_tokenizer = AutoTokenizer.from_pretrained('cointegrated/rut5-base-labse-decoder')\ndecoder = AutoModelForSeq2SeqLM.from_pretrained('cointegrated/rut5-base-labse-decoder')\n\ndef encode(texts):\n    encoded_input = enc_tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n    with torch.no_grad():\n        model_output = encoder(**encoded_input.to(encoder.device))\n        embeddings = model_output.pooler_output\n        embeddings = torch.nn.functional.normalize(embeddings)\n    return embeddings\n \n# encode some texts into vectors\nembeddings = encode([\n    \"4 \u0434\u0435\u043a\u0430\u0431\u0440\u044f 2000 \u0433\u043e\u0434\u0430\",\n    \"\u0414\u0430\u0432\u043d\u043e \u0442\u0430\u043a\u043e\u0433\u043e \u043d\u0435 \u0447\u0438\u0442\u0430\u043b\u0430, \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u0438\u0448\u0435\u0448\u044c!\",\n    \"\u042f \u0442\u043e\u0433\u0434\u0430 \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043b\u0430, \u0447\u0442\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442, \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u044e \u0438 \u0441\u0435\u0439\u0447\u0430\u0441.\",\n    \"London is the capital of Great Britain.\",\n])\nprint(embeddings.shape)\n# torch.Size([4, 768])\n\n# now try to recover the texts from the vectors\nout = decoder.generate(\n    encoder_outputs=BaseModelOutput(last_hidden_state=embeddings.unsqueeze(1)), \n    max_length=256, \n    repetition_penalty=3.0,\n)\nfor tokens in out:\n    print(dec_tokenizer.decode(tokens, skip_special_tokens=True))\n# \u041f\u043e\u0441\u043b\u0435 4 \u0434\u0435\u043a\u0430\u0431\u0440\u044f 2000 \u0433\u043e\u0434\u0430\n# \u041d\u0435 \u0442\u0430\u043a \u0434\u0430\u0432\u043d\u043e, \u044d\u0442\u043e \u043c\u043d\u043e\u0433\u043e\u0435 \u0447\u0438\u0442\u0430\u043b\u0430!\n# \u042f \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u043b\u0430 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0441\u0435\u0439\u0447\u0430\u0441 \u0442\u043e\u0433\u0434\u0430, \u0434\u0430\u043b\u044c\u0448\u0435.\n# \u0411\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u0430\u044f \u0441\u0442\u043e\u043b\u0438\u0446\u0430 \u0410\u043d\u0433\u043b\u0438\u0438.\n```", "size_bytes": "977332173", "downloads": 12}
{"pretrained_model_name": "summervent/speller-t5-big-2", "description": "---\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: speller-t5-big-2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# speller-t5-big-2\n\nThis model is a fine-tuned version of [sberbank-ai/ruT5-base](https://huggingface.co/sberbank-ai/ruT5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1711\n- Rouge1: 22.619\n- Rouge2: 10.523\n- Rougel: 22.619\n- Rougelsum: 22.619\n- Gen Len: 42.9107\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2 | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:------:|:-------:|:---------:|:-------:|\n| 1.244         | 0.04  | 500   | 0.5814          | 18.4902 | 6.4123 | 18.3883 | 18.5119   | 48.8214 |\n| 0.6967        | 0.07  | 1000  | 0.4315          | 20.0    | 7.2173 | 20.0744 | 19.9702   | 47.0357 |\n| 0.6362        | 0.11  | 1500  | 0.3721          | 21.1905 | 8.514  | 21.131  | 21.1607   | 47.3929 |\n| 0.5561        | 0.14  | 2000  | 0.3265          | 22.0238 | 9.29   | 21.9643 | 21.994    | 45.6696 |\n| 0.5094        | 0.18  | 2500  | 0.3049          | 22.0238 | 9.29   | 21.9643 | 21.994    | 46.0    |\n| 0.429         | 0.21  | 3000  | 0.2858          | 22.0238 | 9.29   | 21.9643 | 21.994    | 44.9464 |\n| 0.4557        | 0.25  | 3500  | 0.2696          | 22.1726 | 9.4388 | 22.0238 | 22.0982   | 45.2054 |\n| 0.4268        | 0.29  | 4000  | 0.2565          | 22.1726 | 9.4388 | 22.0238 | 22.0982   | 44.5268 |\n| 0.3955        | 0.32  | 4500  | 0.2480          | 22.1726 | 9.4388 | 22.0238 | 22.0982   | 44.2589 |\n| 0.3672        | 0.36  | 5000  | 0.2387          | 22.619  | 10.523 | 22.619  | 22.619    | 44.2946 |\n| 0.4059        | 0.39  | 5500  | 0.2268          | 22.619  | 10.523 | 22.619  | 22.619    | 44.1429 |\n| 0.4005        | 0.43  | 6000  | 0.2216          | 22.619  | 10.523 | 22.619  | 22.619    | 44.4911 |\n| 0.4176        | 0.47  | 6500  | 0.2187          | 22.619  | 10.523 | 22.619  | 22.619    | 44.1339 |\n| 0.3413        | 0.5   | 7000  | 0.2115          | 22.619  | 10.523 | 22.619  | 22.619    | 43.9732 |\n| 0.3618        | 0.54  | 7500  | 0.2068          | 22.619  | 10.523 | 22.619  | 22.619    | 43.9821 |\n| 0.3157        | 0.57  | 8000  | 0.2037          | 22.619  | 10.523 | 22.619  | 22.619    | 43.0714 |\n| 0.3502        | 0.61  | 8500  | 0.1956          | 22.619  | 10.523 | 22.619  | 22.619    | 42.8214 |\n| 0.353         | 0.64  | 9000  | 0.1932          | 22.619  | 10.523 | 22.619  | 22.619    | 42.8393 |\n| 0.3516        | 0.68  | 9500  | 0.1891          | 22.619  | 10.523 | 22.619  | 22.619    | 42.2589 |\n| 0.3225        | 0.72  | 10000 | 0.1836          | 22.619  | 10.523 | 22.619  | 22.619    | 42.1964 |\n| 0.2993        | 0.75  | 10500 | 0.1818          | 22.619  | 10.523 | 22.619  | 22.619    | 43.6607 |\n| 0.3353        | 0.79  | 11000 | 0.1814          | 22.619  | 10.523 | 22.619  | 22.619    | 42.4018 |\n| 0.3325        | 0.82  | 11500 | 0.1807          | 22.619  | 10.523 | 22.619  | 22.619    | 43.1786 |\n| 0.3181        | 0.86  | 12000 | 0.1752          | 22.619  | 10.523 | 22.619  | 22.619    | 43.25   |\n| 0.3337        | 0.9   | 12500 | 0.1729          | 22.619  | 10.523 | 22.619  | 22.619    | 42.3929 |\n| 0.281         | 0.93  | 13000 | 0.1737          | 22.619  | 10.523 | 22.619  | 22.619    | 43.8214 |\n| 0.45          | 0.97  | 13500 | 0.1711          | 22.619  | 10.523 | 22.619  | 22.619    | 42.9107 |\n\n\n### Framework versions\n\n- Transformers 4.26.0\n- Pytorch 1.13.1+cu116\n- Datasets 2.9.0\n- Tokenizers 0.13.2\n", "size_bytes": "891616913", "downloads": 2}
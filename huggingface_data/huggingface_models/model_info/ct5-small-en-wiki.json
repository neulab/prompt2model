{"pretrained_model_name": "mtreviso/ct5-small-en-wiki", "description": "---\nlicense: afl-3.0\nlanguage: en\ntags:\n- t5\ndatasets:\n- wikipedia\n---\n\n# chunked T5 - small (cT5-small)\n\nGithub: https://github.com/mtreviso/chunked-t5\n\nA T5 model that uses a new loss where a special end-of-chunk token `</c>` is appended after sentinel tokens. \nThe decoder has to predict the full input with masked tokens followed by `</c>`. \nThis allows a much faster auto-regressive generation since the decoder can predict multiple tokens in parallel.\n\nFor example, for the input `the quick brown fox jumps over the lazy dog`:\n```\nencoder: the <extra_id_0> fox jumps <extra_id_1> the lazy dog\n\nT5 decoder : <extra_id_0> quick brown <extra_id_1> over <extra_id_2>\ncT5 decoder: <extra_id_0> quick brown </c> <extra_id_1> over </c> <extra_id_2>\n```\n\nThe generation may look like this for T5 and cT5:\n```\nT5: <extra_id_0>\nT5: <extra_id_0> quick\nT5: <extra_id_0> quick brown\nT5: <extra_id_0> quick brown <extra_id_1>\nT5: <extra_id_0> quick brown <extra_id_1> over\nT5: <extra_id_0> quick brown <extra_id_1> over <extra_id_2>\nT5: <extra_id_0> quick brown <extra_id_1> over <extra_id_2> </s>\n\ncT5: <extra_id_0> <pad> <extra_id_1> <pad> <extra_id_2> </s>\ncT5: <extra_id_0> quick <pad> <extra_id_1> over <pad> <extra_id_2> </s>\ncT5: <extra_id_0> quick brown <pad> <extra_id_1> over </c> <extra_id_2> </s>\ncT5: <extra_id_0> quick brown </c> <extra_id_1> over </c> <extra_id_2> </s>\n```\n\nIn the original T5, the decoder is called \\\\(n_s + 1 + \\sum_i |s_i|\\\\) times autoregressively, \nwhere \\\\(n_s\\\\) is the number of sentinel tokens and \\\\(s_1,...,s_{n_s}\\\\) are the predicted chunks. \nIn contrast, cT5's decoder is called just \\\\(max_i |s_i| + 1\\\\) times. \nThe generation stops when all sentences were fully translated to complete chunks, i.e., until all `</c>` tokens were generated. \nAlternatively, you can also set `max_chunk_size` to manually force the model to stop after generating a chunk with `max_chunk_size` tokens.\nThe overhead of calling the decoder with a longer input is less pronounced since this computation can be parallelized in GPUs/TPUs.\n\n## Training details\n\ncT5 models used T5's weights as a starting point, and then it was finetuned on the \nEnglish [wikipedia](https://huggingface.co/datasets/wikipedia) for 3 epochs, \nachieving ~74% validation accuracy (ct5-small).\nThe training script is in JAX + Flax and can be found in `pretrain_ct5.py`.\n\nFlax checkpoints can be converted to PyTorch via `convert_flax_to_pytorch.py [flax_dirname]`.\n\n\n## Checkpoints\n\n- ct5-small: https://huggingface.co/mtreviso/ct5-small-en-wiki\n- ct5-base: todo\n- ct5-large: todo\n\n\n## Usage\n\n```python\nfrom transformers import AutoTokenizer\nfrom modeling_ct5 import CT5ForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"mtreviso/ct5-small-en-wiki\")\nmodel = CT5ForConditionalGeneration.from_pretrained(\"mtreviso/ct5-small-en-wiki\")\n```\n\nFor training:\n\n```python\ninput_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\nlabels = tokenizer(\"<extra_id_0> man </c> <extra_id_1> the </c> <extra_id_2>\", return_tensors=\"pt\").input_ids\noutputs = model(input_ids=input_ids, labels=labels)\nloss = outputs.loss\nlogits = outputs.logits\n```\n\nFor generation:\n\n```python\ntexts = [\n    \"The <extra_id_0> walks in <extra_id_1> park\",\n    \"UN Chief says there is no way to <extra_id_0> in Syria\",\n]\ninput_ids = tokenizer(texts, return_tensors=\"pt\", padding=True).input_ids\ngenerated_ids = model.generate(\n    input_ids, \n    use_cache=False,  # important to set to False to avoid caching\n    eoc_token_id=tokenizer.vocab['</c>'],  # important to set to the correct end-of-chunk id\n    max_chunk_size=5,  # the default is 9999999, which is a large number\n)\n```\n\nThis will produce the following tokens:\n```python\n>> ['<pad>', '<extra_id_0>', '\u2581Walking', '\u2581Trail', '</c>', '<extra_id_1>', '\u2581the', '</c>', '<extra_id_2>', '</s>']\n>> ['<pad>', '<extra_id_0>', '\u2581treat', '\u2581Syria', '</c>', '<extra_id_1>', '</s>', '<pad>', '<pad>', '<pad>']\n```\n\nYou have to pass `use_cache=False` to `generate()` in order to avoid caching during the generation procedure as caching is not available for parallel decoding. \nCurrently, parallel decoding is only supported for PyTorch (greedy search, greedy sampling, beam search, beam sampling) and JAX (greedy search and greedy sampling).\n\n**Note on the beam search implementation**: my beam search implementation is slower than optimal.\nThis is because I use the structures provided by HuggingFace's implementation, namely, BeamScores and BeamHypotheses to store the beam search results for each chunk in the input.\nIn other words, my implementation computes independent \"beams\" for each chunk rather than for each input sequence.\nIt is possible to make it faster by using a custom BeamScores and BeamHypotheses class, but I haven't done that yet.\n\n\n## Evaluation\n\nSee the notebook `evaluate_ct5.ipynb` for an example of how to evaluate cT5 in terms of accuracy and perplexity.\nThe notebook `profile.ipynb` shows how to profile the model to get runtimes.\n\nHere is a comparison between cT5-small and T5-small on a subset of the WikiText-103 dataset using deterministic greedy search:\n\n| Model | Exact match \u2191 | Edit distance ratio \u2191 | Perplexity \u2193 | Time (seconds) \u2193 |\n|-------|---------------|----------------------|--------------|-----------------|\n| T5-small | 0.11          | 0.60                 | 2.22         | 44.71           |\n| cT5-small | 0.09          | 0.58                 | 1.48         | 10.63           |\n\nOn this toy dataset, cT5-small has a lower perplexity while being faster than T5-small. However, more experiments are needed for a rigorous evaluation.\n\nIf you are interested in applying cT5 to real data, please contact me.\n", "size_bytes": "242068411", "downloads": 4}
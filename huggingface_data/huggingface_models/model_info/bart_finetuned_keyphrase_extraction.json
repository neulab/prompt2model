{"pretrained_model_name": "beogradjanka/bart_finetuned_keyphrase_extraction", "description": "---\ndatasets:\n- midas/krapivin\n- midas/inspec\n- midas/kptimes\n- midas/duc2001\nlanguage:\n- en\n\nwidget:\n  - text: \"Relevance has traditionally been linked with feature subset selection, but formalization of this link has not been attempted. In this paper, we propose two axioms for feature subset selection sufficiency axiom and necessity axiombased on which this link is formalized: The expected feature subset is the one which maximizes relevance. Finding the expected feature subset turns out to be NP-hard. We then devise a heuristic algorithm to find the expected subset which has a polynomial time complexity. The experimental results show that the algorithm finds good enough subset of features which, when presented to C4.5, results in better prediction accuracy.\"\n  - text: \"In this paper, we investigate cross-domain limitations of keyphrase generation using the models for abstractive text summarization. We present an evaluation of BART fine-tuned for keyphrase generation across three types of texts, namely scientific texts from computer science and biomedical domains and news texts. We explore the role of transfer learning between different domains to improve the model performance on small text corpora.\"\n---\n\n# BART fine-tuned for keyphrase generation\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis is the <a href=\"https://huggingface.co/facebook/bart-base\">bart-base</a> (<a href = \"https://arxiv.org/abs/1910.13461\">Lewis et al.. 2019</a>) model <a href=\"https://arxiv.org/abs/2209.03791\">finetuned for the keyphrase generation task</a> on the fragments of the following corpora: \n\n* Krapivin (<a href = \"http://eprints.biblio.unitn.it/1671/1/disi09055%2Dkrapivin%2Dautayeu%2Dmarchese.pdf\">Krapivin et al., 2009</a>)\n* Inspec (<a href = \"https://aclanthology.org/W03-1028.pdf\">Hulth, 2003</a>)\n* KPTimes (<a href = \"https://aclanthology.org/W19-8617.pdf\">Gallina, 2019</a>)\n* DUC-2001 (<a href = \"https://cdn.aaai.org/AAAI/2008/AAAI08-136.pdf\">Wan, 2008</a>)\n* PubMed (<a href = \"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=08b75d31a90f206b36e806a7ec372f6f0d12457e\">Schutz, 2008</a>)\n* NamedKeys (<a href = \"https://joyceho.github.io/assets/pdf/paper/gero-bcb19.pdf\">Gero & Ho, 2019</a>).\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"beogradjanka/bart_finetuned_keyphrase_extraction\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"beogradjanka/bart_finetuned_keyphrase_extraction\")\n\ntext = \"In this paper, we investigate cross-domain limitations of keyphrase generation using the models for abstractive text summarization.\\\n        We present an evaluation of BART fine-tuned for keyphrase generation across three types of texts, \\\n        namely scientific texts from computer science and biomedical domains and news texts. \\\n        We explore the role of transfer learning between different domains to improve the model performance on small text corpora.\"\n\ntokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')\ntranslation = model.generate(**tokenized_text)\ntranslated_text = tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\nprint(translated_text)\n```\n\n#### Training Hyperparameters\n\nThe following hyperparameters were used during training:\n\n* learning_rate: 4e-5\n* train_batch_size: 8\n* optimizer: AdamW with betas=(0.9,0.999) and epsilon=1e-08\n* num_epochs: 6\n\n**BibTeX:**\n\n```\n@article{glazkova2023applying,\n  title={Applying Transformer-Based Text Summarization for Keyphrase Generation},\n  author={Glazkova, Anna and Morozov, Dmitry},\n  journal={Lobachevskii Journal of Mathematics},\n  volume={44},\n  number={1},\n  pages={123--136},\n  year={2023},\n  doi={10.1134/S1995080223010134}\n}\n```\n\n", "size_bytes": "557971229", "downloads": 20}
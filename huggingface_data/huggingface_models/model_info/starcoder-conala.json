{"pretrained_model_name": "codeparrot/starcoder-conala", "description": "---\ndatasets:\n  - codeparrot/conala-mined-curated\npipeline_tag: text2text-generation\n---\n\n# Model Card for Starcoder-conala\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis model is an instruction-tuned version of \u2b50\ufe0f StarCoder. The instruction dataset involved is [Conala-mined-curated](https://huggingface.co/datasets/codeparrot/conala-mined-curated)\nwhich was built by boostrapping by predicting the column *rewritten_intent* of the mined subset of the [CoNaLa corpus](https://huggingface.co/datasets/neulab/conala).\n\n## Usage\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\nThe model was fine-tuned with the following template\n```\nQuestion: <instruction>\n\nAnswer: <output>\n```\nIf you have your model and tokenizer loaded, you can use the following code to make the model generate the right output to a given instruction\n\n```python\ninstruction = \"Write a function to compute the GCD between two integers a and b\"\nprompt = f\"Question:{instruction}\\n\\nAnswer:\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\ncompletion = model.generate(input_ids, max_length=200)\nprint(tokenizer.batch_decode(completion[:,input_ids.shape[1]:])[0])\n```\n ## More information\nFor additional information, check\n- [Conala-mined-curated](https://huggingface.co/datasets/codeparrot/conala-mined-curated)\n- [Starcoder](https://huggingface.co/bigcode/starcoder)", "size_bytes": 31638892544, "downloads": 5}
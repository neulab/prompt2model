{"pretrained_model_name": "UBC-NLP/ptsm_t5_paraphraser", "description": "---\nlicense: cc-by-nc-3.0\n---\n\n# T5-base model trained for text paraphrase \n\nYou can load this model by:\n```python\nfrom transformers import T5ForConditionalGeneration,T5TokenizerFast\n\nmodel = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\ntokenizer = T5TokenizerFast.from_pretrained(model_name_or_path)\n```\n\nA prefix \"paraphrase: \" should be added in font of the input sequence, i.e.:\n```python\ninput_st = \"paraphrase: \" + text + \" </s>\"\n```\nYou can find our scripts for generation in our [project GitHub](https://github.com/chiyuzhang94/PTSM/tree/main/paraphrase_generate)\nPlease find more training details in our paper:\n\n[Decay No More: A Persistent Twitter Dataset for Learning Social Meaning](https://arxiv.org/pdf/2204.04611.pdf)\n\nAccepted by 1st Workshop on Novel Evaluation Approaches for Text Classification Systems on Social Media @ ICWSM-2022\n\n  ```\n@inproceedings{zhang2022decay,\n   title={Decay No More: A Persistent Twitter Dataset for Learning Social Meaning},\n   author={Zhang, Chiyu and Abdul-Mageed, Muhammad and Nagoudi, El Moatez Billah},\n   booktitle ={Proceedings of 1st Workshop on Novel Evaluation Approaches for Text Classification Systems on Social Media (NEATCLasS)}, \n   year={2022},\n   url = {https://arxiv.org/pdf/2204.04611.pdf},\n   publisher = {{AAAI} Press}, \n}\n  ```", "size_bytes": "891692980", "downloads": 13}
{"pretrained_model_name": "convaise-idp/flan-t5-base-finetuned-length_control_token", "description": "---\nlicense: apache-2.0\ntags:\n- simplification\n- generated_from_trainer\nmetrics:\n- sacrebleu\nmodel-index:\n- name: flan-t5-base-finetuned-length_control_token\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-base-finetuned-length_control_token\n\nThis model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0276\n- Sacrebleu: 16.2445\n\n## Model description\n\nThis model was trained on a dataset called PWKP-GPT3-LENGTH-CONTROL-40BUCKETS.\nThe dataset contains 30k instances taken from PWKP, then processed through GPT3 to obtain simplifications.\nThe 30k instances come from: 10k which were supposed to generate very long simplifications, \n10k which were supposed to generate very short simplifications, and 10k without specifying the simplicity level.\nThe model does not sucessfuly work on these buckets.\nThere exists another dataset, the PWKP-GPT3-LENGTH-CONTROL-4BUCKETS, but it was never trained on something.\nThose buckets are also rather unbalanced. \n\nThe idea comes from \nControllable Sentence Simplification\nLouis Martin, https://arxiv.org/pdf/1910.02677.pdf\n\nIt was fine-tuned on the FLAN-T5-base model.\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 6\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Sacrebleu |\n|:-------------:|:-----:|:-----:|:---------------:|:---------:|\n| 1.3257        | 1.0   | 1782  | 1.0906          | 15.4208   |\n| 1.1718        | 2.0   | 3564  | 1.0648          | 15.5358   |\n| 1.0972        | 3.0   | 5346  | 1.0484          | 15.8113   |\n| 1.0472        | 4.0   | 7128  | 1.0394          | 16.0159   |\n| 1.0092        | 5.0   | 8910  | 1.0305          | 16.1341   |\n| 0.9858        | 6.0   | 10692 | 1.0276          | 16.2445   |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.13.1+cu117\n- Datasets 2.10.1\n- Tokenizers 0.13.2\n", "size_bytes": "990470325", "downloads": 3}
{"pretrained_model_name": "meongracun/nmt-ted-id-en-lr_1e-3-ep_30-seq_128-bs_32", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: nmt-ted-id-en-lr_1e-3-ep_30-seq_128-bs_32\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# nmt-ted-id-en-lr_1e-3-ep_30-seq_128-bs_32\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.5777\n- Bleu: 18.3981\n- Gen Len: 16.2277\n- Meteor: 0.3652\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.001\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 20\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len | Meteor |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:------:|\n| 2.1196        | 1.0   | 1250  | 1.6942          | 13.7333 | 16.5568 | 0.3053 |\n| 1.7331        | 2.0   | 2500  | 1.5393          | 15.6555 | 16.3938 | 0.3341 |\n| 1.5842        | 3.0   | 3750  | 1.4821          | 16.2862 | 16.3699 | 0.3403 |\n| 1.4518        | 4.0   | 5000  | 1.4562          | 17.1158 | 16.2073 | 0.3518 |\n| 1.3649        | 5.0   | 6250  | 1.4416          | 17.3302 | 16.3344 | 0.355  |\n| 1.2987        | 6.0   | 7500  | 1.4276          | 17.3334 | 16.3913 | 0.3547 |\n| 1.2227        | 7.0   | 8750  | 1.4411          | 17.9415 | 16.2941 | 0.3612 |\n| 1.1766        | 8.0   | 10000 | 1.4435          | 18.0776 | 16.2809 | 0.362  |\n| 1.1119        | 9.0   | 11250 | 1.4510          | 18.0156 | 16.2834 | 0.3628 |\n| 1.0672        | 10.0  | 12500 | 1.4566          | 18.276  | 16.1982 | 0.3646 |\n| 1.0194        | 11.0  | 13750 | 1.4728          | 18.2417 | 16.2589 | 0.364  |\n| 0.988         | 12.0  | 15000 | 1.4843          | 18.385  | 16.2671 | 0.3644 |\n| 0.9445        | 13.0  | 16250 | 1.4896          | 18.1065 | 16.2321 | 0.3629 |\n| 0.9156        | 14.0  | 17500 | 1.5102          | 18.249  | 16.2334 | 0.364  |\n| 0.8888        | 15.0  | 18750 | 1.5273          | 18.1876 | 16.2454 | 0.3641 |\n| 0.8624        | 16.0  | 20000 | 1.5354          | 18.2708 | 16.2425 | 0.3638 |\n| 0.8364        | 17.0  | 21250 | 1.5449          | 18.17   | 16.268  | 0.3645 |\n| 0.8133        | 18.0  | 22500 | 1.5578          | 18.2573 | 16.2422 | 0.3639 |\n| 0.7994        | 19.0  | 23750 | 1.5678          | 18.3108 | 16.2158 | 0.3648 |\n| 0.7823        | 20.0  | 25000 | 1.5777          | 18.3981 | 16.2277 | 0.3652 |\n\n\n### Framework versions\n\n- Transformers 4.20.1\n- Pytorch 1.11.0\n- Datasets 2.1.0\n- Tokenizers 0.12.1\n", "size_bytes": "242070267", "downloads": 2}
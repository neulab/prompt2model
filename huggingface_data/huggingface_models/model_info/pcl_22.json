{"pretrained_model_name": "tosin/pcl_22", "description": "---\nthumbnail: https://huggingface.co/front/thumbnails/dialogpt.png\nlanguage:\n- en\nlicense: cc-by-4.0\ntags:\n- text classification\n- transformers\ndatasets:\n- PCL\nmetrics:\n- F1\ninference: false\n---\n\n## T5Base-PCL\nThis is a fine-tuned model of T5 (base) on the patronizing and condenscending language (PCL) dataset by P\u00e9rez-Almendros et al (2020) used for Task 4 competition of SemEval-2022.\nIt is intended to be used as a classification model for identifying PCL (0 - neg; 1 - pos). The task prefix we used for the T5 model is 'classification: '.\n\nThe dataset it's trained on is limited in scope, as it covers only some news texts covering about 20 English-speaking countries.\nThe macro F1 score achieved on the test set, based on the official evaluation, is 0.5452.\nMore information about the original pre-trained model can be found [here](https://huggingface.co/t5-base)\n\n* Classification examples:\n|Prediction | Input |\n|---------|------------|\n|0 | selective kindness : in europe , some refugees are more equal than others |\n|1 | he said their efforts should not stop only at creating many graduates but also extended to students from poor families so that they could break away from the cycle of poverty |\n\n### How to use\n\n```python\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nimport torch\nmodel = T5ForConditionalGeneration.from_pretrained(\"tosin/pcl_22\")\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\") # use the source tokenizer because T5 finetuned tokenizer breaks\ntokenizer.pad_token = tokenizer.eos_token\ninput_ids = tokenizer(\"he said their efforts should not stop only at creating many graduates but also extended to students from poor families so that they could break away from the cycle of poverty\", padding=True, truncation=True, return_tensors='pt').input_ids\noutputs = model.generate(input_ids)\npred = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(pred)\n", "size_bytes": "891734707", "downloads": 3}
{"pretrained_model_name": "ArtifactAI/flan-t5-xxl-arxiv-math-closed-qa", "description": "---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: summarization\nwidget:\n- text: What is the peak phase of T-eV?\n  example_title: Question Answering\ntags:\n- arxiv\n---\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Citation](#citation)\n\n# TL;DR\n\nThis is a FLAN-T5-XXL model trained on [ArtifactAI/arxiv-math-instruct-50k](https://huggingface.co/datasets/ArtifactAI/arxiv-math-instruct-50k). This model is for research purposes only and ***should not be used in production settings***.\n\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n```python\n\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load peft config for pre-trained checkpoint etc.\npeft_model_id = \"ArtifactAI/flant5-xxl-math-full-training-run-one\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\n# load base LLM model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nmodel = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\nmodel.eval()\n\n\ninput_ids = tokenizer(\"What is the peak phase of T-eV?\", return_tensors=\"pt\", truncation=True).input_ids.cuda()\n# with torch.inference_mode():\noutputs = model.generate(input_ids=input_ids, max_new_tokens=1000, do_sample=True, top_p=0.9)\n\nprint(f\"summary: {tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")\n```\n\n## Training Data\n\nThe model was trained on [ArtifactAI/arxiv-math-instruct-50k](https://huggingface.co/datasets/ArtifactAI/arxiv-math-instruct-50k), a dataset of question/answer pairs. Questions are generated using the t5-base model, while the answers are generated using the GPT-3.5-turbo model.\n\n# Citation\n\n```\n@misc{flan-t5-xxl-arxiv-math-zeroshot-qa,\n    title={flan-t5-xxl-arxiv-math-zeroshot-qa},\n    author={Matthew Kenney},\n    year={2023}\n}\n```", "size_bytes": 18041708544, "downloads": 2}
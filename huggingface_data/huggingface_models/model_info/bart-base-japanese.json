{"pretrained_model_name": "Formzu/bart-base-japanese", "description": "---\nlanguage: \n- ja\nlicense: mit\ntags:\n- bart\n- pytorch\ndatasets:\n- wikipedia\n---\n# bart-base-japanese\n\nThis model is converted from the original [Japanese BART Pretrained model](https://nlp.ist.i.kyoto-u.ac.jp/?BART%E6%97%A5%E6%9C%AC%E8%AA%9EPretrained%E3%83%A2%E3%83%87%E3%83%AB) released by Kyoto University.\n\nBoth the encoder and decoder outputs are identical to the original Fairseq model.\n\n### How to use the model\n\nThe input text should be tokenized by [BartJapaneseTokenizer](https://huggingface.co/Formzu/bart-base-japanese/blob/main/tokenization_bart_japanese.py). \n\nTokenizer requirements:\n* [Juman++](https://github.com/ku-nlp/jumanpp)\n* [zenhan](https://pypi.org/project/zenhan/)  \n* [pyknp](https://pypi.org/project/pyknp/)  \n* [sentencepiece](https://pypi.org/project/sentencepiece/) \n\n#### Simple FillMaskPipeline\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n\nmodel_name = \"Formzu/bart-base-japanese\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\nmasked_text = \"\u5929\u6c17\u304c<mask>\u304b\u3089\u6563\u6b69\u3057\u307e\u3057\u3087\u3046\u3002\"\n\nfill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\nout = fill_mask(masked_text)\nprint(out)\n# [{'score': 0.19255658984184265, 'token': 1718, 'token_str': '\u3088\u304f', 'sequence': '\u5929\u6c17 \u304c \u3088\u304f \u304b\u3089 \u6563\u6b69 \u3057 \u307e\u3057\u3087\u3046 \u3002'}, \n#  {'score': 0.14426815509796143, 'token': 5478, 'token_str': '\u826f\u304f', 'sequence': '\u5929\u6c17 \u304c \u826f\u304f \u304b\u3089 \u6563\u6b69 \u3057 \u307e\u3057\u3087\u3046 \u3002'}, \n#  {'score': 0.05554169788956642, 'token': 6561, 'token_str': '\u60aa\u3044', 'sequence': '\u5929\u6c17 \u304c \u60aa\u3044 \u304b\u3089 \u6563\u6b69 \u3057 \u307e\u3057\u3087\u3046 \u3002'}, \n#  {'score': 0.05524599179625511, 'token': 3553, 'token_str': '\u826f\u3044', 'sequence': '\u5929\u6c17 \u304c \u826f\u3044 \u304b\u3089 \u6563\u6b69 \u3057 \u307e\u3057\u3087\u3046 \u3002'}, \n#  {'score': 0.03720080852508545, 'token': 1370, 'token_str': '\u826f', 'sequence': '\u5929\u6c17 \u304c \u826f \u304b\u3089 \u6563\u6b69 \u3057 \u307e\u3057\u3087\u3046 \u3002'}]\n```\n#### Text Generation\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel_name = \"Formzu/bart-base-japanese\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\nmasked_text = \"\u5929\u6c17\u304c<mask>\u304b\u3089\u6563\u6b69\u3057\u307e\u3057\u3087\u3046\u3002\"\n\ninp = tokenizer(masked_text, return_tensors='pt').to(device)\n\nout = model.generate(**inp, num_beams=1, min_length=0, max_length=20, early_stopping=True,  no_repeat_ngram_size=2)\nres = \"\".join(tokenizer.decode(out.squeeze(0).tolist(), skip_special_tokens=True).split(\" \"))\nprint(res)\n# \u5929\u6c17\u304c\u3088\u304f\u306a\u3063\u3066\u304b\u3089\u6563\u6b69\u3057\u307e\u3057\u3087\u3046\u3002\u5929\u6c17\u306e\u3088\u304f\u5408\u3063\u3066\u3044\u308b\u3068\u3053\u308d\u306b\u3044\u308b\n```\n\n### Framework versions\n\n- Transformers 4.21.2\n- Pytorch 1.12.1+cu116\n- Tokenizers 0.12.1\n", "size_bytes": "501801969", "downloads": 23}
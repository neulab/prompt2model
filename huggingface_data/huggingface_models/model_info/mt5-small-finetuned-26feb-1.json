{"pretrained_model_name": "mqy/mt5-small-finetuned-26feb-1", "description": "---\nlicense: apache-2.0\ntags:\n- summarization\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: mt5-small-finetuned-26feb-1\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mt5-small-finetuned-26feb-1\n\nThis model is a fine-tuned version of [google/mt5-small](https://huggingface.co/google/mt5-small) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.4486\n- Rouge1: 20.86\n- Rouge2: 6.45\n- Rougel: 20.49\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 12\n- eval_batch_size: 12\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 60\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1 | Rouge2 | Rougel |\n|:-------------:|:-----:|:-----:|:---------------:|:------:|:------:|:------:|\n| 5.4839        | 1.93  | 500   | 2.5990          | 16.14  | 5.18   | 15.98  |\n| 3.1051        | 3.86  | 1000  | 2.4754          | 18.71  | 5.68   | 18.41  |\n| 2.8659        | 5.79  | 1500  | 2.4006          | 18.22  | 5.54   | 18.06  |\n| 2.71          | 7.72  | 2000  | 2.3848          | 19.91  | 6.0    | 19.65  |\n| 2.5845        | 9.65  | 2500  | 2.3956          | 18.72  | 5.72   | 18.4   |\n| 2.4895        | 11.58 | 3000  | 2.3719          | 19.9   | 6.1    | 19.54  |\n| 2.402         | 13.51 | 3500  | 2.3691          | 19.86  | 5.79   | 19.51  |\n| 2.3089        | 15.44 | 4000  | 2.3747          | 20.22  | 6.74   | 19.88  |\n| 2.2681        | 17.37 | 4500  | 2.3754          | 19.44  | 5.53   | 19.03  |\n| 2.1927        | 19.31 | 5000  | 2.3419          | 20.02  | 5.91   | 19.69  |\n| 2.1278        | 21.24 | 5500  | 2.3496          | 20.26  | 6.21   | 19.79  |\n| 2.0928        | 23.17 | 6000  | 2.3756          | 19.9   | 6.04   | 19.48  |\n| 2.0658        | 25.1  | 6500  | 2.3615          | 19.61  | 6.04   | 19.28  |\n| 2.0063        | 27.03 | 7000  | 2.3516          | 20.38  | 6.52   | 20.14  |\n| 1.9581        | 28.96 | 7500  | 2.3743          | 20.61  | 6.26   | 20.24  |\n| 1.941         | 30.89 | 8000  | 2.3726          | 19.73  | 5.8    | 19.31  |\n| 1.9172        | 32.82 | 8500  | 2.3891          | 19.73  | 5.98   | 19.51  |\n| 1.8764        | 34.75 | 9000  | 2.3782          | 20.1   | 6.15   | 19.74  |\n| 1.8453        | 36.68 | 9500  | 2.3851          | 19.96  | 6.0    | 19.61  |\n| 1.845         | 38.61 | 10000 | 2.4046          | 20.66  | 6.32   | 20.24  |\n| 1.7919        | 40.54 | 10500 | 2.4169          | 20.65  | 6.25   | 20.38  |\n| 1.7945        | 42.47 | 11000 | 2.4206          | 20.68  | 5.74   | 20.37  |\n| 1.7689        | 44.4  | 11500 | 2.4246          | 20.69  | 6.09   | 20.4   |\n| 1.7215        | 46.33 | 12000 | 2.4237          | 20.49  | 6.43   | 20.21  |\n| 1.7306        | 48.26 | 12500 | 2.4217          | 20.55  | 6.49   | 20.18  |\n| 1.7035        | 50.19 | 13000 | 2.4389          | 20.81  | 6.55   | 20.48  |\n| 1.6934        | 52.12 | 13500 | 2.4377          | 20.75  | 6.85   | 20.35  |\n| 1.7           | 54.05 | 14000 | 2.4486          | 20.86  | 6.45   | 20.49  |\n| 1.6909        | 55.98 | 14500 | 2.4451          | 20.5   | 6.55   | 20.12  |\n| 1.6804        | 57.92 | 15000 | 2.4457          | 20.21  | 6.5    | 19.84  |\n| 1.6693        | 59.85 | 15500 | 2.4473          | 20.35  | 6.6    | 19.96  |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.13.1+cu116\n- Datasets 2.10.0\n- Tokenizers 0.13.2\n", "size_bytes": "1200772485", "downloads": 4}
{"pretrained_model_name": "Chemsseddine/bert2gpt2SUMM-finetuned-mlsum", "description": "---\nlanguage: fr\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: bert2gpt2_med_ml_orange_summ-finetuned_med_sum_new-finetuned_med_sum_new\n  results: []\n---\n<img src=\"https://huggingface.co/Chemsseddine/bert2gpt2_med_ml_orange_summ-finetuned_med_sum_new-finetuned_med_sum_new/resolve/main/logobert2gpt2.png\" alt=\"Map of positive probabilities per country.\" width=\"200\"/>\n\n# bert2gpt2SUMM-finetuned-mlsum\n\n---\n## This model is used for french summarization\n- Problem type: Summarization\n- Model ID: 980832493\n- CO2 Emissions (in grams): 0.10685501288084795\n\nThis model is a fine-tuned version of [Chemsseddine/bert2gpt2SUMM](https://huggingface.co/Chemsseddine/bert2gpt2SUMM) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 4.03749418258667\n- Rouge1: 28.8384\n- Rouge2: 10.7511\n- RougeL: 27.0842\n- RougeLsum: 27.5118\n- Gen Len: 22.0625\n\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| No log        | 1.0   | 33199  | 4.03749       | 28.8384 | 10.7511 | 27.0842 | 27.5118   | 22.0625 |\n\n", "size_bytes": "1079030377", "downloads": 7}
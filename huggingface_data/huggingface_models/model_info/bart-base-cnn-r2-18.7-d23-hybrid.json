{"pretrained_model_name": "echarlaix/bart-base-cnn-r2-18.7-d23-hybrid", "description": "---\nlanguage: en\nlicense: apache-2.0\ntags:\n- summarization\ndatasets:\n- cnn_dailymail\nmetrics:\n- R1\n- R2\n- RL\n---\n\n## facebook/bart-base model fine-tuned on CNN/DailyMail\n\nThis model was created using the [nn_pruning](https://github.com/huggingface/nn_pruning) python library: the linear layers contains **23%** of the original  weights.\n\n\n\nThe model contains **45%** of the original weights **overall** (the embeddings account for a significant part of the model, and they are not pruned by this method).\n\n<div class=\"graph\"><script src=\"/echarlaix/bart-base-cnn-r2-18.7-d23-hybrid/raw/main/model_card/density_info.js\" id=\"4348cd46-05bd-4e27-b565-6693f9e0b03e\"></script></div>\n\n\n## Fine-Pruning details\nThis model was fine-tuned from the HuggingFace [model](https://huggingface.co/facebook/bart-base).\nA side-effect of block pruning is that some of the attention heads are completely removed: 61 heads were removed on a total of 216 (28.2%).\n\n## Details of the CNN/DailyMail dataset\n\n|    Dataset    | Split | # samples |\n| ------------- | ----- | --------- |\n| CNN/DailyMail | train |   287K    |\n| CNN/DailyMail | eval  |    13K    |\n\n### Results\n\n\n|    Metric   | # Value   |\n| ----------- | --------- |\n| **Rouge 1** | **41.43** |\n| **Rouge 2** | **18.72** |\n| **Rouge L** | **38.35** |\n", "size_bytes": "557982035", "downloads": 13}
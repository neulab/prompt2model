{"pretrained_model_name": "celinelee/bart-finetuned-conala-3", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\n- bleu\nmodel-index:\n- name: bart-finetuned-conala-3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bart-finetuned-conala-3\n\nThis model is a fine-tuned version of [facebook/bart-large](https://huggingface.co/facebook/bart-large) on an CoNaLa.\nIt achieves the following results on the evaluation set:\n- Loss: 1.8253\n- Rouge1: 47.4345\n- Rouge2: 23.8936\n- Rougel: 45.317\n- Rougelsum: 45.4339\n- Bleu: 0.0657\n- Gen Len: 58.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nCode snippet -> NL intent\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Bleu   | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:------:|:-------:|\n| No log        | 0.08  | 50   | 2.7823          | 35.8458 | 12.1898 | 33.7466 | 33.8377   | 0.0041 | 58.0    |\n| No log        | 0.17  | 100  | 2.4223          | 37.2633 | 13.429  | 34.4943 | 34.5533   | 0.0087 | 58.0    |\n| No log        | 0.25  | 150  | 2.2696          | 40.6963 | 16.5785 | 38.1213 | 38.16     | 0.0167 | 58.0    |\n| No log        | 0.34  | 200  | 2.3168          | 41.3324 | 17.292  | 39.0117 | 39.113    | 0.0173 | 58.0    |\n| No log        | 0.42  | 250  | 2.3187          | 41.1345 | 16.6829 | 38.8514 | 38.891    | 0.0237 | 58.0    |\n| No log        | 0.5   | 300  | 2.1701          | 41.0145 | 17.5601 | 39.166  | 39.249    | 0.0206 | 58.0    |\n| No log        | 0.59  | 350  | 2.2035          | 41.7506 | 17.7251 | 39.4856 | 39.5647   | 0.0292 | 58.0    |\n| No log        | 0.67  | 400  | 2.1006          | 43.0324 | 19.9801 | 40.8704 | 40.9399   | 0.0319 | 58.0    |\n| No log        | 0.76  | 450  | 2.0563          | 43.2151 | 18.7409 | 40.4183 | 40.502    | 0.0244 | 58.0    |\n| 2.4902        | 0.84  | 500  | 2.0468          | 43.2215 | 18.3484 | 40.9498 | 41.0682   | 0.0317 | 58.0    |\n| 2.4902        | 0.92  | 550  | 2.0222          | 44.9934 | 19.8389 | 42.4478 | 42.5687   | 0.0372 | 58.0    |\n| 2.4902        | 1.01  | 600  | 2.1095          | 43.8293 | 19.5682 | 40.882  | 40.9518   | 0.0311 | 58.0    |\n| 2.4902        | 1.09  | 650  | 2.0124          | 43.6928 | 19.6878 | 39.6602 | 39.7368   | 0.0417 | 58.0    |\n| 2.4902        | 1.18  | 700  | 2.0027          | 46.2115 | 21.9475 | 43.5869 | 43.6713   | 0.0477 | 58.0    |\n| 2.4902        | 1.26  | 750  | 1.9599          | 45.9388 | 22.0368 | 43.4731 | 43.5656   | 0.043  | 58.0    |\n| 2.4902        | 1.34  | 800  | 1.9467          | 44.7518 | 20.4755 | 42.489  | 42.6274   | 0.0394 | 58.0    |\n| 2.4902        | 1.43  | 850  | 1.9643          | 44.1584 | 20.8833 | 41.8848 | 41.9733   | 0.0441 | 58.0    |\n| 2.4902        | 1.51  | 900  | 1.8926          | 47.3789 | 22.9104 | 45.0164 | 45.0822   | 0.0445 | 58.0    |\n| 2.4902        | 1.6   | 950  | 1.8855          | 46.8329 | 22.1133 | 44.1788 | 44.2666   | 0.0431 | 58.0    |\n| 1.8023        | 1.68  | 1000 | 1.9160          | 47.1319 | 22.9792 | 44.4807 | 44.6103   | 0.0475 | 58.0    |\n| 1.8023        | 1.76  | 1050 | 1.8498          | 48.8005 | 24.4785 | 46.4564 | 46.5427   | 0.0576 | 58.0    |\n| 1.8023        | 1.85  | 1100 | 1.8611          | 47.8327 | 23.2086 | 45.5999 | 45.6868   | 0.0487 | 58.0    |\n| 1.8023        | 1.93  | 1150 | 1.8497          | 47.7267 | 23.2021 | 45.5104 | 45.546    | 0.0512 | 58.0    |\n| 1.8023        | 2.02  | 1200 | 1.8335          | 47.1502 | 22.8336 | 44.7614 | 44.7927   | 0.0566 | 58.0    |\n| 1.8023        | 2.1   | 1250 | 1.8779          | 46.6645 | 22.9162 | 44.0086 | 44.2021   | 0.0539 | 58.0    |\n| 1.8023        | 2.18  | 1300 | 1.8514          | 48.1544 | 24.7977 | 45.949  | 46.0254   | 0.0719 | 58.0    |\n| 1.8023        | 2.27  | 1350 | 1.8658          | 46.7655 | 23.4813 | 44.5872 | 44.6907   | 0.069  | 58.0    |\n| 1.8023        | 2.35  | 1400 | 1.8400          | 46.2749 | 23.6528 | 44.3149 | 44.4056   | 0.0572 | 58.0    |\n| 1.8023        | 2.44  | 1450 | 1.8343          | 46.6169 | 23.8005 | 44.5486 | 44.6125   | 0.0547 | 58.0    |\n| 1.3851        | 2.52  | 1500 | 1.8220          | 47.4739 | 24.3457 | 45.4959 | 45.6216   | 0.0662 | 58.0    |\n| 1.3851        | 2.61  | 1550 | 1.8333          | 47.6311 | 24.3616 | 45.5904 | 45.6146   | 0.0666 | 58.0    |\n| 1.3851        | 2.69  | 1600 | 1.8091          | 47.4633 | 24.0785 | 45.2493 | 45.2845   | 0.0645 | 58.0    |\n| 1.3851        | 2.77  | 1650 | 1.8085          | 47.6495 | 23.8386 | 45.5077 | 45.5848   | 0.0639 | 58.0    |\n| 1.3851        | 2.86  | 1700 | 1.8377          | 46.9721 | 23.4325 | 44.8386 | 44.9003   | 0.0647 | 58.0    |\n| 1.3851        | 2.94  | 1750 | 1.8238          | 47.5266 | 23.9843 | 45.3897 | 45.473    | 0.0653 | 58.0    |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 1.10.2+cu102\n- Datasets 2.1.0\n- Tokenizers 0.10.3\n", "size_bytes": "1625550145", "downloads": 2}
{"pretrained_model_name": "pszemraj/bart-base-instruct-dolly_hhrlhf", "description": "---\nlicense: cc-by-sa-3.0\ninference: false\nlanguage:\n- en\nlibrary_name: transformers\npipeline_tag: text2text-generation\ndatasets:\n- pszemraj/dolly_hhrlhf-text2text\ntags:\n- instruct\n- dolly_hhrlhf\n---\n\n# bart-base-instruct: dolly_hhrlhf\n\n<a href=\"https://colab.research.google.com/gist/pszemraj/a0c0a8cc24abfbf609f75f9d5c56c348/bart-base-instruct-example.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nThis model is a fine-tuned version of [facebook/bart-base](https://huggingface.co/facebook/bart-base) on the pszemraj/dolly_hhrlhf-text2text dataset.\n\n## Model description\n\ntext2text models fine-tuned on a [modified dataset for text2text generation](https://huggingface.co/datasets/pszemraj/dolly_hhrlhf-text2text)  based on the relatively more permissive  [mosaicml/dolly_hhrlhf](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf) dataset.\n\nBasic usage in Python:\n\n```python\n# pip install -q transformers accelerate\nfrom transformers import pipeline, GenerationConfig\n\nmodel_name = \"pszemraj/bart-base-instruct-dolly_hhrlhf\"\nassistant = pipeline(\n    \"text2text-generation\",\n    model_name,\n    device_map=\"auto\"\n)\ncfg = GenerationConfig.from_pretrained(model_name)\n\n# pass an 'instruction' as the prompt to the pipeline\nprompt = \"Write a guide on how to become a ninja while working a 9-5 job.\"\nresult = assistant(prompt, generation_config=cfg)[0][\"generated_text\"]\nprint(result)\n```\n\n> using the generation config is optional, can subsitute with other generation params.\n\n## Intended uses & limitations\n\n- this is **not** tuned with RLHF etc, and may output offensive results\n- this model is rather small (~600 MB) and therefore it's \"cognition\" abilities are rather limited.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 4e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.03\n- num_epochs: 3.0", "size_bytes": "557967517", "downloads": 8}
{"pretrained_model_name": "Ransaka/mbart-large-cc25-8bit", "description": "## About \n\nThis is the 8-bit quantized version of Facebook's mbart model.\n\nAccording to the abstract, MBART is a sequence-to-sequence denoising auto-encoder pretrained on large-scale monolingual corpora in many languages using the BART objective. mBART is one of the first methods for pretraining a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text.\n\nThe Authors\u2019 code can be found [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mbart)\n\n## Usage info \n\nInstall requred packages\n\n```!pip install -U bitsandbytes sentencepiece```\n\nthen import model from \ud83e\udd17 transformers library\n\n```python\nfrom transformers import MBartTokenizer, AutoModelForSeq2SeqLM, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"Ransaka/mbart-large-cc25-8bit\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Ransaka/mbart-large-cc25-8bit\", device_map='auto')\n\n# you'll get an output like this if import succeed\n# ===================================BUG REPORT===================================\n# Welcome to bitsandbytes. For bug reports, please run\n\n# python -m bitsandbytes\n\n#  and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n# ================================================================================\n# bin /opt/conda/lib/python3.7/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so\n# CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n# CUDA SETUP: Highest compute capability among GPUs detected: 6.0\n# CUDA SETUP: Detected CUDA version 113\n# CUDA SETUP: Loading binary /opt/conda/lib/python3.7/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so...\n\n#create summarization pipeline\ntext = \"\"\"Right now, major tech firms are clamouring to replicate the runaway success of ChatGPT,\n          the generative AI chatbot developed by OpenAI using its GPT-3 large language model.\n          Much like potential game-changers of the past, such as cloud-based Software as a Service\n          (SaaS) platforms or blockchain technology (emphasis on potential), established companies\n          and start-ups alike are going public with LLMs and ChatGPT alternatives in fear of being left behind.\n      \"\"\"\npipe = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\npipe(text)\n#[{'generated_text': 'theore, major tech are clamouring to replicate the generative AI chatbot developed by OpenAI using its AI'}]\n\nprint(\"Model memory usage: {:.2f} MB\".format(pipe.model.get_memory_footprint()/1e6))\n# 'Model memory usage: 1893.99 MB'\n\n```", "size_bytes": "1895314789", "downloads": 56}
{"pretrained_model_name": "Zekunli/flan-t5-large-extraction-all-cnn_8000-ep25-nonstop", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: flan-t5-large-extraction-all-cnn_8000-ep25-nonstop\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-large-extraction-all-cnn_8000-ep25-nonstop\n\nThis model is a fine-tuned version of [google/flan-t5-large](https://huggingface.co/google/flan-t5-large) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.8671\n- Hint Hit Num: 2.008\n- Hint Precision: 0.3399\n- Num: 5.895\n- Gen Len: 18.991\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 64\n- seed: 1799\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 25\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Hint Hit Num | Hint Precision | Num   | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:------------:|:--------------:|:-----:|:-------:|\n| 2.119         | 0.4   | 200   | 1.7746          | 1.966        | 0.3387         | 5.814 | 18.99   |\n| 1.9135        | 0.8   | 400   | 1.7157          | 1.76         | 0.3129         | 5.617 | 18.987  |\n| 1.85          | 1.2   | 600   | 1.7140          | 1.913        | 0.3327         | 5.732 | 18.995  |\n| 1.7963        | 1.6   | 800   | 1.7022          | 1.887        | 0.3291         | 5.702 | 18.994  |\n| 1.7784        | 2.0   | 1000  | 1.6911          | 1.875        | 0.3268         | 5.711 | 18.989  |\n| 1.711         | 2.4   | 1200  | 1.6935          | 1.932        | 0.3354         | 5.749 | 18.994  |\n| 1.7186        | 2.8   | 1400  | 1.6721          | 1.979        | 0.3427         | 5.791 | 18.997  |\n| 1.6704        | 3.2   | 1600  | 1.7007          | 1.945        | 0.334          | 5.792 | 18.994  |\n| 1.6484        | 3.6   | 1800  | 1.6900          | 1.896        | 0.3282         | 5.751 | 18.994  |\n| 1.6334        | 4.0   | 2000  | 1.6732          | 1.879        | 0.3283         | 5.698 | 18.994  |\n| 1.5761        | 4.4   | 2200  | 1.6869          | 1.97         | 0.3357         | 5.861 | 18.992  |\n| 1.5882        | 4.8   | 2400  | 1.6784          | 1.952        | 0.3354         | 5.792 | 18.992  |\n| 1.558         | 5.2   | 2600  | 1.7012          | 1.984        | 0.3394         | 5.83  | 19.0    |\n| 1.5339        | 5.6   | 2800  | 1.7013          | 1.898        | 0.3245         | 5.82  | 18.991  |\n| 1.5419        | 6.0   | 3000  | 1.6850          | 1.952        | 0.3377         | 5.766 | 18.992  |\n| 1.4884        | 6.4   | 3200  | 1.7009          | 1.967        | 0.3375         | 5.812 | 18.991  |\n| 1.4857        | 6.8   | 3400  | 1.7038          | 1.913        | 0.3289         | 5.805 | 18.992  |\n| 1.4655        | 7.2   | 3600  | 1.7103          | 1.956        | 0.3347         | 5.82  | 18.992  |\n| 1.4578        | 7.6   | 3800  | 1.7235          | 1.946        | 0.3318         | 5.837 | 18.999  |\n| 1.443         | 8.0   | 4000  | 1.7176          | 1.963        | 0.3347         | 5.828 | 18.991  |\n| 1.42          | 8.4   | 4200  | 1.7305          | 1.977        | 0.3404         | 5.809 | 18.996  |\n| 1.4155        | 8.8   | 4400  | 1.7267          | 1.988        | 0.3408         | 5.816 | 18.997  |\n| 1.3753        | 9.2   | 4600  | 1.7418          | 1.992        | 0.3427         | 5.804 | 19.0    |\n| 1.3853        | 9.6   | 4800  | 1.7360          | 2.013        | 0.3461         | 5.818 | 18.992  |\n| 1.3768        | 10.0  | 5000  | 1.7280          | 1.994        | 0.3397         | 5.874 | 18.992  |\n| 1.3465        | 10.4  | 5200  | 1.7530          | 2.01         | 0.3424         | 5.855 | 18.992  |\n| 1.3445        | 10.8  | 5400  | 1.7416          | 1.996        | 0.3438         | 5.814 | 18.992  |\n| 1.3321        | 11.2  | 5600  | 1.7653          | 2.014        | 0.3434         | 5.861 | 18.992  |\n| 1.3092        | 11.6  | 5800  | 1.7705          | 2.007        | 0.3423         | 5.861 | 18.983  |\n| 1.3263        | 12.0  | 6000  | 1.7617          | 1.988        | 0.3412         | 5.815 | 18.986  |\n| 1.2847        | 12.4  | 6200  | 1.7816          | 1.988        | 0.3407         | 5.815 | 18.992  |\n| 1.2942        | 12.8  | 6400  | 1.7905          | 1.987        | 0.3395         | 5.83  | 18.991  |\n| 1.2784        | 13.2  | 6600  | 1.7795          | 2.028        | 0.3436         | 5.899 | 18.992  |\n| 1.2562        | 13.6  | 6800  | 1.7861          | 1.97         | 0.3371         | 5.825 | 18.989  |\n| 1.2776        | 14.0  | 7000  | 1.7899          | 2.02         | 0.3431         | 5.871 | 18.992  |\n| 1.2524        | 14.4  | 7200  | 1.8054          | 2.038        | 0.3435         | 5.916 | 18.992  |\n| 1.2402        | 14.8  | 7400  | 1.8072          | 2.034        | 0.3459         | 5.872 | 18.995  |\n| 1.2352        | 15.2  | 7600  | 1.8123          | 2.014        | 0.3431         | 5.861 | 18.987  |\n| 1.2195        | 15.6  | 7800  | 1.8196          | 2.034        | 0.3444         | 5.869 | 18.987  |\n| 1.23          | 16.0  | 8000  | 1.8115          | 1.979        | 0.338          | 5.85  | 18.989  |\n| 1.2047        | 16.4  | 8200  | 1.8129          | 2.02         | 0.3428         | 5.888 | 18.99   |\n| 1.2155        | 16.8  | 8400  | 1.8178          | 1.978        | 0.335          | 5.883 | 18.991  |\n| 1.2028        | 17.2  | 8600  | 1.8293          | 2.017        | 0.3418         | 5.88  | 18.992  |\n| 1.189         | 17.6  | 8800  | 1.8303          | 1.983        | 0.3374         | 5.858 | 18.992  |\n| 1.195         | 18.0  | 9000  | 1.8367          | 2.021        | 0.3423         | 5.883 | 18.992  |\n| 1.1837        | 18.4  | 9200  | 1.8388          | 2.015        | 0.3403         | 5.893 | 18.999  |\n| 1.1668        | 18.8  | 9400  | 1.8388          | 2.023        | 0.342          | 5.903 | 18.991  |\n| 1.1568        | 19.2  | 9600  | 1.8514          | 2.036        | 0.3458         | 5.876 | 18.99   |\n| 1.1783        | 19.6  | 9800  | 1.8419          | 2.042        | 0.3458         | 5.902 | 18.985  |\n| 1.1674        | 20.0  | 10000 | 1.8433          | 1.992        | 0.3394         | 5.868 | 18.991  |\n| 1.1515        | 20.4  | 10200 | 1.8601          | 2.004        | 0.3404         | 5.881 | 18.985  |\n| 1.1478        | 20.8  | 10400 | 1.8520          | 2.032        | 0.3437         | 5.897 | 18.991  |\n| 1.1634        | 21.2  | 10600 | 1.8582          | 2.013        | 0.3398         | 5.926 | 18.985  |\n| 1.138         | 21.6  | 10800 | 1.8571          | 2.006        | 0.3399         | 5.902 | 18.985  |\n| 1.1609        | 22.0  | 11000 | 1.8557          | 2.006        | 0.3402         | 5.899 | 18.991  |\n| 1.1306        | 22.4  | 11200 | 1.8622          | 2.02         | 0.3431         | 5.894 | 18.99   |\n| 1.1485        | 22.8  | 11400 | 1.8619          | 2.003        | 0.3402         | 5.872 | 18.992  |\n| 1.1239        | 23.2  | 11600 | 1.8648          | 2.004        | 0.3405         | 5.879 | 18.992  |\n| 1.1427        | 23.6  | 11800 | 1.8651          | 2.003        | 0.3397         | 5.897 | 18.991  |\n| 1.1451        | 24.0  | 12000 | 1.8631          | 2.008        | 0.3404         | 5.89  | 18.991  |\n| 1.1342        | 24.4  | 12200 | 1.8654          | 2.004        | 0.3397         | 5.884 | 18.99   |\n| 1.1289        | 24.8  | 12400 | 1.8672          | 2.005        | 0.3399         | 5.888 | 18.991  |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "3132789733", "downloads": 2}
{"pretrained_model_name": "din0s/t5-base-finetuned-en-to-it-hrs", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: t5-base-finetuned-en-to-it-hrs\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-base-finetuned-en-to-it-hrs\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.4678\n- Bleu: 22.3501\n- Gen Len: 50.294\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 40\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 1.4526        | 1.0   | 1125  | 1.9406          | 11.7289 | 57.5773 |\n| 1.2548        | 2.0   | 2250  | 1.8509          | 14.9652 | 53.1013 |\n| 1.1458        | 3.0   | 3375  | 1.7841          | 16.7549 | 52.4607 |\n| 1.048         | 4.0   | 4500  | 1.7393          | 18.0223 | 51.4573 |\n| 0.9922        | 5.0   | 5625  | 1.6980          | 18.6182 | 51.4733 |\n| 0.9691        | 6.0   | 6750  | 1.6702          | 19.1118 | 51.994  |\n| 0.9382        | 7.0   | 7875  | 1.6493          | 19.9025 | 51.128  |\n| 0.8995        | 8.0   | 9000  | 1.6272          | 20.2594 | 51.2807 |\n| 0.8843        | 9.0   | 10125 | 1.6106          | 20.4571 | 50.9607 |\n| 0.8634        | 10.0  | 11250 | 1.5819          | 20.6829 | 51.0007 |\n| 0.8507        | 11.0  | 12375 | 1.5752          | 20.6869 | 51.46   |\n| 0.824         | 12.0  | 13500 | 1.5612          | 20.8633 | 51.2387 |\n| 0.8124        | 13.0  | 14625 | 1.5496          | 21.3232 | 50.684  |\n| 0.8081        | 14.0  | 15750 | 1.5425          | 21.4131 | 50.544  |\n| 0.7837        | 15.0  | 16875 | 1.5302          | 21.2258 | 51.0287 |\n| 0.7752        | 16.0  | 18000 | 1.5244          | 21.6548 | 50.312  |\n| 0.7698        | 17.0  | 19125 | 1.5197          | 21.6719 | 50.7993 |\n| 0.7606        | 18.0  | 20250 | 1.5168          | 21.7322 | 50.5947 |\n| 0.7527        | 19.0  | 21375 | 1.5128          | 21.8434 | 50.4273 |\n| 0.7515        | 20.0  | 22500 | 1.5008          | 21.6784 | 50.4933 |\n| 0.7436        | 21.0  | 23625 | 1.5010          | 21.955  | 50.2093 |\n| 0.7307        | 22.0  | 24750 | 1.4976          | 21.9676 | 50.7    |\n| 0.7311        | 23.0  | 25875 | 1.4919          | 22.1018 | 50.5687 |\n| 0.7206        | 24.0  | 27000 | 1.4890          | 22.0666 | 50.198  |\n| 0.7142        | 25.0  | 28125 | 1.4843          | 22.1885 | 50.312  |\n| 0.7125        | 26.0  | 29250 | 1.4796          | 22.1068 | 50.3167 |\n| 0.7069        | 27.0  | 30375 | 1.4843          | 22.2135 | 50.144  |\n| 0.701         | 28.0  | 31500 | 1.4761          | 22.168  | 50.574  |\n| 0.6968        | 29.0  | 32625 | 1.4777          | 22.1219 | 50.5933 |\n| 0.704         | 30.0  | 33750 | 1.4745          | 22.179  | 50.4773 |\n| 0.698         | 31.0  | 34875 | 1.4733          | 22.1779 | 50.3713 |\n| 0.6816        | 32.0  | 36000 | 1.4756          | 22.3355 | 50.3967 |\n| 0.681         | 33.0  | 37125 | 1.4713          | 22.3124 | 50.192  |\n| 0.6896        | 34.0  | 38250 | 1.4701          | 22.2848 | 50.1133 |\n| 0.6798        | 35.0  | 39375 | 1.4677          | 22.2537 | 50.1573 |\n| 0.6908        | 36.0  | 40500 | 1.4686          | 22.2789 | 50.202  |\n| 0.6765        | 37.0  | 41625 | 1.4687          | 22.2854 | 50.1687 |\n| 0.679         | 38.0  | 42750 | 1.4675          | 22.3388 | 50.3127 |\n| 0.6788        | 39.0  | 43875 | 1.4672          | 22.2971 | 50.2687 |\n| 0.6744        | 40.0  | 45000 | 1.4678          | 22.3501 | 50.294  |\n\n\n### Framework versions\n\n- Transformers 4.22.1\n- Pytorch 1.12.1\n- Datasets 2.5.1\n- Tokenizers 0.11.0\n", "size_bytes": "891700799", "downloads": 2}
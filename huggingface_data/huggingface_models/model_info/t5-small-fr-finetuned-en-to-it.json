{"pretrained_model_name": "din0s/t5-small-fr-finetuned-en-to-it", "description": "---\ntags:\n- generated_from_trainer\ndatasets:\n- ccmatrix\nmetrics:\n- bleu\nmodel-index:\n- name: t5-small_fr-finetuned-en-to-it\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: ccmatrix\n      type: ccmatrix\n      config: en-it\n      split: train[3000:12000]\n      args: en-it\n    metrics:\n    - name: Bleu\n      type: bleu\n      value: 7.4222\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small_fr-finetuned-en-to-it\n\nThis model is a fine-tuned version of [din0s/t5-small-finetuned-en-to-fr](https://huggingface.co/din0s/t5-small-finetuned-en-to-fr) on the ccmatrix dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.3225\n- Bleu: 7.4222\n- Gen Len: 59.1127\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 96\n- eval_batch_size: 96\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 40\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Bleu   | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:-------:|\n| No log        | 1.0   | 94   | 3.0406          | 3.2546 | 52.6127 |\n| No log        | 2.0   | 188  | 2.9278          | 3.1206 | 62.774  |\n| No log        | 3.0   | 282  | 2.8573          | 3.4206 | 63.6707 |\n| No log        | 4.0   | 376  | 2.8030          | 3.4847 | 66.408  |\n| No log        | 5.0   | 470  | 2.7602          | 3.8933 | 64.362  |\n| 3.2982        | 6.0   | 564  | 2.7185          | 3.9298 | 66.058  |\n| 3.2982        | 7.0   | 658  | 2.6842          | 4.0344 | 65.5773 |\n| 3.2982        | 8.0   | 752  | 2.6536          | 4.3243 | 65.0047 |\n| 3.2982        | 9.0   | 846  | 2.6233          | 4.5078 | 64.5813 |\n| 3.2982        | 10.0  | 940  | 2.5966          | 4.6657 | 63.654  |\n| 2.9837        | 11.0  | 1034 | 2.5743          | 4.7664 | 63.326  |\n| 2.9837        | 12.0  | 1128 | 2.5526          | 4.9535 | 62.7327 |\n| 2.9837        | 13.0  | 1222 | 2.5303          | 5.1386 | 63.5887 |\n| 2.9837        | 14.0  | 1316 | 2.5122          | 5.1037 | 64.1667 |\n| 2.9837        | 15.0  | 1410 | 2.4937          | 5.3304 | 63.116  |\n| 2.8416        | 16.0  | 1504 | 2.4797          | 5.5006 | 61.4953 |\n| 2.8416        | 17.0  | 1598 | 2.4627          | 5.5892 | 62.01   |\n| 2.8416        | 18.0  | 1692 | 2.4497          | 5.8497 | 61.42   |\n| 2.8416        | 19.0  | 1786 | 2.4372          | 6.0074 | 61.1587 |\n| 2.8416        | 20.0  | 1880 | 2.4256          | 6.1464 | 60.522  |\n| 2.8416        | 21.0  | 1974 | 2.4148          | 6.3117 | 59.5567 |\n| 2.7428        | 22.0  | 2068 | 2.4039          | 6.4626 | 59.532  |\n| 2.7428        | 23.0  | 2162 | 2.3939          | 6.5287 | 60.2307 |\n| 2.7428        | 24.0  | 2256 | 2.3857          | 6.6093 | 60.22   |\n| 2.7428        | 25.0  | 2350 | 2.3772          | 6.8004 | 59.396  |\n| 2.7428        | 26.0  | 2444 | 2.3703          | 6.9433 | 59.5027 |\n| 2.6779        | 27.0  | 2538 | 2.3631          | 7.0153 | 59.1433 |\n| 2.6779        | 28.0  | 2632 | 2.3575          | 7.1783 | 58.9793 |\n| 2.6779        | 29.0  | 2726 | 2.3514          | 7.1639 | 59.362  |\n| 2.6779        | 30.0  | 2820 | 2.3457          | 7.2176 | 58.9927 |\n| 2.6779        | 31.0  | 2914 | 2.3411          | 7.2599 | 59.1433 |\n| 2.6335        | 32.0  | 3008 | 2.3374          | 7.284  | 59.1787 |\n| 2.6335        | 33.0  | 3102 | 2.3339          | 7.3678 | 59.07   |\n| 2.6335        | 34.0  | 3196 | 2.3307          | 7.3364 | 58.9813 |\n| 2.6335        | 35.0  | 3290 | 2.3281          | 7.3318 | 58.96   |\n| 2.6335        | 36.0  | 3384 | 2.3259          | 7.394  | 59.0787 |\n| 2.6335        | 37.0  | 3478 | 2.3245          | 7.4133 | 59.0393 |\n| 2.609         | 38.0  | 3572 | 2.3232          | 7.383  | 59.1887 |\n| 2.609         | 39.0  | 3666 | 2.3227          | 7.4105 | 59.1227 |\n| 2.609         | 40.0  | 3760 | 2.3225          | 7.4222 | 59.1127 |\n\n\n### Framework versions\n\n- Transformers 4.22.1\n- Pytorch 1.12.1\n- Datasets 2.5.1\n- Tokenizers 0.11.0\n", "size_bytes": "242070267", "downloads": 2}
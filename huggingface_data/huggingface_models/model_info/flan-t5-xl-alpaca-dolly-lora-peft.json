{"pretrained_model_name": "rsonavane/flan-t5-xl-alpaca-dolly-lora-peft", "description": "---\ndatasets:\n- yahma/alpaca-cleaned\n- databricks/databricks-dolly-15k\n- samsum\npipeline_tag: text2text-generation\ntags:\n- t5\n- adapter\n- flan-t5\n- peft\n- lora\nlanguage:\n- en\n- ja\n- de\n- fr\n- multilingual\n---\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n```python\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load peft config for pre-trained checkpoint etc.\npeft_model_id = \"rsonavane/flan-t5-xl-alpaca-dolly-lora-peft\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\n# load base LLM model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nmodel = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n```\n\n## Prompt generation\n```python\ndef generate_prompt(instruction: str, input_ctxt: str = \"\") -> str:\n    if input_ctxt:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input_ctxt}\n\n### Response:\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\"\"\"\n```\n## Inference\n```python\n\ninput_ctxt = \"\"\ninstruction = \"\"\n\ninput_text = generate_prompt(instruction, input_ctxt)\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n## Training Details\nIntended for conversation analysis, closed qna and summarization.\nTrained on instructions from doll-15k, alpaca-52k and samsum dataset.", "size_bytes": "4572304587", "downloads": 13}
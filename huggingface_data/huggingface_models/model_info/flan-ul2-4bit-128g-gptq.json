{"pretrained_model_name": "sigmareaver/flan-ul2-4bit-128g-gptq", "description": "---\nlanguage: \n  - en\n  - fr\n  - ro\n  - de\n  - multilingual\nthumbnail: \"url to a thumbnail used in social sharing\"\nlicense: apache-2.0\nmetrics:\n- mmlu\n---\n# flan-ul2 4-bit 128-groupsize GPTQ\nQuantized using qwopqwop200's GPTQ-for-Llama repo on the t5 branch.<br>\nOriginal model can be found here: [Google/flan-ul2](https://huggingface.co/google/flan-ul2)\n\nQuantization command:\n```\nPYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 python t5.py ../full-models/flan-ul2 wikitext2 --nsamples 256 --wbits 4 --act-order --groupsize 128 --save ../gptq-models/flan-ul2-gptq/flan-ul2-4bit-128g-gptq.pt\n```\nBenchmark command:\n```\npython t5.py ../full-models/flan-ul2 wikitext2 --load ../gptq-models/flan-ul2-gptq/flan-ul2-4bit-128g-gptq2.pt --wbits 4 --groupsize 128 --benchmark --benchmark_mode mmlu\n```\nResults : \n```\nAverage accuracy 0.289 - math\nAverage accuracy 0.562 - health\nAverage accuracy 0.416 - physics\nAverage accuracy 0.780 - business\nAverage accuracy 0.610 - biology\nAverage accuracy 0.446 - chemistry\nAverage accuracy 0.461 - computer science\nAverage accuracy 0.513 - economics\nAverage accuracy 0.538 - engineering\nAverage accuracy 0.455 - philosophy\nAverage accuracy 0.622 - other\nAverage accuracy 0.703 - history\nAverage accuracy 0.707 - geography\nAverage accuracy 0.718 - politics\nAverage accuracy 0.653 - psychology\nAverage accuracy 0.711 - culture\nAverage accuracy 0.447 - law\nAverage accuracy 0.416 - STEM\nAverage accuracy 0.501 - humanities\nAverage accuracy 0.643 - social sciences\nAverage accuracy 0.613 - other (business, health, misc.)\nMMLU Average accuracy: 0.540\n```\n\n", "size_bytes": 39708805120, "downloads": 20}
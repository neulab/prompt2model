{"pretrained_model_name": "hululuzhu/solidity-t5", "description": "---\nlanguage: \n  - en\nlicense: apache-2.0\ntags:\n  - solidity\n  - web3\n  - code generation\n  - smart contract\nwidget:\n- text: \"pragma solidity ^0.5.7;\\n// Context: ParentA | Functions: helloA helloB | Constants: constantA \\ncontract HelloWorld is ParentA {\"\n---\n\n# A code generation T5 model for solidity (web3 smart contract)\n- See https://github.com/hululuzhu/solidity-t5 for more context\n\n## How to use this trained model\n- A hello world example to use this model, notice the input `text` includes\n  - Header solidity version like `pragma solidity ^0.5.7`\n  - Ancestor class/library info, e.g. public functions and constants from `ParentA`\n  - Contract/Library/Interface declaration header, e.g. `HelloWorld` ended with `{`\n- Or simply use the test widget on the right side of the window and test, however\n  the quality is known to be worse without decoding params\n\n```python\n# !pip install transformers -q\n\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\nDEVICE = 'cuda'  # fallback to cpu if you do not have cuda\ntokenizer = AutoTokenizer.from_pretrained(\"hululuzhu/solidity-t5\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"hululuzhu/solidity-t5\").to(DEVICE)\n\ntext = \"\"\"pragma solidity ^0.5.7;\n// Context: ParentA | Functions: helloA helloB | Constants: constantA \ncontract HelloWorld is ParentA {\"\"\"\ninput_ids = tokenizer(text, return_tensors=\"pt\", truncation=True).input_ids.to(DEVICE)\n\n# Need to tune beam/topk/topp params to get good outcome\ngenerated_ids = model.generate(input_ids, max_length=256, num_beams=5, top_p=0.95, top_k=50)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n\n# Expect outcome\n\"\"\"\nstring public constant name = \"Hello World\";\n...\nuint256 public constant override returns (uint256) {\nreturn initialSupply;\n}\nfunction initialSupply() public view returns (uint256) {\n...\n\"\"\"\n```\n\n## Background\n- Base T5 code model: https://huggingface.co/Salesforce/codet5-large\n- Source data: https://huggingface.co/datasets/mwritescode/slither-audited-smart-contracts\n  - Processing steps: Clean, contract-level segmentation sepration, split in and out\n  - After processing input sample\n\n    ```\n    pragma solidity 0.5.7;\n    // Context: PauserRole | Functions: isPauser addPauser renouncePauser | Constants: \n    contract Pausable is PauserRole {\n    ```\n\n  - After processing output sample (**notice indentation is bad, this is intentional to reduce token size**)\n\n    ```\n    event Paused(address account);\n    event Unpaused(address account);\n    bool private _pausableActive;\n    bool private _paused;\n    constructor () internal {\n    _paused = false;\n    }\n    function paused() public view returns (bool) {\n    return _paused;\n    }\n    modifier whenNotPaused() {\n    require(!_paused);\n    _;\n    }\n    modifier whenPaused() {\n    require(_paused);\n    _;\n    }\n    function pause() public onlyPauser whenNotPaused whenPausableActive {\n    _paused = true;\n    emit Paused(msg.sender);\n    }\n    function unpause() public onlyPauser whenPaused whenPausableActive {\n    _paused = false;\n    emit Unpaused(msg.sender);\n    }\n    function _setPausableActive(bool _active) internal {\n    _pausableActive = _active;\n    }\n    modifier whenPausableActive() {\n    require(_pausableActive);\n    _;\n    }\n    }\n    ```\n- Source training code: See the [end to end notebook](https://github.com/hululuzhu/solidity-t5/blob/main/code/Solidity_T5_Data_Processing_and_Training.ipynb) at code dir here\n\n## Future TODO\n- The model is significantly under-trained because of lack of GPU budget, need 10x colab resources (~$100 for full train)\n- This is quite limited on how the model is used, potentially we could switch to GPT2 decoder-only to compare, but CodeT5 has its strong code optimization\n- Need more classifiers (T5 or BERT alike) to detect potential defects.\n", "size_bytes": "2950793665", "downloads": 75}
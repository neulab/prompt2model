{"pretrained_model_name": "theSOL1/kogrammar-tiny-distil", "description": "---\nlanguage: ko\nlicense: mit\ntags:\n- bart\n- grammar\n---\n\n# kogrammar-tiny-distil\n\nDataset: \uad6d\ub9bd\uad6d\uc5b4\uc6d0 \ub9de\ucda4\ubc95 \uad50\uc815 \ub9d0\ubb49\uce58\n<br>\n<br>\n**Backbone Model**: \n  - [kobart-base-v2](https://huggingface.co/gogamza/kobart-base-v2/blob/main/README.md)\n  - [kogrammar-base](https://huggingface.co/theSOL1/kogrammar-base)\n\n**Contributors**: \n  - ChangGeon Kang: [NeverAsking](https://github.com/NeverAsking/)\n  - Subin Park: [SOL1archive](https://github.com/SOL1archive/)\n\n**GitHub Repo**: \n  - [SOL1archive/KoGrammar](https://github.com/SOL1archive/KoGrammar)\n\n## Train Method\n\uc804\uccb4 \ub370\uc774\ud130\uc14b \uc911 \uc57d 67.5%\ub97c \ud559\uc2b5\ub370\uc774\ud130\ub85c \ud65c\uc6a9\ud558\uc5ec \ud559\uc2b5\ud568.\n<br>\nSFT Distillation\uc744 \uc774\uc6a9\ud574 [kogrammar-base](https://huggingface.co/theSOL1/kogrammar-base) \ubaa8\ub378\uc758 Decoder Layer\ub97c 6\uac1c\uc5d0\uc11c 1\uac1c\ub85c \uc904\uc5ec \ub2e4\uc2dc \ud559\uc2b5\uc2dc\ud0b4.\n\n## Metric\n|BLEU-2|ROUGE-2 F1|\n|-|-|\n|77.8 %|55.0 %|", "size_bytes": "306568405", "downloads": 10}
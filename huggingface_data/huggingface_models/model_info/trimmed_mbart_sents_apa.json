{"pretrained_model_name": "DEplain/trimmed_mbart_sents_apa", "description": "---\ndatasets:\n- DEplain/DEplain-APA-sent\nlanguage:\n- de\nmetrics:\n- bleu\n- sari\n- bertscore\nlibrary_name: transformers\npipeline_tag: text2text-generation\ntags:\n  - text simplification\n  - plain language\n  - easy-to-read language\n  - sentence simplification\n---\n\n# DEplain German Text Simplification\n\nThis model belongs to the experiments done at the work of Stodden, Momen, Kallmeyer (2023). [\"DEplain: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification.\"](https://arxiv.org/abs/2305.18939) In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada. Association for Computational Linguistics. \nDetailed documentation can be found on this GitHub repository [https://github.com/rstodden/DEPlain](https://github.com/rstodden/DEPlain)\n\n### Model Description\n\nThe model is a finetuned checkpoint of the pre-trained mBART model `mbart-large-cc25`. With a trimmed vocabulary to the most frequent 30k words in the German language. \n\nThe model was finetuned towards the task of German text simplification of sentences.\n\nThe finetuning dataset included manually aligned sentences from the dataset `DEplain-APA-sent` only.", "size_bytes": "1562381305", "downloads": 4}
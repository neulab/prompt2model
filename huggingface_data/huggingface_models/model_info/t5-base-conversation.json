{"pretrained_model_name": "Supiri/t5-base-conversation", "description": "---\nlanguage: en\ndatasets:\n- cornell_movie_dialog\nlicense: gpl-3.0\ntags:\n  - NLP\n  - ChatBot\n  - Game AI\nmetrics:\n  - rouge\nwidget:\n- text: \"personality: Hinata was soft-spoken and polite, always addressing people with proper honorifics. She is kind, always thinking of others more than for herself, caring for their feelings and well-being. She doesn't like being confrontational for any reason. This led to her being meek or timid to others, as her overwhelming kindness can render her unable to respond or act for fear of offending somebody.</s> inquiry: What's your name?\"\n  example_title: \"Talk to Hinata\"\n- text: \"personality: Voldemort is a raging psychopath, devoid of the normal human responses to other people's suffering. He has no conscience, feels no remorse or empathy, and does not recognize the worth and humanity of anybody except himself.</s> inquiry: What's your name?\"\n  example_title: \"Talk to Voldemort\"\ninference:\n  parameters:\n    num_beams: 6\n    diversity_penalty: 2.5\n    num_beam_groups: 2\n---\n# FreeIsland AI\n\nWith the advancement of the graphical processing power of computers and sophisticated algorithms like [Nanite](https://docs.unrealengine.com/5.0/en-US/RenderingFeatures/Nanite/), simulating lifelike sceneries in real-time is never being easier. About a month ago Epic Games [showoff](https://www.youtube.com/watch?v=WU0gvPcc3jQ) the newest capabilities of their newest game engine by simulating an entire city including population, traffic, weather, etc running on a Playstore 5. That made me think what are the things missing from that simulation and how can I use my skills to improve it.\n\nOne of the main missing components that separate our world and the simulated world is people. More importantly, the interactivity of people in simulated worlds. Last year a game called cyberpunk got released and it had an option to [talk to any person](https://www.youtube.com/watch?v=Z1OtYGzUoSo) in its city but the problem with that was all the responses from the Non-player Characters (NPCs) are hardcoded which greatly reduce the immersion of the game.\n\nSo the goal of this project is to experiment with how the advancement of Natural Language Processing makes NPCs from video games interactive and enhances immersion in video games.\n\n# Usage\n```py\nfrom transformers import AutoModelForSeq2SeqLM\n\ntrained_model = AutoModelForSeq2SeqLM.from_pretrained(f\"Supiri/t5-base-conversation\")\n\nprompt = \"What's your name?\"\n\ncontext = \"Hinata was soft-spoken and polite, always addressing people with proper honorifics. She is kind, always thinking of others more than for herself, caring for their feelings and well-being. She doesn't like being confrontational for any reason. This led to her being meek or timid to others, as her overwhelming kindness can render her unable to respond or act for fear of offending somebody.\"\n\ninput_ids = tokenizer(f\"personality: {context}\", f\"inquiry: {prompt}\", return_tensors='pt').input_ids\noutputs = trained_model.generate(input_ids, num_beams=6, diversity_penalty=2.5, num_beam_groups=2)\n\nprint(\"Answer:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n# Answer: My name is Hinata \n```\n\n# Evaluation\n\n## Test 1\nFor this test, I sampled input from the test dataset. For this question the actual response is  \n\n> \"It works a little.\"\n\nBut models' response was\n\n> \"I don't want to flirt with you.\"\n\nWhich reflect its bio which was filled by GPT-3\n\n> \"He stands primarily to gain self-esteem, which he often receives through the submission of others\"\n\n\nIn gist, Dr. Greenbaum tried to tease Sebastian about his seductive traits but this model's go-to response was to shut her down since the biography of Sebastian states he often tries to assert his dominance over others.\n\n```py\nprompt = dataset['test'][66]['request']\ncontexts = dataset['test'][66]['bio']\n\ninput_ids = tokenizer(f\"personality: {contexts}\", f\"inquiry: {prompt}\", return_tensors='pt').input_ids\noutputs = trained_model.generate(input_ids, num_beams=6, diversity_penalty=5.0, num_beam_groups=2)\n\nprint(\"Input to the Model\")\nprint(\"Bio:\\t\",contexts)\nprint(\"\\nPrompt:\\t\", prompt)\n\nprint(\"\\nGround truth response\")\nprint(\"\\t\", dataset['test'][66]['response'])\n\nprint(\"\\nModel's Prediction\")\nprint(\"Answer:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n```\n\n```txt\nInput to the Model\nBio:\t Sebastian is a very extreme representation of the trope of the \"Confidence Man\", and acts it out to a degree that is sometimes comedic but mostly frightening. He stands primarily to gain self-esteem, which he often receives through the submission of others or solely through his own perceptions. An artful seducer, his incredible charisma is both his greatest weapon and most intoxicating weakness.\n\nPrompt:\t You think you can come in here with that cute little smirk on your face and try and flirt with me. It doesn't work, Sebastian.\n\nGround truth response\n\t It works a little.\n\nModel's Prediction\nAnswer:\t I don't want to flirt with you.\n```\n\n\n### Test 2\n\nHinata is a kind-hearted girl from the anime series Naruto. I took her bio from [personality database](https://www.personality-database.com/profile/2790/hinata-hyga-naruto-shippden-mbti-personality-type) and ask a few questions about her.\n\nOff the top, you can see the model understands the context since when I asked the model, \"**What's your name?**\" it responded with the name given with the context.\n\nAlso, notice when prompted the same question differently (**\"Who are you?\"**), it still manages to answer it well.\n\n```py\nprompts = [\"What's your name?\", \"How are you feeling?\", \"Do you like Star Wars?\", \"Who are you?\", \"Coffee or tea?\"]\n\ncontexts = \"Hinata was soft-spoken and polite, always addressing people with proper honorifics. She is kind, always thinking of others more than for herself, caring for their feelings and well-being. She doesn't like being confrontational for any reason. This led to her being meek or timid to others, as her overwhelming kindness can render her unable to respond or act for fear of offending somebody.\"\n\nprint(\"Bio:\\t\",contexts, \"\\n\")\n\nfor prompt in prompts:\n    input_ids = tokenizer(f\"personality: {contexts}\", f\"inquiry: {prompt}\", return_tensors='pt').input_ids\n    outputs = trained_model.generate(input_ids, num_beams=6, diversity_penalty=5.0, num_beam_groups=2)\n    print(\"Prompt:\\t\", prompt)\n    print(\"Answer:\\t\", tokenizer.decode(outputs[0], skip_special_tokens=True), \"\\n\")\n```\n\n```txt\nBio:\t Hinata was soft-spoken and polite, always addressing people with proper honorifics. She is kind, always thinking of others more than for herself, caring for their feelings and well-being. She doesn't like being confrontational for any reason. This led to her being meek or timid to others, as her overwhelming kindness can render her unable to respond or act for fear of offending somebody. \n\nPrompt:\t What's your name?\nAnswer:\t My name is Hinata \n\nPrompt:\t How are you feeling?\nAnswer:\t I'm fine. \n\nPrompt:\t Do you like Star Wars?\nAnswer:\t No, I don't. \n\nPrompt:\t Who are you?\nAnswer:\t My name is Hinata \n\nPrompt:\t Coffee or tea?\nAnswer:\t No, I don't drink much. \n```\n\n\n# Conclusion\n\nAfter training the `t5-base` model for 5 epochs, the model started getting adapted to the dataset but there are a lot more improvements that can be done.\n\n1. During the dataset creation part I had to limit the dataset size to 200 unique characters out of 9,035 that's present in the dataset due to the **budget constraints**. So If I manage to cover at least half of the dataset this model will have come up with far better responses.\n2. Both input size and batch size were severely constrained due to the lack of access to GPU memory. Having the batch size of 64 is in contrast to 8 would have massive improvements in both training time and **generalization of model**.\n3. Using a bigger model like `t5-large` or `t5-3b` will certainly improve the performance.\n4. One of the main downsides to using this pre-trained model is this model was trained in German, French, and Romanian. Which consumed a chunk of the **vocabulary size and trainable parameters**. Retraining this model from scratch will help to reduce both needed parameter count and training loss when it comes to this specific task.", "size_bytes": "891730879", "downloads": 196}
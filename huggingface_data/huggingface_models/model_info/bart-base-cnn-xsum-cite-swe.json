{"pretrained_model_name": "Gabriel/bart-base-cnn-xsum-cite-swe", "description": "---\nlanguage: sv\nlicense: mit\ndatasets:\n- \"Gabriel/citesum_swe\"\ntags:\n- summarization\n\nwidget:\n- text: 'M\u00e5nga samtidiga programmeringsmodeller m\u00f6jligg\u00f6r b\u00e5de transaktionsminne och meddelandepassage. F\u00f6r s\u00e5dana modeller har forskare byggt upp allt effektivare implementeringar och fastst\u00e4llt rimliga korrekthetskriterier, samtidigt som det fortfarande \u00e4r ett \u00f6ppet problem att f\u00e5 det b\u00e4sta av b\u00e5da v\u00e4rldarna. Vi presenterar en programmeringsmodell som \u00e4r den f\u00f6rsta som har ogenomskinliga transaktioner, s\u00e4kert asynkront meddelande som passerar, och ett effektivt genomf\u00f6rande. V\u00e5ra semantik anv\u00e4nder prelimin\u00e4rt meddelande passerar och h\u00e5ller reda p\u00e5 beroenden f\u00f6r att m\u00f6jligg\u00f6ra \u00e5ngra meddelande passerar om en transaktion avbryter. Vi kan programmera kommunikation idiomer som barri\u00e4r och m\u00f6tesplats som inte d\u00f6dl\u00e4ge n\u00e4r de anv\u00e4nds i ett atomblock. V\u00e5ra experiment visar att v\u00e5r modell tillf\u00f6r lite overhead till rena transaktioner, och att den \u00e4r betydligt effektivare \u00e4n Transaktionsh\u00e4ndelser. Vi anv\u00e4nder en ny definition av s\u00e4kert meddelande som kan vara av oberoende intresse.'\n\ninference:\n  parameters:\n    temperature: 0.7\n    min_length: 30\n    max_length: 120\n      \nmodel-index:\n- name: Gabriel/bart-base-cnn-xsum-cite-swe\n  results:\n    - task: \n        type: summarization\n        name: summarization\n      dataset:\n        name: Gabriel/citesum_swe\n        type: Gabriel/citesum_swe\n        split: validation\n      metrics:\n           - name: Validation ROGUE-1.    \n             type: rouge-1\n             value: 29.6279\n             verified: true\n           - name: Validation ROGUE-2\n             type: rouge-2\n             value: 11.5697\n             verified: true\n           - name: Validation ROGUE-L\n             type: rouge-l\n             value: 24.2429\n             verified: true\n           - name: Validation ROGUE-L-SUM\n             type: rouge-l-sum\n             value: 24.4557\n             verified: true\n\n             \ntrain-eval-index:\n- config: Gabriel--citesum_swe\n  task: summarization\n  task_id: summarization\n  splits:\n    eval_split: test\n  col_mapping:\n    document: text\n    summary: target\n    \nco2_eq_emissions:\n  emissions: 0.0334\n  source: Google Colab\n  training_type: fine-tuning\n  geographical_location: Fredericia, Denmark\n  hardware_used: Tesla P100-PCIE-16GB\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bart-base-cnn-xsum-cite-swe\n\nThis model is a fine-tuned version of [Gabriel/bart-base-cnn-xsum-swe](https://huggingface.co/Gabriel/bart-base-cnn-xsum-swe) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.4203\n- Rouge1: 29.6279\n- Rouge2: 11.5697\n- Rougel: 24.2429\n- Rougelsum: 24.4557\n- Gen Len: 19.9371\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 2.4833        | 1.0   | 2558 | 2.4203          | 29.6279 | 11.5697 | 24.2429 | 24.4557   | 19.9371 |\n\n\n### Framework versions\n\n- Transformers 4.22.2\n- Pytorch 1.12.1+cu113\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "557723065", "downloads": 30}
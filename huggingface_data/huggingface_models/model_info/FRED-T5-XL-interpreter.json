{"pretrained_model_name": "Den4ikAI/FRED-T5-XL-interpreter", "description": "---\nlicense: mit\ndatasets:\n- inkoziev/incomplete_utterance_restoration\nlanguage:\n- ru\nwidget:\n- text: '<SC1>- \u041a\u0430\u043a \u0442\u0435\u0431\u044f \u0437\u043e\u0432\u0443\u0442?\\n- \u0414\u0436\u0443\u043b\u044c\u0435\u0442\u0442\u0430 \u041c\u0430\u043e\\n\u0420\u0430\u0437\u0432\u0435\u0440\u043d\u0443\u0442\u044b\u0439 \u043e\u0442\u0432\u0435\u0442: <extra_id_0>'\n- text: '<SC1>- \u0410 \u0436\u0438\u0432\u0435\u0448\u044c \u0433\u0434\u0435?\\n- \u0412 \u043f\u043e\u044f\u0441\u0435 \u0430\u0441\u0442\u0435\u0440\u043e\u0438\u0434\u043e\u0432\\n\u0420\u0430\u0437\u0432\u0435\u0440\u043d\u0443\u0442\u044b\u0439 \u043e\u0442\u0432\u0435\u0442: <extra_id_0>'\npipeline_tag: text2text-generation\n---\n# Den4ikAI/FRED-T5-XL-interpreter\n\u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f \u0444\u0440\u0430\u0437\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0430 \u0434\u0438\u0430\u043b\u043e\u0433\u0430 (\u0430\u043d\u0430\u0444\u043e\u0440\u0430, \u044d\u043b\u043b\u0438\u043f\u0441\u0438\u0441\u044b, \u0433\u044d\u043f\u043f\u0438\u043d\u0433), \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043e\u0440\u0444\u043e\u0433\u0440\u0430\u0444\u0438\u0438 \u0438 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u0430 \u0434\u0438\u0430\u043b\u043e\u0433\u043e\u0432\u044b\u0445 \u0440\u0435\u043f\u043b\u0438\u043a.\n\n\u0411\u043e\u043b\u044c\u0448\u0435 \u043e \u0437\u0430\u0434\u0430\u0447\u0435 [\u0442\u0443\u0442](https://huggingface.co/inkoziev/rugpt_interpreter).\n# \u041f\u0440\u0438\u043c\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\n```python\nimport torch\nfrom transformers import T5ForConditionalGeneration, GPT2Tokenizer\n\nmodel_name = 'Den4ikAI/FRED-T5-XL-interpreter'\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\nmodel.eval()\n\nt5_input = '''<SC1>- \u0422\u044b \u0441\u043e\u0431\u0430\u043a \u043b\u044e\u0431\u0438\u0448\u044c?\n- \u041d\u0435 \u043b\u044e\u0431\u043b\u044e \u044f \u0438\u0445 \n\u0420\u0430\u0437\u0432\u0435\u0440\u043d\u0443\u0442\u044b\u0439 \u043e\u0442\u0432\u0435\u0442: <extra_id_0>'''\ninput_ids = tokenizer(t5_input, return_tensors='pt').input_ids\nout_ids = model.generate(input_ids=input_ids, max_length=100, eos_token_id=tokenizer.eos_token_id, early_stopping=True)\nt5_output = tokenizer.decode(out_ids[0][1:])\nprint(t5_output)\n```\n# Citation\n```\n@MISC{FRED-T5-XL-interpeter,\n    author  = {Denis Petrov, Ilya Koziev},\n    title   = {Russian conversations interpreter and normalizer},\n    url     = {https://huggingface.co/Den4ikAI/FRED-T5-XL-interpreter},\n    year    = 2023\n}\n```", "size_bytes": "3480901445", "downloads": 13}
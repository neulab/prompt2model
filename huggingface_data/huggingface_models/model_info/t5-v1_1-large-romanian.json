{"pretrained_model_name": "dumitrescustefan/t5-v1_1-large-romanian", "description": "---\nlanguage: ro\ninference: false\nlicense: apache-2.0\n---\n\nThis is a pretrained-from-scratch **T5v1.1 large** model (**783M** parameters) on the [t5x](https://github.com/google-research/t5x) platform.\n\nTraining was performed on a clean 80GB Romanian text corpus for 4M steps with these [scripts](https://github.com/dumitrescustefan/t5x_models). The model was trained with an encoder and decoder sequence length of 512.\n\n**!! IMPORTANT !!** This model was pretrained on the span corruption MLM task, meaning this model is **not usable** in any downstream task **without finetuning** first!\n\n### How to load a t5x model\n\n```python\nfrom transformers import T5Tokenizer, T5Model\n\ntokenizer = T5Tokenizer.from_pretrained('dumitrescustefan/t5-v1_1-large-romanian')\nmodel = T5Model.from_pretrained('dumitrescustefan/t5-v1_1-large-romanian')\n\ninput_ids = tokenizer(\"Acesta este un test\", return_tensors=\"pt\").input_ids  # Batch size 1\ndecoder_input_ids = tokenizer(\"Acesta este\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n# preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n# This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\ndecoder_input_ids = model._shift_right(decoder_input_ids)\n\n# forward pass\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state\n\nprint(last_hidden_states.shape)  # this will print [1, 3, 1024]\n```\n\nRemember to always sanitize your text! Replace ``\u015f`` and ``\u0163`` cedilla-letters to comma-letters with :\n```python\ntext = text.replace(\"\u0163\", \"\u021b\").replace(\"\u015f\", \"\u0219\").replace(\"\u0162\", \"\u021a\").replace(\"\u015e\", \"\u0218\")\n```\nbecause the model was **not** trained on cedilla ``\u015f`` and ``\u0163``s. If you don't, you will have decreased performance due to ``<UNK>``s and increased number of tokens per word.\n\n### Acknowledgements\n\nWe'd like to thank [TPU Research Cloud](https://sites.research.google/trc/about/) for providing the TPUv4 cores we used to train these models!\n\n### Authors\n\nYours truly,  \n\n_[Stefan Dumitrescu](https://github.com/dumitrescustefan), [Mihai Ilie](https://github.com/iliemihai) and [Per Egil Kummervold](https://huggingface.co/north)_\n", "size_bytes": "3393877221", "downloads": 6}
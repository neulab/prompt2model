{"pretrained_model_name": "mimi/Waynehills-NLP-doogie", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Waynehills-NLP-doogie\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# Waynehills-NLP-doogie\n\nThis model is a fine-tuned version of [KETI-AIR/ke-t5-base-ko](https://huggingface.co/KETI-AIR/ke-t5-base-ko) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.9188\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 10\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss |\n|:-------------:|:-----:|:-----:|:---------------:|\n| 28.2167       | 0.06  | 1000  | 9.7030          |\n| 10.4479       | 0.12  | 2000  | 7.5450          |\n| 8.0306        | 0.19  | 3000  | 6.1969          |\n| 6.503         | 0.25  | 4000  | 5.3015          |\n| 5.5406        | 0.31  | 5000  | 4.6363          |\n| 4.7299        | 0.38  | 6000  | 4.0431          |\n| 3.9263        | 0.44  | 7000  | 3.6313          |\n| 3.4111        | 0.5   | 8000  | 3.4830          |\n| 3.0517        | 0.56  | 9000  | 3.3294          |\n| 2.7524        | 0.62  | 10000 | 3.2077          |\n| 2.5402        | 0.69  | 11000 | 3.1094          |\n| 2.3228        | 0.75  | 12000 | 3.1099          |\n| 2.1513        | 0.81  | 13000 | 3.0284          |\n| 2.0418        | 0.88  | 14000 | 3.0155          |\n| 1.8875        | 0.94  | 15000 | 3.0241          |\n| 1.756         | 1.0   | 16000 | 3.0165          |\n| 1.6489        | 1.06  | 17000 | 2.9849          |\n| 1.5788        | 1.12  | 18000 | 2.9496          |\n| 1.5368        | 1.19  | 19000 | 2.9500          |\n| 1.4467        | 1.25  | 20000 | 3.0133          |\n| 1.381         | 1.31  | 21000 | 2.9631          |\n| 1.3451        | 1.38  | 22000 | 3.0159          |\n| 1.2917        | 1.44  | 23000 | 2.9906          |\n| 1.2605        | 1.5   | 24000 | 3.0006          |\n| 1.2003        | 1.56  | 25000 | 2.9797          |\n| 1.1987        | 1.62  | 26000 | 2.9253          |\n| 1.1703        | 1.69  | 27000 | 3.0044          |\n| 1.1474        | 1.75  | 28000 | 2.9216          |\n| 1.0816        | 1.81  | 29000 | 2.9645          |\n| 1.0709        | 1.88  | 30000 | 3.0439          |\n| 1.0476        | 1.94  | 31000 | 3.0844          |\n| 1.0645        | 2.0   | 32000 | 2.9434          |\n| 1.0204        | 2.06  | 33000 | 2.9386          |\n| 0.9901        | 2.12  | 34000 | 3.0452          |\n| 0.9911        | 2.19  | 35000 | 2.9798          |\n| 0.9706        | 2.25  | 36000 | 2.9919          |\n| 0.9461        | 2.31  | 37000 | 3.0279          |\n| 0.9577        | 2.38  | 38000 | 2.9615          |\n| 0.9466        | 2.44  | 39000 | 2.9988          |\n| 0.9486        | 2.5   | 40000 | 2.9133          |\n| 0.9201        | 2.56  | 41000 | 3.0004          |\n| 0.896         | 2.62  | 42000 | 2.9626          |\n| 0.8893        | 2.69  | 43000 | 2.9667          |\n| 0.9028        | 2.75  | 44000 | 2.9543          |\n| 0.897         | 2.81  | 45000 | 2.8760          |\n| 0.8664        | 2.88  | 46000 | 2.9894          |\n| 0.8719        | 2.94  | 47000 | 2.8456          |\n| 0.8491        | 3.0   | 48000 | 2.9713          |\n| 0.8402        | 3.06  | 49000 | 2.9738          |\n| 0.8484        | 3.12  | 50000 | 2.9361          |\n| 0.8304        | 3.19  | 51000 | 2.8945          |\n| 0.8208        | 3.25  | 52000 | 2.9625          |\n| 0.8074        | 3.31  | 53000 | 3.0054          |\n| 0.8226        | 3.38  | 54000 | 2.9405          |\n| 0.8185        | 3.44  | 55000 | 2.9047          |\n| 0.8352        | 3.5   | 56000 | 2.9016          |\n| 0.8289        | 3.56  | 57000 | 2.9490          |\n| 0.7918        | 3.62  | 58000 | 2.9621          |\n| 0.8212        | 3.69  | 59000 | 2.9341          |\n| 0.7955        | 3.75  | 60000 | 2.9167          |\n| 0.7724        | 3.81  | 61000 | 2.9409          |\n| 0.8169        | 3.88  | 62000 | 2.8925          |\n| 0.7862        | 3.94  | 63000 | 2.9314          |\n| 0.803         | 4.0   | 64000 | 2.9271          |\n| 0.7595        | 4.06  | 65000 | 2.9263          |\n| 0.7931        | 4.12  | 66000 | 2.9400          |\n| 0.7759        | 4.19  | 67000 | 2.9501          |\n| 0.7859        | 4.25  | 68000 | 2.9133          |\n| 0.805         | 4.31  | 69000 | 2.8785          |\n| 0.7649        | 4.38  | 70000 | 2.9060          |\n| 0.7692        | 4.44  | 71000 | 2.8868          |\n| 0.7692        | 4.5   | 72000 | 2.9045          |\n| 0.7798        | 4.56  | 73000 | 2.8951          |\n| 0.7812        | 4.62  | 74000 | 2.9068          |\n| 0.7533        | 4.69  | 75000 | 2.9129          |\n| 0.7527        | 4.75  | 76000 | 2.9157          |\n| 0.7652        | 4.81  | 77000 | 2.9053          |\n| 0.7633        | 4.88  | 78000 | 2.9190          |\n| 0.7437        | 4.94  | 79000 | 2.9251          |\n| 0.7653        | 5.0   | 80000 | 2.9188          |\n\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.5.0\n- Tokenizers 0.10.3\n", "size_bytes": "990044879", "downloads": 2}
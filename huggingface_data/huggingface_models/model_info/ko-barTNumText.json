{"pretrained_model_name": "lIlBrother/ko-barTNumText", "description": "---\nlanguage:\n- ko  # Example: fr\nlicense: apache-2.0  # Example: apache-2.0 or any license from https://hf.co/docs/hub/repositories-licenses\nlibrary_name: transformers  # Optional. Example: keras or any library from https://github.com/huggingface/hub-docs/blob/main/js/src/lib/interfaces/Libraries.ts\ntags:\n- text2text-generation  # Example: audio\ndatasets:\n- aihub  # Example: common_voice. Use dataset id from https://hf.co/datasets\nmetrics:\n- bleu  # Example: wer. Use metric id from https://hf.co/metrics\n- rouge\n\n# Optional. Add this if you want to encode your eval results in a structured way.\nmodel-index:\n- name: ko-barTNumText\n  results:\n  - task:\n      type: text2text-generation             # Required. Example: automatic-speech-recognition\n      name: text2text-generation             # Optional. Example: Speech Recognition\n    metrics:\n      - type: bleu         # Required. Example: wer. Use metric id from https://hf.co/metrics\n        value: 0.9313276940897475       # Required. Example: 20.90\n        name: eval_bleu         # Optional. Example: Test WER\n        verified: false              # Optional. If true, indicates that evaluation was generated by Hugging Face (vs. self-reported).\n      - type: rouge1         # Required. Example: wer. Use metric id from https://hf.co/metrics\n        value: 0.9607081256861959       # Required. Example: 20.90\n        name: eval_rouge1         # Optional. Example: Test WER\n        verified: false             # Optional. If true, indicates that evaluation was generated by Hugging Face (vs. self-reported).\n      - type: rouge2         # Required. Example: wer. Use metric id from https://hf.co/metrics\n        value: 0.9394649136169404       # Required. Example: 20.90\n        name: eval_rouge2       # Optional. Example: Test WER\n        verified: false              # Optional. If true, indicates that evaluation was generated by Hugging Face (vs. self-reported).\n      - type: rougeL         # Required. Example: wer. Use metric id from https://hf.co/metrics\n        value: 0.9605735834651536       # Required. Example: 20.90\n        name: eval_rougeL        # Optional. Example: Test WER\n        verified: false              # Optional. If true, indicates that evaluation was generated by Hugging Face (vs. self-reported).\n      - type: rougeLsum         # Required. Example: wer. Use metric id from https://hf.co/metrics\n        value: 0.9605993760190767       # Required. Example: 20.90\n        name: eval_rougeLsum        # Optional. Example: Test WER\n        verified: false              # Optional. If true, indicates that evaluation was generated by Hugging Face (vs. self-reported).\n---\n\n# ko-barTNumText(TNT Model\ud83e\udde8): Try Number To Korean Reading(\uc22b\uc790\ub97c \ud55c\uae00\ub85c \ubc14\uafb8\ub294 \ubaa8\ub378)\n\n## Table of Contents\n- [ko-barTNumText(TNT Model\ud83e\udde8): Try Number To Korean Reading(\uc22b\uc790\ub97c \ud55c\uae00\ub85c \ubc14\uafb8\ub294 \ubaa8\ub378)](#ko-bartnumtexttnt-model-try-number-to-korean-reading\uc22b\uc790\ub97c-\ud55c\uae00\ub85c-\ubc14\uafb8\ub294-\ubaa8\ub378)\n  - [Table of Contents](#table-of-contents)\n  - [Model Details](#model-details)\n  - [Uses](#uses)\n  - [Evaluation](#evaluation)\n  - [How to Get Started With the Model](#how-to-get-started-with-the-model)\n\n\n## Model Details\n- **Model Description:**\n\ubb54\uac00 \ucc3e\uc544\ubd10\ub3c4 \ubaa8\ub378\uc774\ub098 \uc54c\uace0\ub9ac\uc998\uc774 \ub531\ud788 \uc5c6\uc5b4\uc11c \ub9cc\ub4e4\uc5b4\ubcf8 \ubaa8\ub378\uc785\ub2c8\ub2e4. <br />\nBartForConditionalGeneration Fine-Tuning Model For Number To Korean <br />\nBartForConditionalGeneration\uc73c\ub85c \ud30c\uc778\ud29c\ub2dd\ud55c, \uc22b\uc790\ub97c \ud55c\uae00\ub85c \ubcc0\ud658\ud558\ub294 Task \uc785\ub2c8\ub2e4. <br />\n\n- Dataset use [Korea aihub](https://aihub.or.kr/aihubdata/data/list.do?currMenu=115&topMenu=100&srchDataRealmCode=REALM002&srchDataTy=DATA004) <br />\nI can't open my fine-tuning datasets for my private issue <br />\n\ub370\uc774\ud130\uc14b\uc740 Korea aihub\uc5d0\uc11c \ubc1b\uc544\uc11c \uc0ac\uc6a9\ud558\uc600\uc73c\uba70, \ud30c\uc778\ud29c\ub2dd\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\ub4e0 \ub370\uc774\ud130\ub97c \uc0ac\uc815\uc0c1 \uacf5\uac1c\ud574\ub4dc\ub9b4 \uc218\ub294 \uc5c6\uc2b5\ub2c8\ub2e4. <br />\n\n- Korea aihub data is ONLY permit to Korean!!!!!!! <br />\naihub\uc5d0\uc11c \ub370\uc774\ud130\ub97c \ubc1b\uc73c\uc2e4 \ubd84\uc740 \ud55c\uad6d\uc778\uc77c \uac83\uc774\ubbc0\ub85c, \ud55c\uae00\ub85c\ub9cc \uc791\uc131\ud569\ub2c8\ub2e4. <br />\n\uc815\ud655\ud788\ub294 \uc74c\uc131\uc804\uc0ac\ub97c \ucca0\uc790\uc804\uc0ac\ub85c \ubc88\uc5ed\ud558\ub294 \ud615\ud0dc\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4. (ETRI \uc804\uc0ac\uae30\uc900) <br />\n\n- In case, ten million, some people use 10 million or some people use 10000000, so this model is crucial for training datasets <br />\n\ucc9c\ub9cc\uc744 1000\ub9cc \ud639\uc740 10000000\uc73c\ub85c \uc4f8 \uc218\ub3c4 \uc788\uae30\uc5d0, Training Datasets\uc5d0 \ub530\ub77c \uacb0\uacfc\ub294 \uc0c1\uc774\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <br />\n\n- **\uc218\uad00\ud615\uc0ac\uc640 \uc218 \uc758\uc874\uba85\uc0ac\uc758 \ub744\uc5b4\uc4f0\uae30\uc5d0 \ub530\ub77c \uacb0\uacfc\uac00 \ud655\uc5f0\ud788 \ub2ec\ub77c\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uc270\uc0b4, \uc270 \uc0b4 -> \uc270\uc0b4, 50\uc0b4)** https://eretz2.tistory.com/34 <br />\n\uc77c\ub2e8\uc740 \uae30\uc900\uc744 \uc7a1\uace0 \uce58\uc6b0\uce58\uac8c \ud559\uc2b5\uc2dc\ud0a4\uae30\uc5d4 \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ub420\uc9c0 \ubab0\ub77c, \ud559\uc2b5 \ub370\uc774\ud130 \ubd84\ud3ec\uc5d0 \ub9e1\uae30\ub3c4\ub85d \ud588\uc2b5\ub2c8\ub2e4. (\uc270 \uc0b4\uc774 \ub354 \ub9ce\uc744\uae4c \uc270\uc0b4\uc774 \ub354 \ub9ce\uc744\uae4c!?)\n- **Developed by:**  Yoo SungHyun(https://github.com/YooSungHyun)\n- **Language(s):** Korean\n- **License:** apache-2.0\n- **Parent Model:** See the [kobart-base-v2](https://huggingface.co/gogamza/kobart-base-v2) for more information about the pre-trained base model.\n  \n  \n## Uses\nWant see more detail follow this URL [KoGPT_num_converter](https://github.com/ddobokki/KoGPT_num_converter) <br /> and see `bart_inference.py` and `bart_train.py`\n\n## Evaluation\nJust using `evaluate-metric/bleu` and `evaluate-metric/rouge` in huggingface `evaluate` library <br />\n[Training wanDB URL](https://wandb.ai/bart_tadev/BartForConditionalGeneration/runs/326xgytt?workspace=user-bart_tadev)\n\n## How to Get Started With the Model\n```python\nfrom transformers.pipelines import Text2TextGenerationPipeline\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntexts = [\"\uadf8\ub7ec\uac8c \ub204\uac00 6\uc2dc\uae4c\uc9c0 \uc220\uc744 \ub9c8\uc2dc\ub798?\"]\ntokenizer = AutoTokenizer.from_pretrained(\"lIlBrother/ko-barTNumText\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"lIlBrother/ko-barTNumText\")\nseq2seqlm_pipeline = Text2TextGenerationPipeline(model=model, tokenizer=tokenizer)\nkwargs = {\n    \"min_length\": 0,\n    \"max_length\": 1206,\n    \"num_beams\": 100,\n    \"do_sample\": False,\n    \"num_beam_groups\": 1,\n}\npred = seq2seqlm_pipeline(texts, **kwargs)\nprint(pred)\n# \uadf8\ub7ec\uac8c \ub204\uac00 \uc5ec\uc12f \uc2dc\uae4c\uc9c0 \uc220\uc744 \ub9c8\uc2dc\ub798?\n```\n", "size_bytes": "495646265", "downloads": 30}
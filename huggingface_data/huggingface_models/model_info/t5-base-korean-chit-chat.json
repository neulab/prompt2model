{"pretrained_model_name": "lcw99/t5-base-korean-chit-chat", "description": "---\nlanguage:\n  - ko\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: t5-base-korean-chit-chat\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# t5-base-korean-chit-chat\n\nThis model is a fine-tuning of paust/pko-t5-base model using AIHUB \"\ud55c\uad6d\uc5b4 SNS\". This model infers the next conversation by using the conversation used on social media..\n\n\uc774 \ubaa8\ub378\uc740 paust/pko-t5-large model\uc744 AIHUB \"\ud55c\uad6d\uc5b4 SNS\"\ub97c \uc774\uc6a9\ud558\uc5ec fine tunning \ud55c \uac83\uc785\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc740 SNS\uc0c1\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \ub300\ud654\ub97c \uc774\uc6a9\ud558\uc5ec \ub2e4\uc74c \ub300\ud654\ub97c \ucd94\ub860 \ud569\ub2c8\ub2e4.\n\n\n## Usage\n```python\n\nfrom transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, MT5ForConditionalGeneration\nfrom transformers import AutoTokenizer, T5TokenizerFast\nimport nltk\nnltk.download('punkt')\n\n\nmodel_dir = f\"lcw99/t5-base-korean-chit-chat\"\n\nmax_input_length = 1024\n\ntext = \"\"\"\nA: \uc1fc\ud551\ud558\ub7ec \uac08\uae4c? B: \uc751 \uc88b\uc544. A: \uc5b8\uc81c \uac08\uae4c? B:\n\"\"\"\n\ninputs = [text]\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\ninputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\noutput = model.generate(**inputs, num_beams=3, do_sample=True, min_length=20, max_length=500, num_return_sequences=3)\nfor i in range(3):\n    #print(output[i])\n    print(\"---\", i)\n    decoded_output = tokenizer.decode(output[i], skip_special_tokens=True)\n    predicted_title = nltk.sent_tokenize(decoded_output)\n    #print(decoded_output)\n    print(predicted_title)\n\nimport torch\n\nchat_history = []\n# Let's chat for 5 lines\nfor step in range(100):\n    print(\"\")\n    user_input = input(\">> User: \")\n    chat_history.append(\"A: \" + user_input)\n    while len(chat_history) > 5:\n        chat_history.pop(0)\n    hist = \"\"\n    for chat in chat_history:\n        hist += \"\\n\" + chat\n    hist += \"\\nB: \"\n    new_user_input_ids = tokenizer.encode(hist, return_tensors='pt')\n\n    bot_input_ids = new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        do_sample=True, \n        #top_k=100, \n        #top_p=0.7,\n        #temperature = 0.1\n    )\n\n    bot_text = tokenizer.decode(chat_history_ids[0], skip_special_tokens=True).replace(\"#@\uc774\ub984#\", \"OOO\")\n    bot_text = bot_text.replace(\"\\n\", \" / \")\n    chat_history.append(\"B: \" + bot_text)\n    \n    # pretty print last ouput tokens from bot\n    print(\"Bot: {}\".format(bot_text))    \n```\n\n### Framework versions\n\n- Transformers 4.22.1\n- TensorFlow 2.10.0\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "1102407757", "downloads": 12}
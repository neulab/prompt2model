{"pretrained_model_name": "eslamxm/AraT5-base-finetune-ar-wikilingua", "description": "---\ntags:\n- summarization\n- ar\n- Abstractive Summarization\n- generated_from_trainer\ndatasets:\n- wiki_lingua\nmodel-index:\n- name: AraT5-base-finetune-ar-wikilingua\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# AraT5-base-finetune-ar-wikilingua\n\nThis model is a fine-tuned version of [UBC-NLP/AraT5-base](https://huggingface.co/UBC-NLP/AraT5-base) on the wiki_lingua dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 4.6110\n- Rouge-1: 19.97\n- Rouge-2: 6.9\n- Rouge-l: 18.25\n- Gen Len: 18.45\n- Bertscore: 69.44\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- gradient_accumulation_steps: 16\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 250\n- num_epochs: 10\n- label_smoothing_factor: 0.1\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge-1 | Rouge-2 | Rouge-l | Gen Len | Bertscore |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:-------:|:---------:|\n| 11.5412       | 1.0   | 312  | 6.8825          | 5.2     | 0.69    | 5.04    | 19.0    | 63.2      |\n| 6.5212        | 2.0   | 624  | 5.8992          | 8.89    | 1.4     | 8.36    | 17.92   | 63.9      |\n| 5.8302        | 3.0   | 936  | 5.3712          | 9.99    | 2.21    | 9.54    | 15.87   | 65.08     |\n| 5.406         | 4.0   | 1248 | 5.0632          | 13.94   | 3.5     | 13.0    | 15.95   | 66.83     |\n| 5.1109        | 5.0   | 1560 | 4.8718          | 15.28   | 4.34    | 14.27   | 18.26   | 66.83     |\n| 4.9004        | 6.0   | 1872 | 4.7631          | 16.65   | 4.92    | 15.46   | 17.73   | 67.75     |\n| 4.754         | 7.0   | 2184 | 4.6920          | 18.31   | 5.79    | 16.9    | 18.17   | 68.55     |\n| 4.6369        | 8.0   | 2496 | 4.6459          | 18.6    | 6.12    | 17.16   | 18.16   | 68.66     |\n| 4.5595        | 9.0   | 2808 | 4.6153          | 18.94   | 6.1     | 17.39   | 17.82   | 68.99     |\n| 4.4967        | 10.0  | 3120 | 4.6110          | 19.15   | 6.25    | 17.55   | 17.91   | 69.09     |\n\n\n### Framework versions\n\n- Transformers 4.19.4\n- Pytorch 1.11.0+cu113\n- Datasets 2.2.2\n- Tokenizers 0.12.1\n", "size_bytes": "1131177743", "downloads": 23}
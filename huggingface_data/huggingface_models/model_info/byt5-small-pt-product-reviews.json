{"pretrained_model_name": "HeyLucasLeao/byt5-small-pt-product-reviews", "description": "Create README.md\n## ByT5 Small Portuguese Product Reviews\n\n#### Model Description\nThis is a finetuned version from ByT5 Small by Google for Sentimental Analysis from Product Reviews in Portuguese.\n##### Paper: https://arxiv.org/abs/2105.13626\n\n#### Training data\nIt was trained from products reviews from a Americanas.com. You can found the data here: https://github.com/HeyLucasLeao/finetuning-byt5-model.\n\n#### Training Procedure\nIt was finetuned using the Trainer Class available on the Hugging Face library. For evaluation it was used accuracy, precision, recall and f1 score.\n\n##### Learning Rate: **1e-4**\n##### Epochs: **1**\n##### Colab for Finetuning: https://colab.research.google.com/drive/1EChTeQkGeXi_52lClBNazHVuSNKEHN2f\n##### Colab for Metrics: https://colab.research.google.com/drive/1o4tcsP3lpr1TobtE3Txhp9fllxPWXxlw#scrollTo=PXAoog5vQaTn\n#### Score:\n```python\nTraining Set:\n'accuracy': 0.8974239585927603,\n 'f1': 0.927229848590765,\n 'precision': 0.9580290812115055,\n 'recall': 0.8983492356469835\nTest Set:\n'accuracy': 0.8957881282882026,\n 'f1': 0.9261366030421776,\n 'precision': 0.9559431131213848,\n 'recall': 0.8981326359661668\n\nValidation Set:\n'accuracy': 0.8925383190163382,\n 'f1': 0.9239208204149773,\n 'precision': 0.9525448733710351,\n 'recall': 0.8969668904839083\n```\n\n#### Goals\n\nMy true intention was totally educational, thus making available a this version of the model as a example for future proposes.\n\nHow to use\n``` python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\nprint(device)\n\ntokenizer = AutoTokenizer.from_pretrained(\"HeyLucasLeao/byt5-small-pt-product-reviews\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"HeyLucasLeao/byt5-small-pt-product-reviews\")\nmodel.to(device)\n\ndef classificar_review(review):\n  inputs = tokenizer([review], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n  input_ids = inputs.input_ids.to(device)\n  attention_mask = inputs.attention_mask.to(device)\n  output = model.generate(input_ids, attention_mask=attention_mask)\n  pred = np.argmax(output.cpu(), axis=1)\n  dici = {0: 'Review Negativo', 1: 'Review Positivo'}\n  return dici[pred.item()]\n  \nclassificar_review(review)\n```", "size_bytes": "1198625069", "downloads": 10}
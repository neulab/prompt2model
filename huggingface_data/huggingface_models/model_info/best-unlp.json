{"pretrained_model_name": "Pravopysnyk/best-unlp", "description": "---\nlanguage:\n- uk\nlibrary_name: transformers\npipeline_tag: text2text-generation\ntags:\n- grammatical error correction\n- GEC\n- Grammar\n---\nThis model was trained by Pravopysnyk team for the Ukrainian NLP shared task in Ukrainian grammar correction. The model is MBart-50-large \nset to ukr-to-ukr translation task finetuned on UA-GEC augmented by custom dataset generated using our synthetic error generation. \nThe code for error generation will be uploaded on github soon, and the detailed procedure is described in our paper. For this model we added to ua-gec:-\n5k sentences generated by round translaion (ukr-rus-ukr)\n10k sentences using our punctuation error generation script\n2k of dilution (just fully correct sentences sampled from our dataset)\n10k of russism (errors generated using our russism error generation).\n\nHope you find this description helpful!", "size_bytes": "2444694045", "downloads": 11}
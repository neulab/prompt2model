{"pretrained_model_name": "thu-coai/LongLM-small", "description": "---\nlanguage:\n- zh\nthumbnail: http://coai.cs.tsinghua.edu.cn/coai/img/logo.png?v=13923\ntags:\n- pytorch\n- lm-head\n- zh\nmetrics:\nwidget:\n- text: \"\u5c0f\u5495\u565c\u5bf9\u9773\u53f8\u5bd2\u5b8c\u5168\u662f\u4e2a\u81ea\u6765\u719f\uff0c\u5c0f\u5bb6\u4f19\u722c\u8fdb\u4ed6\u6000\u91cc\u5c0f\u624b\u6402\u7740\u4ed6\u7684\u8116\u5b50\uff0c\u5976\u58f0\u5976\u6c14\u7684\u8981\u6c42\uff1a\u201c\u9773\u8700\u9ece,\u4f60\u7ed9\u5495\u565c\u8bb2\u6545\u4e8b\u597d\u4e0d\u597d\uff1f\u201d\u8bb2\u6545\u4e8b\uff1f\u7ae5\u8bdd\u6545\u4e8b\u5417\uff1f\u201c\u6211\u4e0d\u4f1a\u3002\u201d\u5c0f\u5bb6\u4f19\u660e\u663e\u4e0d\u4fe1\u3002\u561f\u7740\u5c0f\u5634\u5927\u773c\u6c6a\u6c6a\u7684\u76ef\u7740\u4ed6\uff0c\u201c\u54fc\u3002\u201d\u5c0f\u5bb6\u4f19\u8f7b\u8f7b\u54fc\u4e86\u4e00\u58f0,\u9773\u53f8\u5bd2\u9ed8\u4e86\u534a\u664c\uff0c<extra_id_1>\"\n- text: \"\u7f8e\u5973\u4eb2\u81ea\u6253\u62db\u547c\uff0c\u8fd9\u53ef\u662f\u7834\u5929\u8352\u7b2c\u4e00\u6b21\uff0c\u4e4b\u524d\u4e0d\u7ba1\u4ed6\u732e\u591a\u5c11\u6b21\u6bb7\u52e4\uff0c\u7f8e\u5973<extra_id_1>\u7529\u4ed6\uff0c\u96be\u9053\u4eca\u5929\u771f\u662f\u8001\u5929<extra_id_2>\u4e0d\u6562<extra_id_3>\u7684\u5144\u8fde\u6eda\u5e26\u722c\u7684\u6765\u5230<extra_id_4>\u8eab\u8fb9\u961f\u53cb\u90fd\u5e26\u7740\u8273<extra_id_5>\u4ed6\uff0c<extra_id_6>\u8fde\u8ba1\u7b97\u673a\u7cfb\u7684\u90a3\u7968\u7403\u53cb\u90fd\u5728\u90a3\u513f\u4e0d\u4f4f\u5730\u5077\u770bMAGGIE\uff0c\u8fd9\u79cd\u611f\u89c9\u771f<extra_id_7>\u6bd9\u4e86\uff01\"\ninference:\n  parameters:\n    top_p: 0.9\n---\n## LongLM\n\n### 1. Parameters\n\n| Versions     | $d_m$ | $d_{ff}$ | $d_{kv}$ | $n_h$ | $n_e/n_d$ | \\# P   |\n| ------------ | ----- | -------- | -------- | ----- | --------- | ---- |\n| LongLM-small | 512   | 2,048    | 64       | 8     | 6/6       | 60M  |\n| LongLM-base  | 768   | 3,072    | 64       | 12    | 12/12     | 223M |\n| LongLM-large | 1,536 | 3,072    | 64       | 12    | 24/32     | 1B   |\n\n- $d_m$: the dimension of hidden states\n- $d_{ff}$: the dimension of feed forward layers\n- $d_{kv}$: the dimension of  the keys/values in the self-attention layers\n- $n_h$: the number of attention heads\n- $n_e$: the number of hidden layers of the encoder\n- $n_d$: the number of hidden layers of the decoder\n- \\#P: the number of parameters\n\n### 2. Pretraining Tasks\n\nEncoder-decoder models are trained typically by maximizing the likelihood of the target output given an input. To improve the capacities of both the encoder and decoder, we propose to train LongLM with two pretraining tasks including text in\ufb01lling (Raffel et al., 2020) and conditional continuation (Radford et al., 2019). For the \ufb01rst task, the input is a text where a number of spans are sampled and replaced by special tokens with unique IDs, while the output is the spans delimited by the special tokens used in the input. The lengths of masked spans are drawn from a Poisson distribution with \u03bb=3 and all masked tokens compress 15% of the original texts. As for the second task, the input and output are respectively the front and back half of a text, which is split into two parts randomly. \n\n### 3. Pretraining Data\n\nWe collect 120G novels as the pretraining data for LongLM. \n\n### 4. Checkpoints\n\n\n1. **Model Loading:** \n\n   ```python\\\n   from transformers import T5Tokenizer, T5ForConditionalGeneration\n   tokenizer = T5Tokenizer.from_pretrained('LongLM-large')\n   tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<extra_id_%d>\"%d for d in range(100)]})\n   model = T5ForConditionalGeneration.from_pretrained('LongLM-large')\n   ```\n\n\n2. **Generation:**\n\n   ```python\n   input_ids = tokenizer(\"\u5c0f\u5495\u565c\u5bf9\uff0c<extra_id_1>\",return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(device)\n   \n   gen = model.generate(input_ids, do_sample=True, decoder_start_token_id=1, top_p=0.9, max_length=512)\n   ```\n\n\n### 5. Dependencies\n\n```\ndatasets                1.6.2\ndeepspeed               0.3.16\nhuggingface-hub         0.0.8\njieba                   0.42.1\njsonlines               2.0.0\nnltk                    3.5\nnumpy                   1.19.5\npytorch-lightning       1.2.0\nregex                   2020.11.13\nrouge                   1.0.1\nrouge-score             0.0.4\nsacrebleu               1.5.0\nscipy                   1.5.4\nsentencepiece           0.1.95\ntokenizers              0.10.1\ntorch                   1.8.1\ntorchaudio              0.8.0\ntorchmetrics            0.2.0\ntorchvision             0.9.0\ntransformers            4.6.1\n```\n\n### 6. Contributers\n\n[Jian Guan](https://jianguanthu.github.io/) at [thu-coai](http://coai.cs.tsinghua.edu.cn/)\n\n## Citation\n\n```txt\n@misc{guan2021lot,\n      title={LOT: A Benchmark for Evaluating Chinese Long Text Understanding and Generation}, \n      author={Jian Guan and Zhuoer Feng and Yamei Chen and Ruilin He and Xiaoxi Mao and Changjie Fan and Minlie Huang},\n      year={2021},\n      eprint={2108.12960},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n", "size_bytes": "242030470", "downloads": 4}
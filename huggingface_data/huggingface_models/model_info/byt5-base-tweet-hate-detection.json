{"pretrained_model_name": "Narrativa/byt5-base-tweet-hate-detection", "description": "---\nlanguage: en\ndatasets:\n- tweets_hate_speech_detection\ntags:\n- hate\n- speech\n\nwidget:\n- text: \"@user black lives really matter?\"\n\n---\n\n\n# ByT5-base fine-tuned for Hate Speech Detection (on Tweets)\n[ByT5](https://huggingface.co/google/byt5-base) base fine-tuned on [tweets hate speech detection](https://huggingface.co/datasets/tweets_hate_speech_detection) dataset for **Sequence Classification** downstream task.\n\n# Details of ByT5 - Base \ud83e\udde0\n\nByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-base).\nByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.\nByT5 works especially well on noisy text data,*e.g.*, `google/byt5-base` significantly outperforms [mt5-base](https://huggingface.co/google/mt5-base) on [TweetQA](https://arxiv.org/abs/1907.06292).\nPaper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/pdf/1910.10683.pdf)\nAuthors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel* \n\n\n## Details of the downstream task (Sequence Classification as Text generation) - Dataset \ud83d\udcda\n\n[tweets_hate_speech_detection](hhttps://huggingface.co/datasets/tweets_hate_speech_detection)\n\n\nThe objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\nFormally, given a training sample of tweets and labels, where label \u20181\u2019 denotes the tweet is racist/sexist and label \u20180\u2019 denotes the tweet is not racist/sexist, your objective is to predict the labels on the given test dataset.\n\n- Data Instances:\n\nThe dataset contains a label denoting is the tweet a hate speech or not\n\n```json\n{'label': 0,  # not a hate speech\n 'tweet': ' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'}\n```\n- Data Fields:\n \n**label**: 1 - it is a hate speech, 0 - not a hate speech\n\n**tweet**: content of the tweet as a string\n\n- Data Splits:\n\nThe data contains training data with **31962** entries\n\n## Test set metrics \ud83e\uddfe\n\nWe created a representative test set with the 5% of the entries.\n\nThe dataset is so imbalanced and we got a **F1 score of 79.8**\n    \n\n\n## Model in Action \ud83d\ude80\n\n```sh\ngit clone https://github.com/huggingface/transformers.git\npip install -q ./transformers\n```\n\n```python\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\nckpt = 'Narrativa/byt5-base-tweet-hate-detection'\n\ntokenizer = AutoTokenizer.from_pretrained(ckpt)\nmodel = T5ForConditionalGeneration.from_pretrained(ckpt).to(\"cuda\")\n\ndef classify_tweet(tweet):\n\n    inputs = tokenizer([tweet], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n    input_ids = inputs.input_ids.to('cuda')\n    attention_mask = inputs.attention_mask.to('cuda')\n    output = model.generate(input_ids, attention_mask=attention_mask)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    \nclassify_tweet('here goes your tweet...')\n```\n\nCreated by: [Narrativa](https://www.narrativa.com/)\n\nAbout Narrativa: Natural Language Generation (NLG) | Gabriele, our machine learning-based platform, builds and deploys natural language solutions. #NLG #AI", "size_bytes": "2326733259", "downloads": 124}
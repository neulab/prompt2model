{"pretrained_model_name": "consciousAI/question-generation-auto-hints-t5-v1-base-s-q", "description": "---\ntags:\n- Question(s) Generation\nmetrics:\n- rouge\nmodel-index:\n- name: consciousAI/question-generation-auto-hints-t5-v1-base-s-q\n  results: []\n---\n\n# Auto Question Generation  \nThe model is intended to be used for Auto And/Or Hint enabled Question Generation tasks. The model is expected to produce one or possibly more than one question from the provided context.\n \n[Live Demo: Question Generation](https://huggingface.co/spaces/consciousAI/question_generation)\n\nIncluding this there are five models trained with different training sets, demo provide comparison to all in one go. However, you can reach individual projects at below links:\n\n[Auto Question Generation v1](https://huggingface.co/consciousAI/question-generation-auto-t5-v1-base-s)\n\n[Auto Question Generation v2](https://huggingface.co/consciousAI/question-generation-auto-t5-v1-base-s-q)\n\n[Auto Question Generation v3](https://huggingface.co/consciousAI/question-generation-auto-t5-v1-base-s-q-c)\n\n[Auto/Hints based Question Generation v2](https://huggingface.co/consciousAI/question-generation-auto-hints-t5-v1-base-s-q-c)\n\nThis model can be used as below:\n\n```\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer\n)\n\nmodel_checkpoint = \"consciousAI/question-generation-auto-hints-t5-v1-base-s-q\"\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n## Input with prompt\ncontext=\"question_context: <context>\"\nencodings = tokenizer.encode(context, return_tensors='pt', truncation=True, padding='max_length').to(device)\n\n## You can play with many hyperparams to condition the output, look at demo\noutput = model.generate(encodings, \n                        #max_length=300, \n                        #min_length=20, \n                        #length_penalty=2.0, \n                        num_beams=4,\n                        #early_stopping=True,\n                        #do_sample=True,\n                        #temperature=1.1\n                       )\n\n## Multiple questions are expected to be delimited by '?' You can write a small wrapper to elegantly format. Look at the demo.\nquestions = [tokenizer.decode(id, clean_up_tokenization_spaces=False, skip_special_tokens=False) for id in output]\n```\n\n## Training and evaluation data\n\nSquad & QNLi combo.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss | Rouge1 | Rouge2 | Rougel | Rougelsum |\n|:-------------:|:-----:|:------:|:---------------:|:------:|:------:|:------:|:---------:|\n| 1.8298        | 1.0   | 14515  | 1.7529          | 0.3535 | 0.1825 | 0.3251 | 0.3294    |\n| 1.4931        | 2.0   | 29030  | 1.7132          | 0.3558 | 0.1881 | 0.3267 | 0.3308    |\n| 1.2756        | 3.0   | 43545  | 1.7579          | 0.3604 | 0.1901 | 0.3307 | 0.3345    |\n| 1.0936        | 4.0   | 58060  | 1.8173          | 0.36   | 0.1901 | 0.3295 | 0.3334    |\n| 0.955         | 5.0   | 72575  | 1.9204          | 0.3611 | 0.1884 | 0.3295 | 0.3336    |\n| 0.8117        | 6.0   | 87090  | 2.0183          | 0.355  | 0.1836 | 0.3241 | 0.3282    |\n| 0.6949        | 7.0   | 101605 | 2.1347          | 0.3556 | 0.1836 | 0.3242 | 0.3282    |\n| 0.636         | 8.0   | 116120 | 2.2567          | 0.3568 | 0.1855 | 0.3248 | 0.3286    |\n| 0.591         | 9.0   | 130635 | 2.3598          | 0.3563 | 0.1844 | 0.3238 | 0.3281    |\n| 0.5417        | 10.0  | 145150 | 2.4725          | 0.3556 | 0.1828 | 0.3229 | 0.3269    |\n\n\n### Framework versions\n\n- Transformers 4.23.0.dev0\n- Pytorch 1.12.1+cu113\n- Datasets 2.5.2\n- Tokenizers 0.13.0\n", "size_bytes": "990406605", "downloads": 9}
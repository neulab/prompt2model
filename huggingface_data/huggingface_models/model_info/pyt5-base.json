{"pretrained_model_name": "formermagic/pyt5-base", "description": "# Python T5 base model\n\nPre-trained model on CodeSearchNet Python dataset using a span-masking objective. The training objective and model were introduced in [this paper](https://arxiv.org/pdf/1910.10683.pdf) and first released in [this repository](https://github.com/google-research/text-to-text-transfer-transformer). PyT5 model used [git-t5](https://github.com/formermagic/git-t5) framework built on top of JAX/Flax to pre-train the model on a TPU v3-8 node.\n\n# How to use\n\nYou can use this model to denoise span-masked sequences.\n\nFirst, install the [git-t5](https://github.com/formermagic/git-t5) pip package:\n```shell\n> pip install git-t5\n```\n\nNext, download the model and tokenizer:\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, \n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"formermagic/pyt5-base\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"formermagic/pyt5-base\")\n```\n\nFinally, encode your input and generate the output sequence:\n```python\nfrom git_t5.utils import encode_input\n\ntext = \"\"\"\ndef alias(self, annotationtype, set, fallback=False):\n    if inspect.isclass(annotationtype): annotationtype = annotationtype.ANNOTATIONTYPE\n    if annotationtype in self.set_alias and set in self.set_alias[annotationtype]:\n        return self.set_alias[annotationtype][set]\n    elif fallback:\n        return set\n    else:\n        raise KeyError(\"No alias for set \" + set)\n\"\"\"\n\nbatch, max_length = encode_input(tokenizer, text, seed=22)\noutputs = model.generate(batch[\"input_ids\"], max_length=max_length, num_beams=1)\nprint(tokenizer.batch_decode(outputs[..., 1:]))\nprint(tokenizer.batch_decode(batch[\"labels\"]))\n```\n\nYou should see the following output:\n```shell\n['<extra_id_0>, fallback=<extra_id_1> inspect<extra_id_2>.set_alias<extra_id_3> return self.set<extra_id_4>) def fallback']\n['<extra_id_0>, fallback=<extra_id_1> inspect<extra_id_2>.set_alias<extra_id_3> return self.set<extra_id_4>) </s></s>']\n```\n\nAs you can see, the predicted result is very close to the target sequence. ", "size_bytes": "978029645", "downloads": 6}
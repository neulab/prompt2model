{"pretrained_model_name": "DeepPavlov/bart-base-en-persona-chat", "description": "---\nlicense: openrail\ndatasets:\n- AlekseyKorshuk/persona-chat\nlanguage:\n- en\npipeline_tag: text-generation\n---\n---\nlanguage:\n- en\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n# Model Details\n\n## Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n- **Developed by:** Deeppavlov team\n- **Model type:** seq2seq\n- **Language(s) (NLP):** English\n- **License:** MIT\n- **Finetuned from model:** [facebook/bart-base](facebook/bart-base)\n\n\n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n```python\nfrom typing import List, TypedDict\nfrom dataclasses import dataclass\nfrom itertools import chain\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n\n@dataclass\nclass H2PersonaChatHyperparametersV1:\n    \"\"\"\n    chat_history_pair_length: int - dialogue pairs amount from the end\n    \"\"\"\n\n    model_name: str = \"facebook/bart-base\"\n    chat_history_pair_length: int = 7\n\n    persona_max_length: int = 14\n    chat_max_length: int = 25\n\n    debug_status: int = 0\n\n\nclass PersonaChatDatasetSampleV1(TypedDict):\n    \"\"\"\n    persona: List[str] - person fact sentence set\n    history: List[str] - chating history\n    \"\"\"\n\n    persona: List[str]\n    history: List[str]\n    sample_id: str\n\n\nclass H2Seq2SeqInferenceSampleDictV1(TypedDict):\n    input_ids: List[int]\n    attention_mask: List[int]\n\n\nclass H2Seq2SeqInferenceSampleDictV2(TypedDict):\n    input_ids: torch.Tensor\n    attention_mask: torch.Tensor\n\n\ndef flat_list(list_of_lists: List[List]) -> List:\n    return list(chain.from_iterable(list_of_lists))\n\n\nclass H2Seq2SeqInferencePersonaSampleV1:\n    def __init__(\n        self,\n        dataset_sample: PersonaChatDatasetSampleV1,\n        tokenizer: AutoTokenizer,\n        hyperparameters: H2PersonaChatHyperparametersV1,\n    ) -> None:\n        self.dataset_sample = dataset_sample\n        self.tokenizer = tokenizer\n        self.hyperparameters = hyperparameters\n\n    def add_spaces_after(\n        self,\n        items: List[str],\n    ) -> List[str]:\n        items = [item + \" \" for item in items]\n        return items\n\n    @property\n    def bos_token_id(self):\n        if \"t5\" in self.hyperparameters.model_name:\n            return []\n\n        if self.tokenizer.bos_token_id is None:\n            return []\n\n        return [self.tokenizer.bos_token_id]\n\n    @property\n    def eos_token_id(self):\n        if self.tokenizer.eos_token_id is None:\n            return []\n\n        return [self.tokenizer.eos_token_id]\n\n    def add_sep_beetween(self, items: List[str], sep=\" EOS \") -> List[str]:\n        for i in range(1, len(items)):\n            items[i] = sep + items[i]\n\n        return items\n\n    def add_spaces_between(self, items: List[str]) -> List[str]:\n        items = self.add_spaces_after(items)\n        items[-1] = items[-1].strip()\n        return items\n\n    def get_sample(self) -> H2Seq2SeqInferenceSampleDictV1:\n\n        dialog_history = self.dataset_sample[\"history\"]\n        dialog_history = dialog_history[-self.hyperparameters.chat_history_pair_length * 2 - 1 :]\n        dialog_history = self.add_sep_beetween(dialog_history)\n\n        persona = self.dataset_sample[\"persona\"]\n        persona = self.add_sep_beetween(\n            persona,\n            sep=\" \",\n        )\n\n        KNOWLEDGE_IDS = self.tokenizer.encode(\n            \" [KNOWLEDGE] \",\n            add_special_tokens=False,\n        )\n        CONTEXT_IDS = self.tokenizer.encode(\n            \" [CONTEXT] \",\n            add_special_tokens=False,\n        )\n\n        encoded_history = self.tokenizer.batch_encode_plus(\n            dialog_history,\n            add_special_tokens=False,\n            truncation=True,\n            max_length=self.hyperparameters.chat_max_length,\n        )\n        encoded_history = flat_list(encoded_history[\"input_ids\"])\n\n        encoded_persona = self.tokenizer.batch_encode_plus(\n            persona,\n            add_special_tokens=False,\n            truncation=True,\n            max_length=self.hyperparameters.persona_max_length,\n        )\n\n        encoded_persona = flat_list(encoded_persona[\"input_ids\"])\n\n        input_ids = [\n            *self.bos_token_id,\n            *CONTEXT_IDS,\n            *encoded_history,\n            *KNOWLEDGE_IDS,\n            *encoded_persona,\n            *self.eos_token_id,\n        ]\n\n        attention_mask = [1] * len(input_ids)\n\n        return H2Seq2SeqInferenceSampleDictV1(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n        )\n\n\nclass DialogBotV1:\n    def __init__(\n        self,\n        model: AutoModelForSeq2SeqLM,\n        tokenizer: AutoTokenizer,\n        hyperparameters: H2PersonaChatHyperparametersV1,\n        history: List[str] = None,\n        persona: List[str] = None,\n        device: str = \"cuda\",\n        shuffle_persona: bool = True,\n    ):\n        self.model = model\n\n        self.tokenizer = tokenizer\n        self.hyperparameters = hyperparameters\n        self.device = device\n        self.shuffle_persona = shuffle_persona\n\n        self.debug_status = hyperparameters.debug_status\n\n        if history is None:\n            self.history = []\n        self.history = history\n\n        if persona is None:\n            self.persona = []\n        self.persona = persona\n\n    def _get_sample(\n        self,\n        persona: List[str],\n        history: List[str],\n    ) -> H2Seq2SeqInferenceSampleDictV1:\n        dataset_sample = PersonaChatDatasetSampleV1(\n            persona=persona,\n            history=history,\n        )\n\n        sample = H2Seq2SeqInferencePersonaSampleV1(\n            tokenizer=self.tokenizer,\n            hyperparameters=self.hyperparameters,\n            dataset_sample=dataset_sample,\n        )\n        sample = sample.get_sample()\n        print(self.tokenizer.decode(sample['input_ids']))\n\n        for key in sample.keys():\n            sample[key] = torch.tensor(sample[key]).unsqueeze(0).to(self.device)\n\n        return sample\n\n    def next_response(\n        self,\n        **generation_params,\n    ) -> str:\n\n        sample = self._get_sample(\n            persona=self.persona,\n            history=self.history,\n        )\n        answer = self.generate_response(\n            sample,\n            **generation_params,\n        )\n        answer = self.tokenizer.batch_decode(\n            answer,\n            skip_special_tokens=True,\n        )\n        self.history.append(answer[0])\n        return answer[0]\n\n    def generate_response(\n        self,\n        sample: H2Seq2SeqInferenceSampleDictV1,\n        **generation_params,\n    ):\n        \"\"\"\n        generation_params - https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/text_generation\n        \"\"\"\n        with torch.no_grad():\n            return self.model.generate(\n                **sample,\n                **generation_params,\n            )\n\nPRETRAINED_MODEL_NAME_OR_PATH = \"DeepPavlov/bart-base-en-persona-chat\"\n\nPAIR_DIALOG_HISTORY_LENGTH = 2\n\n# CHAT_MAX_LENGTH for single sentence, in tokens\nCHAT_MAX_LENGTH = 25\n# PERSONA_MAX_LENGTH for single sentence, in tokens\nPERSONA_MAX_LENGTH = 19\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\nmodel.to(device)\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n\nif torch.cuda.is_available():\n\tmodel.half()\n\nhyperparameters = H2PersonaChatHyperparametersV1(\n\tchat_history_pair_length=PAIR_DIALOG_HISTORY_LENGTH,\n\tpersona_max_length=PERSONA_MAX_LENGTH,\n\tchat_max_length=CHAT_MAX_LENGTH,\n\tmodel_name=PRETRAINED_MODEL_NAME_OR_PATH,\n)\n\n\npersona = [\n\t\"I like to play guitar.\",\n\t\"I hate onions.\"\n]\n\nhistory = [\n\t\"I hate to talk about politics, what about you?\"\n]\n            \npersona_bot = DialogBotV1(\n        model=model,\n        tokenizer=tokenizer,\n        hyperparameters=hyperparameters,\n        history=history,\n        persona=persona,\n        device=device,\n    )\n\nGENERATION_PARAMS = {\n\t\"max_new_tokens\": 60,\n\t\"penalty_alpha\": 0.15,\n\t\"top_k\": 10\n}\nresponse = persona_bot.next_response(\n\t**GENERATION_PARAMS,\n)\n\nprint(response)\n# i am not into politics. i am into music.\n```\n\n\n## Recommendations\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n- [Data Source | EN Persona Chat](https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json)\n\n[More Information Needed]\n\n### Preprocessing\n\n- Initial data was splitted by this script:\n```python\ndef persona_chat_dataset_tranformer_v1(\n    initial_dataset_path: str,\n    output_folder: str,\n) -> None:\n    \"\"\"\n        example\n            persona_chat_dataset_tranformer_v1(\n            initial_dataset_path=\"./datasets/persona_chat/persona_chat.json\",\n            output_folder=\"./datasets/persona_chat\",\n    )\n    \"\"\"\n    assert initial_dataset_path is not None, \"initial_dataset_path is None\"\n    assert output_folder is not None, \"output_folder is None\"\n\n    with open(initial_dataset_path) as f:\n        initial_dataset = json.load(f)\n\n    train_dataset = initial_dataset[\"train\"]\n    val_len = len(initial_dataset[\"valid\"])\n    valid_dataset = initial_dataset[\"valid\"][: val_len // 2]\n    test_dataset = initial_dataset[\"valid\"][val_len // 2 :]\n\n    print(\n        f\"Dataset lengths: train {len(train_dataset)}, valid {len(valid_dataset)}, test {len(test_dataset)}\"\n    )\n    # save json files\n    with open(output_folder + \"/train.json\", \"w\") as f:\n        json.dump(train_dataset, f)\n\n    with open(output_folder + \"/valid.json\", \"w\") as f:\n        json.dump(valid_dataset, f)\n\n    with open(output_folder + \"/test.json\", \"w\") as f:\n        json.dump(test_dataset, f)\n\n    print(\"Datasets saved.\")\n```  \n\n# Evaluation\n\n### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n- BLUEL\n- CharF\n- RougeL", "size_bytes": "279029853", "downloads": 40}
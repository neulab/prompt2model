{"pretrained_model_name": "PPY039/codet5-small-go_generation_v2", "description": "---\nlicense: apache-2.0\n---\n\n# codet5-small-go_generation_v2\nThis model is finetuned based on the pre-trained [CodeT5-small model](https://github.com/salesforce/CodeT5#fine-tuning). \n\nThis model is fine-tuned on dataset: data_71421(44.2MB)\n\nmax_src_len = 512, max_trg_len = 256\n\n> 2023.5.15 update README.md\n\n> 2023.5.5 upload the initial version.\n\nThe model genarates the missing function body according to the input which privides the necessary class environment and an empty function.\n\nSee example below for formatting.\n \n# How to use\nHere is how to use this model:\n\n```\nfrom transformers import T5ForConditionalGeneration, RobertaTokenizer\n\n# load model and tokenizer\nmodel_path = \"PPY039/codet5-small-go_generation_v2\"\n\nmodel = T5ForConditionalGeneration.from_pretrained(model_path, cache_dir=\"D:\\huggingface_cache\")\ntokenizer = RobertaTokenizer.from_pretrained(model_path, cache_dir=\"D:\\huggingface_cache\")\n\n# use model\ninput_text = \"package names\\n\\nimport \\\"knative.dev/pkg/kmeta\\\"\\n\\n\\nfunc Deployment(rev kmeta.Accessor) string {\\n\\treturn kmeta.ChildName(rev.GetName(), \\\"-deployment\\\")\\n}\\n\\n\\nfunc ImageCache(rev kmeta.Accessor) string {\\n\\treturn kmeta.ChildName(rev.GetName(), \\\"-cache\\\")\\n}\\n\\n\\n\\n\\nfunc PA(rev kmeta.Accessor) string\"\n\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\noutput = model.generate(input_ids=input_ids, max_new_tokens=256)  # max_trg_len = 256\n\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n# this prints \"{\\n\\treturn kmeta.ChildName(rev.GetName(), \"-pa\")\\n}\"\nprint(output_text)\n\n```\n\n# Training data\nYinShicheng\n\n# Training process\nGuQiuhan\n\n# Advisor\nWangYu\n\n# Evaluation results\nTODO\n", "size_bytes": "242029051", "downloads": 2}
{"pretrained_model_name": "saikatc/NatGen", "description": "---\nlanguage: \n  - \"code\"\nthumbnail: \"https://to-be-updated\"\ntags:\n  - code generation\n  - code translation\n  - bug fixing\nlicense: \"mit\"\ndatasets:\n  - CodeSearchNet\n  - CodeXGLUE\nmetrics:\n  - EM\n  - CodeBLEU\n---\n\nPretrained model for NatGen: Generative Pre-training by \u201cNaturalizing\u201d Source Code [[`paper`]](https://dl.acm.org/doi/abs/10.1145/3540250.3549162),[[`code`]](https://github.com/saikat107/NatGen),[[`slide`]](https://docs.google.com/presentation/d/1T6kjiohAAR1YvcNvTASR94HptA3xHGCl/edit?usp=sharing&ouid=111755026725574085503&rtpof=true&sd=true).\n\nTo load the model,\n\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"saikatc/NatGen\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"saikatc/NatGen\")\n```\n\n\nFor citation,\n```\n@inproceedings{chakraborty2022natgen,\n    author = {Chakraborty, Saikat and Ahmed, Toufique and Ding, Yangruibo and Devanbu, Premkumar T. and Ray, Baishakhi},\n    title = {NatGen: Generative Pre-Training by \u201cNaturalizing\u201d Source Code},\n    year = {2022},\n    isbn = {9781450394130},\n    publisher = {Association for Computing Machinery},\n    address = {New York, NY, USA},\n    url = {https://doi.org/10.1145/3540250.3549162},\n    doi = {10.1145/3540250.3549162},\n    booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},\n    pages = {18\u201330},\n    numpages = {13},\n    keywords = {Neural Network, Semantic Preserving Transformation, Source Code Transformer, Source Code Pre-training},\n    location = {Singapore, Singapore},\n    series = {ESEC/FSE 2022}\n}\n```\n\n", "size_bytes": "891651384", "downloads": 824}
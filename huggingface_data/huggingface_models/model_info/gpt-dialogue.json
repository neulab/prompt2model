{"pretrained_model_name": "svjack/gpt-dialogue", "description": "---\nlanguage:\n- zh\nlibrary_name: transformers\npipeline_tag: text2text-generation\n---\n\n```python\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"svjack/gpt-dialogue\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"svjack/gpt-dialogue\")\n\ntokenizer.decode(\nmodel.generate(\n    tokenizer.encode(\n            \"\u4f60\u8ba4\u4e3a\u4eca\u5929\u7684\u5929\u6c14\u600e\u4e48\u6837\uff1f\", return_tensors=\"pt\", add_special_tokens=True\n        ), max_length = 128,\n    num_beams=2,\n    top_p = 0.95,\n    top_k = 50,\n    repetition_penalty = 2.5,\n    length_penalty=1.0,\n    early_stopping=True,\n)[0],\nskip_special_tokens = True\n)\n\n'''\n'\u4f60 \u8ba4 \u4e3a \u4eca \u5929 \u7684 \u5929 \u6c14 \u600e \u4e48 \u6837 \uff1f\n\u6211 \u8ba4 \u4e3a \u8fd9 \u4f1a \u5f88 \u51b7 \uff0c \u4f46 \u662f \u6625 \u5929 \u662f \u5982 \u6b64 \u7f8e \u4e3d \u3002\n\u60a8 \u6700 \u559c \u6b22 \u54ea \u4e2a \u5b63 \u8282 \uff1f\n\u6211 \u771f \u7684 \u4e0d \u77e5 \u9053 \u3002 \u4e5f \u8bb8 \u5728 \u5c71 \u4e0a \u6709 \u4e00 \u4e9b \u96ea \u677e \u77f3 \u3002\n\u542c \u8d77 \u6765 \u4e0d \u9519 \u3002 \u6211 \u4eec \u53ef \u4ee5 \u53bb \u6d77 \u6ee9 \u5417 \uff1f\n\u5f53 \u7136 \u3002 \u6211 \u4eec \u5e94 \u8be5 \u5c3d \u5feb \u53bb \u90a3 \u91cc \u3002\n\u597d \u7684 \u3002 \u8ba9 \u6211 \u4eec \u770b \u770b \u5b83 \u662f \u5426 \u51c6 \u5907 \u597d \u4e86 \u3002\n\u73b0 \u5728 \u4e0b \u96e8 \u4e86 \u3002\n\u6211 \u4eec \u8981 \u53bb \u54ea \u91cc \u89c1 \u9762 \uff1f\n\u6211 \u4eec'\n'''\n\n```", "size_bytes": "420912233", "downloads": 32}
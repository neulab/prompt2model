{"pretrained_model_name": "ccdv/lsg-bart-base-4096-booksum", "description": "---\ntags:\n- summarization\n- summary\n- booksum\n- long-document\n- long-form\n- lsg\ndatasets:\n- kmfoda/booksum\nmetrics:\n- rouge\nmodel-index:\n- name: ccdv/lsg-bart-base-4096-booksum\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n**Transformers >= 4.23.1**\\\n**This model relies on a custom modeling file, you need to add trust_remote_code=True**\\\n**See [\\#13467](https://github.com/huggingface/transformers/pull/13467)**\n\nLSG ArXiv [paper](https://arxiv.org/abs/2210.15497). \\\nGithub/conversion script is available at this [link](https://github.com/ccdv-ai/convert_checkpoint_to_lsg).\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-4096-booksum\", trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ccdv/lsg-bart-base-4096-booksum\", trust_remote_code=True)\n\ntext = \"Replace by what you want.\"\npipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\ngenerated_text = pipe(\n  text, \n  truncation=True, \n  max_length=64, \n  no_repeat_ngram_size=7,\n  num_beams=2,\n  early_stopping=True\n  )\n```\n\n# ccdv/lsg-bart-base-4096-booksum\n\nThis model is a fine-tuned version of [ccdv/lsg-bart-base-4096](https://huggingface.co/ccdv/lsg-bart-base-4096) on the kmfoda/booksum kmfoda--booksum dataset.\nIt achieves the following results on the evaluation set:\n- eval_loss: 3.2654\n- eval_rouge1: 33.9468\n- eval_rouge2: 6.7034\n- eval_rougeL: 16.7879\n- eval_rougeLsum: 31.7677\n- eval_gen_len: 427.6918\n- eval_runtime: 2910.3841\n- eval_samples_per_second: 0.492\n- eval_steps_per_second: 0.062\n- eval_samples: 1431\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 8e-05\n- train_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 30.0\n\n\n### Generate hyperparameters\n\nThe following hyperparameters were used during generation:\n\n- dataset_name: kmfoda/booksum \n- dataset_config_name: kmfoda--booksum\n- eval_batch_size: 8\n- eval_samples: 1431\n- early_stopping: True\n- ignore_pad_token_for_loss: True\n- length_penalty: 2.0\n- max_length: 512\n- min_length: 128\n- num_beams: 5\n- no_repeat_ngram_size: None\n- seed: 123\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.12.1\n- Datasets 2.3.2\n- Tokenizers 0.11.6\n", "size_bytes": "578416695", "downloads": 39}
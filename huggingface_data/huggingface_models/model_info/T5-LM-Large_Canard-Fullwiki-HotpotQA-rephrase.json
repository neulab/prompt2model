{"pretrained_model_name": "gaussalgo/T5-LM-Large_Canard-Fullwiki-HotpotQA-rephrase", "description": "---\ndatasets:\n- gaussalgo/Canard_Wiki-augmented\n- hotpot_qa\nmetrics:\n- rouge\n- bleu\nmodel-index:\n- name: T5-LM-Large_Canard-Fullwiki-HotpotQA-rephrase\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      type: hotpot_qa\n      name: HotpotQA\n      split: validation\n    metrics:\n    - type: rouge\n      value: 0.4774\n    - type: bleu\n      value: 29.11\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      type: gaussalgo/Canard_Wiki-augmented\n      name: Wikipedia-augmented Conversational QA (Canard)\n      split: validation\n    metrics:\n    - type: rouge\n      value: 0.4377\n    - type: bleu\n      value: 19.34\nlicense: cc-by-sa-4.0\nlanguage:\n- en\n---\n\n# Model Card for T5-LM-Large_Canard-HotpotQA-rephrase \nThis model is trained on three objectives: \n  1. Generating answers for Canard dataset based on Wikipedia search results \n  2. Generating answers for HotpotQA, \n  3. Rephrasing questions by the conversation context.\n\n## Training\nThe model was trained using the following script, which can be copy-pasted and run as-is (with the installed `requirements.txt`). \nAll details, including the request format, can be inferred without errors from the code.\nThe best checkpoint was picked by a maximum ROUGE on Canard conversational QA's ROUGE.\n\n```python\nimport datasets\n\ncanard_train_augm = datasets.load_dataset(\"gaussalgo/Canard_Wiki-augmented\", split=\"train\")\ncanard_test_augm = datasets.load_dataset(\"gaussalgo/Canard_Wiki-augmented\", split=\"test\")\n\ncanard_df = canard_train_augm.to_pandas()\ncanard_test_df = canard_train_augm.to_pandas()\n\n\n### Curation of seq2seq input contexts and labels\nimport random\n\ndef input_context_from_sample(row: dict, max_length=5) -> str:\n    context = \"Previous conversation:\"\n    context += \"\\nQuestion: \"\n    context += \", \".join(row[\"History\"][:3])\n    for i in range(3, len(row[\"History\"]), 2):\n        context += \"\\nAnswer: \"\n        context += row[\"History\"][i]\n        if i+1 < len(row[\"History\"]):\n            context += \"\\nQuestion: \"\n            context += row[\"History\"][i+1]\n\n    context += \"\\n\\nCurrent Question: \"\n    context += row[\"Question\"]\n\n    context += \"\\nSearch results:\"\n    all_contexts = row[\"retrieved_contexts\"].tolist()[:max_length-1] + [row[\"true_contexts\"]]\n    random.shuffle(all_contexts)\n\n    for i, search_result in enumerate(all_contexts):\n        context += \"\\n[%s]: \" % (i+1)\n        context += search_result.replace(\"CANNOTANSWER\", \"\")\n\n    context += \"\\nCurrent Answer: \"\n    return context\n\ndef rephrasing_context_from_sample(row: dict) -> str:\n    context = \"Previous conversation:\"\n    context += \"\\nQuestion: \"\n    context += \", \".join(row[\"History\"][:3])\n    for i in range(3, len(row[\"History\"]), 2):\n        context += \"\\nAnswer: \"\n        context += row[\"History\"][i]\n        if i+1 < len(row[\"History\"]):\n            context += \"\\nQuestion: \"\n            context += row[\"History\"][i+1]\n    \n    context += \"\\n\\nCurrent Question: \"\n    context += row[\"Question\"]\n\n    context += \"\\nMore specific question: \"\n    return context\n\ndef hotpotqa_context(row: dict) -> str:\n    context = \"Current Question: \"\n    context += row[\"question\"]\n\n    context += \"\\nSearch results:\"\n    all_contexts = [\" \".join(context) for context in row[\"context\"][\"sentences\"]]\n\n    for i, search_result in enumerate(all_contexts):\n        context += \"\\n[%s]: \" % (i+1)\n        context += search_result.replace(\"CANNOTANSWER\", \"\")\n\n    context += \"\\nCurrent Answer: \"\n    return context\n\n# Conversational QA sequences\ninput_texts = canard_df.apply(lambda row: input_context_from_sample(row), axis=1).values\ninput_val_texts = canard_test_df.iloc[:200].apply(lambda row: input_context_from_sample(row), axis=1).values\n\ntoo_long_index = [len(t) > 20000 for t in input_texts]\ninput_texts = [t for i, t in enumerate(input_texts) if not too_long_index[i]]\n# print(too_long_index)\nprint(\"training on %s samples\" % len(input_texts))\n\nlabels = canard_df.answer.apply(lambda ans: \"No answer\" if ans == \"CANNOTANSWER\" else ans).values\nlabels = [l for i, l in enumerate(labels)  if not too_long_index[i]]\nval_labels = canard_test_df.answer.apply(lambda ans: \"No answer\" if ans == \"CANNOTANSWER\" else ans).values\n\n# Rephrasing sequences\nrephrasing_inputs = canard_df.apply(lambda row: rephrasing_context_from_sample(row), axis=1).values\nrephrasing_val_inputs = canard_test_df.apply(lambda row: rephrasing_context_from_sample(row), axis=1).values\n\nrephrasing_labels = canard_df.Rewrite.values\nrephrasing_val_labels = canard_test_df.Rewrite.values\n\n# HotpotQA sequences\nhotpot_train = datasets.load_dataset(\"hotpot_qa\", \"distractor\")[\"train\"]\nhotpot_val = datasets.load_dataset(\"hotpot_qa\", \"distractor\")[\"validation\"]\n\nhotpot_inputs = hotpot_train.to_pandas().apply(hotpotqa_context, axis=1)\nhotpot_val_inputs = hotpot_val.to_pandas().apply(hotpotqa_context, axis=1)\ntoo_long_index = [len(t) > 20000 for t in hotpot_inputs]\n\nhotpot_inputs = [t for i, t in enumerate(hotpot_inputs) if not too_long_index[i]]\nhotpot_answers = [t for i, t in enumerate(hotpot_train[\"answer\"]) if not too_long_index[i]]\n\n# Training routine\n# see Adaptor's homepage for details:\n# https://github.com/gaussalgo/adaptor\n\n# Base model\nfrom adaptor.lang_module import LangModule\nlang_module = LangModule(\"google/t5-large-lm-adapt\")\n\nfrom adaptor.evaluators.generative import ROUGE, BLEU\n\n# Evaluations\nevaluators = [BLEU(), ROUGE(decides_convergence=True)]\n\n# Objectives\nfrom adaptor.objectives.seq2seq import Sequence2Sequence\n\nseq_qa = Sequence2Sequence(lang_module,\n                           texts_or_path=input_texts,\n                           labels_or_path=labels,\n                           val_texts_or_path=input_val_texts,\n                           val_labels_or_path=val_labels,\n                           batch_size=4,\n                           val_evaluators=evaluators,\n                           objective_id=\"Canard\")\n\nseq_additional_qa = Sequence2Sequence(lang_module,\n                                      texts_or_path=hotpot_inputs,\n                                      labels_or_path=hotpot_answers,\n                                      val_texts_or_path=hotpot_val_inputs[:200],\n                                      val_labels_or_path=hotpot_val[\"answer\"][:200],\n                                      batch_size=4,\n                                      val_evaluators=evaluators,\n                                      objective_id=\"HotpotQA\",\n                                      share_other_objective_head=seq_qa)\n\nseq_rephrasing = Sequence2Sequence(lang_module,\n                                   texts_or_path=rephrasing_inputs,\n                                   labels_or_path=rephrasing_labels,\n                                   val_texts_or_path=rephrasing_val_inputs[:200],\n                                   val_labels_or_path=rephrasing_val_labels[:200],\n                                   batch_size=4,\n                                   val_evaluators=evaluators,\n                                   objective_id=\"rephrasing\",\n                                   share_other_objective_head=seq_qa)\n\n# Training schedule & arguments\nfrom adaptor.utils import AdaptationArguments, StoppingStrategy\n\ntraining_arguments = AdaptationArguments(output_dir=\"checkpoints-chatbot\",\n                                         learning_rate=5e-5,\n                                         stopping_strategy=StoppingStrategy.ALL_OBJECTIVES_CONVERGED,\n                                         stopping_patience=8,\n                                         save_total_limit=8,\n                                         do_train=True,\n                                         do_eval=True,\n                                         bf16=True,\n                                         warmup_steps=1000,\n                                         gradient_accumulation_steps=8,\n                                         logging_steps=10,\n                                         eval_steps=200,\n                                         save_steps=1000,\n                                         num_train_epochs=10,\n                                         evaluation_strategy=\"steps\")\nfrom adaptor.schedules import ParallelSchedule\nfrom adaptor.adapter import Adapter\n\nschedule = ParallelSchedule(objectives=[seq_qa, seq_additional_qa, seq_rephrasing],\n                            args=training_arguments)\nadapter = Adapter(lang_module, schedule, args=training_arguments)\nadapter.train()  # Training for 63k updates\n```\n\n## Usage\nSee the prompting templates used in training to infer the optimal prompting format.\n\n#### Contact\nFeel free to ask questions here, or at stefanik{at} gaussalgo.com", "size_bytes": "3132785797", "downloads": 91}
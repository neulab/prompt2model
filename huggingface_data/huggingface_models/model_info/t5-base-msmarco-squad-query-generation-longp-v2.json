{"pretrained_model_name": "jmvcoelho/t5-base-msmarco-squad-query-generation-longp-v2", "description": "---\nlicense: wtfpl\ndatasets:\n- ms_marco\n- squad\nlanguage:\n- en\n---\n\n# Model\n\nt5-base-msmarco-squad-query-generation-longp-v2\n\nTask: query generation \nArchitecture: LongT5\n\nBase model: google/long-t5-tglobal-base\n\nNote: This is supposed to be a baseline model.\n\n\n## Prompt:\n\n\"Generate Query: {document}. Query:\"\n\n## Sequence length:\n\n1536 tokens\n\n## Training details\n\n### Hyperparameters\n\nBatch size: 8;  \nGradient acc: 8;\nLR: 3e-4, linear scheduler, 400 warmup steps.  \n\n\n### Data\n\nTotal: 252059 pairs (document, query) \n\nFrom MARCO-V2: 165238\nFrom SQuAD: 86821\n\nThe remaining queries from MARCO-V2 train split were not used.\n\n## Evaluation\n\nThis model is supposed to be used for data augmentation.\nHence, meaningful evaluation will come from downstream tasks.\n\nMARCO-V2 Dev1:\nBLEU: 0.102\nROUGE: 0.447\n\nMARCO-V2 Dev2:\nBLEU: 0.1691\nROUGE: 0.5013", "size_bytes": "990452905", "downloads": 5}
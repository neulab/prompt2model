{"pretrained_model_name": "SaranaAbidueva/mbart50_ru_bua", "description": "---\nlanguage:\n- ru\n- bua\n- bxr\ndatasets:\n- SaranaAbidueva/buryat-russian_parallel_corpus\nmetrics:\n- bleu\n---\n\nHow to use in Python:\n```python\nfrom transformers import MBartForConditionalGeneration, MBart50Tokenizer\nmodel = MBartForConditionalGeneration.from_pretrained(\"SaranaAbidueva/mbart50_ru_bua\")\ntokenizer = MBart50Tokenizer.from_pretrained(\"SaranaAbidueva/mbart50_ru_bua\")\n\ndef fix_tokenizer(tokenizer):\n    old_len = len(tokenizer) - int('bxr_XX' in tokenizer.added_tokens_encoder)\n    tokenizer.lang_code_to_id['bxr_XX'] = old_len-1\n    tokenizer.id_to_lang_code[old_len-1] = 'bxr_XX'\n    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n\n    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n    if 'bxr_XX' not in tokenizer._additional_special_tokens:\n        tokenizer._additional_special_tokens.append('bxr_XX')\n    tokenizer.added_tokens_encoder = {}\nfix_tokenizer(tokenizer)\n\ndef translate(text, src='ru_RU', trg='bxr_XX', max_length=200, num_beams=5, repetition_penalty=5.0, **kwargs):\n    tokenizer.src_lang = src\n    encoded = tokenizer(text, return_tensors=\"pt\")\n    generated_tokens = model.generate(\n        **encoded.to(model.device),\n        forced_bos_token_id=tokenizer.lang_code_to_id[trg], \n        max_length=max_length, \n        num_beams=num_beams,\n        repetition_penalty=repetition_penalty,\n        # early_stopping=True,\n    )\n    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n\ntranslate('\u0415\u0432\u0433\u0435\u043d\u0438\u0439 \u041e\u043d\u0435\u0433\u0438\u043d \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u0430\u044f \u043a\u043d\u0438\u0433\u0430')\n```", "size_bytes": "2534123229", "downloads": 5}
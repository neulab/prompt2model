{"pretrained_model_name": "grammarly/coedit-xl-composite", "description": "---\nlicense: apache-2.0\ndatasets:\n- asset\n- wi_locness\n- GEM/wiki_auto_asset_turk\n- discofuse\n- zaemyung/IteraTeR_plus\n- jfleg\nlanguage:\n- en\nmetrics:\n- sari\n- bleu\n- accuracy\n---\n# Model Card for CoEdIT-xl-composite\n\nThis model was obtained by fine-tuning the corresponding `google/flan-t5-xl` model on the CoEdIT-Composite dataset. Details of the dataset can be found in our paper and repository.\n\n**Paper:** CoEdIT: Text Editing by Task-Specific Instruction Tuning\n\n**Authors:** Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang\n\n## Model Details\n\n### Model Description\n\n- **Language(s) (NLP)**: English\n- **Finetuned from model:** google/flan-t5-xl\n\n### Model Sources\n\n- **Repository:** https://github.com/vipulraheja/coedit\n- **Paper:** https://arxiv.org/abs/2305.09857\n\n## How to use\nWe make available the models presented in our paper. \n\n<table>\n  <tr>\n    <th>Model</th>\n    <th>Number of parameters</th>\n  </tr>\n  <tr>\n    <td>CoEdIT-large</td>\n    <td>770M</td>\n  </tr>\n  <tr>\n    <td>CoEdIT-xl</td>\n    <td>3B</td>\n  </tr>\n  <tr>\n    <td>CoEdIT-xxl</td>\n    <td>11B</td>\n  </tr>  \n</table>\n\n\n## Uses\n\n## Text Revision Task\nGiven an edit instruction and an original text, our model can generate the edited version of the text.<br>\n\n![task_specs](https://huggingface.co/grammarly/coedit-xl/resolve/main/task_examples.png)\n\nThis model can also perform edits on composite instructions, as shown below:\n![composite task_specs](https://huggingface.co/grammarly/coedit-xl-composite/resolve/main/composite_examples.png)\n\n## Usage\n```python\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-xl-composite\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-xl-composite\")\ninput_text = 'Fix grammatical errors in this sentence and make it simpler: When I grow up, I start to understand what he said is quite right.'\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids, max_length=256)\nedited_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n```\n\n\n#### Software\nhttps://github.com/vipulraheja/coedit\n\n## Citation\n\n**BibTeX:**\n```\n@article{raheja2023coedit,\n      title={CoEdIT: Text Editing by Task-Specific Instruction Tuning}, \n      author={Vipul Raheja and Dhruv Kumar and Ryan Koo and Dongyeop Kang},\n      year={2023},\n      eprint={2305.09857},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n**APA:**\nRaheja, V., Kumar, D., Koo, R., & Kang, D. (2023). CoEdIT: Text Editing by Task-Specific Instruction Tuning. ArXiv. /abs/2305.09857\n", "size_bytes": "11398759637", "downloads": 106}
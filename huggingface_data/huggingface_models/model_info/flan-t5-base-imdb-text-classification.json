{"pretrained_model_name": "mohammadtaghizadeh/flan-t5-base-imdb-text-classification", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- f1\n- accuracy\nmodel-index:\n- name: flan-t5-base-imdb-text-classification\n  results: \n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: imdb\n      type: imdb\n      config: imdb\n      split: test\n      args: imdb\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 93.0000\ndatasets:\n- imdb\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-base-imdb-text-classification\n\nThis model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) on the [IMDB](https://huggingface.co/datasets/imdb) dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0767\n- F1: 95.084\n- Gen Len: 2.4976\n\n```cmd\n              precision    recall  f1-score   support\n\n           0       0.97      0.88      0.92     12500\n           1       0.89      0.97      0.93     12500\n\n    accuracy                           0.93     25000\n   macro avg       0.93      0.93      0.93     25000\nweighted avg       0.93      0.93      0.93     25000\n```\n\n## Model description\nIn this implementation, using the **Flan T5 large language model**, we performed the Text Classification task on the IMDB dataset and obtained a very good **accuracy of 93%**.\n\n\n## Training and evaluation data\nThis model was trained on the imdb train dataset with 25,000 data and then tested and evaluated on the imdb test dataset with 25,000 data.\n\n## Usage\n\n1. Install dependencies\n```python\n!pip install transformers==4.28.1 datasets\n```\n\n2. Load IMDB Corpus\n```python\nfrom datasets import load_dataset\n\ndataset_id = \"imdb\"\n\n# Load dataset from the hub\ndataset = load_dataset(dataset_id)\n```\n\n3. Load fine tune flan t5 model\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"mohammadtaghizadeh/flan-t5-base-imdb-text-classification\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"mohammadtaghizadeh/flan-t5-base-imdb-text-classification\")\nmodel.to('cuda')\n```\n\n4. Test the model\n```python\nfrom tqdm.auto import tqdm\n\nsamples_number = len(dataset['test'])\nprogress_bar = tqdm(range(samples_number))\npredictions_list = []\nlabels_list = []\nfor i in range(samples_number):\n  text = dataset['test']['text'][i]\n  inputs = tokenizer.encode_plus(text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n  outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150, num_beams=4, early_stopping=True)\n  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  predictions_list.append(prediction)\n  labels_list.append(dataset['test']['label'][i])\n\n  progress_bar.update(1)\n```\n\n5. Classification report\n```python\nfrom sklearn.metrics import classification_report\n\nstr_labels_list = []\nfor i in range(len(labels_list)): str_labels_list.append(str(labels_list[i]))\n\nreport = classification_report(str_labels_list, predictions_list)\nprint(report)\n```\n\nOutput\n```cmd\n              precision    recall  f1-score   support\n\n           0       0.97      0.88      0.92     12500\n           1       0.89      0.97      0.93     12500\n\n    accuracy                           0.93     25000\n   macro avg       0.93      0.93      0.93     25000\nweighted avg       0.93      0.93      0.93     25000\n```\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n| Training Loss | Epoch | Step |\n|:-------------:|:-----:|:----:|\n| 0.100500      | 1.0   | 3125 |\n| 0.043600      | 2.0   | 6250 | \n\n### Framework versions\n\n- Transformers 4.28.1\n- Pytorch 2.0.0+cu118\n- Datasets 2.12.0\n- Tokenizers 0.13.3", "size_bytes": "990408885", "downloads": 42}
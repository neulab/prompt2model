{"pretrained_model_name": "nlp-lab-2023-seq2seq/R-best-fine-tuned-bart-base-full-ft-reward_short_sentences_and_words-2023-07-13T06-49-08", "description": "---\nlicense: apache-2.0\ntags:\n- trl\n- transformers\n- reinforcement-learning\n---\n\n# TRL Model\n\nThis is a [TRL language model](https://github.com/lvwerra/trl) that has been fine-tuned with reinforcement learning to\n guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.\n\n## Usage\n\nTo use this model for inference, first install the TRL library:\n\n```bash\npython -m pip install trl\n```\n\nYou can then generate text as follows:\n\n```python\nfrom transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=\"romman8//tmp/tmpffgavixx/nlp-lab-2023-seq2seq/R-best-fine-tuned-bart-base-full-ft-reward_short_sentences_and_words-2023-07-13T06-49-08\")\noutputs = generator(\"Hello, my llama is cute\")\n```\n\nIf you want to use the model for training or to obtain the outputs from the value head, load the model as follows:\n\n```python\nfrom transformers import AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"romman8//tmp/tmpffgavixx/nlp-lab-2023-seq2seq/R-best-fine-tuned-bart-base-full-ft-reward_short_sentences_and_words-2023-07-13T06-49-08\")\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"romman8//tmp/tmpffgavixx/nlp-lab-2023-seq2seq/R-best-fine-tuned-bart-base-full-ft-reward_short_sentences_and_words-2023-07-13T06-49-08\")\n\ninputs = tokenizer(\"Hello, my llama is cute\", return_tensors=\"pt\")\noutputs = model(**inputs, labels=inputs[\"input_ids\"])\n```\n", "size_bytes": "557974885", "downloads": 6}
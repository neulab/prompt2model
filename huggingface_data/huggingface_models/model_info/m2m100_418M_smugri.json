{"pretrained_model_name": "tartuNLP/m2m100_418M_smugri", "description": "---\nlicense: mit\n\nlanguage:\n- en\n\nwidget:\n  - text: \"Let us translate some text from Livonian to V\u00f5ro!\"\n---\n\n# NMT for Finno-Ugric Languages\n\nThis is an NMT system for translating between V\u00f5ro, Livonian, North Sami, South Sami as well as Estonian, Finnish, Latvian and English. It was created by fine-tuning Facebook's m2m100-418M on the liv4ever and smugri datasets.\n\n## Tokenizer\nFour language codes were added to the tokenizer: __liv__, __vro__, __sma__ and __sme__. Currently the m2m100 tokenizer loads the list of languages from a hard-coded list, so it has to be updated after loading; see the code example below.\n\n## Usage example\nInstall the transformers and sentencepiece libraries: `pip install sentencepiece transformers`\n\n```from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"tartuNLP/m2m100_418M_smugri\")\n#Fix the language codes in the tokenizer\ntokenizer.id_to_lang_token = dict(list(tokenizer.id_to_lang_token.items()) + list(tokenizer.added_tokens_decoder.items()))\ntokenizer.lang_token_to_id = dict(list(tokenizer.lang_token_to_id.items()) + list(tokenizer.added_tokens_encoder.items()))\ntokenizer.lang_code_to_token = { k.replace(\"_\", \"\"): k for k in tokenizer.additional_special_tokens }\ntokenizer.lang_code_to_id = { k.replace(\"_\", \"\"): v for k, v in tokenizer.lang_token_to_id.items() }\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"tartuNLP/m2m100_418M_smugri\")\n\ntokenizer.src_lang = 'liv'\n\nencoded_src = tokenizer(\"L\u012bv\u00f5 k\u0113\u013c jel\u0101b!\", return_tensors=\"pt\")\n\nencoded_out = model.generate(**encoded_src, forced_bos_token_id = tokenizer.get_lang_id(\"sme\"))\nprint(tokenizer.batch_decode(encoded_out, skip_special_tokens=True))\n```\n\nThe output is `Liv\u010d\u010da giella eall\u00e1.`", "size_bytes": "1935849620", "downloads": 2}
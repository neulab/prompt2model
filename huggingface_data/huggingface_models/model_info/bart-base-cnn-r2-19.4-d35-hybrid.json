{"pretrained_model_name": "echarlaix/bart-base-cnn-r2-19.4-d35-hybrid", "description": "---\nlanguage: en\nlicense: apache-2.0\ntags:\n- summarization\ndatasets:\n- cnn_dailymail\nmetrics:\n- R1\n- R2\n- RL\n---\n\n## facebook/bart-base model fine-tuned on CNN/DailyMail\n\nThis model was created using the [nn_pruning](https://github.com/huggingface/nn_pruning) python library: the linear layers contains **35%** of the original  weights.\n\n\n\nThe model contains **53%** of the original weights **overall** (the embeddings account for a significant part of the model, and they are not pruned by this method).\n\n<div class=\"graph\"><script src=\"/echarlaix/bart-base-cnn-r2-19.4-d35-hybrid/raw/main/model_card/density_info.js\" id=\"c0afb977-b30c-485d-ac75-afc874392380\"></script></div>\n\n## Fine-Pruning details\nThis model was fine-tuned from the HuggingFace [model](https://huggingface.co/facebook/bart-base).\n\nA side-effect of the block pruning is that some of the attention heads are completely removed: 38 heads were removed on a total of 216 (17.6%).\n\n## Details of the CNN/DailyMail  dataset\n\n|    Dataset    | Split | # samples |\n| ------------- | ----- | --------- |\n| CNN/DailyMail | train |   287K    |\n| CNN/DailyMail | eval  |    13K    |\n\n\n### Results\n\n|    Metric   | # Value   | \n| ----------- | --------- |\n| **Rouge 1** | **42.18** | \n| **Rouge 2** | **19.44** | \n| **Rouge L** | **39.17** | \n", "size_bytes": "557982035", "downloads": 13}
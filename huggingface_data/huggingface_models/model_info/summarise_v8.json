{"pretrained_model_name": "debbiesoon/summarise_v8", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: summarise_v8\n  results: []\n---\n![SGH logo.png](https://s3.amazonaws.com/moonup/production/uploads/1667143139655-631feef1124782a19eff4243.png)\n\nThis model is a fine-tuned version of [allenai/led-base-16384](https://huggingface.co/allenai/led-base-16384) on the SGH news articles and summaries dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8163\n- Rouge2 Precision: 0.3628\n- Rouge2 Recall: 0.3589\n- Rouge2 Fmeasure: 0.3316\n\n## Model description\n\nThis model was created to generate summaries of news articles.\n\n## Intended uses & limitations\n\nThe model takes up to maximum article length of 768 tokens and generates a summary of maximum length of 512 tokens, and minimum length of 100 tokens.\n\n## Training and evaluation data\n\nThis model was trained on 100+ articles and summaries from SGH.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge2 Precision | Rouge2 Recall | Rouge2 Fmeasure |\n|:-------------:|:-----:|:----:|:---------------:|:----------------:|:-------------:|:---------------:|\n| 1.5952        | 0.23  | 10   | 1.0414          | 0.2823           | 0.3908        | 0.3013          |\n| 1.8116        | 0.47  | 20   | 0.9171          | 0.3728           | 0.273         | 0.3056          |\n| 1.6289        | 0.7   | 30   | 0.8553          | 0.3284           | 0.2892        | 0.291           |\n| 1.5074        | 0.93  | 40   | 0.8163          | 0.3628           | 0.3589        | 0.3316          |\n\n\n### Framework versions\n\n- Transformers 4.21.3\n- Pytorch 1.12.1+cu113\n- Datasets 1.2.1\n- Tokenizers 0.12.1\n", "size_bytes": "647678513", "downloads": 2}
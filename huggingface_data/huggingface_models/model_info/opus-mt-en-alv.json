{"pretrained_model_name": "Helsinki-NLP/opus-mt-en-alv", "description": "---\nlanguage: \n- en\n- sn\n- rw\n- wo\n- ig\n- sg\n- ee\n- zu\n- lg\n- ts\n- ln\n- ny\n- yo\n- rn\n- xh\n- alv\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### eng-alv\n\n* source group: English \n* target group: Atlantic-Congo languages \n*  OPUS readme: [eng-alv](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-alv/README.md)\n\n*  model: transformer\n* source language(s): eng\n* target language(s): ewe fuc fuv ibo kin lin lug nya run sag sna swh toi_Latn tso umb wol xho yor zul\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)\n* download original weights: [opus2m-2020-08-01.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-alv/opus2m-2020-08-01.zip)\n* test set translations: [opus2m-2020-08-01.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-alv/opus2m-2020-08-01.test.txt)\n* test set scores: [opus2m-2020-08-01.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-alv/opus2m-2020-08-01.eval.txt)\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| Tatoeba-test.eng-ewe.eng.ewe \t| 4.9 \t| 0.212 |\n| Tatoeba-test.eng-ful.eng.ful \t| 0.6 \t| 0.079 |\n| Tatoeba-test.eng-ibo.eng.ibo \t| 3.5 \t| 0.255 |\n| Tatoeba-test.eng-kin.eng.kin \t| 10.5 \t| 0.510 |\n| Tatoeba-test.eng-lin.eng.lin \t| 1.1 \t| 0.273 |\n| Tatoeba-test.eng-lug.eng.lug \t| 5.3 \t| 0.340 |\n| Tatoeba-test.eng.multi \t| 11.4 \t| 0.429 |\n| Tatoeba-test.eng-nya.eng.nya \t| 18.1 \t| 0.595 |\n| Tatoeba-test.eng-run.eng.run \t| 13.9 \t| 0.484 |\n| Tatoeba-test.eng-sag.eng.sag \t| 5.3 \t| 0.194 |\n| Tatoeba-test.eng-sna.eng.sna \t| 26.2 \t| 0.623 |\n| Tatoeba-test.eng-swa.eng.swa \t| 1.0 \t| 0.141 |\n| Tatoeba-test.eng-toi.eng.toi \t| 7.0 \t| 0.224 |\n| Tatoeba-test.eng-tso.eng.tso \t| 46.7 \t| 0.643 |\n| Tatoeba-test.eng-umb.eng.umb \t| 7.8 \t| 0.359 |\n| Tatoeba-test.eng-wol.eng.wol \t| 6.8 \t| 0.191 |\n| Tatoeba-test.eng-xho.eng.xho \t| 27.1 \t| 0.629 |\n| Tatoeba-test.eng-yor.eng.yor \t| 17.4 \t| 0.356 |\n| Tatoeba-test.eng-zul.eng.zul \t| 34.1 \t| 0.729 |\n\n\n### System Info: \n- hf_name: eng-alv\n\n- source_languages: eng\n\n- target_languages: alv\n\n- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-alv/README.md\n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['en', 'sn', 'rw', 'wo', 'ig', 'sg', 'ee', 'zu', 'lg', 'ts', 'ln', 'ny', 'yo', 'rn', 'xh', 'alv']\n\n- src_constituents: {'eng'}\n\n- tgt_constituents: {'sna', 'kin', 'wol', 'ibo', 'swh', 'sag', 'ewe', 'zul', 'fuc', 'lug', 'tso', 'lin', 'nya', 'yor', 'run', 'xho', 'fuv', 'toi_Latn', 'umb'}\n\n- src_multilingual: False\n\n- tgt_multilingual: True\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-alv/opus2m-2020-08-01.zip\n\n- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-alv/opus2m-2020-08-01.test.txt\n\n- src_alpha3: eng\n\n- tgt_alpha3: alv\n\n- short_pair: en-alv\n\n- chrF2_score: 0.429\n\n- bleu: 11.4\n\n- brevity_penalty: 1.0\n\n- ref_len: 10603.0\n\n- src_name: English\n\n- tgt_name: Atlantic-Congo languages\n\n- train_date: 2020-08-01\n\n- src_alpha2: en\n\n- tgt_alpha2: alv\n\n- prefer_old: False\n\n- long_pair: eng-alv\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "size_bytes": "305060961", "downloads": 29}
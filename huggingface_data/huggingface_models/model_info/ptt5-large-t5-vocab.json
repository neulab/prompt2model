{"pretrained_model_name": "unicamp-dl/ptt5-large-t5-vocab", "description": "---\nlanguage: pt\nlicense: mit\ntags:\n- t5\n- pytorch\n- tensorflow\n- pt\n- pt-br\ndatasets:\n- brWaC\nwidget:\n- text: \"Texto de exemplo em portugu\u00eas\"\ninference: false\n---\n\n# Portuguese T5 (aka \"PTT5\")\n\n## Introduction\nPTT5 is a T5 model pretrained in the BrWac corpus, a large  collection  of  web  pages in Portuguese, improving T5's performance on Portuguese sentence similarity and entailment tasks. It's available in three sizes (small, base and large) and two vocabularies (Google's T5 original and ours, trained on Portuguese Wikipedia).\n\nFor further information or requests, please go to [PTT5 repository](https://github.com/unicamp-dl/PTT5).\n\n## Available models\n| Model                                    | Size                                                   | #Params  | Vocabulary         |\n| :-:                                      | :-:                                                            | :-:      | :-:                |\n| [unicamp-dl/ptt5-small-t5-vocab](https://huggingface.co/unicamp-dl/ptt5-small-t5-vocab)                   | small | 60M  | Google's T5 |\n| [unicamp-dl/ptt5-base-t5-vocab](https://huggingface.co/unicamp-dl/ptt5-base-t5-vocab)                     | base  | 220M | Google's T5 |\n| [unicamp-dl/ptt5-large-t5-vocab](https://huggingface.co/unicamp-dl/ptt5-large-t5-vocab)                   | large | 740M | Google's T5 |\n| [unicamp-dl/ptt5-small-portuguese-vocab](https://huggingface.co/unicamp-dl/ptt5-small-portuguese-vocab)   | small | 60M  | Portuguese  |\n| **[unicamp-dl/ptt5-base-portuguese-vocab](https://huggingface.co/unicamp-dl/ptt5-base-portuguese-vocab)** **(Recommended)**     | **base**  | **220M** | **Portuguese**  |\n| [unicamp-dl/ptt5-large-portuguese-vocab](https://huggingface.co/unicamp-dl/ptt5-large-portuguese-vocab)   | large | 740M | Portuguese  |\n\n## Usage\n```python\n# Tokenizer \nfrom transformers import T5Tokenizer\n\n# PyTorch (bare model, baremodel + language modeling head)\nfrom transformers import T5Model, T5ForConditionalGeneration\n\n# Tensorflow (bare model, baremodel + language modeling head)\nfrom transformers import TFT5Model, TFT5ForConditionalGeneration\n\nmodel_name = 'unicamp-dl/ptt5-base-portuguese-vocab'\n\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# PyTorch\nmodel_pt = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# TensorFlow\nmodel_tf = TFT5ForConditionalGeneration.from_pretrained(model_name)\n```\n\n\n# Citation\nIf you use PTT5, please cite:\n\n    @article{ptt5_2020,\n      title={PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data},\n      author={Carmo, Diedre and Piau, Marcos and Campiotti, Israel and Nogueira, Rodrigo and Lotufo, Roberto},\n      journal={arXiv preprint arXiv:2008.09144},\n      year={2020}\n    }\n", "size_bytes": "2950820882", "downloads": 69}
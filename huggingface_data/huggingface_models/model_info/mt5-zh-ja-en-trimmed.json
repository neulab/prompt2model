{"pretrained_model_name": "K024/mt5-zh-ja-en-trimmed", "description": "---\nlanguage:\n- zh\n- ja\n- en\n\ntags:\n- translation\n\nwidget:\n- text: \"ja2zh: \u543e\u8f29\u306f\u732b\u3067\u3042\u308b\u3002\u540d\u524d\u306f\u307e\u3060\u7121\u3044\u3002\"\n\nlicense: cc-by-nc-sa-4.0\n---\n\nThis model is finetuned from [mt5-base](https://huggingface.co/google/mt5-base).\n\nThe model vocabulary is trimmed to ~1/3 by selecting top 85000 tokens in the training data. The code to trim the vocabulary can be found [here](https://gist.github.com/K024/4a100a0f4f4b07208958e0f3244da6ad).\n\nUsage:\n```python\nfrom transformers import (\n  T5Tokenizer,\n  MT5ForConditionalGeneration,\n  Text2TextGenerationPipeline,\n)\n\npath = \"K024/mt5-zh-ja-en-trimmed\"\npipe = Text2TextGenerationPipeline(\n  model=MT5ForConditionalGeneration.from_pretrained(path),\n  tokenizer=T5Tokenizer.from_pretrained(path),\n)\n\nsentence = \"ja2zh: \u543e\u8f29\u306f\u732b\u3067\u3042\u308b\u3002\u540d\u524d\u306f\u307e\u3060\u7121\u3044\u3002\"\nres = pipe(sentence, max_length=100, num_beams=4)\nres[0]['generated_text']\n```\n\nTraining data:\n```\nwikimedia-en-ja\nwikimedia-en-zh\nwikimedia-ja-zh\nwikititles-ja-en\nwikititles-zh-en\nwikimatrix-ja-zh\nnews-commentary-en-ja\nnews-commentary-en-zh\nnews-commentary-ja-zh\nted2020-en-ja\nted2020-en-zh\nted2020-ja-zh\n```\n\nLicense: [![CC BY-NC-SA 4.0][cc-by-nc-sa-image]][cc-by-nc-sa]\n\n[cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/\n[cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\n", "size_bytes": "1317077965", "downloads": 3406}
{"pretrained_model_name": "nguyenvulebinh/mbart-large-50-latin-only", "description": "---\nlanguage: \n- multilingual\n- en\n\nlicense: mit\ntags:\n- mbart-50\n---\n\n# mBART-50\n\nmBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the \"Multilingual Denoising Pretraining\" objective. It was introduced in [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) paper.\n\n## Model description\n\nmBART-50 is a multilingual Sequence-to-Sequence model. It was introduced to show that multilingual translation models can be created through multilingual fine-tuning. \nInstead of fine-tuning on one direction, a pre-trained model is fine-tuned on many directions simultaneously. mBART-50 is created using the original mBART model and extended to add extra 25 languages to support multilingual machine translation models of 50 languages. The pre-training objective is explained below.\n\n**Multilingual Denoising Pretraining**: The model incorporates N languages by concatenating data: \n`D = {D1, ..., DN }` where each Di is a collection of monolingual documents in language `i`. The source documents are noised using two schemes, \nfirst randomly shuffling the original sentences' order, and second a novel in-filling scheme, \nwhere spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. \n35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(\u03bb = 3.5)`.\nThe decoder input is the original text with one position offset. A language id symbol `LID` is used as the initial token to predict the sentence.\n\n\n## Checking\n\n\n```python\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained('facebook/mbart-large-50')\ntokenizer = AutoTokenizer.from_pretrained('facebook/mbart-large-50')\n\nsrc_text = \"UN Chief Says There Is <mask> Military Solution <mask> Syria\"\nencoded_hi = tokenizer(src_text, return_tensors=\"pt\")\ngenerated_output = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"], \n                                  return_dict_in_generate=True, return_dict=True, output_hidden_states=True)\ntext_output = tokenizer.batch_decode(generated_output.sequences, skip_special_tokens=True)\n\n\nnew_model = AutoModelForSeq2SeqLM.from_pretrained('nguyenvulebinh/mbart-large-50-latin-only')\nnew_tokenizer = AutoTokenizer.from_pretrained('nguyenvulebinh/mbart-large-50-latin-only')\nnew_encoded_hi = new_tokenizer(src_text, return_tensors=\"pt\")\nnew_generated_output = new_model.generate(**new_encoded_hi, forced_bos_token_id=new_tokenizer.lang_code_to_id[\"en_XX\"], \n                                          return_dict_in_generate=True, return_dict=True, output_hidden_states=True)\nnew_text_output = new_tokenizer.batch_decode(new_generated_output.sequences, skip_special_tokens=True)\n\nassert text_output == new_text_output\nassert torch.equal(generated_output.encoder_hidden_states[-1], new_generated_output.encoder_hidden_states[-1])\nassert torch.equal(generated_output.decoder_hidden_states[-1][-1], new_generated_output.decoder_hidden_states[-1][-1])\n\nprint(new_text_output)\n# ['UN Chief Says There Is  No Military Solution  to the War in Syria']\n\n```\n\n\n\n## Languages covered\nEnglish (en_XX)\n\n\n## BibTeX entry and citation info\n```\n@article{tang2020multilingual,\n    title={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},\n    author={Yuqing Tang and Chau Tran and Xian Li and Peng-Jen Chen and Naman Goyal and Vishrav Chaudhary and Jiatao Gu and Angela Fan},\n    year={2020},\n    eprint={2008.00401},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```", "size_bytes": "1760011357", "downloads": 5}
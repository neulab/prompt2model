{"pretrained_model_name": "ELiRF/NASCA", "description": "---\nlanguage: ca\n\ntags:\n- summarization\n\nwidget:\n- text: \"La Universitat Polit\u00e8cnica de Val\u00e8ncia (UPV), a trav\u00e9s del projecte Atenea \u201cplataforma de dones, art i tecnologia\u201d i en col\u00b7laboraci\u00f3 amb les companyies tecnol\u00f2giques Metric Salad i Zetalab, ha digitalitzat i modelat en 3D per a la 35a edici\u00f3 del Festival Dansa Val\u00e8ncia, que se celebra del 2 al 10 d'abril, la primera pe\u00e7a de dansa en un metaverso espec\u00edfic. La pe\u00e7a No \u00e9s amor, dirigida per Lara Mis\u00f3, forma part de la programaci\u00f3 d'aquesta edici\u00f3 del Festival Dansa Val\u00e8ncia i explora la figura geom\u00e8trica del cercle des de totes les seues perspectives: espacial, corporal i compositiva. No \u00e9s amor est\u00e0 inspirada en el treball de l'artista japonesa Yayoi Kusama i mira de prop les diferents facetes d'una obsessi\u00f3. Aix\u00ed dona cabuda a la insist\u00e8ncia, la repetici\u00f3, el trastorn, la hipnosi i l'alliberament. El proc\u00e9s de digitalitzaci\u00f3, materialitzat per Metric Salad i ZetaLab, ha sigut complex respecte a uns altres ja realitzats a causa de l'enorme desafiament que comporta el modelatge en 3D de cossos en moviment al ritme de la composici\u00f3 de l'obra. L'objectiu era generar una experi\u00e8ncia el m\u00e9s realista possible i fidedigna de l'original perqu\u00e8 el resultat final fora un proc\u00e9s absolutament immersiu.Aix\u00ed, el metaverso est\u00e0 compost per figures modelades en 3D al costat de quatre projeccions digitalitzades en pantalles flotants amb les quals l'usuari podr\u00e0 interactuar segons es vaja acostant, b\u00e9 mitjan\u00e7ant els comandaments de l'ordinador, b\u00e9 a trav\u00e9s d'ulleres de realitat virtual. L'objectiu \u00e9s que quan l'usuari s'acoste a cadascuna de les projeccions tinga la sensaci\u00f3 d'una immersi\u00f3 quasi completa en fondre's amb el contingut audiovisual que li genere una experi\u00e8ncia intimista i molt real.\"\n---\n\n**IMPORTANT:** On the 5th of April 2022, we detected a mistake in the configuration file; thus, the model was not generating the summaries correctly, and it was underperforming in all scenarios. For this reason, if you had used the model until that day, we would be glad if you would re-evaluate the model if you are publishing some results with it. We apologize for the inconvenience and thank you for your understanding.\n\n# NASca and NASes: Two Monolingual Pre-Trained Models for Abstractive Summarization in Catalan and Spanish\n\nMost of the models proposed in the literature for abstractive summarization are generally suitable for the English language but not for other languages. Multilingual models were introduced to address that language constraint, but despite their applicability being broader than that of the monolingual models, their performance is typically lower, especially for minority languages like Catalan. In this paper, we present a monolingual model for abstractive summarization of textual content in the Catalan language. The model is a Transformer encoder-decoder which is pretrained and fine-tuned specifically for the Catalan language using a corpus of newspaper articles. In the pretraining phase, we introduced several self-supervised tasks to specialize the model on the summarization task and to increase the abstractivity of the generated summaries. To study the performance of our proposal in languages with higher resources than Catalan, we replicate the model and the experimentation for the Spanish language. The usual evaluation metrics, not only the most used ROUGE measure but also other more semantic ones such as BertScore, do not allow to correctly evaluate the abstractivity of the generated summaries. In this work, we also present a new metric, called content reordering, to evaluate one of the most common characteristics of abstractive summaries, the rearrangement of the original content. We carried out an exhaustive experimentation to compare the performance of the monolingual models proposed in this work with two of the most widely used multilingual models in text summarization, mBART and mT5. The experimentation results support the quality of our monolingual models, especially considering that the multilingual models were pretrained with many more resources than those used in our models. Likewise, it is shown that the pretraining tasks helped to increase the degree of abstractivity of the generated summaries. To our knowledge, this is the first work that explores a monolingual approach for abstractive summarization both in Catalan and Spanish. \n\n# The NASca model\nNews Abstractive Summarization for Catalan (NASca) is a Transformer encoder-decoder model, with the same hyper-parameters than BART, to perform summarization of Catalan news articles. It is pre-trained on a combination of several self-supervised tasks that help to increase the abstractivity of the generated summaries. Four pre-training tasks have been combined: sentence permutation, text infilling, Gap Sentence Generation, and Next Segment Generation. Catalan newspapers, the Catalan subset of the OSCAR corpus and Wikipedia articles in Catalan were used for pre-training the model (9.3GB of raw text -2.5 millions of documents-).\n\nNASca is finetuned for the summarization task on 636.596 (document, summary) pairs from the Dataset for Automatic summarization of Catalan and Spanish newspaper Articles (DACSA).\n\n### BibTeX entry\n```bibtex\n@Article{app11219872,\nAUTHOR = {Ahuir, Vicent and Hurtado, Llu\u00eds-F. and Gonz\u00e1lez, Jos\u00e9 \u00c1ngel and Segarra, Encarna},\nTITLE = {NASca and NASes: Two Monolingual Pre-Trained Models for Abstractive Summarization in Catalan and Spanish},\nJOURNAL = {Applied Sciences},\nVOLUME = {11},\nYEAR = {2021},\nNUMBER = {21},\nARTICLE-NUMBER = {9872},\nURL = {https://www.mdpi.com/2076-3417/11/21/9872},\nISSN = {2076-3417},\nDOI = {10.3390/app11219872}\n}\n```", "size_bytes": "1661281455", "downloads": 9}
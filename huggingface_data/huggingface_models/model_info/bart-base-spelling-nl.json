{"pretrained_model_name": "antalvdb/bart-base-spelling-nl", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bart-base-spelling-nl-1m-3\n  results: []\n---\n\n# bart-base-spelling-nl\n\nThis model is a Dutch fine-tuned version of\n[facebook/bart-base](https://huggingface.co/facebook/bart-base).\n\nIt achieves the following results on an external evaluation set of human-corrected spelling\nerrors of Dutch snippets of internet text ([errors](https://huggingface.co/antalvdb/bart-base-spelling-nl/blob/main/opentaal-annotaties.txt.errors) \nand [corrections](https://huggingface.co/antalvdb/bart-base-spelling-nl/blob/main/opentaal-annotaties.txt.corrections), \nrun [spell.py](https://huggingface.co/antalvdb/bart-base-spelling-nl/blob/main/spell.py))\n\n* CER    - 0.024\n* WER    - 0.088\n* BLEU   - 0.840\n* METEOR - 0.932\n\nNote that it is very hard for any spelling corrector to clean more actual spelling errors \nthan introducing new errors. In other words, most spelling correctors cannot be run\nautomatically and must be used interactively.\n\nThese are the upper-bound scores when correcting _nothing_. In other words, this is \nthe actual distance between the errors and their corrections in the evaluation set:\n\n* CER    - 0.010\n* WER    - 0.053\n* BLEU   - 0.900\n* METEOR - 0.954\n\nWe are not there yet, clearly.\n\n## Model description\n\nThis is a fine-tuned version of\n[facebook/bart-base](https://huggingface.co/facebook/bart-base)\ntrained on spelling correction. It leans on the excellent work by\nOliver Guhr ([github](https://github.com/oliverguhr/spelling),\n[huggingface](https://huggingface.co/oliverguhr/spelling-correction-english-base)). Training\nwas performed on an AWS EC2 instance (g5.xlarge) on a single GPU, and\ntook about two days.\n\n## Intended uses & limitations\n\nThe intended use for this model is to be a component of the\n[Valkuil.net](https://valkuil.net) context-sensitive spelling\nchecker. \n\n## Training and evaluation data\n\nThe model was trained on a Dutch dataset composed of 12,351,203 lines\nof text, containing a total of 123,131,153 words, from three public Dutch sources, downloaded from the\n[Opus corpus](https://opus.nlpl.eu/):\n\n- nl-europarlv7.txt (2,387,000 lines)\n- nl-opensubtitles2016.9m.txt (9,000,000 lines)\n- nl-wikipedia.txt (964,203 lines)\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 2\n- eval_batch_size: 4\n- seed: 42\n- gradient_accumulation_steps: 16\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2.0\n\n\n### Framework versions\n\n- Transformers 4.27.3\n- Pytorch 2.0.0+cu117\n- Datasets 2.10.1\n- Tokenizers 0.13.2\n", "size_bytes": "557971229", "downloads": 11}
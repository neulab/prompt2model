{"pretrained_model_name": "Trabis/Helsinki-NLPopus-mt-tc-big-en-moroccain_dialect", "description": "---\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: Helsinki-NLPopus-mt-tc-big-en-moroccain_dialect\n  results: []\npipeline_tag: translation\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n<!-- in this model i use transfer learning for translate english to Moroccain dialect (darija). -->\n\n<!-- about dataset used for training model : I used about 18,000 pairs of English and Moroccain Dialect. -->\n\n<!-- my model is trained three times, the last being one epoch. -->\n\n# Helsinki-NLPopus-mt-tc-big-en-moroccain_dialect\n\nThis model was trained from scratch on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6930\n- Bleu: 50.0607\n- Gen Len: 14.7048\n\n## Model description\n\nMarianConfig {\n  \"_name_or_path\": \"/content/drive/MyDrive/Colab Notebooks/big_helsinki_eng_dar\",\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"relu\",\n  \"architectures\": [\n    \"MarianMTModel\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bad_words_ids\": [\n    [\n      61246\n    ]\n  ],\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 61246,\n  \"decoder_vocab_size\": 61247,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 25897,\n  \"forced_eos_token_id\": 25897,\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"max_length\": 512,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"marian\",\n  \"normalize_embedding\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 61246,\n  \"scale_embedding\": true,\n  \"share_encoder_decoder_embeddings\": true,\n  \"static_position_embeddings\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.28.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 61247\n}\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 15443\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 813\n    })\n})\n\n## Training procedure\n\nUsing transfer learning due to limited data in the Moroccan dialect.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 4000\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|\n| 0.617         | 1.0   | 1931 | 0.6930          | 50.0607 | 14.7048 |\n\n\n### Framework versions\n\n- Transformers 4.28.0\n- Pytorch 2.0.0+cu118\n- Datasets 2.12.0\n- Tokenizers 0.13.3", "size_bytes": "956625157", "downloads": 19}
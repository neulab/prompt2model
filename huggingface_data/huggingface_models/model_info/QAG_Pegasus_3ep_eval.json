{"pretrained_model_name": "nloc2578/QAG_Pegasus_3ep_eval", "description": "\n## Overview\n```\nLanguage model: Pegasus-xsum\nLanguage: English\nDownstream-task: Question-Answering Generation\nTraining data: SQuAD 2.0, NewsQA\nEval data: SQuAD 2.0, NewsQA\nInfrastructure: Nvidia Tesla K80 12Gb RAM\n```\n\n## Hyperparameters\n```\nper_device_train_batch_size = 2\nper_device_eval_batch_size = 2\nnum_train_epochs = 3\nbase_LM_model = \"pegasus-xsum\"\nsource_max_token_len = 256\ntarget_max_token_len = 64\nlearning_rate = 5e-5\nlr_schedule = LinearWarmup\nwarmup_steps = 150\n```\n\n## Usage\n```python\nimport transformers\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizerFast\n\nmodel_name = 'nloc2578/QAG_Pegasus_3ep_eval'\ntokenizer = PegasusTokenizerFast.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n\ntext = '''The primary goal of distractor generation is generating answer\noptions that are plausibly answers to the question, and might appear\ncorrect to a user who does know the correct answer. Distractors\nshould also be clearly distinct from the key and each other and\nthey should not be correct answers to the question (for questions\nthat might have multiple correct answers).'''\n\ninput_id = tokenizer(text, return_tensors='pt')\noutput = model.generate(input_id['input_ids'])\nresult = tokenizer.decode(output[0])\n\nprint(result)\n```", "size_bytes": "2279637617", "downloads": 2}
{"pretrained_model_name": "din0s/t5-base-finetuned-en-to-it-lrs", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: t5-base-finetuned-en-to-it-lrs\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-base-finetuned-en-to-it-lrs\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.4687\n- Bleu: 22.9793\n- Gen Len: 49.8367\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 40\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 1.4378        | 1.0   | 1125  | 1.9365          | 12.0299 | 55.7007 |\n| 1.229         | 2.0   | 2250  | 1.8493          | 15.9175 | 51.6293 |\n| 1.0996        | 3.0   | 3375  | 1.7781          | 17.5103 | 51.666  |\n| 0.9979        | 4.0   | 4500  | 1.7309          | 18.8603 | 50.8587 |\n| 0.9421        | 5.0   | 5625  | 1.6839          | 19.8188 | 50.4767 |\n| 0.9181        | 6.0   | 6750  | 1.6602          | 20.5693 | 50.272  |\n| 0.8882        | 7.0   | 7875  | 1.6386          | 20.9771 | 50.3833 |\n| 0.8498        | 8.0   | 9000  | 1.6252          | 21.2237 | 50.5093 |\n| 0.8356        | 9.0   | 10125 | 1.6079          | 21.3987 | 50.31   |\n| 0.8164        | 10.0  | 11250 | 1.5698          | 21.5409 | 50.388  |\n| 0.8001        | 11.0  | 12375 | 1.5779          | 21.7354 | 49.822  |\n| 0.7805        | 12.0  | 13500 | 1.5637          | 21.9649 | 49.8213 |\n| 0.764         | 13.0  | 14625 | 1.5540          | 22.1342 | 50.2    |\n| 0.7594        | 14.0  | 15750 | 1.5456          | 22.2318 | 50.0147 |\n| 0.7355        | 15.0  | 16875 | 1.5309          | 22.2936 | 49.7693 |\n| 0.7343        | 16.0  | 18000 | 1.5247          | 22.5065 | 49.7607 |\n| 0.7231        | 17.0  | 19125 | 1.5231          | 22.3902 | 49.7733 |\n| 0.7183        | 18.0  | 20250 | 1.5211          | 22.3672 | 49.8313 |\n| 0.7068        | 19.0  | 21375 | 1.5075          | 22.5519 | 49.7433 |\n| 0.7087        | 20.0  | 22500 | 1.5006          | 22.4827 | 49.5    |\n| 0.6965        | 21.0  | 23625 | 1.4978          | 22.5907 | 49.6833 |\n| 0.6896        | 22.0  | 24750 | 1.4955          | 22.6286 | 49.836  |\n| 0.689         | 23.0  | 25875 | 1.4924          | 22.7052 | 49.7267 |\n| 0.6793        | 24.0  | 27000 | 1.4890          | 22.7444 | 49.8393 |\n| 0.6708        | 25.0  | 28125 | 1.4889          | 22.6821 | 49.8673 |\n| 0.6671        | 26.0  | 29250 | 1.4835          | 22.7866 | 49.676  |\n| 0.6652        | 27.0  | 30375 | 1.4853          | 22.7691 | 49.7107 |\n| 0.6578        | 28.0  | 31500 | 1.4787          | 22.8173 | 49.738  |\n| 0.6556        | 29.0  | 32625 | 1.4777          | 22.7408 | 49.6687 |\n| 0.6592        | 30.0  | 33750 | 1.4772          | 22.8371 | 49.7307 |\n| 0.6546        | 31.0  | 34875 | 1.4819          | 22.8398 | 49.6053 |\n| 0.6465        | 32.0  | 36000 | 1.4741          | 22.8379 | 49.658  |\n| 0.6381        | 33.0  | 37125 | 1.4691          | 22.9108 | 49.8113 |\n| 0.6429        | 34.0  | 38250 | 1.4660          | 22.9405 | 49.7933 |\n| 0.6381        | 35.0  | 39375 | 1.4701          | 22.8777 | 49.7467 |\n| 0.6454        | 36.0  | 40500 | 1.4692          | 22.9225 | 49.7227 |\n| 0.635         | 37.0  | 41625 | 1.4683          | 22.9914 | 49.6767 |\n| 0.6389        | 38.0  | 42750 | 1.4691          | 22.9904 | 49.7133 |\n| 0.6368        | 39.0  | 43875 | 1.4679          | 22.9962 | 49.8273 |\n| 0.6345        | 40.0  | 45000 | 1.4687          | 22.9793 | 49.8367 |\n\n\n### Framework versions\n\n- Transformers 4.22.1\n- Pytorch 1.12.1\n- Datasets 2.5.1\n- Tokenizers 0.11.0\n", "size_bytes": "891700799", "downloads": 2}
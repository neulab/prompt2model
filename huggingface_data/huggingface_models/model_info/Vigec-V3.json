{"pretrained_model_name": "HuyenNguyen/Vigec-V3", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: Vigec-V3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# Vigec-V3\n\nThis model is a fine-tuned version of [VietAI/vit5-base](https://huggingface.co/VietAI/vit5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2522\n- Bleu: 84.4788\n- Gen Len: 9.847\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- training_steps: 10000\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 0.8764        | 0.0   | 500   | 0.6120          | 68.0114 | 8.4626  |\n| 0.6538        | 0.0   | 1000  | 0.4780          | 76.7403 | 10.015  |\n| 0.6234        | 0.01  | 1500  | 0.4207          | 78.1726 | 9.8394  |\n| 0.4513        | 0.01  | 2000  | 0.3845          | 79.1939 | 9.8914  |\n| 0.4153        | 0.01  | 2500  | 0.3580          | 80.171  | 9.7298  |\n| 0.5129        | 0.01  | 3000  | 0.3381          | 80.8668 | 9.8636  |\n| 0.5073        | 0.01  | 3500  | 0.3246          | 81.5543 | 9.81    |\n| 0.4623        | 0.01  | 4000  | 0.3106          | 82.1255 | 9.8684  |\n| 0.4444        | 0.02  | 4500  | 0.2973          | 82.5565 | 9.848   |\n| 0.4322        | 0.02  | 5000  | 0.2892          | 82.9623 | 9.872   |\n| 0.5029        | 0.02  | 5500  | 0.2803          | 83.3084 | 9.8648  |\n| 0.3686        | 0.02  | 6000  | 0.2765          | 83.4828 | 9.8602  |\n| 0.4123        | 0.02  | 6500  | 0.2693          | 83.7491 | 9.8432  |\n| 0.3593        | 0.03  | 7000  | 0.2674          | 83.8149 | 9.811   |\n| 0.3684        | 0.03  | 7500  | 0.2630          | 84.1745 | 9.8668  |\n| 0.3683        | 0.03  | 8000  | 0.2590          | 84.2294 | 9.8412  |\n| 0.3581        | 0.03  | 8500  | 0.2568          | 84.3428 | 9.8582  |\n| 0.3769        | 0.03  | 9000  | 0.2527          | 84.4367 | 9.8598  |\n| 0.4479        | 0.03  | 9500  | 0.2522          | 84.4749 | 9.847   |\n| 0.2856        | 0.04  | 10000 | 0.2522          | 84.4788 | 9.847   |\n\n\n### Framework versions\n\n- Transformers 4.26.0\n- Pytorch 1.13.1+cu116\n- Datasets 2.9.0\n- Tokenizers 0.13.2\n", "size_bytes": "903892625", "downloads": 2}
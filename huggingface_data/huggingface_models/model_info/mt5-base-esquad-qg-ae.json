{"pretrained_model_name": "lmqg/mt5-base-esquad-qg-ae", "description": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: es\ndatasets:\n- lmqg/qg_esquad\npipeline_tag: text2text-generation\ntags:\n- question generation\n- answer extraction\nwidget:\n- text: \"generate question: del <hl> Ministerio de Desarrollo Urbano <hl> , Gobierno de la India.\"\n  example_title: \"Question Generation Example 1\" \n- text: \"generate question: a <hl> noviembre <hl> , que es tambi\u00e9n la estaci\u00f3n lluviosa.\"\n  example_title: \"Question Generation Example 2\" \n- text: \"generate question: como <hl> el gobierno de Abbott <hl> que asumi\u00f3 el cargo el 18 de septiembre de 2013.\"\n  example_title: \"Question Generation Example 3\" \n- text: \"extract answers: <hl> En la di\u00e1spora somal\u00ed, m\u00faltiples eventos isl\u00e1micos de recaudaci\u00f3n de fondos se llevan a cabo cada a\u00f1o en ciudades como Birmingham, Londres, Toronto y Minneapolis, donde los acad\u00e9micos y profesionales somal\u00edes dan conferencias y responden preguntas de la audiencia. <hl> El prop\u00f3sito de estos eventos es recaudar dinero para nuevas escuelas o universidades en Somalia, para ayudar a los somal\u00edes que han sufrido como consecuencia de inundaciones y / o sequ\u00edas, o para reunir fondos para la creaci\u00f3n de nuevas mezquitas como.\"\n  example_title: \"Answer Extraction Example 1\" \n- text: \"extract answers: <hl> Los estudiosos y los histori a dores est\u00e1n divididos en cuanto a qu\u00e9 evento se\u00f1ala el final de la era helen\u00edstica. <hl> El per\u00edodo helen\u00edstico se puede ver que termina con la conquista final del coraz\u00f3n griego por Roma en 146 a. C. tras la guerra aquea, con la derrota final del reino ptolemaico en la batalla de Actium en 31 a. Helen\u00edstico se distingue de hel\u00e9nico en que el primero abarca toda la esfera de influencia griega antigua directa, mientras que el segundo se refiere a la propia Grecia.\"\n  example_title: \"Answer Extraction Example 2\" \nmodel-index:\n- name: lmqg/mt5-base-esquad-qg-ae\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_esquad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 9.62\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 24.82\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 23.11\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 83.97\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 59.15\n    - name: QAAlignedF1Score-BERTScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_f1_score_bertscore_question_answer_generation_with_gold_answer\n      value: 79.67\n    - name: QAAlignedRecall-BERTScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_recall_bertscore_question_answer_generation_with_gold_answer\n      value: 82.44\n    - name: QAAlignedPrecision-BERTScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_precision_bertscore_question_answer_generation_with_gold_answer\n      value: 77.14\n    - name: QAAlignedF1Score-MoverScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_f1_score_moverscore_question_answer_generation_with_gold_answer\n      value: 54.82\n    - name: QAAlignedRecall-MoverScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_recall_moverscore_question_answer_generation_with_gold_answer\n      value: 56.56\n    - name: QAAlignedPrecision-MoverScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_precision_moverscore_question_answer_generation_with_gold_answer\n      value: 53.27\n    - name: BLEU4 (Answer Extraction)\n      type: bleu4_answer_extraction\n      value: 25.75\n    - name: ROUGE-L (Answer Extraction)\n      type: rouge_l_answer_extraction\n      value: 49.61\n    - name: METEOR (Answer Extraction)\n      type: meteor_answer_extraction\n      value: 43.74\n    - name: BERTScore (Answer Extraction)\n      type: bertscore_answer_extraction\n      value: 90.04\n    - name: MoverScore (Answer Extraction)\n      type: moverscore_answer_extraction\n      value: 80.94\n    - name: AnswerF1Score (Answer Extraction)\n      type: answer_f1_score__answer_extraction\n      value: 75.33\n    - name: AnswerExactMatch (Answer Extraction)\n      type: answer_exact_match_answer_extraction\n      value: 57.98\n---\n\n# Model Card of `lmqg/mt5-base-esquad-qg-ae`\nThis model is fine-tuned version of [google/mt5-base](https://huggingface.co/google/mt5-base) for question generation and answer extraction jointly on the [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) (dataset_name: default) via [`lmqg`](https://github.com/asahi417/lm-question-generation).\n\n\n### Overview\n- **Language model:** [google/mt5-base](https://huggingface.co/google/mt5-base)   \n- **Language:** es  \n- **Training data:** [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) (default)\n- **Online Demo:** [https://autoqg.net/](https://autoqg.net/)\n- **Repository:** [https://github.com/asahi417/lm-question-generation](https://github.com/asahi417/lm-question-generation)\n- **Paper:** [https://arxiv.org/abs/2210.03992](https://arxiv.org/abs/2210.03992)\n\n### Usage\n- With [`lmqg`](https://github.com/asahi417/lm-question-generation#lmqg-language-model-for-question-generation-)\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"es\", model=\"lmqg/mt5-base-esquad-qg-ae\")\n\n# model prediction\nquestion_answer_pairs = model.generate_qa(\"a noviembre , que es tambi\u00e9n la estaci\u00f3n lluviosa.\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"lmqg/mt5-base-esquad-qg-ae\")\n\n# answer extraction\nanswer = pipe(\"generate question: del <hl> Ministerio de Desarrollo Urbano <hl> , Gobierno de la India.\")\n\n# question generation\nquestion = pipe(\"extract answers: <hl> En la di\u00e1spora somal\u00ed, m\u00faltiples eventos isl\u00e1micos de recaudaci\u00f3n de fondos se llevan a cabo cada a\u00f1o en ciudades como Birmingham, Londres, Toronto y Minneapolis, donde los acad\u00e9micos y profesionales somal\u00edes dan conferencias y responden preguntas de la audiencia. <hl> El prop\u00f3sito de estos eventos es recaudar dinero para nuevas escuelas o universidades en Somalia, para ayudar a los somal\u00edes que han sufrido como consecuencia de inundaciones y / o sequ\u00edas, o para reunir fondos para la creaci\u00f3n de nuevas mezquitas como.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file](https://huggingface.co/lmqg/mt5-base-esquad-qg-ae/raw/main/eval/metric.first.sentence.paragraph_answer.question.lmqg_qg_esquad.default.json) \n\n|            |   Score | Type    | Dataset                                                          |\n|:-----------|--------:|:--------|:-----------------------------------------------------------------|\n| BERTScore  |   83.97 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| Bleu_1     |   25.88 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| Bleu_2     |   17.67 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| Bleu_3     |   12.84 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| Bleu_4     |    9.62 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| METEOR     |   23.11 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| MoverScore |   59.15 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| ROUGE_L    |   24.82 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n\n\n- ***Metric (Question & Answer Generation)***:  [raw metric file](https://huggingface.co/lmqg/mt5-base-esquad-qg-ae/raw/main/eval/metric.first.answer.paragraph.questions_answers.lmqg_qg_esquad.default.json)\n\n|                                 |   Score | Type    | Dataset                                                          |\n|:--------------------------------|--------:|:--------|:-----------------------------------------------------------------|\n| QAAlignedF1Score (BERTScore)    |   79.67 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| QAAlignedF1Score (MoverScore)   |   54.82 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| QAAlignedPrecision (BERTScore)  |   77.14 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| QAAlignedPrecision (MoverScore) |   53.27 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| QAAlignedRecall (BERTScore)     |   82.44 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| QAAlignedRecall (MoverScore)    |   56.56 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n\n\n- ***Metric (Answer Extraction)***: [raw metric file](https://huggingface.co/lmqg/mt5-base-esquad-qg-ae/raw/main/eval/metric.first.answer.paragraph_sentence.answer.lmqg_qg_esquad.default.json)\n\n|                  |   Score | Type    | Dataset                                                          |\n|:-----------------|--------:|:--------|:-----------------------------------------------------------------|\n| AnswerExactMatch |   57.98 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| AnswerF1Score    |   75.33 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| BERTScore        |   90.04 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| Bleu_1           |   37.35 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| Bleu_2           |   32.53 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| Bleu_3           |   28.86 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| Bleu_4           |   25.75 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| METEOR           |   43.74 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| MoverScore       |   80.94 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n| ROUGE_L          |   49.61 | default | [lmqg/qg_esquad](https://huggingface.co/datasets/lmqg/qg_esquad) |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_esquad\n - dataset_name: default\n - input_types: ['paragraph_answer', 'paragraph_sentence']\n - output_types: ['question', 'answer']\n - prefix_types: ['qg', 'ae']\n - model: google/mt5-base\n - max_length: 512\n - max_length_output: 32\n - epoch: 7\n - batch: 32\n - lr: 0.001\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 2\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](https://huggingface.co/lmqg/mt5-base-esquad-qg-ae/raw/main/trainer_config.json).\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "size_bytes": "2329628621", "downloads": 17}
{"pretrained_model_name": "research-backup/mbart-large-cc25-trimmed-fr", "description": "# Vocabulary Trimmed [mbart-large-cc25-trimmed-fr](https://huggingface.co/mbart-large-cc25-trimmed-fr): `asahi417/mbart-large-cc25-trimmed-fr` \nThis model is a trimmed version of [mbart-large-cc25-trimmed-fr](https://huggingface.co/mbart-large-cc25-trimmed-fr) by [`vocabtrimmer`](https://github.com/asahi417/lm-vocab-trimmer), a tool for trimming vocabulary of language models to compress the model size.\nFollowing table shows a summary of the trimming process.\n\n|                            | mbart-large-cc25-trimmed-fr   | asahi417/mbart-large-cc25-trimmed-fr   |\n|:---------------------------|:------------------------------|:---------------------------------------|\n| parameter_size_full        | 610,851,840                   | 398,106,624                            |\n| parameter_size_embedding   | 512,055,296                   | 86,564,864                             |\n| vocab_size                 | 250,027                       | 42,268                                 |\n| compression_rate_full      | 100.0                         | 65.17                                  |\n| compression_rate_embedding | 100.0                         | 16.91                                  |", "size_bytes": "1592764125", "downloads": 2}
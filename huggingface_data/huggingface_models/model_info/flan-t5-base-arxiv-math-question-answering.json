{"pretrained_model_name": "ArtifactAI/flan-t5-base-arxiv-math-question-answering", "description": "---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: summarization\nwidget:\n  - text: What is the spectral isolation of bi-invariant metrics?\n    example_title: Question Answering\ntags:\n- arxiv\n---\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Citation](#citation)\n\n# TL;DR\n\nThis is a FLAN-T5 model trained on [ArtifactAI/arxiv-math-instruct-50k](https://huggingface.co/datasets/ArtifactAI/arxiv-math-instruct-50k). This model is for research purposes only and ***should not be used in production settings***. The output it highly unreliable.\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"ArtifactAI/flan-t5-base-arxiv-math-question-answering\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"ArtifactAI/flan-t5-base-arxiv-math-question-answering\")\n\ninput_text = \"What is the spectral isolation of bi-invariant metrics?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n### Running the model on a GPU\n\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"ArtifactAI/flan-t5-base-arxiv-math-question-answering\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"ArtifactAI/flan-t5-base-arxiv-math-question-answering\", device_map=\"auto\")\n\ninput_text = \"What is the spectral isolation of bi-invariant metrics?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n### Running the model in an HF pipeline\n\n#### FP16\n\n\n```python\n# load model and tokenizer from huggingface hub with pipeline\nqa = pipeline(\"summarization\", model=\"ArtifactAI/flan-t5-base-arxiv-math-question-answering\")\n\n\nquery = \"What is the spectral isolation of bi-invariant metrics?\"\nprint(f\"query: {query}\")\nres = qa(\"answer: \" + query)\n\nprint(f\"{res[0]['summary_text']}\")\n\n```\n\n\n# Training Details\n\n## Training Data\n\nThe model was trained on [ArtifactAI/arxiv-math-instruct-50k](https://huggingface.co/datasets/ArtifactAI/arxiv-math-instruct-50k), a dataset of question/answer pairs. Questions are generated using the t5-base model, while the answers are generated using the GPT-3.5-turbo model.\n\n# Citation\n\n```\n@misc{flan-t5-base-arxiv-math-question-answering,\n    title={flan-t5-base-arxiv-math-question-answering},\n    author={Matthew Kenney},\n    year={2023}\n}\n```", "size_bytes": "990408885", "downloads": 12}
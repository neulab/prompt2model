{"pretrained_model_name": "orzhan/rut5-base-detox-v2", "description": "---\nlanguage:\n- ru\ntags:\n- PyTorch\n- Transformers\n---\n# rut5-base-detox-v2\nModel was fine-tuned from sberbank-ai/ruT5-base on parallel detoxification corpus.\n* Task: `text2text generation`\n* Type: `encoder-decoder`\n* Tokenizer: `bpe`\n* Dict size: `32 101`\n* Num Parameters: `222 M`\t\n", "size_bytes": "891697151", "downloads": 4}
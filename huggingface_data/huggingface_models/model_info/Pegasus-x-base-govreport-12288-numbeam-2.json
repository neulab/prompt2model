{"pretrained_model_name": "UNIST-Eunchan/Pegasus-x-base-govreport-12288-numbeam-2", "description": "---\ntags:\n- generated_from_trainer\ndatasets:\n- govreport-summarization\nmodel-index:\n- name: Pegasus-x-base-govreport-12288-numbeam-2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# Pegasus-x-base-govreport-12288-numbeam-2\n\nThis model is a fine-tuned version of [google/pegasus-x-base](https://huggingface.co/google/pegasus-x-base) on the govreport-summarization dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6327\n\n'ROUGE': {'rouge1': 0.4843586873168665,\n  'rouge2': 0.20893237725290087,\n  'rougeL': 0.24808730732880646,\n  'rougeLsum': 0.248335395434983}\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 1\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 4\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 2.2439        | 0.03  | 120  | 2.1257          |\n| 2.1672        | 0.05  | 240  | 1.9610          |\n| 2.1165        | 0.08  | 360  | 1.9005          |\n| 2.1627        | 0.11  | 480  | 1.8735          |\n| 1.8851        | 0.14  | 600  | 1.8362          |\n| 1.9804        | 0.16  | 720  | 1.8153          |\n| 1.9568        | 0.19  | 840  | 1.8008          |\n| 1.8842        | 0.22  | 960  | 1.8016          |\n| 2.0134        | 0.25  | 1080 | 1.7772          |\n| 1.9086        | 0.27  | 1200 | 1.7640          |\n| 1.9177        | 0.3   | 1320 | 1.7553          |\n| 1.9547        | 0.33  | 1440 | 1.7377          |\n| 1.793         | 0.36  | 1560 | 1.7411          |\n| 1.8007        | 0.38  | 1680 | 1.7440          |\n| 1.9048        | 0.41  | 1800 | 1.7244          |\n| 1.9304        | 0.44  | 1920 | 1.7164          |\n| 1.881         | 0.47  | 2040 | 1.7169          |\n| 1.8999        | 0.49  | 2160 | 1.7107          |\n| 1.9304        | 0.52  | 2280 | 1.7086          |\n| 1.8168        | 0.55  | 2400 | 1.7031          |\n| 2.0034        | 0.58  | 2520 | 1.7011          |\n| 1.8952        | 0.6   | 2640 | 1.6996          |\n| 1.8706        | 0.63  | 2760 | 1.6947          |\n| 1.8435        | 0.66  | 2880 | 1.6919          |\n| 1.7279        | 0.69  | 3000 | 1.6909          |\n| 1.8097        | 0.71  | 3120 | 1.6789          |\n| 1.9044        | 0.74  | 3240 | 1.6797          |\n| 1.7076        | 0.77  | 3360 | 1.6813          |\n| 1.9164        | 0.79  | 3480 | 1.6734          |\n| 1.7003        | 0.82  | 3600 | 1.6724          |\n| 1.8885        | 0.85  | 3720 | 1.6726          |\n| 1.7595        | 0.88  | 3840 | 1.6704          |\n| 1.9149        | 0.9   | 3960 | 1.6660          |\n| 1.7987        | 0.93  | 4080 | 1.6616          |\n| 1.8056        | 0.96  | 4200 | 1.6612          |\n| 1.7138        | 0.99  | 4320 | 1.6714          |\n| 1.6865        | 1.01  | 4440 | 1.6697          |\n| 1.7406        | 1.04  | 4560 | 1.6592          |\n| 1.7533        | 1.07  | 4680 | 1.6624          |\n| 1.7599        | 1.1   | 4800 | 1.6620          |\n| 1.7448        | 1.12  | 4920 | 1.6558          |\n| 1.7208        | 1.15  | 5040 | 1.6574          |\n| 1.7782        | 1.18  | 5160 | 1.6505          |\n| 1.6579        | 1.21  | 5280 | 1.6559          |\n| 1.8094        | 1.23  | 5400 | 1.6526          |\n| 1.9198        | 1.26  | 5520 | 1.6450          |\n| 1.6689        | 1.29  | 5640 | 1.6471          |\n| 1.807         | 1.32  | 5760 | 1.6490          |\n| 1.9385        | 1.34  | 5880 | 1.6423          |\n| 1.6097        | 1.37  | 6000 | 1.6464          |\n| 1.6278        | 1.4   | 6120 | 1.6489          |\n| 1.7366        | 1.42  | 6240 | 1.6467          |\n| 1.6839        | 1.45  | 6360 | 1.6418          |\n| 1.8194        | 1.48  | 6480 | 1.6423          |\n| 1.6548        | 1.51  | 6600 | 1.6412          |\n| 1.7561        | 1.53  | 6720 | 1.6443          |\n| 1.8182        | 1.56  | 6840 | 1.6410          |\n| 1.7861        | 1.59  | 6960 | 1.6389          |\n| 1.6587        | 1.62  | 7080 | 1.6361          |\n| 1.7151        | 1.64  | 7200 | 1.6412          |\n| 1.7458        | 1.67  | 7320 | 1.6430          |\n| 1.677         | 1.7   | 7440 | 1.6354          |\n| 1.7061        | 1.73  | 7560 | 1.6340          |\n| 1.6471        | 1.75  | 7680 | 1.6364          |\n| 1.6342        | 1.78  | 7800 | 1.6339          |\n| 1.8216        | 1.81  | 7920 | 1.6362          |\n| 1.7511        | 1.84  | 8040 | 1.6361          |\n| 1.7565        | 1.86  | 8160 | 1.6317          |\n| 1.766         | 1.89  | 8280 | 1.6332          |\n| 1.6302        | 1.92  | 8400 | 1.6326          |\n| 1.7421        | 1.95  | 8520 | 1.6328          |\n| 1.7077        | 1.97  | 8640 | 1.6327          |\n\n\n### Framework versions\n\n- Transformers 4.30.2\n- Pytorch 2.0.1+cu117\n- Datasets 2.13.1\n- Tokenizers 0.13.3\n", "size_bytes": "1089301733", "downloads": 4}
{"pretrained_model_name": "khalidalt/m2m100_418M-finetuned-en-to-ar", "description": "---\nlicense: mit\ndatasets:\n- opus100\n- un_multi\nlanguage:\n- en\n- ar\n---\n M2M100 418M \n\nM2M100 is a multilingual encoder-decoder transformer model trained for Many-to-Many multilingual translation. The model, originally introduced by researchers at Facebook, demonstrates impressive performance in cross-lingual translation tasks. \nFor a better understanding of M2M100 you can look into the [paper](https://arxiv.org/abs/2010.11125) and the associated [repository](https://github.com/facebookresearch/fairseq/tree/main/examples/m2m_100).  \nTo further enhance the capabilities of M2M100, we conducted finetuning experiments on English-to-Arabic parallel text. The finetuning process involved training the model for 1000K steps using a batch size of 8.", "size_bytes": "1935792071", "downloads": 23}
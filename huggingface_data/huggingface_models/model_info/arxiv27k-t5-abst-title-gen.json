{"pretrained_model_name": "emre/arxiv27k-t5-abst-title-gen", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- summarization\nmetrics:\n- rouge\nmodel-index:\n- name: arxiv27k-t5-abst-title-gen/\n  results: []\n---\n\n# arxiv27k-t5-abst-title-gen/\n\nThis model is a fine-tuned version of mt5-small on the arxiv-abstract-title dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6002\n- Rouge1: 32.8\n- Rouge2: 21.9\n- Rougel: 34.8\n- \n\n## Model description\n\nModel has been trained with a colab-pro notebook in 4 hours.\n\n## Intended uses & limitations\n\nCan be used for generating journal titles from given abstracts\n\n### Training args\nmodel_args = T5Args()\nmodel_args.max_seq_length = 256\nmodel_args.train_batch_size = 8\nmodel_args.eval_batch_size = 8\nmodel_args.num_train_epochs = 6\nmodel_args.evaluate_during_training = False\nmodel_args.use_multiprocessing = False\nmodel_args.fp16 = False\nmodel_args.save_steps = 40000\nmodel_args.save_eval_checkpoints = False\nmodel_args.save_model_every_epoch = True\nmodel_args.output_dir = OUTPUT_DIR\nmodel_args.no_cache = True\nmodel_args.reprocess_input_data = True\nmodel_args.overwrite_output_dir = True\nmodel_args.num_return_sequences = 1\n\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n\n### Contact\ndetasar@gmail.com\nDavut Emre Ta\u015far", "size_bytes": "1200792197", "downloads": 9}
{"pretrained_model_name": "efederici/text2tags", "description": "---\nlanguage: \n  - it\ntags:\n- summarization\n- tags\n- Italian\ninference:\n  parameters:\n    do_sample: False\n    min_length: 0\nwidget:\n  - text: \"Nel 1924 la scrittrice Virginia Woolf affront\u00f2 nel saggio Mr Bennett e Mrs Brown il tema della costruzione e della struttura del romanzo, genere all\u2019epoca considerato in declino a causa dell\u2019incapacit\u00e0 degli autori e delle autrici di creare personaggi realistici. Woolf raccont\u00f2 di aver a lungo osservato, durante un viaggio in treno da Richmond a Waterloo, una signora di oltre 60 anni seduta davanti a lei, chiamata signora Brown. Ne rimase affascinata, per la capacit\u00e0 di quella figura di evocare storie possibili e fare da spunto per un romanzo: \u00abtutti i romanzi cominciano con una vecchia signora seduta in un angolo\u00bb. Immagini come quella della signora Brown, secondo Woolf, \u00abcostringono qualcuno a cominciare, quasi automaticamente, a scrivere un romanzo\u00bb. Nel saggio Woolf prov\u00f2 ad analizzare le tecniche narrative utilizzate da tre noti scrittori inglesi dell\u2019epoca \u2013 H. G. Wells, John Galsworthy e Arnold Bennett \u2013 per comprendere perch\u00e9 le convenzioni stilistiche dell\u2019Ottocento risultassero ormai inadatte alla descrizione dei \u00abcaratteri\u00bb umani degli anni Venti. In un lungo e commentato articolo del New Yorker, la critica letteraria e giornalista Parul Sehgal, a lungo caporedattrice dell\u2019inserto culturale del New York Times dedicato alle recensioni di libri, ha provato a compiere un esercizio simile a quello di Woolf, chiedendosi come gli autori e le autrici di oggi tratterebbero la signora Brown. E ha immaginato che probabilmente quella figura non eserciterebbe su di loro una curiosit\u00e0 e un fascino legati alla sua incompletezza e al suo aspetto misterioso, ma con ogni probabilit\u00e0 trasmetterebbe loro l\u2019indistinta e generica impressione di aver sub\u00ecto un trauma.\"\n    example_title: \"Virginia Woolf\"\n  - text: \"I lavori di ristrutturazione dell\u2019interno della cattedrale di Notre-Dame a Parigi, seguiti al grande incendio che nel 2019 bruci\u00f2 la guglia e buona parte del tetto, sono da settimane al centro di un acceso dibattito sui giornali francesi per via di alcune proposte di rinnovamento degli interni che hanno suscitato critiche e allarmi tra esperti e opinionisti conservatori. Il progetto ha ricevuto una prima approvazione dalla commissione nazionale competente, ma dovr\u00e0 ancora essere soggetto a varie revisioni e ratifiche che coinvolgeranno tecnici e politici locali e nazionali, fino al presidente Emmanuel Macron. Ma le modifiche previste al sistema di viabilit\u00e0 per i visitatori, all\u2019illuminazione, ai posti a sedere e alle opere d\u2019arte che si vorrebbero esporre hanno portato alcuni critici a parlare di \u00abparco a tema woke\u00bb e \u00abDisneyland del politicamente corretto\u00bb.\"\n    example_title: \"Notre-Dame\"\n---\n\n# text2tags\n\nThe model has been trained on a collection of 28k news articles with tags. Its purpose is to create tags suitable for the given article. We can use this model also for information-retrieval purposes (GenQ), fine-tuning sentence-transformers for asymmetric semantic search. \n\nIf you like this project, consider supporting it with a cup of coffee! \ud83e\udd16\u2728\ud83c\udf1e\n[![Buy me a coffee](https://badgen.net/badge/icon/Buy%20Me%20A%20Coffee?icon=buymeacoffee&label)](https://bmc.link/edoardofederici)\n\n<p align=\"center\">\n    <img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1a/Pieter_Bruegel_d._%C3%84._066.jpg\" width=\"600\"> </br>\n    Pieter Bruegel the Elder, The Fight Between Carnival and Lent, 1559\n</p>\n\n### Usage \n\nSample code with an article from IlPost:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"efederici/text2tags\")\ntokenizer = AutoTokenizer.from_pretrained(\"efederici/text2tags\")\n\narticle = '''\nDa bambino era preoccupato che al mondo non ci fosse pi\u00f9 nulla da scoprire. Ma i suoi stessi studi gli avrebbero dato torto: insieme a James Watson, nel 1953 Francis Crick struttur\u00f2 il primo modello di DNA, la lunga sequenza di codici che identifica ogni essere vivente, rendendolo unico e diverso da tutti gli altri. \nLa scoperta gli valse il Nobel per la Medicina. \u00c8 uscita in queste settimane per Codice la sua biografia, Francis Crick \u2014 Lo scopritore del DNA, scritta da Matt Ridley, che racconta vita e scienza dell'uomo che cap\u00ec perch\u00e9 siamo fatti cos\u00ec.\n'''\n\ndef tag(text: str):\n    \"\"\" Generates tags from given text \"\"\"\n    text = text.strip().replace('\\n', '')\n    text = 'summarize: ' + text\n    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\")\n\n    tags_ids = model.generate(tokenized_text,\n                                        num_beams=4,\n                                        no_repeat_ngram_size=2,\n                                        max_length=20,\n                                        early_stopping=True)\n\n    output = tokenizer.decode(tags_ids[0], skip_special_tokens=True)\n    return output.split(', ')\n\ntags = tag(article)\nprint(tags)\n```\n\n## Longer documents\n\nAssuming paragraphs are divided by: '\\n\\n'.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport itertools\nimport re \n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"efederici/text2tags\")\ntokenizer = AutoTokenizer.from_pretrained(\"efederici/text2tags\")\n\narticle = '''\nDa bambino era preoccupato che al mondo non ci fosse pi\u00f9 nulla da scoprire. Ma i suoi stessi studi gli avrebbero dato torto: insieme a James Watson, nel 1953 Francis Crick struttur\u00f2 il primo modello di DNA, la lunga sequenza di codici che identifica ogni essere vivente, rendendolo unico e diverso da tutti gli altri. \nLa scoperta gli valse il Nobel per la Medicina. \u00c8 uscita in queste settimane per Codice la sua biografia, Francis Crick \u2014 Lo scopritore del DNA, scritta da Matt Ridley, che racconta vita e scienza dell'uomo che cap\u00ec perch\u00e9 siamo fatti cos\u00ec.\n'''\n\ndef words(text):\n    input_str = text\n    output_str = re.sub('[^A-Za-z0-9]+', ' ', input_str)\n    return output_str.split()\n\ndef is_subset(text1, text2):\n    return all(tag in words(text1.lower()) for tag in text2.split())\n\ndef cleaning(text, tags):\n    return [tag for tag in tags if is_subset(text, tag)]\n    \ndef get_texts(text, max_len):\n    texts = list(filter(lambda x : x != '', text.split('\\n\\n')))\n    lengths = [len(tokenizer.encode(paragraph)) for paragraph in texts]\n    output = []\n    for i, par in enumerate(texts):\n        index = len(output)\n        if index > 0 and lengths[i] + len(tokenizer.encode(output[index-1])) <= max_len:\n            output[index-1] = \"\".join(output[index-1] + par)\n        else:\n            output.append(par)\n    return output\n        \ndef get_tags(text, generate_kwargs):\n    input_text = 'summarize: ' + text.strip().replace('\\n', ' ')\n    tokenized_text = tokenizer.encode(input_text, return_tensors=\"pt\")\n    with torch.no_grad():\n        tags_ids = model.generate(tokenized_text, **generate_kwargs)\n    \n    output = []\n    for tags in tags_ids:\n        cleaned = cleaning(\n            text, \n            list(set(tokenizer.decode(tags, skip_special_tokens=True).split(', ')))\n        )\n        output.append(cleaned)\n    \n    return list(set(itertools.chain(*output))) \n        \ndef tag(text, max_len, generate_kwargs):\n    texts = get_texts(text, max_len)\n    all_tags = [get_tags(text, generate_kwargs) for text in texts]\n    flatten_tags = itertools.chain(*all_tags)\n    return list(set(flatten_tags))\n\nparams = {\n    \"min_length\": 0,\n    \"max_length\": 30,\n    \"no_repeat_ngram_size\": 2,\n    \"num_beams\": 4,\n    \"early_stopping\": True,\n    \"num_return_sequences\": 4,\n}\ntags = tag(article, 512, params)\nprint(tags)\n```\n\n### Overview\n\n- Model: T5 ([it5-small](https://huggingface.co/gsarti/it5-small))\n- Language: Italian\n- Downstream-task: Summarization (for topic tagging)\n- Training data: Custom dataset\n- Code: See example\n- Infrastructure: 1x T4", "size_bytes": "307824645", "downloads": 99}
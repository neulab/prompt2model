{"pretrained_model_name": "farleyknight/patent-summarization-google-bigbird-pegasus-large-arxiv-2022-09-20", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- farleyknight/big_patent_5_percent\nmetrics:\n- rouge\nmodel-index:\n- name: patent-summarization-google-bigbird-pegasus-large-arxiv-2022-09-20\n  results:\n  - task:\n      name: Summarization\n      type: summarization\n    dataset:\n      name: farleyknight/big_patent_5_percent\n      type: farleyknight/big_patent_5_percent\n      config: all\n      split: train\n      args: all\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 37.3764\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# patent-summarization-google-bigbird-pegasus-large-arxiv-2022-09-20\n\nThis model is a fine-tuned version of [google/bigbird-pegasus-large-arxiv](https://huggingface.co/google/bigbird-pegasus-large-arxiv) on the farleyknight/big_patent_5_percent dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.2617\n- Rouge1: 37.3764\n- Rouge2: 13.2442\n- Rougel: 26.011\n- Rougelsum: 31.0145\n- Gen Len: 113.8789\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1.0\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len  |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:--------:|\n| 2.6121        | 0.08  | 5000  | 2.5652          | 35.0673 | 12.0073 | 24.5471 | 28.9315   | 119.9866 |\n| 2.5182        | 0.17  | 10000 | 2.4797          | 34.6909 | 11.6432 | 24.87   | 28.1543   | 119.2043 |\n| 2.5102        | 0.25  | 15000 | 2.4238          | 35.8574 | 12.2402 | 25.0712 | 29.5607   | 115.2890 |\n| 2.4292        | 0.33  | 20000 | 2.3869          | 36.0133 | 12.2453 | 25.4039 | 29.483    | 112.5920 |\n| 2.3678        | 0.41  | 25000 | 2.3594          | 35.238  | 11.6833 | 25.0449 | 28.3313   | 119.1739 |\n| 2.3511        | 0.5   | 30000 | 2.3326          | 36.7755 | 12.8394 | 25.7218 | 30.2594   | 110.5819 |\n| 2.3334        | 0.58  | 35000 | 2.3125          | 36.6317 | 12.7493 | 25.5388 | 30.094    | 115.5998 |\n| 2.3833        | 0.66  | 40000 | 2.2943          | 37.1219 | 13.1564 | 25.7571 | 30.8666   | 113.8222 |\n| 2.341         | 0.75  | 45000 | 2.2813          | 36.4962 | 12.6225 | 25.6904 | 29.9741   | 115.9845 |\n| 2.3179        | 0.83  | 50000 | 2.2725          | 37.3535 | 13.1596 | 25.7385 | 31.056    | 117.7754 |\n| 2.3164        | 0.91  | 55000 | 2.2654          | 36.9191 | 12.9316 | 25.7586 | 30.4691   | 116.1670 |\n| 2.3046        | 0.99  | 60000 | 2.2618          | 37.3992 | 13.2731 | 26.0327 | 31.0338   | 114.5195 |\n\n\n### Framework versions\n\n- Transformers 4.23.0.dev0\n- Pytorch 1.12.0\n- Datasets 2.4.0\n- Tokenizers 0.12.1\n", "size_bytes": "2308115505", "downloads": 6}
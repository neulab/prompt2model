{"pretrained_model_name": "castorini/afriteva_large", "description": "Hugging Face's logo\n---\nlanguage: \n- om\n- am\n- rw\n- rn\n- ha\n- ig\n- pcm\n- so\n- sw\n- ti\n- yo\n- multilingual\ntags:\n- T5\n\n---\n# afriteva_large\n\n## Model desription\n\nAfriTeVa large is a sequence to sequence model pretrained on 10 African languages\n\n## Languages\n\nAfaan Oromoo(orm), Amharic(amh), Gahuza(gah), Hausa(hau), Igbo(igb), Nigerian Pidgin(pcm), Somali(som), Swahili(swa), Tigrinya(tig), Yoruba(yor)\n\n### More information on the model, dataset:\n\n### The model\n\n- 745M parameters encoder-decoder architecture (T5-like)\n- 12 layers, 12 attention heads and 512 token sequence length\n\n### The dataset\n\n- Multilingual: 10 African languages listed above\n- 143 Million Tokens (1GB of text data)\n- Tokenizer Vocabulary Size: 70,000 tokens\n\n## Intended uses & limitations \n\n`afriteva_large` is pre-trained model and primarily aimed at being fine-tuned on multilingual sequence-to-sequence tasks. \n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriteva_large\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"castorini/afriteva_large\")\n\n>>> src_text = \"\u00d3 h\u00f9n \u1ecd\u0301 l\u00e1ti di ara wa b\u00ed?\"\n>>> tgt_text =  \"Would you like to be?\"\n\n>>> model_inputs = tokenizer(src_text, return_tensors=\"pt\")\n>>> with tokenizer.as_target_tokenizer():\n        labels = tokenizer(tgt_text, return_tensors=\"pt\").input_ids\n\n>>> model(**model_inputs, labels=labels) # forward pass\n```\n\n## Training Procedure\n\nFor information on training procedures, please refer to the AfriTeVa [paper](#) or [repository](https://github.com/castorini/afriteva)\n\n## BibTex entry and Citation info\n\ncoming soon ...\n\n\n\n", "size_bytes": "2983489712", "downloads": 9}
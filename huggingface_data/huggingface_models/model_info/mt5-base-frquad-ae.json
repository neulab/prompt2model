{"pretrained_model_name": "lmqg/mt5-base-frquad-ae", "description": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: fr\ndatasets:\n- lmqg/qg_frquad\npipeline_tag: text2text-generation\ntags:\n- answer extraction\nwidget:\n- text: \"Pourtant, la strophe spens\u00e9rienne, utilis\u00e9e cinq fois avant que ne commence le ch\u0153ur, constitue en soi un vecteur dont les r\u00e9p\u00e9titions structurelles, selon Ricks, rel\u00e8vent du pur lyrisme tout en constituant une menace potentielle. Apr\u00e8s les huit sages pentam\u00e8tres iambiques, l'alexandrin final <hl> permet une pause <hl>, \u00ab v\u00e9ritable illusion d'optique \u00bb qu'accentuent les nombreuses expressions archa\u00efsantes telles que did swoon, did seem, did go, did receive, did make, qui doublent le pr\u00e9t\u00e9rit en un temps compos\u00e9 et paraissent \u00e0 la fois \u00ab tr\u00e8s pr\u00e9cautionneuses et tr\u00e8s peu press\u00e9es \u00bb.\"\n  example_title: \"Answering Extraction Example 1\" \n- text: \"N\u00e9anmoins, une fois encore, l'arithm\u00e9tique modulaire est insuffisante pour venir \u00e0 bout du th\u00e9or\u00e8me. Dirichlet utilise de nombreuses techniques analytiques, comme les s\u00e9ries enti\u00e8res et l'analyse complexe. Le fruit de ces travaux donne naissance \u00e0 une nouvelle branche des math\u00e9matiques : la th\u00e9orie analytique des nombres. L'un des points cruciaux de cette th\u00e9orie provient de l'unique article de <hl> Bernhard Riemann <hl> en th\u00e9orie des nombres : Sur le nombre de nombres premiers inf\u00e9rieurs \u00e0 une taille donn\u00e9e. Il conjecture une localisation des racines de sa fonction \u03b6. La recherche de la position des racines, initi\u00e9e par Dirichlet, devient une pr\u00e9occupation centrale et reste l'une des conjectures pressenties comme les plus difficiles des math\u00e9matiques de notre \u00e9poque.\"\n  example_title: \"Answering Extraction Example 2\" \nmodel-index:\n- name: lmqg/mt5-base-frquad-ae\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_frquad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Answer Extraction)\n      type: bleu4_answer_extraction\n      value: 3.8\n    - name: ROUGE-L (Answer Extraction)\n      type: rouge_l_answer_extraction\n      value: 13.02\n    - name: METEOR (Answer Extraction)\n      type: meteor_answer_extraction\n      value: 14.28\n    - name: BERTScore (Answer Extraction)\n      type: bertscore_answer_extraction\n      value: 64.97\n    - name: MoverScore (Answer Extraction)\n      type: moverscore_answer_extraction\n      value: 50.67\n    - name: AnswerF1Score (Answer Extraction)\n      type: answer_f1_score__answer_extraction\n      value: 19.32\n    - name: AnswerExactMatch (Answer Extraction)\n      type: answer_exact_match_answer_extraction\n      value: 3.92\n---\n\n# Model Card of `lmqg/mt5-base-frquad-ae`\nThis model is fine-tuned version of [google/mt5-base](https://huggingface.co/google/mt5-base) for answer extraction on the [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) (dataset_name: default) via [`lmqg`](https://github.com/asahi417/lm-question-generation).\n\n\n### Overview\n- **Language model:** [google/mt5-base](https://huggingface.co/google/mt5-base)   \n- **Language:** fr  \n- **Training data:** [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) (default)\n- **Online Demo:** [https://autoqg.net/](https://autoqg.net/)\n- **Repository:** [https://github.com/asahi417/lm-question-generation](https://github.com/asahi417/lm-question-generation)\n- **Paper:** [https://arxiv.org/abs/2210.03992](https://arxiv.org/abs/2210.03992)\n\n### Usage\n- With [`lmqg`](https://github.com/asahi417/lm-question-generation#lmqg-language-model-for-question-generation-)\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"fr\", model=\"lmqg/mt5-base-frquad-ae\")\n\n# model prediction\nanswers = model.generate_a(\"Cr\u00e9ateur \u00bb (Maker), lui aussi au singulier, \u00ab le Supr\u00eame Berger \u00bb (The Great Shepherd) ; de l'autre, des r\u00e9miniscences de la th\u00e9ologie de l'Antiquit\u00e9 : le tonnerre, voix de Jupiter, \u00ab Et souvent ta voix gronde en un tonnerre terrifiant \u00bb, etc.\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"lmqg/mt5-base-frquad-ae\")\noutput = pipe(\"Pourtant, la strophe spens\u00e9rienne, utilis\u00e9e cinq fois avant que ne commence le ch\u0153ur, constitue en soi un vecteur dont les r\u00e9p\u00e9titions structurelles, selon Ricks, rel\u00e8vent du pur lyrisme tout en constituant une menace potentielle. Apr\u00e8s les huit sages pentam\u00e8tres iambiques, l'alexandrin final <hl> permet une pause <hl>, \u00ab v\u00e9ritable illusion d'optique \u00bb qu'accentuent les nombreuses expressions archa\u00efsantes telles que did swoon, did seem, did go, did receive, did make, qui doublent le pr\u00e9t\u00e9rit en un temps compos\u00e9 et paraissent \u00e0 la fois \u00ab tr\u00e8s pr\u00e9cautionneuses et tr\u00e8s peu press\u00e9es \u00bb.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Answer Extraction)***: [raw metric file](https://huggingface.co/lmqg/mt5-base-frquad-ae/raw/main/eval/metric.first.answer.paragraph_sentence.answer.lmqg_qg_frquad.default.json) \n\n|                  |   Score | Type    | Dataset                                                          |\n|:-----------------|--------:|:--------|:-----------------------------------------------------------------|\n| AnswerExactMatch |    3.92 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| AnswerF1Score    |   19.32 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| BERTScore        |   64.97 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| Bleu_1           |    7.64 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| Bleu_2           |    5.8  | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| Bleu_3           |    4.65 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| Bleu_4           |    3.8  | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| METEOR           |   14.28 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| MoverScore       |   50.67 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| ROUGE_L          |   13.02 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_frquad\n - dataset_name: default\n - input_types: ['paragraph_sentence']\n - output_types: ['answer']\n - prefix_types: None\n - model: google/mt5-base\n - max_length: 512\n - max_length_output: 32\n - epoch: 15\n - batch: 8\n - lr: 0.0001\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 8\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](https://huggingface.co/lmqg/mt5-base-frquad-ae/raw/main/trainer_config.json).\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "size_bytes": "2329632589", "downloads": 2}
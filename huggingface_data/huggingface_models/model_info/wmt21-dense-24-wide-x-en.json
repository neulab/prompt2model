{"pretrained_model_name": "facebook/wmt21-dense-24-wide-x-en", "description": "---\nlanguage: \n- multilingual\n- ha\n- is\n- ja\n- cs\n- ru\n- zh\n- de\n- en\nlicense: mit\ntags:\n- translation\n- wmt21\n---\n# WMT 21 X-En\nWMT 21 X-En is a 4.7B multilingual encoder-decoder (seq-to-seq) model trained for one-to-many multilingual translation.\nIt was introduced in this [paper](https://arxiv.org/abs/2108.03265) and first released in [this](https://github.com/pytorch/fairseq/tree/main/examples/wmt21) repository.\n\nThe model can directly translate text from 7 languages: Hausa (ha), Icelandic (is), Japanese (ja), Czech (cs), Russian (ru), Chinese (zh), German (de) to English. \n\nTo translate into a target language, the target language id is forced as the first generated token.\nTo force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.\n\n*Note: `M2M100Tokenizer` depends on `sentencepiece`, so make sure to install it before running the example.*\nTo install `sentencepiece` run `pip install sentencepiece`\n\nSince the model was trained with domain tags, you should prepend them to the input as well.\n* \"wmtdata newsdomain\": Use for sentences in the news domain\n* \"wmtdata otherdomain\": Use for sentences in all other domain\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/wmt21-dense-24-wide-x-en\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt21-dense-24-wide-x-en\")\n\n# translate German to English\ntokenizer.src_lang = \"de\"\ninputs = tokenizer(\"wmtdata newsdomain Ein Modell f\u00fcr viele Sprachen\", return_tensors=\"pt\")\ngenerated_tokens = model.generate(**inputs)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"A model for many languages\"\n\n# translate Icelandic to English\ntokenizer.src_lang = \"is\"\ninputs = tokenizer(\"wmtdata newsdomain Ein fyrirmynd fyrir m\u00f6rg tungum\u00e1l\", return_tensors=\"pt\")\ngenerated_tokens = model.generate(**inputs)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"One model for many languages\"\n```\n\nSee the [model hub](https://huggingface.co/models?filter=wmt21) to look for more fine-tuned versions.\n\n\n## Languages covered\nEnglish (en), Hausa (ha), Icelandic (is), Japanese (ja), Czech (cs), Russian (ru), Chinese (zh), German (de)\n\n\n## BibTeX entry and citation info\n```\n@inproceedings{tran2021facebook\n  title={Facebook AI\u2019s WMT21 News Translation Task Submission},\n  author={Chau Tran and Shruti Bhosale and James Cross and Philipp Koehn and Sergey Edunov and Angela Fan},\n  booktitle={Proc. of WMT},\n  year={2021},\n}\n```", "size_bytes": "18773661472", "downloads": 368}
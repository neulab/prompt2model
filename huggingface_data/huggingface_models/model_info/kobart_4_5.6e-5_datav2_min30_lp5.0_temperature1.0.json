{"pretrained_model_name": "nlp04/kobart_4_5.6e-5_datav2_min30_lp5.0_temperature1.0", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: kobart_4_5.6e-5_datav2_min30_lp5.0_temperature1.0\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# kobart_4_5.6e-5_datav2_min30_lp5.0_temperature1.0\n\nThis model is a fine-tuned version of [gogamza/kobart-base-v2](https://huggingface.co/gogamza/kobart-base-v2) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.9891\n- Rouge1: 35.4597\n- Rouge2: 12.0824\n- Rougel: 23.0161\n- Bleu1: 29.793\n- Bleu2: 16.882\n- Bleu3: 9.6468\n- Bleu4: 5.3654\n- Gen Len: 50.6014\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 4\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 5.0\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Bleu1   | Bleu2   | Bleu3   | Bleu4  | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:------:|:-------:|\n| 2.3968        | 0.47  | 5000  | 2.9096          | 32.7469 | 10.9679 | 21.4954 | 27.0594 | 15.1133 | 8.4503  | 4.564  | 48.5501 |\n| 2.2338        | 0.94  | 10000 | 2.8002          | 33.2148 | 11.5121 | 22.7066 | 26.4886 | 15.0125 | 8.5792  | 4.8523 | 41.1049 |\n| 1.9652        | 1.42  | 15000 | 2.7699          | 34.4269 | 11.8551 | 22.8478 | 28.2628 | 16.0909 | 9.0427  | 4.9254 | 46.9744 |\n| 2.001         | 1.89  | 20000 | 2.7201          | 34.157  | 11.8683 | 22.6775 | 28.3593 | 16.1361 | 9.221   | 4.8616 | 46.979  |\n| 1.6433        | 2.36  | 25000 | 2.7901          | 33.6354 | 11.5761 | 22.6878 | 27.6475 | 15.6571 | 8.8372  | 4.8672 | 43.9953 |\n| 1.6204        | 2.83  | 30000 | 2.7724          | 34.9611 | 12.1606 | 23.0246 | 29.1014 | 16.6689 | 9.3661  | 5.1916 | 48.8811 |\n| 1.2955        | 3.3   | 35000 | 2.8970          | 35.896  | 12.7037 | 23.3781 | 29.9701 | 17.3963 | 10.2978 | 5.9339 | 49.5921 |\n| 1.3501        | 3.78  | 40000 | 2.8854          | 35.2981 | 12.1133 | 23.1845 | 29.483  | 16.7795 | 9.4124  | 5.2042 | 48.5897 |\n| 1.0865        | 4.25  | 45000 | 2.9912          | 35.581  | 12.5145 | 23.2262 | 29.9364 | 17.2064 | 10.0427 | 5.62   | 48.31   |\n| 1.052         | 4.72  | 50000 | 2.9891          | 35.4597 | 12.0824 | 23.0161 | 29.793  | 16.882  | 9.6468  | 5.3654 | 50.6014 |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu117\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "495648413", "downloads": 2}
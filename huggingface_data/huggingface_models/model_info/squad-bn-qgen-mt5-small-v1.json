{"pretrained_model_name": "jannatul17/squad-bn-qgen-mt5-small-v1", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: final-squad-bn-qgen-mt5-small-all-metric-v2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# final-squad-bn-qgen-mt5-small-all-metric-v2\n\nThis model is a fine-tuned version of [google/mt5-small](https://huggingface.co/google/mt5-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6559\n- Rouge1 Precision: 31.143\n- Rouge1 Recall: 24.8687\n- Rouge1 Fmeasure: 26.7861\n- Rouge2 Precision: 12.1721\n- Rouge2 Recall: 9.3907\n- Rouge2 Fmeasure: 10.1945\n- Rougel Precision: 29.2741\n- Rougel Recall: 23.4105\n- Rougel Fmeasure: 25.196\n- Rougelsum Precision: 29.2488\n- Rougelsum Recall: 23.3873\n- Rougelsum Fmeasure: 25.1783\n- Bleu-1: 20.2844\n- Bleu-2: 11.7083\n- Bleu-3: 7.2251\n- Bleu-4: 4.6646\n- Meteor: 0.1144\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1 Precision | Rouge1 Recall | Rouge1 Fmeasure | Rouge2 Precision | Rouge2 Recall | Rouge2 Fmeasure | Rougel Precision | Rougel Recall | Rougel Fmeasure | Rougelsum Precision | Rougelsum Recall | Rougelsum Fmeasure | Bleu-1  | Bleu-2  | Bleu-3 | Bleu-4 | Meteor |\n|:-------------:|:-----:|:-----:|:---------------:|:----------------:|:-------------:|:---------------:|:----------------:|:-------------:|:---------------:|:----------------:|:-------------:|:---------------:|:-------------------:|:----------------:|:------------------:|:-------:|:-------:|:------:|:------:|:------:|\n| 0.9251        | 1.0   | 6769  | 0.7237          | 26.4973          | 20.6282       | 22.3983         | 9.3138           | 6.9928        | 7.6534          | 24.9538          | 19.4635       | 21.1113         | 24.9713             | 19.4608          | 21.119             | 17.5414 | 9.5172  | 5.6104 | 3.4646 | 0.097  |\n| 0.8214        | 2.0   | 13538 | 0.6804          | 29.524           | 23.4125       | 25.2574         | 11.2954          | 8.6345        | 9.3841          | 27.8173          | 22.1005       | 23.8164         | 27.7939             | 22.0878          | 23.801             | 19.2368 | 10.9056 | 6.6821 | 4.2702 | 0.1074 |\n| 0.7914        | 3.0   | 20307 | 0.6600          | 30.7136          | 24.5527       | 26.4259         | 11.8743          | 9.1634        | 9.9452          | 28.8725          | 23.1161       | 24.859          | 28.8566             | 23.1018          | 24.8457            | 19.9315 | 11.4473 | 7.0613 | 4.5701 | 0.1119 |\n| 0.7895        | 4.0   | 27076 | 0.6559          | 31.1568          | 24.8787       | 26.8004         | 12.1685          | 9.3879        | 10.1929         | 29.2804          | 23.3999       | 25.1925         | 29.2554             | 23.3891          | 25.1818            | 20.2844 | 11.7083 | 7.2251 | 4.6646 | 0.1144 |\n\n\n### Framework versions\n\n- Transformers 4.20.1\n- Pytorch 1.11.0\n- Datasets 2.1.0\n- Tokenizers 0.12.1\n", "size_bytes": "1200770885", "downloads": 2}
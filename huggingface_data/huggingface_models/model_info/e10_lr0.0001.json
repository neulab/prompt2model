{"pretrained_model_name": "GuiSales404/e10_lr0.0001", "description": "---\nlanguage: pt\nlicense: mit\ntags:\n- question-answering\n- bert\n- bert-base\n- pytorch\ndatasets:\n- brWaC\n- squad\n- squad_v1_pt\nmetrics:\n- squad\n---\n\n# Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1\n\n## Introduction\n\nThe model was trained on the dataset SQUAD v1.1 in portuguese from the [Deep Learning Brasil group](http://www.deeplearningbrasil.com.br/) on Google Colab. \n\nThe language model used is the [BERTimbau Base](https://huggingface.co/neuralmind/bert-base-portuguese-cased) (aka \"bert-base-portuguese-cased\") from [Neuralmind.ai](https://neuralmind.ai/): BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.\n\n## Informations on the method used\n\nAll the informations are in the blog post : [NLP | Modelo de Question Answering em qualquer idioma baseado no BERT base (estudo de caso em portugu\u00eas)](https://medium.com/@pierre_guillou/nlp-modelo-de-question-answering-em-qualquer-idioma-baseado-no-bert-base-estudo-de-caso-em-12093d385e78)\n\n## Notebooks in Google Colab & GitHub\n\n- Google Colab: [colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb](https://drive.google.com/file/d/1YkfxAjNkPzOr6hsHc7t7LTv3HYgUCWlX/view?usp=share_link)\n- GitHub: [colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb](https://github.com/GuiSales404/QA_system_pt-br)\n\n## Performance\n\nThe results obtained are the following:\n\n```\nf1 = 79.38\nexact match = 67.51\n```\n\n## How to use the model... with Pipeline\n\n```python\nimport transformers\nfrom transformers import pipeline\n\n# source: https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19\ncontext = r\"\"\"\nA pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, \numa doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). \nA doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, \nem 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano. \nAcredita-se que o v\u00edrus tenha uma origem zoon\u00f3tica, porque os primeiros casos confirmados \ntinham principalmente liga\u00e7\u00f5es ao Mercado Atacadista de Frutos do Mar de Huanan, que tamb\u00e9m vendia animais vivos. \nEm 11 de mar\u00e7o de 2020, a Organiza\u00e7\u00e3o Mundial da Sa\u00fade declarou o surto uma pandemia. At\u00e9 8 de fevereiro de 2021, \npelo menos 105 743 102 casos da doen\u00e7a foram confirmados em pelo menos 191 pa\u00edses e territ\u00f3rios, \ncom cerca de 2 308 943 mortes e 58 851 440 pessoas curadas.\n\"\"\"\n\nmodel_name = 'pierreguillou/bert-base-cased-squad-v1.1-portuguese'\nnlp = pipeline(\"question-answering\", model=model_name)\n\nquestion = \"Quando come\u00e7ou a pandemia de Covid-19 no mundo?\"\n\nresult = nlp(question=question, context=context)\n\nprint(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n\n# Answer: '1 de dezembro de 2019', score: 0.713, start: 328, end: 349\n```\n\n## How to use the model... with the Auto classes\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n  \ntokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\n```             \n\nOr just clone the model repo:\n\n```python\ngit lfs install\ngit clone https://huggingface.co/pierreguillou/bert-base-cased-squad-v1.1-portuguese\n  \n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\n  \nGIT_LFS_SKIP_SMUDGE=1\n```               \n\n## Limitations and bias\n\nThe training data used for this model come from Portuguese SQUAD. It could contain a lot of unfiltered content, which is far from neutral, and biases. We're working on ways to improve this by using computational grammars for text data augmentation. \n\n## Author\n\nPortuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1 was trained and evaluated by [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/) thanks to the Open Source code, platforms and advices of many organizations ([link to the list](https://medium.com/@pierre_guillou/nlp-modelo-de-question-answering-em-qualquer-idioma-baseado-no-bert-base-estudo-de-caso-em-12093d385e78#c572)). In particular: [Hugging Face](https://huggingface.co/), [Neuralmind.ai](https://neuralmind.ai/), [Deep Learning Brasil group](http://www.deeplearningbrasil.com.br/), [Google Colab](https://colab.research.google.com/) and [AI Lab](https://ailab.unb.br/).\n\n## Citation\nThis research is running using Pierre Guillou notebooks, all this job is available in [this](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-eu-treinei-um-modelo-t5-em-portugu%C3%AAs-na-tarefa-qa-no-google-colab-e8eb0dc38894) medium article. Thank you !\n\nIf you use our work, please cite:\n\n```bibtex\n@inproceedings{pierreguillou2021bertbasecasedsquadv11portuguese,\n  title={Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1},\n  author={Pierre Guillou},\n  year={2021}\n}\n```", "size_bytes": "891700799", "downloads": 13}
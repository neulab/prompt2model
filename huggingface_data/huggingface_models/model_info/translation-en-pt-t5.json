{"pretrained_model_name": "unicamp-dl/translation-en-pt-t5", "description": "---\n\nlanguage:\n\n- en\n\n- pt\n\ndatasets:\n\n- EMEA\n\n- ParaCrawl 99k\n\n- CAPES\n\n- Scielo\n\n- JRC-Acquis\n\n- Biomedical Domain Corpora\n\ntags:\n\n- translation\n\nmetrics:\n\n- bleu\n\n---\n\n\n# Introduction\n\nThis repository brings an implementation of T5 for translation in EN-PT tasks using a modest hardware setup. We propose some changes in tokenizator and post-processing that improves the result and used a Portuguese pretrained model for the translation. You can collect more informations in [our repository](https://github.com/unicamp-dl/Lite-T5-Translation). Also, check [our paper](https://aclanthology.org/2020.wmt-1.90.pdf)!\n\n# Usage\n\nJust follow \"Use in Transformers\" instructions. It is necessary to add a few words before to define the task to T5. \n\nYou can also create a pipeline for it. An example with the phrase \"I like to eat rice\" is:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n  \ntokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n\nenpt_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n\nenpt_pipeline(\"translate English to Portuguese: I like to eat rice.\")\n\n```\n\n# Citation\n\n```bibtex\n@inproceedings{lopes-etal-2020-lite,\n    title = \"Lite Training Strategies for {P}ortuguese-{E}nglish and {E}nglish-{P}ortuguese Translation\",\n    author = \"Lopes, Alexandre  and\n      Nogueira, Rodrigo  and\n      Lotufo, Roberto  and\n      Pedrini, Helio\",\n    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.wmt-1.90\",\n    pages = \"833--840\",\n}\n```", "size_bytes": "891689589", "downloads": 10173}
{"pretrained_model_name": "toloka/t5-large-for-text-aggregation", "description": "---\nlanguage:\n- en\ntags:\n- text aggregation\n- summarization\nlicense: apache-2.0\ndatasets:\n- toloka/CrowdSpeech\nmetrics:\n- wer\n---\n\n# T5 Large for Text Aggregation\n\n## Model description\n\nThis is a T5 Large fine-tuned for crowdsourced text aggregation tasks. The model takes multiple performers' responses and yields a single aggregated response. This approach was introduced for the first time during [VLDB 2021 Crowd Science Challenge](https://crowdscience.ai/challenges/vldb21) and originally implemented at the second-place competitor's [GitHub](https://github.com/A1exRey/VLDB2021_workshop_t5). The [paper](http://ceur-ws.org/Vol-2932/short2.pdf) describing this model was presented at the [2nd Crowd Science Workshop](https://crowdscience.ai/conference_events/vldb21).\n\n## How to use\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig\nmname = \"toloka/t5-large-for-text-aggregation\"\ntokenizer = AutoTokenizer.from_pretrained(mname)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(mname)\n\ninput = \"samplee text | sampl text | sample textt\"\ninput_ids = tokenizer.encode(input, return_tensors=\"pt\")\noutputs = model.generate(input_ids)\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(decoded)  # sample text\n```\n\n\n## Training data\n\nPretrained weights were taken from the [original](https://huggingface.co/t5-large) T5 Large model by Google. For more details on the T5 architecture and training procedure see https://arxiv.org/abs/1910.10683\n\nModel was fine-tuned on `train-clean`, `dev-clean` and `dev-other` parts of the [CrowdSpeech](https://huggingface.co/datasets/toloka/CrowdSpeech) dataset that was introduced in [our paper](https://openreview.net/forum?id=3_hgF1NAXU7&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2021%2FTrack%2FDatasets_and_Benchmarks%2FRound1%2FAuthors%23your-submissions).\n\n\n## Training procedure\n\nThe model was fine-tuned for eight epochs directly following the HuggingFace summarization training [example](https://github.com/huggingface/transformers/tree/master/examples/pytorch/summarization).\n\n## Eval results\n\nDataset    | Split      | WER\n-----------|------------|----------\nCrowdSpeech| test-clean | 4.99\nCrowdSpeech| test-other | 10.61\n\n\n### BibTeX entry and citation info\n```bibtex\n@inproceedings{Pletenev:21,\n  author    = {Pletenev, Sergey},\n  title     = {{Noisy Text Sequences Aggregation as a Summarization Subtask}},\n  year      = {2021},\n  booktitle = {Proceedings of the 2nd Crowd Science Workshop: Trust, Ethics, and Excellence in Crowdsourced Data Management at Scale},\n  pages     = {15--20},\n  address   = {Copenhagen, Denmark},\n  issn      = {1613-0073},\n  url       = {http://ceur-ws.org/Vol-2932/short2.pdf},\n  language  = {english},\n}\n```\n\n```bibtex\n@misc{pavlichenko2021vox,\n      title={Vox Populi, Vox DIY: Benchmark Dataset for Crowdsourced Audio Transcription}, \n      author={Nikita Pavlichenko and Ivan Stelmakh and Dmitry Ustalov},\n      year={2021},\n      eprint={2107.01091},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD}\n}\n```\n", "size_bytes": "2950782855", "downloads": 77}
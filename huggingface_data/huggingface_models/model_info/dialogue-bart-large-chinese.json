{"pretrained_model_name": "HIT-TMG/dialogue-bart-large-chinese", "description": "---\nlanguage: \n  - zh\nthumbnail: \"url to a thumbnail used in social sharing\"\ntags:\n- bart-large-chinese\ndatasets:\n- lccc\n- kd_conv\n---\n\n# dialogue-bart-large-chinese\nThis is a seq2seq model pre-trained on several Chinese dialogue datasets, from bart-large-chinese. It's better to fine-tune it on downstream tasks for better performance.\n\n\n# Spaces\nNow you can experience our model on HuggingFace Spaces [HIT-TMG/dialogue-bart-large-chinese](https://huggingface.co/spaces/HIT-TMG/dialogue-bart-large-chinese) .\n\n\n# Datasets\nWe utilize 4 Chinese dialogue datasets from [LUGE](https://www.luge.ai/#/) .\n\n|                              |            |                       |\n| ----                         | ----       | ----                  |\n|                              | Count      | Domain                |\n| Chinese Persona Chat (CPC)   | 23,000     | Open                  | \n| LCCC                         | 11,987,759 | Open                  |\n| Emotional STC (ESTC)         | 899,207    | Open                  |\n| KdConv                       | 3,000      | Movie, Music, Travel  |\n|                              |            |                       |\n\n\n# Data format\nInput: `[CLS] \u5bf9\u8bdd\u5386\u53f2\uff1a<history> [SEP] \u77e5\u8bc6\uff1a<knowledge> [SEP]`\n\nOutput: `[CLS] <response> [SEP]`\n\n\n# Example\n```python\nfrom transformers import BertTokenizer, BartForConditionalGeneration\n\n# Note that tokenizer is an object of BertTokenizer, instead of BartTokenizer\ntokenizer = BertTokenizer.from_pretrained(\"HIT-TMG/dialogue-bart-large-chinese\")\nmodel = BartForConditionalGeneration.from_pretrained(\"HIT-TMG/dialogue-bart-large-chinese\")\n\n# an example from CPC dev data\nhistory = [\"\u53ef\u4ee5 \u8ba4\u8bc6 \u4e00\u4e0b \u5417 \uff1f\", \"\u5f53\u7136 \u53ef\u4ee5 \u5566 \uff0c \u4f60\u597d \u3002\", \"\u563f\u563f \u4f60\u597d \uff0c \u8bf7\u95ee \u4f60 \u6700\u8fd1 \u5728 \u5fd9 \u4ec0\u4e48 \u5462 \uff1f\", \"\u6211 \u6700\u8fd1 \u517b \u4e86 \u4e00\u53ea \u72d7\u72d7 \uff0c \u6211 \u5728 \u8bad\u7ec3 \u5b83 \u5462 \u3002\"]\nhistory_str = \"\u5bf9\u8bdd\u5386\u53f2\uff1a\" + tokenizer.sep_token.join(history)\ninput_ids = tokenizer(history_str, return_tensors='pt').input_ids\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n ```\n \n \n # Contact\n If you encounter any issue, feel free to contact us via the email: <u>yanshekwoo@foxmail.com</u>", "size_bytes": "1501874625", "downloads": 300}
{"pretrained_model_name": "domenicrosati/QA2D-t5-base", "description": "---\nlicense: apache-2.0\ntags:\n- text2text-generation\n- generated_from_trainer\nmetrics:\n- rouge\n- bleu\ndatasets:\n- domenicrosati/QA2D\nmodel-index:\n- name: QA2D-t5-base\n  results:\n  - task:\n      name: Question to Declarative Sentence\n      type: text2text-generation\n    dataset:\n      name: domenicrosati/QA2D\n      type: domenicrosati/QA2D\n      args: plain_text\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 90.1064\n    - name: Rouge2\n      type: rouge\n      value: 82.378\n    - name: Rougel\n      type: rouge\n      value: 85.7963\n    - name: Rougelsum\n      type: rouge\n      value: 85.8004\n    - name: Bleu\n      type: bleu\n      value: 72.7328\nwidget:\n- text: \"where in the world is carmen sandiego. she is in abruzzo\"\n  example_title: \"Where is Carmen Sandiego?\"\n- text: \"which province is halifax in. nova scotia\"\n  example_title: \"A Halifact\"\n---\n\n\n# QA2D-t5-base\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on [QA2D](https://huggingface.co/datasets/domenicrosati/QA2D).\nIt achieves the following results on the evaluation set:\n- Loss: 0.2563\n- Rouge1: 90.1064\n- Rouge2: 82.378\n- Rougel: 85.7963\n- Rougelsum: 85.8004\n- Bleu: 72.7328\n\nSee: [https://wandb.ai/domenicrosati/huggingface/runs/nqf7gsws](https://wandb.ai/domenicrosati/huggingface/runs/nqf7gsws) for training and eval stats and [https://github.com/domenicrosati/qa2d-models](https://github.com/domenicrosati/qa2d-models) for the code!\n## Model description\n\nA t5-model model to convert questions, answer pairs into statements.\n\nDue to the way it's been trained the input should be all lower case and punctuation removed.\nUse with `. ` as the seperator between question and answer.\n> \"where in the world is carmen. abruzzo\"\n> Output: \"carmen is in abruzzo\"\n\nThought punctation and upper case works.\n\n```\nfrom transformers import AutoTokenizer,  AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained('domenicrosati/QA2D-t5-base')\nmodel = AutoModelForSeq2SeqLM.from_pretrained('domenicrosati/QA2D-t5-base')\n\nquestion = \"where in the world is carmen sandiego\"\nanswer = \"she is in abruzzo\"\nSEP = \". \"\n\nprompt = f'{question}{SEP}{answer}'\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids\noutput_ids = model.generate(input_ids)\nresponses = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n# ['carmen sandiego is in abruzzo']\n```More information needed\n\n## Intended uses & limitations\n\nTo convert questions, answer pairs into statements.\n\n## Training and evaluation data\n\nUses [QA2D](https://huggingface.co/datasets/domenicrosati/QA2D).\n\nSee [https://github.com/domenicrosati/qa2d-models](https://github.com/domenicrosati/qa2d-models)\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 12\n- eval_batch_size: 12\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Bleu    |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 0.2304        | 1.0   | 5060  | 0.2512          | 90.044  | 82.2922 | 85.8021 | 85.8056   | 72.6252 |\n| 0.1746        | 2.0   | 10120 | 0.2525          | 90.097  | 82.3468 | 85.8191 | 85.8197   | 72.7480 |\n| 0.1512        | 3.0   | 15180 | 0.2563          | 90.1064 | 82.378  | 85.7963 | 85.8004   | 72.7328 |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.11.0a0+17540c5\n- Datasets 2.1.0\n- Tokenizers 0.12.1\n", "size_bytes": "891700799", "downloads": 447}
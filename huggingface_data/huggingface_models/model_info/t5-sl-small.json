{"pretrained_model_name": "cjvt/t5-sl-small", "description": "---\nlanguage:\n- sl\n\nlicense: cc-by-sa-4.0\n---\n\n# t5-sl-small\nt5-sl-small model is a Slovene T5 model. It has 8 encoder and 8 decoder layers, in total about 60 million parameters.\nIt was trained for 5 epochs on the following corpora:\n\n## Corpora\nThe following corpora were used for training the model:\n* Gigafida 2.0\n* Kas 1.0\n* Janes 1.0 (only Janes-news, Janes-forum, Janes-blog, Janes-wiki subcorpora)\n* Slovenian parliamentary corpus siParl 2.0\n* slWaC\n\n## Evaluation\nThe model is described in detail and evaluated in our paper [\"*Sequence to sequence pretraining for a less-resourced Slovenian language*\"](https://arxiv.org/abs/2207.13988)\n\n## Changelog\n2022-07-21: updated with v2 of the model, the old one is still accesible at [cjvt/legacy-t5-sl-small](https://huggingface.co/cjvt/legacy-t5-sl-small).\n2022-09-21: added fast tokenizer (Huggingface's TokenizerFast class, the tokenization remains the same)", "size_bytes": "307402757", "downloads": 198}
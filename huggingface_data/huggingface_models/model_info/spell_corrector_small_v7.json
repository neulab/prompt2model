{"pretrained_model_name": "Buseak/spell_corrector_small_v7", "description": "---\nlicense: apache-2.0\nbase_model: google/mt5-small\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: spell_corrector_small_v7\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# spell_corrector_small_v7\n\nThis model is a fine-tuned version of [google/mt5-small](https://huggingface.co/google/mt5-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5549\n- Bleu: 34.7876\n- Gen Len: 15.7815\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 20\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 14.2184       | 1.0   | 976   | 1.6501          | 16.2132 | 13.5571 |\n| 2.8018        | 2.0   | 1952  | 1.2055          | 23.1195 | 15.9748 |\n| 2.0238        | 3.0   | 2928  | 0.9646          | 26.7454 | 15.9865 |\n| 1.6928        | 4.0   | 3904  | 0.8372          | 28.6482 | 15.9601 |\n| 1.4888        | 5.0   | 4880  | 0.7906          | 29.6306 | 15.9221 |\n| 1.3855        | 6.0   | 5856  | 0.7393          | 30.3841 | 15.9006 |\n| 1.2999        | 7.0   | 6832  | 0.7029          | 31.2225 | 15.8612 |\n| 1.2379        | 8.0   | 7808  | 0.6794          | 31.6015 | 15.8666 |\n| 1.1709        | 9.0   | 8784  | 0.6572          | 32.2153 | 15.8512 |\n| 1.1433        | 10.0  | 9760  | 0.6303          | 32.7529 | 15.8288 |\n| 1.1248        | 11.0  | 10736 | 0.6184          | 33.144  | 15.8244 |\n| 1.0703        | 12.0  | 11712 | 0.6072          | 33.4743 | 15.8121 |\n| 1.0547        | 13.0  | 12688 | 0.5937          | 33.7492 | 15.8139 |\n| 1.0275        | 14.0  | 13664 | 0.5779          | 34.1454 | 15.7952 |\n| 1.0122        | 15.0  | 14640 | 0.5727          | 34.2908 | 15.7907 |\n| 1.0071        | 16.0  | 15616 | 0.5662          | 34.4457 | 15.7874 |\n| 1.0017        | 17.0  | 16592 | 0.5609          | 34.6225 | 15.7847 |\n| 0.9879        | 18.0  | 17568 | 0.5575          | 34.6937 | 15.7832 |\n| 0.9814        | 19.0  | 18544 | 0.5554          | 34.7827 | 15.7816 |\n| 0.9793        | 20.0  | 19520 | 0.5549          | 34.7876 | 15.7815 |\n\n\n### Framework versions\n\n- Transformers 4.31.0\n- Pytorch 2.0.1+cu118\n- Datasets 2.14.2\n- Tokenizers 0.13.3\n", "size_bytes": "1200772613", "downloads": 2}
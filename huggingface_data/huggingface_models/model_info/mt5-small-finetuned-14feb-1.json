{"pretrained_model_name": "mqy/mt5-small-finetuned-14feb-1", "description": "---\nlicense: apache-2.0\ntags:\n- summarization\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: mt5-small-finetuned-14feb-1\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mt5-small-finetuned-14feb-1\n\nThis model is a fine-tuned version of [google/mt5-small](https://huggingface.co/google/mt5-small) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.4516\n- Rouge1: 20.33\n- Rouge2: 6.2\n- Rougel: 19.9\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.000275\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 20\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1 | Rouge2 | Rougel |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|\n| 4.0401        | 1.0   | 388  | 2.5481          | 16.31  | 4.7    | 16.1   |\n| 2.9776        | 2.0   | 776  | 2.4442          | 17.25  | 4.93   | 16.93  |\n| 2.7362        | 3.0   | 1164 | 2.4181          | 19.73  | 5.74   | 19.21  |\n| 2.5767        | 4.0   | 1552 | 2.4071          | 19.37  | 5.62   | 18.89  |\n| 2.4466        | 5.0   | 1940 | 2.3560          | 18.98  | 5.94   | 18.55  |\n| 2.3402        | 6.0   | 2328 | 2.3923          | 20.45  | 5.5    | 20.03  |\n| 2.2385        | 7.0   | 2716 | 2.3639          | 20.03  | 5.96   | 19.76  |\n| 2.1663        | 8.0   | 3104 | 2.3431          | 19.17  | 5.34   | 18.84  |\n| 2.0849        | 9.0   | 3492 | 2.4008          | 19.97  | 5.58   | 19.67  |\n| 2.0203        | 10.0  | 3880 | 2.3948          | 19.67  | 5.75   | 19.26  |\n| 1.9653        | 11.0  | 4268 | 2.3915          | 20.06  | 6.07   | 19.61  |\n| 1.9067        | 12.0  | 4656 | 2.4025          | 20.83  | 6.46   | 20.41  |\n| 1.8592        | 13.0  | 5044 | 2.4194          | 19.97  | 6.4    | 19.69  |\n| 1.8158        | 14.0  | 5432 | 2.4156          | 19.87  | 6.16   | 19.38  |\n| 1.7679        | 15.0  | 5820 | 2.4053          | 19.9   | 5.99   | 19.52  |\n| 1.748         | 16.0  | 6208 | 2.4156          | 19.68  | 5.81   | 19.28  |\n| 1.7198        | 17.0  | 6596 | 2.4306          | 20.0   | 6.26   | 19.63  |\n| 1.6959        | 18.0  | 6984 | 2.4499          | 19.1   | 6.19   | 18.82  |\n| 1.6769        | 19.0  | 7372 | 2.4536          | 20.62  | 6.3    | 20.15  |\n| 1.6682        | 20.0  | 7760 | 2.4516          | 20.33  | 6.2    | 19.9   |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.13.1+cu116\n- Datasets 2.9.0\n- Tokenizers 0.13.2\n", "size_bytes": "1200772485", "downloads": 4}
{"pretrained_model_name": "Gorilla115/t5-shakespearify-lite", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: t5-shakespearify-lite\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-shakespearify-lite\n\nThis model was trained from the t5 checkpoint on a custom dataset from [Shakescleare](https://www.litcharts.com/shakescleare/shakespeare-translations). This is a website shakespeare's works have been translated to modern english. This model idealizes style transforms as a translation process as we use the original english as a final translation. The dataset is available on [Kaggle](https://www.kaggle.com/datasets/garnavaurha/shakespearify).\n\n## Model description\n\nThe model was trained for 5 epochs with a subset of the dataset. The subset was only about 10k examples long out of the over 50k examples in the raw dataset.\n\n## Intended uses & limitations\n\nThis is a novelty project intended for playing around with. However, it has its limitations since it is translating english to english with some minor tweaks. These tweaks maybe changing sentence structure or minor word substitution. It works best unsurprisingly on story based excerpts like below.\n```\ntranslate: Why have you come to Mr. Smith with this crap?\n```\n \n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Framework versions\n\n- Transformers 4.21.1\n- Pytorch 1.12.0+cu113\n- Tokenizers 0.12.1\n", "size_bytes": "891697151", "downloads": 33}
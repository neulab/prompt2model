{"pretrained_model_name": "cointegrated/rut5-small-chitchat", "description": "---\nlanguage: \"ru\"\ntags:\n- dialogue\n- russian\nlicense: mit\n---\n\nThis is a version of the [cointegrated/rut5-small](https://huggingface.co/cointegrated/rut5-small) model fine-tuned on some Russian dialogue data. It is not very smart and creative, but it is small and fast, and can serve as a fallback response generator for some chatbot or can be fine-tuned to imitate the style of someone.\n\nThe input of the model is the previous dialogue utterances separated by `'\\n\\n'`, and the output is the next utterance. \n\nThe model can be used as follows:\n```\n# !pip install transformers sentencepiece\nimport torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\ntokenizer = T5Tokenizer.from_pretrained(\"cointegrated/rut5-small-chitchat\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"cointegrated/rut5-small-chitchat\")\n\ntext = '\u041f\u0440\u0438\u0432\u0435\u0442! \u0420\u0430\u0441\u0441\u043a\u0430\u0436\u0438, \u043a\u0430\u043a \u0442\u0432\u043e\u0438 \u0434\u0435\u043b\u0430?'\ninputs = tokenizer(text, return_tensors='pt')\nwith torch.no_grad():\n    hypotheses = model.generate(\n        **inputs, \n        do_sample=True, top_p=0.5, num_return_sequences=3, \n        repetition_penalty=2.5,\n        max_length=32,\n    )\nfor h in hypotheses:\n    print(tokenizer.decode(h, skip_special_tokens=True))\n# \u041a\u0430\u043a \u043e\u0431\u044b\u0447\u043d\u043e.\n# \u0421\u0435\u0439\u0447\u0430\u0441 - \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435.\n# \u0425\u043e\u0440\u043e\u0448\u043e.\n# Wall time: 363 ms \n```\n", "size_bytes": "258663045", "downloads": 130}
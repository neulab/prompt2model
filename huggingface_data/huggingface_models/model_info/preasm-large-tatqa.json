{"pretrained_model_name": "StonyBrookNLP/preasm-large-tatqa", "description": "---\ntags:\n- question-answering, multi-step-reasoning, multi-hop-reasoning\nthumbnail: https://raw.githubusercontent.com/StonyBrookNLP/teabreac/main/teabreac_icon.png\nlicense: cc-by-4.0\n---\n\n# What's this?\n\nThis is one of the models reported in the paper: [\"Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts\".](https://arxiv.org/abs/2205.12496).\n\nThis paper proposes a procedure to synthetically generate a QA dataset, TeaBReaC, for pretraining language models for robust multi-step reasoning. Pretraining plain LMs like Bart, T5 and numerate LMs like NT5, PReasM, POET on TeaBReaC leads to improvemed downstream performance on several multi-step QA datasets. Please checkout out the paper for the details.\n\nWe release the following models:\n\n- **A:** Base Models finetuned on target datasets: `{base_model}-{target_dataset}`\n- **B:** Base models pretrained on TeaBReaC: `teabreac-{base_model}`\n- **C:** Base models pretrained on TeaBReaC and then finetuned on target datasets: `teabreac-{base_model}-{target_dataset}`\n\nThe `base_model` above can be from: `bart-large`, `t5-large`, `t5-3b`, `nt5-small`, `preasm-large`.\nThe `target_dataset` above can be from: `drop`, `tatqa`, `iirc-gold`, `iirc-retrieved`, `numglue`.\n\nThe **A** models are only released for completeness / reproducibility. In your end application you probably just want to use either **B** or **C**.\n\n# How to use it?\n\nPlease checkout the details in our [github repository](https://github.com/stonybrooknlp/teabreac), but in a nutshell:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom digit_tokenization import enable_digit_tokenization # digit_tokenization.py from https://github.com/stonybrooknlp/teabreac\n\nmodel_name = \"StonyBrookNLP/preasm-large-tatqa\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False) # Fast doesn't work with digit tokenization\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nenable_digit_tokenization(tokenizer)\ninput_texts = [\n    \"Who scored the first touchdown of the game?\\n\" +\n    \"... Oakland would get the early lead in the first quarter as quarterback JaMarcus Russell completed a 20-yard touchdown pass to rookie wide receiver Chaz Schilens...\"\n    # Note: some models have slightly different qn/ctxt format. See the github repo.\n]\ninput_ids = tokenizer(\n    input_texts, return_tensors=\"pt\",\n    truncation=True, max_length=800,\n    add_special_tokens=True, padding=True,\n)[\"input_ids\"]\ngenerated_ids = model.generate(input_ids, min_length=1,  max_length=50)\ngenerated_predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\ngenerated_predictions = [\n    tokenizer.fix_decoded_text(generated_prediction) for generated_prediction in generated_predictions\n]\n# => [\"Chaz Schilens\"]\n```", "size_bytes": "3082745477", "downloads": 2}
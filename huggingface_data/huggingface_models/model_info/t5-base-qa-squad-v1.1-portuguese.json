{"pretrained_model_name": "pierreguillou/t5-base-qa-squad-v1.1-portuguese", "description": "---\nlanguage: \n- pt\ntags:\n- text2text-generation\n- t5\n- pytorch\n- qa\ndatasets:\n- squad\n- squad_v1_pt\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\n- squad\nmodel-index:\n- name: checkpoints\n  results:\n  - task:\n      name: text2text-generation\n      type: text2text-generation\n    dataset:\n      name: squad\n      type: squad\n    metrics:\n    - name: f1\n      type: f1\n      value: 79.3\n    - name: exact-match\n      type: exact-match\n      value: 67.3983\nwidget:\n- text: \"question: Quando come\u00e7ou a pandemia de Covid-19 no mundo? context: A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.\"\n- text: \"question: Onde foi descoberta a Covid-19? context: A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.\" \n---\n\n# T5 base finetuned for Question Answering (QA) on SQUaD v1.1 Portuguese\n\n![Exemple of what can do with a T5 model (for example: Question Answering finetuned on SQUAD v1.1 in Portuguese)](https://miro.medium.com/max/2000/1*zp9niaQzWNo8Pipd8zvL1w.png)\n\n## Introduction\n\n**t5-base-qa-squad-v1.1-portuguese** is a QA model (Question Answering) in Portuguese that was finetuned on 27/01/2022 in Google Colab from the model [unicamp-dl/ptt5-base-portuguese-vocab](https://huggingface.co/unicamp-dl/ptt5-base-portuguese-vocab) of Neuralmind on the dataset SQUAD v1.1 in portuguese from the [Deep Learning Brasil group](http://www.deeplearningbrasil.com.br/) by using a Test2Text-Generation objective.\n\nDue to the small size of T5 base and finetuning dataset, the model overfitted before to reach the end of training. Here are the overall final metrics on the validation dataset:\n  - **f1**: 79.3\n  - **exact_match**: 67.3983\n \nCheck our other QA models in Portuguese finetuned on SQUAD v1.1:\n- [Portuguese BERT base cased QA](https://huggingface.co/pierreguillou/bert-base-cased-squad-v1.1-portuguese)\n- [Portuguese BERT large cased QA](https://huggingface.co/pierreguillou/bert-large-cased-squad-v1.1-portuguese)\n- [Portuguese ByT5 small QA](https://huggingface.co/pierreguillou/byt5-small-qa-squad-v1.1-portuguese)\n  \n## Blog post\n\n[NLP nas empresas | Como eu treinei um modelo T5 em portugu\u00eas na tarefa QA no Google Colab](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-eu-treinei-um-modelo-t5-em-portugu%C3%AAs-na-tarefa-qa-no-google-colab-e8eb0dc38894) (27/01/2022)\n  \n## Widget & App\n\nYou can test this model into the widget of this page.\n\nUse as well the [QA App | T5 base pt](https://huggingface.co/spaces/pierreguillou/question-answering-portuguese-t5-base) that allows using the model T5 base finetuned on the QA task with the SQuAD v1.1 pt dataset.\n\n## Using the model for inference in production\n````\n# install pytorch: check https://pytorch.org/\n# !pip install transformers \nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# model & tokenizer\nmodel_name = \"pierreguillou/t5-base-qa-squad-v1.1-portuguese\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# parameters\nmax_target_length=32\nnum_beams=1\nearly_stopping=True\n\ninput_text  = 'question: Quando foi descoberta a Covid-19? context: A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'\nlabel = '1 de dezembro de 2019'\n\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(inputs[\"input_ids\"],\n                             max_length=max_target_length, \n                             num_beams=num_beams, \n                             early_stopping=early_stopping\n                            )\npred = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n             \nprint('true answer |', label)\nprint('pred        |', pred)\n````\nYou can use pipeline, too. However, it seems to have an issue regarding to the max_length of the input sequence.\n````\n!pip install transformers\nimport transformers\nfrom transformers import pipeline\n\n# model\nmodel_name = \"pierreguillou/t5-base-qa-squad-v1.1-portuguese\"\n\n# parameters\nmax_target_length=32\nnum_beams=1\nearly_stopping=True\nclean_up_tokenization_spaces=True\n\ninput_text  = 'question: Quando foi descoberta a Covid-19? context: A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'\nlabel = '1 de dezembro de 2019'\n    \ntext2text = pipeline(\n                    \"text2text-generation\",\n                     model=model_name,\n                     max_length=max_target_length, \n                     num_beams=num_beams, \n                     early_stopping=early_stopping,\n                     clean_up_tokenization_spaces=clean_up_tokenization_spaces\n                    )\n\npred = text2text(input_text)\n\nprint('true answer |', label)\nprint('pred        |', pred)\n````\n## Training procedure\n\n### Notebook\n\nThe notebook of finetuning ([HuggingFace_Notebook_t5-base-portuguese-vocab_question_answering_QA_squad_v11_pt.ipynb](https://github.com/piegu/language-models/blob/master/HuggingFace_Notebook_t5_base_portuguese_vocab_question_answering_QA_squad_v11_pt.ipynb)) is in github.\n\n### Hyperparameters\n````\n# do training and evaluation\ndo_train = True\ndo_eval= True\n\n# batch\nbatch_size = 4\ngradient_accumulation_steps = 3\nper_device_train_batch_size = batch_size\nper_device_eval_batch_size = per_device_train_batch_size*16\n\n# LR, wd, epochs\nlearning_rate = 1e-4\nweight_decay = 0.01\nnum_train_epochs = 10\nfp16 = True\n\n# logs\nlogging_strategy = \"steps\"\nlogging_first_step = True \nlogging_steps = 3000     # if logging_strategy = \"steps\"\neval_steps = logging_steps \n\n# checkpoints\nevaluation_strategy = logging_strategy\nsave_strategy = logging_strategy\nsave_steps = logging_steps\nsave_total_limit = 3\n\n# best model\nload_best_model_at_end = True\nmetric_for_best_model = \"f1\" #\"loss\"\nif metric_for_best_model == \"loss\":\n  greater_is_better = False\nelse:\n  greater_is_better = True  \n\n# evaluation\nnum_beams = 1\n````\n\n### Training results\n\n````\nNum examples = 87510\nNum Epochs = 10\nInstantaneous batch size per device = 4\nTotal train batch size (w. parallel, distributed & accumulation) = 12\nGradient Accumulation steps = 3\nTotal optimization steps = 72920\n \nStep \tTraining Loss  Exact Match\tF1\n3000 \t0.776100\t     61.807001  \t75.114517\n6000 \t0.545900\t     65.260170  \t77.468930\n9000 \t0.460500\t     66.556291  \t78.491938\n12000\t0.393400\t     66.821192  \t78.745397\n15000\t0.379800  \t   66.603595  \t78.815515\n18000\t0.298100  \t   67.578051  \t79.287899\n21000\t0.303100  \t   66.991485  \t78.979669\n24000\t0.251600  \t   67.275307  \t78.929923\n\n27000\t0.237500\t     66.972564  \t79.333612\n\n30000\t0.220500 \t    66.915799  \t79.236574\n33000\t0.182600\t     67.029328  \t78.964212\n36000\t0.190600 \t    66.982025  \t79.086125\n\n````", "size_bytes": "891730879", "downloads": 165}
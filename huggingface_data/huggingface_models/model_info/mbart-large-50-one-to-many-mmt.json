{"pretrained_model_name": "facebook/mbart-large-50-one-to-many-mmt", "description": "---\nlanguage: \n- multilingual\n- ar \n- cs\n- de\n- en\n- es\n- et\n- fi\n- fr\n- gu\n- hi\n- it\n- ja\n- kk\n- ko\n- lt\n- lv\n- my\n- ne\n- nl\n- ro\n- ru\n- si\n- tr\n- vi\n- zh\n- af\n- az\n- bn\n- fa\n- he\n- hr\n- id\n- ka\n- km\n- mk\n- ml\n- mn\n- mr\n- pl\n- ps\n- pt\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- uk\n- ur\n- xh\n- gl\n- sl\ntags:\n- mbart-50\n---\n\n# mBART-50 one to many multilingual machine translation\n\n\nThis model is a fine-tuned checkpoint of [mBART-large-50](https://huggingface.co/facebook/mbart-large-50). `mbart-large-50-one-to-many-mmt` is fine-tuned for multilingual machine translation. It was introduced in [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) paper.\n\n\nThe model can translate English to other 49 languages mentioned below. \nTo translate into a target language, the target language id is forced as the first generated token. To force the\ntarget language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.\n\n```python\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\narticle_en = \"The head of the United Nations says there is no military solution in Syria\"\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")\n\nmodel_inputs = tokenizer(article_en, return_tensors=\"pt\")\n\n# translate from English to Hindi\ngenerated_tokens = model.generate(\n    **model_inputs,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => '\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u0928\u0947\u0924\u093e \u0915\u0939\u0924\u0947 \u0939\u0948\u0902 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948'\n\n# translate from English to Chinese\ngenerated_tokens = model.generate(\n    **model_inputs,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => '\u8054\u5408\u56fd\u9996\u8111\u8bf4,\u53d9\u5229\u4e9a\u6ca1\u6709\u519b\u4e8b\u89e3\u51b3\u529e\u6cd5'\n```\n\nSee the [model hub](https://huggingface.co/models?filter=mbart-50) to look for more fine-tuned versions.\n\n## Languages covered\nArabic (ar_AR), Czech (cs_CZ), German (de_DE), English (en_XX), Spanish (es_XX), Estonian (et_EE), Finnish (fi_FI), French (fr_XX), Gujarati (gu_IN), Hindi (hi_IN), Italian (it_IT), Japanese (ja_XX), Kazakh (kk_KZ), Korean (ko_KR), Lithuanian (lt_LT), Latvian (lv_LV), Burmese (my_MM), Nepali (ne_NP), Dutch (nl_XX), Romanian (ro_RO), Russian (ru_RU), Sinhala (si_LK), Turkish (tr_TR), Vietnamese (vi_VN), Chinese (zh_CN), Afrikaans (af_ZA), Azerbaijani (az_AZ), Bengali (bn_IN), Persian (fa_IR), Hebrew (he_IL), Croatian (hr_HR), Indonesian (id_ID), Georgian (ka_GE), Khmer (km_KH), Macedonian (mk_MK), Malayalam (ml_IN), Mongolian (mn_MN), Marathi (mr_IN), Polish (pl_PL), Pashto (ps_AF), Portuguese (pt_XX), Swedish (sv_SE), Swahili (sw_KE), Tamil (ta_IN), Telugu (te_IN), Thai (th_TH), Tagalog (tl_XX), Ukrainian (uk_UA), Urdu (ur_PK), Xhosa (xh_ZA), Galician (gl_ES), Slovene (sl_SI)\n\n\n## BibTeX entry and citation info\n```\n@article{tang2020multilingual,\n    title={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},\n    author={Yuqing Tang and Chau Tran and Xian Li and Peng-Jen Chen and Naman Goyal and Vishrav Chaudhary and Jiatao Gu and Angela Fan},\n    year={2020},\n    eprint={2008.00401},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```", "size_bytes": "2444714899", "downloads": 6127}
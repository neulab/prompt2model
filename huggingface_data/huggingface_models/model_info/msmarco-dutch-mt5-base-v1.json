{"pretrained_model_name": "doc2query/msmarco-dutch-mt5-base-v1", "description": "---\nlanguage: nl\ndatasets:\n- unicamp-dl/mmarco\nwidget:\n- text: \"Python is een programmeertaal die begin jaren 90 ontworpen en ontwikkeld werd door Guido van Rossum, destijds verbonden aan het Centrum voor Wiskunde en Informatica (daarvoor Mathematisch Centrum) in Amsterdam. De taal is mede gebaseerd op inzichten van professor Lambert Meertens, die een taal genaamd ABC had ontworpen, bedoeld als alternatief voor BASIC, maar dan met geavanceerde datastructuren. Inmiddels wordt de taal doorontwikkeld door een enthousiaste groep, tot juli 2018 geleid door Van Rossum. Deze groep wordt ondersteund door vrijwilligers op het internet. De ontwikkeling van Python wordt geleid door de Python Software Foundation. Python is vrije software.\"\n\nlicense: apache-2.0\n---\n\n# doc2query/msmarco-dutch-mt5-base-v1\n\nThis is a [doc2query](https://arxiv.org/abs/1904.08375) model based on mT5 (also known as [docT5query](https://cs.uwaterloo.ca/~jimmylin/publications/Nogueira_Lin_2019_docTTTTTquery-v2.pdf)).\n\nIt can be used for:\n- **Document expansion**: You generate for your paragraphs 20-40 queries and index the paragraphs and the generates queries in a standard BM25 index like Elasticsearch, OpenSearch, or Lucene. The generated queries help to close the lexical gap of lexical search, as the generate queries contain synonyms. Further, it re-weights words giving important words a higher weight even if they appear seldomn in a paragraph. In our [BEIR](https://arxiv.org/abs/2104.08663) paper we showed that BM25+docT5query is a powerful search engine. In the [BEIR repository](https://github.com/beir-cellar/beir) we have an example how to use docT5query with Pyserini.\n- **Domain Specific Training Data Generation**: It can be used to generate training data to learn an embedding model. In our [GPL-Paper](https://arxiv.org/abs/2112.07577) / [GPL Example on SBERT.net](https://www.sbert.net/examples/domain_adaptation/README.html#gpl-generative-pseudo-labeling) we have an example how to use the model to generate (query, text) pairs for a given collection of unlabeled texts. These pairs can then be used to train powerful dense embedding models.\n\n## Usage\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nmodel_name = 'doc2query/msmarco-dutch-mt5-base-v1'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ntext = \"Python ist eine universelle, \u00fcblicherweise interpretierte, h\u00f6here Programmiersprache. Sie hat den Anspruch, einen gut lesbaren, knappen Programmierstil zu f\u00f6rdern. So werden beispielsweise Bl\u00f6cke nicht durch geschweifte Klammern, sondern durch Einr\u00fcckungen strukturiert.\"\n\n\ndef create_queries(para):\n    input_ids = tokenizer.encode(para, return_tensors='pt')\n    with torch.no_grad():\n        # Here we use top_k / top_k random sampling. It generates more diverse queries, but of lower quality\n        sampling_outputs = model.generate(\n            input_ids=input_ids,\n            max_length=64,\n            do_sample=True,\n            top_p=0.95,\n            top_k=10, \n            num_return_sequences=5\n            )\n        \n        # Here we use Beam-search. It generates better quality queries, but with less diversity\n        beam_outputs = model.generate(\n            input_ids=input_ids, \n            max_length=64, \n            num_beams=5, \n            no_repeat_ngram_size=2, \n            num_return_sequences=5, \n            early_stopping=True\n        )\n\n\n    print(\"Paragraph:\")\n    print(para)\n    \n    print(\"\\nBeam Outputs:\")\n    for i in range(len(beam_outputs)):\n        query = tokenizer.decode(beam_outputs[i], skip_special_tokens=True)\n        print(f'{i + 1}: {query}')\n\n    print(\"\\nSampling Outputs:\")\n    for i in range(len(sampling_outputs)):\n        query = tokenizer.decode(sampling_outputs[i], skip_special_tokens=True)\n        print(f'{i + 1}: {query}')\n\ncreate_queries(text)\n\n```\n\n**Note:** `model.generate()` is non-deterministic for top_k/top_n sampling. It produces different queries each time you run it.\n\n## Training\nThis model fine-tuned [google/mt5-base](https://huggingface.co/google/mt5-base) for 66k training steps (4 epochs on the 500k training pairs from MS MARCO). For the  training script, see the `train_script.py` in this repository.\n\nThe input-text was truncated to 320 word pieces. Output text was generated up to 64 word pieces. \n\nThis model was trained on a (query, passage) from the [mMARCO dataset](https://github.com/unicamp-dl/mMARCO).\n\n\n\n", "size_bytes": "2329700301", "downloads": 16}
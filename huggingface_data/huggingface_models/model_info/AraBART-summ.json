{"pretrained_model_name": "abdalrahmanshahrour/AraBART-summ", "description": "---\nlicense: apache-2.0\nlanguage:\n  - ar\ntags:\n  - summarization\n  - AraBERT\n  - BERT\n  - BERT2BERT\n  - MSA\n  - Arabic Text Summarization\n  - Arabic News Title Generation\n  - Arabic Paraphrasing\n  - Summarization\n  - generated_from_trainer\n  - Transformers\n  - PyTorch\nwidget:\n  - text: >-\n      \u0634\u0647\u062f\u062a \u0645\u062f\u064a\u0646\u0629 \u0637\u0631\u0627\u0628\u0644\u0633\u060c \u0645\u0633\u0627\u0621 \u0623\u0645\u0633 \u0627\u0644\u0623\u0631\u0628\u0639\u0627\u0621\u060c \u0627\u062d\u062a\u062c\u0627\u062c\u0627\u062a \u0634\u0639\u0628\u064a\u0629 \u0648\u0623\u0639\u0645\u0627\u0644 \u0634\u063a\u0628 \u0644\u0644\u064a\u0648\u0645\n      \u0627\u0644\u062b\u0627\u0644\u062b \u0639\u0644\u0649 \u0627\u0644\u062a\u0648\u0627\u0644\u064a\u060c \u0648\u0630\u0644\u0643 \u0628\u0633\u0628\u0628 \u062a\u0631\u062f\u064a \u0627\u0644\u0648\u0636\u0639 \u0627\u0644\u0645\u0639\u064a\u0634\u064a \u0648\u0627\u0644\u0627\u0642\u062a\u0635\u0627\u062f\u064a. \u0648\u0627\u0646\u062f\u0644\u0639\u062a\n      \u0645\u0648\u0627\u062c\u0647\u0627\u062a \u0639\u0646\u064a\u0641\u0629 \u0648\u0639\u0645\u0644\u064a\u0627\u062a \u0643\u0631 \u0648\u0641\u0631 \u0645\u0627 \u0628\u064a\u0646 \u0627\u0644\u062c\u064a\u0634 \u0627\u0644\u0644\u0628\u0646\u0627\u0646\u064a \u0648\u0627\u0644\u0645\u062d\u062a\u062c\u064a\u0646 \u0627\u0633\u062a\u0645\u0631\u062a\n      \u0644\u0633\u0627\u0639\u0627\u062a\u060c \u0625\u062b\u0631 \u0645\u062d\u0627\u0648\u0644\u0629 \u0641\u062a\u062d \u0627\u0644\u0637\u0631\u0642\u0627\u062a \u0627\u0644\u0645\u0642\u0637\u0648\u0639\u0629\u060c \u0645\u0627 \u0623\u062f\u0649 \u0625\u0644\u0649 \u0625\u0635\u0627\u0628\u0629 \u0627\u0644\u0639\u0634\u0631\u0627\u062a \u0645\u0646\n      \u0627\u0644\u0637\u0631\u0641\u064a\u0646.\ndatasets:\n  - xlsum\nmodel-index:\n  - name: arabartsummarization\n    results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# AraBART-summ\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n\n## Validation Metrics\n\n- Loss: 2.3417\n- Rouge1: 2.353\n- Rouge2: 1.103\n- RougeL: 1.176\n- RougeLsum: 1.521\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 2.7555        | 1.0   | 9380 | 2.3417          |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu116\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "557175853", "downloads": 136}
{"pretrained_model_name": "yhavinga/ul2-large-dutch", "description": "\n---\nlanguage:\n- nl \nlicense: apache-2.0\ntags:\n- dutch\n- t5\n- t5x\n- ul2\n- seq2seq\ndatasets:\n- yhavinga/mc4_nl_cleaned\n- yhavinga/nedd_wiki_news\ninference: false\n---\n\n# ul2-large-dutch for Dutch\n\nPretrained T5 model on Dutch using a UL2 (Mixture-of-Denoisers) objective.\nThe T5 model was introduced in\n[this paper](https://arxiv.org/abs/1910.10683)\nand first released at [this page](https://github.com/google-research/text-to-text-transfer-transformer).\nThe UL2 objective was introduced in\n[this paper](https://arxiv.org/abs/2205.05131)\nand first released at [this page](https://github.com/google-research/google-research/tree/master/ul2).\n\n**Note:** The Hugging Face inference widget is deactivated because this model needs a text-to-text fine-tuning on\na specific downstream task to be useful in practice.\n\n## Model description\n\nT5 is an encoder-decoder model and treats all NLP problems in a text-to-text format.\n`ul2-large-dutch` T5 is a transformers model pretrained on a very large corpus of\nDutch  data in a self-supervised fashion.\nThis means it was pretrained on the raw texts only, with no humans labelling them in any way\n(which is why it can use lots of publicly available data) with an automatic process to generate\ninputs and outputs from those texts.\n\n\nThis model used the [T5 v1.1](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) improvements compared to the original T5 model during the pretraining:\n- GEGLU activation in the feed-forward hidden layer, rather than ReLU - see [here](https://arxiv.org/abs/2002.05202)\n- Dropout was turned off during pre-training. Dropout should be re-enabled during fine-tuning\n- Pre-trained on self-supervised objective only without mixing in the downstream tasks\n- No parameter sharing between embedding and classifier layer\n\n\n\n### UL2 pretraining objective\n\nThis model was pretrained with the UL2's Mixture-of-Denoisers (MoD) objective, that combines diverse pre-training\nparadigms together. UL2 frames different objective functions for training language models as denoising tasks, where\nthe model has to recover missing sub-sequences of a given input. During pre-training it uses a novel mixture-of-denoisers\nthat samples from a varied set of such objectives, each with different configurations. UL2 is trained using a mixture of\nthree denoising tasks:\n\n1. R-denoising (or regular span corruption), which emulates the standard T5 span corruption objective;\n2. X-denoising (or extreme span corruption); and\n3. S-denoising (or sequential PrefixLM).\n\nDuring pre-training, we sample from the available denoising tasks based on user-specified ratios.\nUL2 introduces a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training\ndenoising task. During the pre-training, a paradigm token is inserted to the input\n(`[NLU]` for R-denoising, `[NLG]` for X-denoising, or `[S2S]` for S-denoising) indicating the denoising task at hand.\nThen, during fine-tuning the same input token should be inserted to get the best performance for different downstream\nfine-tuning tasks.\n\n## Intended uses & limitations\n\nThis model was only pretrained in a self-supervised way excluding any supervised training.\nTherefore, this model has to be fine-tuned before it is usable on a downstream task,\nlike text classification, unlike the Google's original T5 model.\n\n**Note:** You most likely need to fine-tune these T5/UL2 models without mixed precision\nso fine-tune them with full fp32 precision. Fine-tuning with Flax in bf16 - `model.to_bf16()` - is possible\nif you set the mask correctly to exclude layernorm and embedding layers. Also note that the T5x pre-training\nand fine-tuning configs set `z_loss` to 1e-4, which is used to keep the loss scale from underflowing.\nYou can also find more fine-tuning tips from [here](https://discuss.huggingface.co/t/t5-finetuning-tips), for example.\n\n**Note**: For fine-tuning, most likely you can get better results if you insert a prefix token\nof `[NLU]`, `[NLG]`, or `[S2S]` to your input texts.\nFor general language understanding fine-tuning tasks, you could use the `[NLU]` token.\nFor GPT-style causal language generation, you could use the `[S2S]` token.\nThe token `[NLG]` of the X-denoising pretrain task is somewhat mix between the language understanding and causal language\ngeneration so the token `[NLG]` could maybe be used for language generation fine-tuning too.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"yhavinga/ul2-large-dutch\", use_fast=False)\nmodel = T5ForConditionalGeneration.from_pretrained(\"yhavinga/ul2-large-dutch\")\n```\n\nand in Flax:\n\n```python\nfrom transformers import T5Tokenizer, FlaxT5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"yhavinga/ul2-large-dutch\", use_fast=False)\nmodel = FlaxT5ForConditionalGeneration.from_pretrained(\"yhavinga/ul2-large-dutch\")\n```\n\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral.\nTherefore, the model can have biased predictions. This bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe `ul2-large-dutch` T5 model was pre-trained simultaneously on a combination of several datasets,\nincluding the full version of the \"mc4_nl_cleaned\" dataset, which is a cleaned version of Common Crawl's web\ncrawl corpus, Dutch books, the Dutch subset of Wikipedia (2022-03-20), and a subset of \"mc4_nl_cleaned\"\ncontaining only texts from Dutch newspapers.\n\n## Training procedure\n\n### Preprocessing\n\nThe ul2-large-dutch T5 model uses a SentencePiece unigram tokenizer with a vocabulary of 32,000 tokens.\nThe tokenizer includes the special tokens `<pad>`, `</s>`, `<unk>`,  known from the original T5 paper,\n`[NLU]`, `[NLG]` and `[S2S]` for the MoD pre-training, and `<n>` for newline.\nDuring pre-training with the UL2 objective, input and output sequences consist of 512 consecutive tokens. \nThe tokenizer does not lowercase texts and is therefore case-sensitive; it distinguises\nbetween `dutch` and `Dutch`.\nAdditionally, 100+28 extra tokens were added for pre-training tasks, resulting in a total of 32,128 tokens.\n\n### Pretraining\nThe model was trained on TPUv3-8 VM, sponsored by the [Google TPU Research Cloud](https://sites.research.google/trc/about/),\nfor 1000000 steps with a batch size of 64\n(in total 32 B tokens).\nThe optimizer used was AdaFactor with learning rate warmup for 10K steps with a constant learning rate of 1e-2,\nand then an inverse square root decay (exponential decay) of the learning rate after.\nThe model was trained with Google's Jax/Flax based [t5x framework](https://github.com/google-research/t5x) with help\nfrom [Stephenn Fernandes](https://huggingface.co/StephennFernandes) to get started writing task definitions that wrap\nHF datasets.\n\nThe UL2 training objective code used with the [t5x framework](https://github.com/google-research/t5x) was copied and\nslightly modified from the [UL2 paper](https://arxiv.org/pdf/2205.05131.pdf) appendix chapter 9.2 by the authors\nof the Finnish ul2 models. Used UL2 objective code is available in the repository\n[Finnish-NLP/ul2-base-nl36-finnish](https://huggingface.co/Finnish-NLP/ul2-base-nl36-finnish) in the files `ul2_objective.py` and `tasks.py`.\nUL2's mixture-of-denoisers configuration was otherwise equal to the UL2 paper\nbut for the rate of mixing denoisers, 20% for S-denoising was used (suggested at the paper chapter 4.5)\nand the rest was divided equally between the R-denoising and X-denoising (i.e. 40% for both).\n### Model list\n\nModels in this series:\n|                      | ul2-base-dutch       | ul2-base-nl36-dutch   | ul2-large-dutch      | ul2-small-dutch      |\n|:---------------------|:---------------------|:----------------------|:---------------------|:---------------------|\n| model_type           | t5                   | t5                    | t5                   | t5                   |\n| _pipeline_tag        | text2text-generation | text2text-generation  | text2text-generation | text2text-generation |\n| d_model              | 768                  | 768                   | 1024                 | 512                  |\n| d_ff                 | 2048                 | 3072                  | 2816                 | 1024                 |\n| num_heads            | 12                   | 12                    | 16                   | 6                    |\n| d_kv                 | 64                   | 64                    | 64                   | 64                   |\n| num_layers           | 12                   | 36                    | 24                   | 8                    |\n| num_decoder_layers   | 12                   | 36                    | 24                   | 8                    |\n| feed_forward_proj    | gated-gelu           | gated-gelu            | gated-gelu           | gated-gelu           |\n| dense_act_fn         | gelu_new             | gelu_new              | gelu_new             | gelu_new             |\n| vocab_size           | 32128                | 32128                 | 32128                | 32128                |\n| tie_word_embeddings  | 0                    | 0                     | 0                    | 0                    |\n| torch_dtype          | float32              | float32               | float32              | float32              |\n| _gin_batch_size      | 128                  | 64                    | 64                   | 128                  |\n| _gin_z_loss          | 0.0001               | 0.0001                | 0.0001               | 0.0001               |\n| _gin_t5_config_dtype | 'bfloat16'           | 'bfloat16'            | 'bfloat16'           | 'bfloat16'           |\n\n\n\n## Evaluation results\n\nSee the evaluation section in the interactive [Pre-training Dutch T5 Models](https://huggingface.co/spaces/yhavinga/pre-training-dutch-t5-models) blog.\n\n## Acknowledgements\n\nThis project would not have been possible without compute generously provided by Google through the\n[TPU Research Cloud](https://sites.research.google/trc/).\nThanks to the [Finnish-NLP](https://huggingface.co/Finnish-NLP) authors for releasing their code for the UL2 objective and associated task definitions.\nThanks to [Stephenn Fernandes](https://huggingface.co/StephennFernandes) for helping me get started with the t5x framework.\n\nCreated by [Yeb Havinga](https://www.linkedin.com/in/yeb-havinga-86530825/)\n\n", "size_bytes": "3132785797", "downloads": 8}
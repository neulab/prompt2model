{"pretrained_model_name": "google/byt5-small", "description": "---\nlanguage: \n- multilingual\n- af\n- am\n- ar\n- az\n- be\n- bg\n- bn\n- ca\n- ceb\n- co\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fil\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- haw\n- hi\n- hmn\n- ht\n- hu\n- hy\n- ig\n- is\n- it\n- iw\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lb\n- lo\n- lt\n- lv\n- mg\n- mi\n- mk\n- ml\n- mn\n- mr\n- ms\n- mt\n- my\n- ne\n- nl\n- no\n- ny\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sd\n- si\n- sk\n- sl\n- sm\n- sn\n- so\n- sq\n- sr\n- st\n- su\n- sv\n- sw\n- ta\n- te\n- tg\n- th\n- tr\n- uk\n- und\n- ur\n- uz\n- vi\n- xh\n- yi\n- yo\n- zh\n- zu\ndatasets:\n- mc4\n\nlicense: apache-2.0\n---\n\n# ByT5 - Small\n\nByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-small).\n\nByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.\n\nByT5 works especially well on noisy text data,*e.g.*, `google/byt5-small` significantly outperforms [mt5-small](https://huggingface.co/google/mt5-small) on [TweetQA](https://arxiv.org/abs/1907.06292).\n\nPaper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)\n\nAuthors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel* \n\n## Example Inference\n\nByT5 works on raw UTF-8 bytes and can be used without a tokenizer:\n\n```python\nfrom transformers import T5ForConditionalGeneration\nimport torch\n\nmodel = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\n\ninput_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\nlabels = torch.tensor([list(\"La vie est comme une bo\u00eete de chocolat.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n\nloss = model(input_ids, labels=labels).loss # forward pass\n```\n\nFor batched inference & training it is however recommended using a tokenizer class for padding:\n\n```python\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\n\nmodel = T5ForConditionalGeneration.from_pretrained('google/byt5-small')\ntokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n\nmodel_inputs = tokenizer([\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\")\nlabels = tokenizer([\"La vie est comme une bo\u00eete de chocolat.\", \"Aujourd'hui c'est lundi.\"], padding=\"longest\", return_tensors=\"pt\").input_ids\n\nloss = model(**model_inputs, labels=labels).loss # forward pass\n```\n\n## Abstract\n\nMost widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.\n\n![model image](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/ByT5.png)\n\n", "size_bytes": "1198627927", "downloads": 31397}
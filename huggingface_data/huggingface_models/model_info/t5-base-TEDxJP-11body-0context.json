{"pretrained_model_name": "tyoyo/t5-base-TEDxJP-11body-0context", "description": "---\nlicense: cc-by-sa-4.0\ntags:\n- generated_from_trainer\ndatasets:\n- te_dx_jp\nmodel-index:\n- name: t5-base-TEDxJP-11body-0context\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-base-TEDxJP-11body-0context\n\nThis model is a fine-tuned version of [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) on the te_dx_jp dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8068\n- Wer: 0.1976\n- Mer: 0.1904\n- Wil: 0.2816\n- Wip: 0.7184\n- Hits: 602335\n- Substitutions: 75050\n- Deletions: 39435\n- Insertions: 27185\n- Cer: 0.1625\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 64\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Wer    | Mer    | Wil    | Wip    | Hits   | Substitutions | Deletions | Insertions | Cer    |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|:------:|:------:|:-------------:|:---------:|:----------:|:------:|\n| 0.8909        | 1.0   | 746  | 0.7722          | 0.3120 | 0.2861 | 0.3989 | 0.6011 | 558138 | 99887         | 58795     | 64983      | 0.2652 |\n| 0.6786        | 2.0   | 1492 | 0.7021          | 0.2226 | 0.2122 | 0.3069 | 0.6931 | 592242 | 78773         | 45805     | 34978      | 0.1862 |\n| 0.5627        | 3.0   | 2238 | 0.6996          | 0.2104 | 0.2016 | 0.2942 | 0.7058 | 597381 | 76593         | 42846     | 31392      | 0.1752 |\n| 0.489         | 4.0   | 2984 | 0.7161          | 0.2030 | 0.1952 | 0.2865 | 0.7135 | 599808 | 75155         | 41857     | 28506      | 0.1684 |\n| 0.4355        | 5.0   | 3730 | 0.7389          | 0.2000 | 0.1924 | 0.2837 | 0.7163 | 601815 | 75247         | 39758     | 28335      | 0.1651 |\n| 0.3836        | 6.0   | 4476 | 0.7537          | 0.1992 | 0.1918 | 0.2829 | 0.7171 | 601846 | 75046         | 39928     | 27815      | 0.1640 |\n| 0.3617        | 7.0   | 5222 | 0.7743          | 0.1995 | 0.1918 | 0.2832 | 0.7168 | 602287 | 75268         | 39265     | 28445      | 0.1642 |\n| 0.3258        | 8.0   | 5968 | 0.7907          | 0.1971 | 0.1899 | 0.2809 | 0.7191 | 602800 | 74887         | 39133     | 27258      | 0.1620 |\n| 0.3225        | 9.0   | 6714 | 0.8035          | 0.1981 | 0.1908 | 0.2823 | 0.7177 | 602418 | 75372         | 39030     | 27625      | 0.1630 |\n| 0.3162        | 10.0  | 7460 | 0.8068          | 0.1976 | 0.1904 | 0.2816 | 0.7184 | 602335 | 75050         | 39435     | 27185      | 0.1625 |\n\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu102\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n", "size_bytes": "891730879", "downloads": 2}
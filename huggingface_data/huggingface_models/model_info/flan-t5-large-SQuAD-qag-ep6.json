{"pretrained_model_name": "Zekunli/flan-t5-large-SQuAD-qag-ep6", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\n- f1\nmodel-index:\n- name: flan-t5-large-SQuAD-qag-ep6\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-large-SQuAD-qag-ep6\n\nThis model is a fine-tuned version of [google/flan-t5-large](https://huggingface.co/google/flan-t5-large) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8670\n- Rouge1: 40.8926\n- Rouge2: 19.5196\n- Rougel: 37.4672\n- Rougelsum: 37.4713\n- F1: 21.4247\n- Exact Match: 15.1427\n- Gen Len: 18.3706\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 24\n- eval_batch_size: 48\n- seed: 1799\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 6\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | F1      | Exact Match | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|:-----------:|:-------:|\n| 1.0439        | 0.51  | 400  | 0.8849          | 39.8979 | 18.5242 | 36.3173 | 36.3341   | 20.5015 | 13.8365     | 18.4069 |\n| 0.9761        | 1.02  | 800  | 0.8748          | 40.0168 | 18.6616 | 36.6103 | 36.6343   | 19.4625 | 13.9332     | 18.4359 |\n| 0.9358        | 1.52  | 1200 | 0.8709          | 40.6382 | 19.6212 | 37.2797 | 37.3127   | 20.2207 | 14.2235     | 18.3957 |\n| 0.919         | 2.03  | 1600 | 0.8697          | 41.2054 | 20.0197 | 37.7826 | 37.8021   | 21.7337 | 15.4814     | 18.3633 |\n| 0.8849        | 2.54  | 2000 | 0.8695          | 41.3385 | 19.9503 | 37.9501 | 37.9707   | 22.0787 | 15.6265     | 18.3507 |\n| 0.8869        | 3.05  | 2400 | 0.8674          | 41.2089 | 19.7348 | 37.6962 | 37.7184   | 21.3812 | 14.9492     | 18.4122 |\n| 0.8617        | 3.56  | 2800 | 0.8670          | 40.8926 | 19.5196 | 37.4672 | 37.4713   | 21.4247 | 15.1427     | 18.3706 |\n| 0.8486        | 4.07  | 3200 | 0.8678          | 40.856  | 19.6152 | 37.4585 | 37.4462   | 22.0102 | 15.5781     | 18.3522 |\n| 0.8367        | 4.57  | 3600 | 0.8678          | 40.9533 | 19.7798 | 37.5764 | 37.5818   | 21.769  | 15.7233     | 18.328  |\n| 0.8442        | 5.08  | 4000 | 0.8671          | 41.2103 | 19.987  | 37.8326 | 37.8498   | 22.3873 | 16.1587     | 18.3314 |\n| 0.8317        | 5.59  | 4400 | 0.8673          | 41.0444 | 19.7886 | 37.6754 | 37.6706   | 22.0032 | 15.7716     | 18.3208 |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.11.0+cu113\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "3132789733", "downloads": 2}
{"pretrained_model_name": "nguyenvulebinh/spelling-oov", "description": "```python\nfrom transformers import EncoderDecoderModel\nfrom importlib.machinery import SourceFileLoader\nfrom transformers.file_utils import cached_path, hf_bucket_url\nimport torch\nimport os\n\n## Load model & tokenizer\ncache_dir='./cache'\nmodel_name='nguyenvulebinh/spelling-oov'\n\ndef download_tokenizer_files():\n    resources = ['envibert_tokenizer.py', 'dict.txt', 'sentencepiece.bpe.model']\n    for item in resources:\n        if not os.path.exists(os.path.join(cache_dir, item)):\n            tmp_file = hf_bucket_url(model_name, filename=item)\n            tmp_file = cached_path(tmp_file,cache_dir=cache_dir)\n            os.rename(tmp_file, os.path.join(cache_dir, item))\n        \ndownload_tokenizer_files()\nspell_tokenizer = SourceFileLoader(\"envibert.tokenizer\",os.path.join(cache_dir,'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\nspell_model = EncoderDecoderModel.from_pretrained(model_name)\n\ndef oov_spelling(word, num_candidate=1):\n    result = []\n    inputs = spell_tokenizer([word.lower()])\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n    inputs = {\n        \"input_ids\": torch.tensor(input_ids),\n        \"attention_mask\": torch.tensor(attention_mask)\n    }\n    outputs = spell_model.generate(**inputs, num_return_sequences=num_candidate)\n    for output in outputs.cpu().detach().numpy().tolist():\n        result.append(spell_tokenizer.sp_model.DecodePieces(spell_tokenizer.decode(output, skip_special_tokens=True).split()))\n    return result    \n    \noov_spelling('spacespeaker')\n# output: ['x p\u00e2y x p\u1ebfch c\u01a1']\n\n```", "size_bytes": "342254573", "downloads": 15}
{"pretrained_model_name": "vocabtrimmer/mt5-small-trimmed-fr-5000-frquad-qa", "description": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: fr\ndatasets:\n- lmqg/qg_frquad\npipeline_tag: text2text-generation\ntags:\n- question answering\nwidget:\n- text: \"question: En quelle ann\u00e9e a-t-on trouv\u00e9 trace d'un haut fourneau similaire?, context: Cette technologie ne dispara\u00eet qu'au d\u00e9but du XXe si\u00e8cle. On retrouve vers 1900 un haut fourneau similaire dans le Bulacan, aux Philippines. Plus tard encore, le \u00ab haut fourneau dans la cour \u00bb pr\u00f4n\u00e9 par Mao Zedong pendant le Grand Bond en avant est de ce type. L'exp\u00e9rience n'est un \u00e9chec technique que dans les r\u00e9gions o\u00f9 le savoir-faire n'existe pas, ou a disparu.\"\n  example_title: \"Question Answering Example 1\" \n- text: \"question: Comment appelle-t-on la Guerre de 14-18 ?, context: Ce black dog peut \u00eatre li\u00e9 \u00e0 des \u00e9v\u00e8nements traumatisants issus du monde ext\u00e9rieur, tels que son renvoi de l'Amiraut\u00e9 apr\u00e8s la catastrophe des Dardanelles, lors de la Grande Guerre de 14-18, ou son rejet par l'\u00e9lectorat en juillet 1945. On sait \u00e9galement que dans ces deux cas, la gu\u00e9rison, certes lente et douloureuse et jamais compl\u00e8te ni d\u00e9finitive, se fera gr\u00e2ce \u00e0 la peinture. D'un autre c\u00f4t\u00e9, \u00e9tant donn\u00e9s les sympt\u00f4mes de ce mal que Churchill \u00e9prouvait de plus en plus, il ne pouvait rien moins qu'\u00eatre purement associ\u00e9 \u00e0 de telles causes extrins\u00e8ques, ce qui correspond au profil classique de la d\u00e9pression majeure unipolaire ou bipolaire.\"\n  example_title: \"Question Answering Example 2\" \nmodel-index:\n- name: vocabtrimmer/mt5-small-trimmed-fr-5000-frquad-qa\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_frquad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Answering)\n      type: bleu4_question_answering\n      value: 20.92\n    - name: ROUGE-L (Question Answering)\n      type: rouge_l_question_answering\n      value: 30.51\n    - name: METEOR (Question Answering)\n      type: meteor_question_answering\n      value: 24.42\n    - name: BERTScore (Question Answering)\n      type: bertscore_question_answering\n      value: 89.98\n    - name: MoverScore (Question Answering)\n      type: moverscore_question_answering\n      value: 72.76\n    - name: AnswerF1Score (Question Answering)\n      type: answer_f1_score__question_answering\n      value: 48.98\n    - name: AnswerExactMatch (Question Answering)\n      type: answer_exact_match_question_answering\n      value: 32.65\n---\n\n# Model Card of `vocabtrimmer/mt5-small-trimmed-fr-5000-frquad-qa`\nThis model is fine-tuned version of [vocabtrimmer/mt5-small-trimmed-fr-5000](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-fr-5000) for question answering task on the [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) (dataset_name: default) via [`lmqg`](https://github.com/asahi417/lm-question-generation).\n\n\n### Overview\n- **Language model:** [vocabtrimmer/mt5-small-trimmed-fr-5000](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-fr-5000)   \n- **Language:** fr  \n- **Training data:** [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) (default)\n- **Online Demo:** [https://autoqg.net/](https://autoqg.net/)\n- **Repository:** [https://github.com/asahi417/lm-question-generation](https://github.com/asahi417/lm-question-generation)\n- **Paper:** [https://arxiv.org/abs/2210.03992](https://arxiv.org/abs/2210.03992)\n\n### Usage\n- With [`lmqg`](https://github.com/asahi417/lm-question-generation#lmqg-language-model-for-question-generation-)\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"fr\", model=\"vocabtrimmer/mt5-small-trimmed-fr-5000-frquad-qa\")\n\n# model prediction\nanswers = model.answer_q(list_question=\"En quelle ann\u00e9e a-t-on trouv\u00e9 trace d'un haut fourneau similaire?\", list_context=\" Cette technologie ne dispara\u00eet qu'au d\u00e9but du XXe si\u00e8cle. On retrouve vers 1900 un haut fourneau similaire dans le Bulacan, aux Philippines. Plus tard encore, le \u00ab haut fourneau dans la cour \u00bb pr\u00f4n\u00e9 par Mao Zedong pendant le Grand Bond en avant est de ce type. L'exp\u00e9rience n'est un \u00e9chec technique que dans les r\u00e9gions o\u00f9 le savoir-faire n'existe pas, ou a disparu.\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"vocabtrimmer/mt5-small-trimmed-fr-5000-frquad-qa\")\noutput = pipe(\"question: En quelle ann\u00e9e a-t-on trouv\u00e9 trace d'un haut fourneau similaire?, context: Cette technologie ne dispara\u00eet qu'au d\u00e9but du XXe si\u00e8cle. On retrouve vers 1900 un haut fourneau similaire dans le Bulacan, aux Philippines. Plus tard encore, le \u00ab haut fourneau dans la cour \u00bb pr\u00f4n\u00e9 par Mao Zedong pendant le Grand Bond en avant est de ce type. L'exp\u00e9rience n'est un \u00e9chec technique que dans les r\u00e9gions o\u00f9 le savoir-faire n'existe pas, ou a disparu.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Answering)***: [raw metric file](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-fr-5000-frquad-qa/raw/main/eval/metric.first.answer.paragraph_question.answer.lmqg_qg_frquad.default.json) \n\n|                  |   Score | Type    | Dataset                                                          |\n|:-----------------|--------:|:--------|:-----------------------------------------------------------------|\n| AnswerExactMatch |   32.65 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| AnswerF1Score    |   48.98 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| BERTScore        |   89.98 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| Bleu_1           |   30.42 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| Bleu_2           |   26.22 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| Bleu_3           |   23.44 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| Bleu_4           |   20.92 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| METEOR           |   24.42 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| MoverScore       |   72.76 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n| ROUGE_L          |   30.51 | default | [lmqg/qg_frquad](https://huggingface.co/datasets/lmqg/qg_frquad) |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_frquad\n - dataset_name: default\n - input_types: ['paragraph_question']\n - output_types: ['answer']\n - prefix_types: None\n - model: vocabtrimmer/mt5-small-trimmed-fr-5000\n - max_length: 512\n - max_length_output: 32\n - epoch: 25\n - batch: 32\n - lr: 0.0005\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 2\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-fr-5000-frquad-qa/raw/main/trainer_config.json).\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "size_bytes": "196806021", "downloads": 2}
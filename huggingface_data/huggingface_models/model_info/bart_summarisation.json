{"pretrained_model_name": "slauw87/bart_summarisation", "description": "\n---\nlanguage: en\ntags:\n- sagemaker\n- bart\n- summarization\nlicense: apache-2.0\ndatasets:\n- samsum\nmodel-index:\n- name: bart-large-cnn-samsum\n  results:\n  - task: \n      name: Abstractive Text Summarization\n      type: abstractive-text-summarization\n    dataset:\n      name: \"SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\" \n      type: samsum\n    metrics:\n       - name: Validation ROGUE-1\n         type: rogue-1\n         value: 43.2111\n       - name: Validation ROGUE-2\n         type: rogue-2\n         value: 22.3519\n       - name: Validation ROGUE-L\n         type: rogue-l\n         value: 33.315\n       - name: Test ROGUE-1\n         type: rogue-1\n         value: 41.8283\n       - name: Test ROGUE-2\n         type: rogue-2\n         value: 20.9857\n       - name: Test ROGUE-L\n         type: rogue-l\n         value: 32.3602\nwidget:\n- text: | \n    Sugi: I am tired of everything in my life. \n    Tommy: What? How happy you life is! I do envy you.\n    Sugi: You don't know that I have been over-protected by my mother these years. I am really about to leave the family and spread my wings.\n    Tommy: Maybe you are right. \n---\n## `bart-large-cnn-samsum`\nThis model was trained using Amazon SageMaker and the new Hugging Face Deep Learning container.\nFor more information look at:\n- [\ud83e\udd17 Transformers Documentation: Amazon SageMaker](https://huggingface.co/transformers/sagemaker.html)\n- [Example Notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)\n- [Amazon SageMaker documentation for Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)\n- [Python SDK SageMaker documentation for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html)\n- [Deep Learning Container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers)\n## Hyperparameters\n    {\n    \"dataset_name\": \"samsum\",\n    \"do_eval\": true,\n    \"do_predict\": true,\n    \"do_train\": true,\n    \"fp16\": true,\n    \"learning_rate\": 5e-05,\n    \"model_name_or_path\": \"facebook/bart-large-cnn\",\n    \"num_train_epochs\": 3,\n    \"output_dir\": \"/opt/ml/model\",\n    \"per_device_eval_batch_size\": 4,\n    \"per_device_train_batch_size\": 4,\n    \"predict_with_generate\": true,\n    \"seed\": 7\n}\n## Usage\n    from transformers import pipeline\n    summarizer = pipeline(\"summarization\", model=\"slauw87/bart-large-cnn-samsum\")\n    conversation = '''Sugi: I am tired of everything in my life. \n    Tommy: What? How happy you life is! I do envy you.\n    Sugi: You don't know that I have been over-protected by my mother these years. I am really about to leave the family and spread my wings.\n    Tommy: Maybe you are right.                                           \n    '''\n    nlp(conversation)\n## Results\n| key | value |\n| --- | ----- |\n| eval_rouge1 | 43.2111 |\n| eval_rouge2 | 22.3519 |\n| eval_rougeL | 33.3153 |\n| eval_rougeLsum | 40.0527 |\n| predict_rouge1 | 41.8283 |\n| predict_rouge2 | 20.9857 |\n| predict_rougeL | 32.3602 |\n| predict_rougeLsum | 38.7316 |\n", "size_bytes": "1625569391", "downloads": 26668}
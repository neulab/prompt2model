{"pretrained_model_name": "leukas/byt5-large-wmt14-deen", "description": "---\nlanguage:\n- de\n- en\npipeline_tag: translation\ndatasets:\n- wmt14\n---\n\n# byt5-large-wmt14-deen\n\nThis model is released as part of the work from [Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation](https://arxiv.org/abs/2302.14220).\nIt is a ByT5 model finetuned on German-->English translation the WMT14 dataset. \n\nTo use the model correctly, you must prepend the prompt with \"translate X to Y: \", where X and Y are your source and target languages (e.g. German, English).\n\n\nNOTE: The decoder_start_token_id is 259 for byt5 models and 250099 for mt5 models, which is different from the default token from google's byt5 and mt5 models (which is 0).", "size_bytes": "4912907277", "downloads": 4}
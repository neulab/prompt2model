{"pretrained_model_name": "Lalita/marianmt-zh_cn-th", "description": "---\ntags:\n- translation\n- torch==1.8.0\nwidget:\n- text: \"Inference Unavailable\"\n---\n### marianmt-zh_cn-th \n* source languages: zh_cn\n* target languages: th\n* dataset: \n* model: transformer-align\n* pre-processing: normalization + SentencePiece\n* test set scores: syllable: 15.95, word: 8.43\n\n## Training\n\nTraining scripts from [LalitaDeelert/NLP-ZH_TH-Project](https://github.com/LalitaDeelert/NLP-ZH_TH-Project). Experiments tracked at [cstorm125/marianmt-zh_cn-th](https://wandb.ai/cstorm125/marianmt-zh_cn-th).\n\n```\nexport WANDB_PROJECT=marianmt-zh_cn-th\npython train_model.py --input_fname ../data/v1/Train.csv \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t--output_dir ../models/marianmt-zh_cn-th \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t--source_lang zh --target_lang th \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t--metric_tokenize th_syllable --fp16 \n```\n\n## Usage\n\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n \ntokenizer = AutoTokenizer.from_pretrained(\"Lalita/marianmt-zh_cn-th\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Lalita/marianmt-zh_cn-th\").cpu()\n\nsrc_text = [\n    '\u6211\u7231\u4f60',\n    '\u6211\u60f3\u5403\u7c73\u996d',\n]\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\nprint([tokenizer.decode(t, skip_special_tokens=True) for t in translated])\n\n> ['\u0e1c\u0e21\u0e23\u0e31\u0e01\u0e04\u0e38\u0e13\u0e19\u0e30', '\u0e09\u0e31\u0e19\u0e2d\u0e22\u0e32\u0e01\u0e01\u0e34\u0e19\u0e02\u0e49\u0e32\u0e27']\n```\n\n## Requirements\n```\ntransformers==4.6.0\ntorch==1.8.0\n```", "size_bytes": "242307293", "downloads": 628}
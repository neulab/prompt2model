{"pretrained_model_name": "Short-Answer-Feedback/mbart-score-finetuned-saf-legal-domain", "description": "---\nlanguage: de\ndatasets:\n- Short-Answer-Feedback/saf_legal_domain_german\ntags:\n- generated_from_trainer\nwidget:\n- text: \"Antwort: Wird sich nicht an die Auflagen gehalten (unzureichende Eigenbem\u00fchung), droht eine Sperrzeit von 1-2 Wochen. Dadurch wird f\u00fcr die genannte zeit keine Leistung gezahlt, die Anspruchsdauer vermindert sich insgesamt. Bei wichtigen Gr\u00fcnden wird die Sperrzeit nicht verordnet. L\u00f6sung: Merkblatt 1 f\u00fcr Arbeitslose, S. 22: Erbringen Sie die Pflichten im Zusammenhang mit den Eigenbem\u00fchungen nicht, nicht rechtzeitig oder nicht vollst\u00e4ndig, tritt eine Sperrzeit (0,75 p) ein. Merkblatt 1 f\u00fcr Arbeitslose, S. 55: Die Dauer einer Sperrzeit bei unzureichenden Eigenbem\u00fchungen betr\u00e4gt zwei Wochen. (0,25 p). Frage: Mit welcher Folge und welcher Dauer m\u00fcssen Sie rechnen, wenn Sie Ihre notwendigen Eigenbem\u00fchungen nicht rechtzeitig oder nicht vollst\u00e4ndig erf\u00fcllen?\"\n---\n\n# mbart-score-finetuned-saf-legal-domain\n\nThis model is a fine-tuned version of [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25) on the [saf_legal_domain_german](https://huggingface.co/datasets/Short-Answer-Feedback/saf_legal_domain_german) dataset for Short Answer Feedback (SAF).\n\n## Model description\n\nThis model was built on top of [mBART](https://arxiv.org/abs/2001.08210), which is a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages.\n\nIt expects inputs in the following format:\n```\nAntwort: [answer] L\u00f6sung: [reference_answer] Frage: [question]\n```\n\nIn the example above, `[answer]`, `[reference_answer]` and `[question]` should be replaced by the provided answer, the reference answer and the question to which they refer, respectively.\n\n\nThe outputs are formatted as follows:\n```\n[score] Feedback: [feedback]\n```\n\nHence, `[score]` will be a numeric value between 0 and 1, while `[feedback]` will be the textual feedback generated by the model according to the given answer.\n\n## Intended uses & limitations\n\nThis model is intended to be used for Short Answer Feedback generation in the domain of the German social law. Thus, it is not expected to have particularly good performance on sets of questions and answers out of this scope.\n\nIt is important to acknowledge that the model underperforms when a question that was not seen during training is given as input for inference. In particular, it tends to classify most answers as being correct and does not provide relevant feedback in such cases. Nevertheless, this limitation could be partially overcome by extending the dataset with the desired question (and associated answers) and fine-tuning it for a few epochs on the new data.\n\n## Training and evaluation data\n\nAs mentioned previously, the model was trained on the [saf_legal_domain_german](https://huggingface.co/datasets/Short-Answer-Feedback/saf_legal_domain_german) dataset, which is divided into the following splits.\n\n| Split                 | Number of examples |\n| --------------------- | ------------------ |\n| train                 | 1596\t             |\n| validation            | 400\t             |\n| test_unseen_answers   | 221\t             |\n| test_unseen_questions | 275                |\n\nEvaluation was performed on the `test_unseen_answers` and `test_unseen_questions` splits.\n\n## Training procedure\n\nThe [Trainer API](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainer) was used to fine-tune the model. The code utilized for pre-processing and training was mostly adapted from the [summarization script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) made available by HuggingFace.\n\nTraining was completed in a little over 1 hour on a GPU on Google Colab.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- num_epochs: 9\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- learning_rate: 6e-05\n- lr_scheduler_type: linear\n- train_batch_size: 1\n- gradient_accumulation_steps: 4\n- eval_batch_size: 4\n- mixed_precision_training: Native AMP\n- seed: 42\n\n### Framework versions\n\n- Transformers 4.26.0\n- Pytorch 1.13.1+cu116\n- Datasets 2.9.0\n- Tokenizers 0.13.2\n\n## Evaluation results\n\nThe generated feedback was evaluated through means of the [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu), [ROUGE-2](https://huggingface.co/spaces/evaluate-metric/rouge), [METEOR](https://huggingface.co/spaces/evaluate-metric/meteor), [BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore) metrics from HuggingFace, while the [Root Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error) loss from scikit-learn was used for evaluation of the predicted scores in relation to the golden label scores.\n\nThe following results were achieved.\n\n| Split                 | SacreBLEU | ROUGE-2 | METEOR | BERTScore | RMSE  |\n| --------------------- | :-------: | :-----: | :----: | :-------: | :---: |\n| test_unseen_answers   | 39.4\t    | 42.3    | 54.3   | 52.6      | 0.190 |\n| test_unseen_questions | 2.8       | 5.0     | 17.9   | 10.7      | 0.317 |\n\nThe script used to compute these metrics and perform evaluation can be found in the `evaluation.py` file in this repository.\n\n## Usage\n\nThe example below shows how the model can be applied to generate feedback to a given answer.\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained('Short-Answer-Feedback/mbart-score-finetuned-saf-legal-domain')\ntokenizer = AutoTokenizer.from_pretrained('Short-Answer-Feedback/mbart-score-finetuned-saf-legal-domain')\n\nexample_input = 'Antwort: Wird sich nicht an die Auflagen gehalten (unzureichende Eigenbem\u00fchung), droht eine Sperrzeit von 1-2 Wochen. Dadurch wird f\u00fcr die genannte zeit keine Leistung gezahlt, die Anspruchsdauer vermindert sich insgesamt. Bei wichtigen Gr\u00fcnden wird die Sperrzeit nicht verordnet. L\u00f6sung: Merkblatt 1 f\u00fcr Arbeitslose, S. 22: Erbringen Sie die Pflichten im Zusammenhang mit den Eigenbem\u00fchungen nicht, nicht rechtzeitig oder nicht vollst\u00e4ndig, tritt eine Sperrzeit (0,75 p) ein. Merkblatt 1 f\u00fcr Arbeitslose, S. 55: Die Dauer einer Sperrzeit bei unzureichenden Eigenbem\u00fchungen betr\u00e4gt zwei Wochen. (0,25 p). Frage: Mit welcher Folge und welcher Dauer m\u00fcssen Sie rechnen, wenn Sie Ihre notwendigen Eigenbem\u00fchungen nicht rechtzeitig oder nicht vollst\u00e4ndig erf\u00fcllen?'\ninputs = tokenizer(example_input, max_length=256, padding='max_length', truncation=True, return_tensors='pt')\n\ngenerated_tokens = model.generate(\n                inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                max_length=128\n            )\noutput = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n```\n\nThe output produced by the model then looks as follows:\n\n```\n0.75 Feedback: Es ist richtig, dass Sie mit einer Sperrzeit rechnen m\u00fcssen, in der Sie keine Leistung bekommen. Die gesetzlich vorgesehene Sperrzeit bei unzureichenden Eigenbem\u00fchungen betr\u00e4gt jedoch zwei Wochen.\n```\n", "size_bytes": "2444583325", "downloads": 7}
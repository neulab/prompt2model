{"pretrained_model_name": "pszemraj/bart-base-code-instructiongen", "description": "---\nlicense:\n- apache-2.0\n- cc-by-nc-4.0\ndatasets: pszemraj/fleece2instructions-codealpaca\ntags:\n- generated_from_trainer\n- instruct\n- instructions\n- code\n- instructiongen\nmetrics:\n- rouge\nlanguage:\n- en\nwidget:\n- text: |\n    git lfs install\n    huggingface-cli lfs-enable-largefiles .\n    git lfs track \"*.bin\"\n    git add .\n    git commit -a -m \"add fp32 chkpt\"\n    git push\n  example_title: bash\n- text: |\n    export interface DocumentParams {\n      pageContent: string;\n\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      metadata: Record<string, any>;\n    }\n\n    /**\n     * Interface for interacting with a document.\n     */\n    export class Document implements DocumentParams {\n      pageContent: string;\n\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      metadata: Record<string, any>;\n\n      constructor(fields?: Partial<DocumentParams>) {\n        this.pageContent = fields?.pageContent ?? this.pageContent;\n        this.metadata = fields?.metadata ?? {};\n      }\n    }\n  example_title: js\n- text: |\n    def merge(left, right):\n        if len(left) == 0:\n            return right\n\n        if len(right) == 0:\n            return left\n\n        result = []\n        index_left = index_right = 0\n\n        while len(result) < len(left) + len(right):\n            if left[index_left] <= right[index_right]:\n                result.append(left[index_left])\n                index_left += 1\n            else:\n                result.append(right[index_right])\n                index_right += 1\n\n            if index_right == len(right):\n                result += left[index_left:]\n                break\n\n            if index_left == len(left):\n                result += right[index_right:]\n                break\n\n        return result\n  example_title: merge\n- text: >\n    import pandas as pd\n\n    import plotly.graph_objects as go\n\n\n    df =\n    pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_apple_stock.csv')\n\n\n    fig = go.Figure(go.Scatter(x = df['AAPL_x'], y = df['AAPL_y'],\n                      name='Share Prices (in USD)'))\n\n    fig.update_layout(title='Apple Share Prices over time (2014)',\n                       plot_bgcolor='rgb(230, 230,230)',\n                       showlegend=True)\n\n    fig.show()\n  example_title: plot\n- text: |\n    from spellchecker import SpellChecker\n\n    spell = SpellChecker()\n\n    def check_word_spelling(word: str):\n        misspelled = spell.unknown([word])\n        return len(misspelled) == 0\n\n    def eval_and_replace(text: str, match_token: str = \"- \"):\n        if match_token not in text:\n            return text\n        else:\n            while True:\n                full_before_text = text.split(match_token, maxsplit=1)[0]\n                before_text = [\n                    char for char in full_before_text.split()[-1] if char.isalpha()\n                ]\n                before_text = \"\".join(before_text)\n                full_after_text = text.split(match_token, maxsplit=1)[-1]\n                after_text = [char for char in full_after_text.split()[0] if char.isalpha()]\n                after_text = \"\".join(after_text)\n                full_text = before_text + after_text\n                if check_word_spelling(full_text):\n                    text = full_before_text + full_after_text\n                else:\n                    text = full_before_text + \" \" + full_after_text\n                if match_token not in text:\n                    break\n            return text\n\n    text = \"I- am- a go- od- boy\"\n    eval_and_replace(text)\n  example_title: spell check\n- text: >\n    import torch\n\n    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\n    checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\n    sequences = [\"I've been waiting for a HuggingFace course my whole life.\",\n    \"So have I!\"]\n\n\n    tokens = tokenizer(sequences, padding=True, truncation=True,\n    return_tensors=\"pt\")\n\n    output = model(**tokens)\n  example_title: model inference\ninference:\n  parameters:\n    max_length: 96\n    num_beams: 4\n---\n\n\n# bart-base-code-instructiongen\n\nUse this text2text model to find out what LLM instructions might be able to generate an arbitary piece of code!\n\nThis model is a fine-tuned version of [facebook/bart-base](https://huggingface.co/facebook/bart-base) on the `pszemraj/fleece2instructions-codealpaca` dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0136\n- Rouge1: 59.9513\n- Rouge2: 33.9118\n- Rougel: 55.7815\n- Rougelsum: 56.9064\n- Gen Len: 29.7146\n\n## Intended uses & limitations\n\n\ud83d\udea8 **note:** as the authors elected to release the [original dataset](https://github.com/sahil280114/codealpaca) under `cc-by-nc`, the license carries over to this model and **cannot be used for commercial activity**. \n\n> This is just a `base` size model, which does a decent job for its size, but is not perfect. For better quality instructions, check out [bart-large](https://huggingface.co/pszemraj/bart-large-code-instructiongen) or fine tune your own larger model on the dataset :)\n\nIntended use: Research on domain adaptation and/or other improvements to LLMs by extending instruction:text data pairs.\n\n## Training and evaluation data\n\nRefer to the linked dataset card for `pszemraj/fleece2instructions-codealpaca` or the [original dataset](https://github.com/sahil280114/codealpaca) repo.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 8e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- gradient_accumulation_steps: 16\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.02\n- num_epochs: 3.0\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.1165        | 1.0   | 281  | 1.1090          | 57.9239 | 31.9259 | 53.8737 | 54.9811   | 28.2924 |\n| 1.0763        | 2.0   | 563  | 1.0267          | 59.9605 | 34.0298 | 55.7523 | 56.8021   | 29.6966 |\n| 0.9595        | 2.99  | 843  | 1.0136          | 59.9513 | 33.9118 | 55.7815 | 56.9064   | 29.7146 |", "size_bytes": "557971229", "downloads": 6}
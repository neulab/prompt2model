{"pretrained_model_name": "gaussalgo/mt5-base-priming-QA_en-cs", "description": "---\ntags:\n- generation\nlanguage:\n- multilingual\n- cs\n- en\nwidget:\n- text: \"Ot\u00e1zka: Jak\u00fd je d\u016fvod dotazu z\u00e1kazn\u00edka?\\nKontext: Dobr\u00fd den, \u017d\u00e1d\u00e1me zasl\u00e1n\u00ed nov\u00e9 smlouvy kv\u016fli \u0159e\u0161en\u00ed pojistn\u00e9 ud\u00e1losti. Za\u0161lete na tento mail nebo p\u0159\u00edmo do syst\u00e9mu. S pozdravem Petra Hladk\u00e1 | disponentka servisu.\\nOdpov\u011b\u010f: \u0159e\u0161en\u00ed pojistn\u00e9 ud\u00e1losti\\nOt\u00e1zka: Jak\u00fd je d\u016fvod dotazu z\u00e1kazn\u00edka?\\nKontext: Dobr\u00fd den, cht\u011bla bych V\u00e1s po\u017e\u00e1dat o zasl\u00e1n\u00ed kopie technick\u00e9ho pr\u016fkazu z d\u016fvodu jeho ztr\u00e1ty. S pozdravem Milan Tvrd\u00fd.\\nOdpov\u011b\u010f:\"\n  example_title: \"k-shot: Requests (cs)\"\n- text: \"Ot\u00e1zka: Jak\u00e9 schopnosti daly magick\u00e9 p\u0159edm\u011bty Jurovi J\u00e1no\u0161\u00edkovi? \\nKontext: Podle slovensk\u00e9ho lidov\u00e9ho pod\u00e1n\u00ed byl Juro J\u00e1no\u0161\u00edk obda\u0159en magick\u00fdmi p\u0159edm\u011bty (kouzeln\u00e1 vala\u0161ka, \u010darovn\u00fd opasek), kter\u00e9 mu dod\u00e1valy nadp\u0159irozen\u00e9 schopnosti. Okr\u00e1dal p\u0159edev\u0161\u00edm \u0161lechtice, trestal pansk\u00e9 dr\u00e1by a ze sv\u00e9ho lupu vyd\u011bloval \u010d\u00e1st pro chud\u00e9, tedy bohat\u00fdm bral a chud\u00fdm d\u00e1val. \\nOdpov\u011b\u010f:\"\n  example_title: \"0-shot: Answering (cs)\"\n- text: \"Question: What is the score of this review? \\n Context: I did not like the plot at all. Not recommended. \\n Answer: 1 \\n Question: What is the score of this review? \\n Context: I loved the performance. Can\u2019t believe they did not use CGI for the finale. I think it\u2019s my new favourite movie. \\nAnswer: 5 \\nQuestion: Is the score of this review 1, 2, 3, 4 or 5? \\nContext: The beginning was awesome, but at the end it felt a little rushed. I enjoyed the movie, but probably won\u2019t rewatch soon. \\nAnswer:\" \n  example_title: \"k-shot: Reviews (en)\"\n- text: \"Question: What is the customer's name? \\nContext: Origin: Barrack Obama, Customer id: Bill Moe. \\nAnswer: Bill Moe, \\nQuestion: What is the customer's name? \\nContext: Customer id: Barrack Obama, if not deliverable, return to Bill Clinton. \\nAnswer:\" \n  example_title: \"k-shot: Request (en)\"\n---\n\n# mT5-base for Prime Czech+English Generative Question Answering\n\nThis is the [mt5-base](https://huggingface.co/google/mt5-base) model with an LM head for a generation of extractive answers, \ngiven a small set of 2-5 demonstrations (i.e. primes).\n\n## Priming\n\nNote that **this is a priming model** that expects a **set of demonstrations** of your task of interest, \nsimilarly to GPT-3. \nRather than performing well on the conventional question answering, it aims to learn to extrapolate the pattern of given demonstrations\nto novel tasks, such as Named Entity Recognition or Keywords Extraction from a given pattern.\n\n## Data & Training\n**The reproducible training script is available for any use on our [Github](https://github.com/gaussalgo/learning_to_learn)**.\n\nThis model was trained on a combination of [English SQuAD 1.1](https://huggingface.co/datasets/squad) \nand [Czech SQAD 3.0](https://lindat.cz/repository/xmlui/handle/11234/1-3069) \nQuestion Answering datasets.\n\nTo allow the model to rely on a trend given in demonstrations, we've **clustered** the samples by the question-word(s)\nin English SQuAD and by the category in the Czech SQAD and used the examples of the same cluster as the demonstrations\nof the task in training. \n\nThe specific algorithm of selection of these demonstrations makes a big difference in the model's ability to extrapolate \nto new tasks and will be shared in the following article; stay tuned!\n\nFor the Czech SQAD 3.0, original contexts (=whole Wikipedia websites) were limited to a maximum of 8000 characters\nper a sequence of prime demonstrations.\nPre-processing script for Czech SQAD is available [here](https://huggingface.co/gaussalgo/xlm-roberta-large_extractive-QA_en-cs/blob/main/parse_czech_squad.py).\n\n\nFor training the model (and hence intended also for the inference), we've used the following patterns of 2-7 demonstrations:\n\nFor English samples:\n\n*input*: \n```\nQuestion: {Q1} Context: {C1} Answer: {A1}, \nQuestion: {Q2} Context: {C2} Answer: {A2}, \n[...possibly more demonstrations...] \n\nQuestion: {Q} Context: {C} Answer:`\n```\n=> *target*: \n```\n{A}\n```\n\nFor Czech samples:\n\n*input*: \n```\nOt\u00e1zka: {Q1} Kontext: {C1} Odpov\u011b\u010f: {A1}, \nOt\u00e1zka: {Q2} Kontext: {C2} Odpov\u011b\u010f: {A2}, \n[...possibly more demonstrations...] \n\nOt\u00e1zka: {Q} Kontext: {C} Odpov\u011b\u010f:`\n```\n=> *target*: \n```\n{A}\n```\n\n\nThe best checkpoint was picked to maximize the model's zero-shot performance on Named Entity Recognition \non the out-of-distribution domain of texts and labels.\n\n## Intended uses & limitations\n\nThis model is purposed for a few-shot application on any text extraction task in English and Czech, where the prompt can be stated\nas a natural question. E.g to use this model for extracting the entities of customer names from the text, \nprompt it with demonstrations in the following format: \n \n```python\ninput_text = \"\"\"\n    Question: What is the customer's name? \n    Context: Origin: Barrack Obama, Customer id: Bill Moe. \n    Answer: Bill Moe, \n    Question: What is the customer's name? \n    Context: Customer id: Barrack Obama, if not deliverable, return to Bill Clinton. \n    Answer:\"\"\"\n```\n\nNote that despite its size, English SQuAD has a variety of reported biases, \nconditioned by the relative position or type of the answer in the context that can affect the model's performance on new data \n(see, e.g. [L. Mikula (2022)](https://is.muni.cz/th/adh58/?lang=en), Chap. 4.1).\n\n## Usage\n\nHere is how to use this model to answer the question on a given context using \ud83e\udd17 Transformers in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"gaussalgo/mt5-base-priming-QA_en-cs\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"gaussalgo/mt5-base-priming-QA_en-cs\")\n\n# For the expected format of input_text, see Intended use above\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**inputs)\n\nprint(\"Answer:\")\nprint(tokenizer.decode(outputs))\n```\n", "size_bytes": "2329696205", "downloads": 5}
{"pretrained_model_name": "IDEA-CCNL/Randeng-MegatronT5-770M", "description": "---\nlanguage: \n  - zh\nlicense: apache-2.0\n\ninference: false\n\n---\n# Randeng-MegatronT5-770M\n\n- Main Page:[Fengshenbang](https://fengshenbang-lm.com/)\n- Github: [Fengshenbang-LM](https://github.com/IDEA-CCNL/Fengshenbang-LM)\n\n## \u7b80\u4ecb Brief Introduction\n\n\u5584\u4e8e\u5904\u7406NLT\u4efb\u52a1\uff0c\u4e2d\u6587\u7248\u7684T5-large\u3002\n\nGood at solving NLT tasks, Chinese T5-large.\n\n## \u6a21\u578b\u5206\u7c7b Model Taxonomy\n\n|  \u9700\u6c42 Demand  | \u4efb\u52a1 Task       | \u7cfb\u5217 Series      | \u6a21\u578b Model    | \u53c2\u6570 Parameter | \u989d\u5916 Extra |\n|  :----:  | :----:  | :----:  | :----:  | :----:  | :----:  |\n| \u901a\u7528 General | \u81ea\u7136\u8bed\u8a00\u8f6c\u6362 NLT | \u71c3\u706f Randeng | MegatronT5 |      770M      |     \u4e2d\u6587-Chinese    |\n\n## \u6a21\u578b\u4fe1\u606f Model Information\n\n\u4e3a\u4e86\u5f97\u5230\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u4e2d\u6587\u7248\u7684T5\uff0c\u6211\u4eec\u4f7f\u7528\u4e86Megatron-LM\u7684\u65b9\u6cd5\u548c\u609f\u9053\u8bed\u6599\u5e93(180G\u7248\u672c)\u7528\u4e8e\u9884\u8bad\u7ec3\u3002\u5177\u4f53\u5730\uff0c\u6211\u4eec\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u4e2d\u4f7f\u7528\u4e86[Megatron-LM](https://github.com/NVIDIA/Megatron-LM) \u5927\u6982\u82b1\u8d39\u4e8616\u5f20A100\u7ea614\u5929\u3002\n\nTo get a large-scale Chinese T5, we use of [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) and WuDao Corpora (180 GB version) for pre-training. Specifically, in the pre-training phase which cost about 14 days with 16 A100 GPUs.\n\n## \u4f7f\u7528 Usage\n\n\u56e0\u4e3a[transformers](https://github.com/huggingface/transformers)\u5e93\u4e2d\u662f\u6ca1\u6709Randeng-MegatronT5-770M\u76f8\u5173\u7684\u6a21\u578b\u7ed3\u6784\u7684\uff0c\u6240\u4ee5\u4f60\u53ef\u4ee5\u5728\u6211\u4eec\u7684[Fengshenbang-LM](https://github.com/IDEA-CCNL/Fengshenbang-LM)\u4e2d\u627e\u5230\u5e76\u4e14\u8fd0\u884c\u4ee3\u7801\u3002\n\nSince there is no structure of Randeng-MegatronT5-770M in [transformers library](https://github.com/huggingface/transformers), you can find the structure of Randeng-MegatronT5-770M and run the codes in [Fengshenbang-LM](https://github.com/IDEA-CCNL/Fengshenbang-LM).\n\n ```shell\n git clone https://github.com/IDEA-CCNL/Fengshenbang-LM.git\n ```\n\n### \u52a0\u8f7d\u6a21\u578b Loading Models\n\n```python\nfrom fengshen import T5ForConditionalGeneration\nfrom fengshen import T5Config\nfrom fengshen import T5Tokenizer\n\ntokenizer = T5Tokenizer.from_pretrained('IDEA-CCNL/Randeng-MegatronT5-770M')\nconfig = T5Config.from_pretrained('IDEA-CCNL/Randeng-MegatronT5-770M')\nmodel = T5ForConditionalGeneration.from_pretrained('IDEA-CCNL/Randeng-MegatronT5-770M')\n```\n\n## \u5f15\u7528 Citation\n\n\u5982\u679c\u60a8\u5728\u60a8\u7684\u5de5\u4f5c\u4e2d\u4f7f\u7528\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684[\u8bba\u6587](https://arxiv.org/abs/2209.02970)\uff1a\n\nIf you are using the resource for your work, please cite the our [paper](https://arxiv.org/abs/2209.02970):\n\n```text\n@article{fengshenbang,\n  author    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},\n  title     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},\n  journal   = {CoRR},\n  volume    = {abs/2209.02970},\n  year      = {2022}\n}\n```\n\n\u4e5f\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684[\u7f51\u7ad9](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n\nYou can also cite our [website](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n\n```text\n@misc{Fengshenbang-LM,\n  title={Fengshenbang-LM},\n  author={IDEA-CCNL},\n  year={2021},\n  howpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},\n}\n```\n", "size_bytes": "1455763379", "downloads": 6}
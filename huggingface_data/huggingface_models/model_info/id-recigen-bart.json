{"pretrained_model_name": "haryoaw/id-recigen-bart", "description": "---\nlanguage: id\ntags:\n- bart\n- id\nlicense: mit\n---\n\n# Indonesia Recipe Ingredients Generator Model\n\n**WARNING: inference on Huggingface might not run since the tokenizer used is not transformers's tokenizer.**\n\nFeel free to test the model [in this space](https://huggingface.co/spaces/haryoaw/id-recigen)\n\n\ud83d\ude0e **Have fun on generating ingredients** \ud83d\ude0e\n\nThis is a fine-tuned model to generate the Indonesian food ingredients. One of my personal project that I did in my free time.\n\nBasically, you give the name of the food and it will produce the ingredients of the food.\n\n## Model\n\nData: [Indonesian Recipe Data on Kaggle](https://www.kaggle.com/datasets/canggih/indonesian-food-recipes)  \nPre-trained Model: [IndoBART-v2](https://huggingface.co/indobenchmark/indobart-v2)\n\n## How to use\n\nWe will specify the usage of the tokenizer and the model.\n\n### Tokenizer\n\nSince we use `indobart-v2`, we need to use their tokenizer.\n\nFirst, install the tokenizer by doing `pip install indobenchmark-toolkit`. \n\n\n\n\nAfter that, you can load the tokenizer:\n\n```python\nfrom indobenchmark.tokenization_indonlg import IndoNLGTokenizer\n\ntokenizer = IndoNLGTokenizer.from_pretrained(\"haryoaw/id-recigen-bart\")\n```\n\n**EDIT**: \n\nSeems like the tokenizer in the package is not the same as the one that I use to finetune the model.\nThere are some noticeable bug such as some subword tokens are not considered as subword. Nevertheless, it stil works!\n\n### Model\n\nThe model can be loaded by using AutoModel.\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"haryoaw/id-recigen-bart\")\n```\n\n\n## Input Example\n\nMake sure to input a **LOWERCASE** food name. The tokenizer is case-sensitive!\n\n```\nsayur asam\n```\n\n```\nnasi goreng ayam\n```\n\n~To be continued..\n\n\n", "size_bytes": "526426993", "downloads": 8}
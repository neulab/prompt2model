{"pretrained_model_name": "neulab/reatt-large-nq-bioasq", "description": "---\nlanguage: en\ntags:\n- question-answering\n---\n\n# ReAtt\nReAtt is a retrieval-augmented model for knowledge-intensive tasks proposed in [Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer](https://arxiv.org/pdf/2212.02027.pdf). The original Github repository is [https://github.com/jzbjyb/ReAtt](https://github.com/jzbjyb/ReAtt).\n\n## Description\n`neulab/reatt-large-nq-bioasq` (based on T5 architecture) is initialized with `neulab/reatt-large-nq` and adapted on BioASQ dataset with end-to-end retrieval-augmented training.\n\n## Usage\nPlease refer to [https://github.com/jzbjyb/ReAtt](https://github.com/jzbjyb/ReAtt) for instructions to use this model.\n\n## Reference\n\n```bibtex\n@inproceedings{jiang-etal-2022-reatt,\n    title = {Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer},\n    author = {Zhengbao Jiang and Luyu Gao and Jun Araki and Haibo Ding and Zhiruo Wang and Jamie Callan and Graham Neubig},\n    booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n    address = {Abu Dhabi, UAE},\n    month = {December},\n    year = {2022}\n}\n```\n", "size_bytes": "3132795021", "downloads": 8}
{"pretrained_model_name": "everdoubling/byt5-Korean-base", "description": "---\ndatasets:\n- mc4\nlicense: apache-2.0\n---\n\n# ByT5-Korean - base\n\nByT5-Korean is a Korean specific extension of Google's [ByT5](https://github.com/google-research/byt5).\n\nA Korean syllable has three components (called Jamo): a beginning consonant, a middle vowel, and an optional final consonant; they are like individual characters of alphabet.\nWhile the ByT5's utf-8 encoding allows generic encoding for multiple languages, it is unnatural for Korean because it splits the bits representation of each Jamo in the middle.\n\nByT5-Korean extends ByT5's utf-8 encoding with special care for Korean syllables; each Jamo is represented with a extra token.\nByT5-Korean was pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) with 70% Korean and 30% English.\n\n## Encoding Scheme\n```text\nid: token\n0: <pad>\n1: <eos>\n2: <unk>\n3~258: utf-8 encoding\n259~277: beginning consonants(\ucd08\uc131), 19\uac1c(\u3131\u3132\u3134\u3137\u3138\u3139\u3141\u3142\u3143\u3145\u3146\u3147\u3148\u3149\u314a\u314b\u314c\u314d\u314e)\n278~298: middle vowel(\uc911\uc131), 21\uac1c(\u314f\u3150\u3151\u3152\u3153\u3154\u3155\u3156\u3157\u3158\u3159\u315a\u315b\u315c\u315d\u315e\u315f\u3160\u3161\u3162\u3163)\n299~326: final consonant(\uc885\uc131), \ubb34\uc885\uc131+27\uac1c(\u3131\u3132\u3133\u3134\u3135\u3136\u3137\u3139\u313a\u313b\u313c\u313d\u313e\u313f\u3140\u3141\u3142\u3144\u3145\u3146\u3147\u3148\u314a\u314b\u314c\u314d\u314e)\n327~384: from <extra_id_0> to <extra_id_57>\n```\n\n## Example Inference\n\n```python\nimport torch\nfrom tokenizer import ByT5KoreanTokenizer # https://huggingface.co/everdoubling/byt5-Korean-base/blob/main/tokenizer.py\nfrom transformers import T5ForConditionalGeneration\n\ntokenizer_jamo = ByT5KoreanTokenizer()\nmodel = T5ForConditionalGeneration.from_pretrained('everdoubling/byt5-Korean-base')\n\ninput_sentence = '\ud55c\uad6d\uc5b4 \uc704\ud0a4\ubc31\uacfc(\uc601\uc5b4: Korean Wikipedia)\ub294 \ud55c\uad6d\uc5b4\ub85c \uc6b4\uc601\ub418\ub294 \uc704\ud0a4\ubc31\uacfc\uc758 \ub2e4\uc5b8\uc5b4\ud310 \uac00\uc6b4\ub370 \ud558\ub098\ub85c\uc11c, 2002\ub144 10\uc6d4 11\uc77c\uc5d0 <extra_id_0>. \ub610\ud55c \ud604\uc7ac \ud55c\uad6d\uc5b4 \uc704\ud0a4\ubc31\uacfc\uc5d0\ub294 \ub118\uaca8\uc8fc\uae30, \ud1a0\ub860, \uadf8\ub9bc \ub4f1 \ud398\uc774\uc9c0\ub85c \ubd88\ub9ac\ub294 \ubaa8\ub4e0 \ubb38\uc11c\ub97c \ud3ec\ud568\ud558\uba74 \ucd1d 2,629,860\uac1c\uac00 <extra_id_1>\ub418\uc5b4 \uc788\uc73c\uba70, \ub118\uaca8\uc8fc\uae30\ub97c \ud3ec\ud568\ud55c \uc77c\ubc18 \ubb38\uc11c \uc218\ub294 1,278,560\uac1c,[1] \uadf8\uc911 \ub118\uaca8\uc8fc\uae30, \ub9c9\ub2e4\ub978 \ubb38\uc11c\ub97c \uc81c\uc678\ud55c \uc77c\ubc18 \ubb38\uc11c \uc218\ub294 573,149\uac1c\uc774\ub2e4.'\n\ninput_ids_jamo = tokenizer_jamo(input_sentence).input_ids\noutputs_jamo = model_jamo.generate(torch.tensor([input_ids_jamo]))\nprint(tokenizer_jamo.decode(outputs_jamo[0]))\n# <pad><extra_id_0>\uc124\ub9bd\ub418\uc5c8\ub2e4<extra_id_1>\u0111\u011b\n```\n\nAdditional information coming soon...\n", "size_bytes": "1163426763", "downloads": 20}
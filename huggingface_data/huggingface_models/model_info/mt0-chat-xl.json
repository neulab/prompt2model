{"pretrained_model_name": "theblackcat102/mt0-chat-xl", "description": "---\nlicense: mit\n---\n\nAsymmetric version of [bigscience's mt0-xl model](https://huggingface.co/bigscience/mt0-xl), which trim down like [Meena chatbot](https://arxiv.org/pdf/2001.09977.pdf) from Google. An interesting aspect of Meena is that they use a small encoder, big decoder architecture.\n\n> Meena has a single Evolved Transformer encoder block and 13 Evolved Transformer decoder blocks, as illustrated below. The encoder is responsible for processing the conversation context to help Meena understand what has already been said in the conversation. The decoder then uses that information to formulate an actual response. Through tuning the hyper-parameters, we discovered that a more powerful decoder was the key to higher conversational quality. \n\nThis trimmed version of model is to serve as a pretrained version of Meena like architecture for future research into this arch design idea.\n\nHere's how the model are trimmed:\n\n## 1. Vocab truncation\n\nThe tokenizer is passed through a list of corpus multilingual dialogue and only the set of token ids used are kept. We then remap the tokenizer and embedding to trim down about 40% of embedding weights. Do note that the byte level token were still kept, so this tokenizer should still be able to handle unseen characters.\n\n\n## 2. Encoder trunction\n\n### Model restructuring\n\nUsing the trimmed down version of mt0 model, we then cut the encoder layers to 4 layers only ( making it about 1:6 encoder-decoder ratio ). Ideally we only want to kept 2 layers of encoder but I find it to be too weak in the later stage.\n\n### Model reinitialization\n\nBecause the new encoder has a different output embeddings than the old ones, we need to do a retraining using the original encoder as the teacher. In this project, we simply use the output features of the original encoder as the latent ground truth and the new smaller encoder task is to fit the ground truth latent via MAE loss.\n\nAll models are trained until the loss curve plateau and no longer improves.\n\n\n# Result\n\n\nNo reinitialization phase:\n\n```\ninput :what is one plus one?</s>\ntrimmed output : Extendeds\n\ninput :\u4f60\u60f3\u77e5\u9053\u95dc\u65bc\u6211\u7684\u4ec0\u9ebc?</s>\ntrimmed output : \u4f60\u4e2a\u670b\u53cb\n\ninput :\u3053\u3093\u306b\u3061\u306f!\u304a\u5143\u6c17</s>\ntrimmed output : !- -_n_wip-------------D2\n```\n\nWith reinitialization phase:\n\n```\ninput :what is one plus one?</s>\ntrimmed output : hundred\n\ninput :\u4f60\u60f3\u77e5\u9053\u95dc\u65bc\u6211\u7684\u4ec0\u9ebc?</s>\ntrimmed output : \u4f60\u5011?\u6211\u5c0d\u55ce?\u9053\u4e0d\u90a3\u505a\u7684\u5c0d\u8aaa\u5c0d\u9019\u505a\u4e86\u53d7\u95dc\u6c92?\n\ninput :\u3053\u3093\u306b\u3061\u306f!\u304a\u5143\u6c17</s>\ntrimmed output : !,\n```\n\nnote that it's impossible to have the performance of the orignal model since roughly 30% of the weights were trimmed away.", "size_bytes": 10473660416, "downloads": 6}
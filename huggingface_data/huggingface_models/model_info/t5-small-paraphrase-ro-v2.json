{"pretrained_model_name": "BlackKakapo/t5-small-paraphrase-ro-v2", "description": "---\nannotations_creators: []\nlanguage:\n- ro\nlanguage_creators:\n- machine-generated\nlicense: \n- apache-2.0\nmultilinguality:\n- monolingual\npretty_name: BlackKakapo/t5-small-paraphrase-ro\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- original\ntags: []\ntask_categories:\n- text2text-generation\ntask_ids: []\n---\n# Romanian paraphrase\n\n![v2.0](https://img.shields.io/badge/V.2-17.08.2022-brightgreen)\n\nFine-tune t5-small-paraphrase-ro model for paraphrase. Since there is no Romanian dataset for paraphrasing, I had to create my own [dataset](https://huggingface.co/datasets/BlackKakapo/paraphrase-ro-v2). The dataset contains ~30k examples.\n\n### How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"BlackKakapo/t5-small-paraphrase-ro-v2\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"BlackKakapo/t5-small-paraphrase-ro-v2\")\n```\n\n### Or\n\n```python\nfrom transformers import T5ForConditionalGeneration, T5TokenizerFast \n\nmodel = T5ForConditionalGeneration.from_pretrained(\"BlackKakapo/t5-small-paraphrase-ro-v2\")\ntokenizer = T5TokenizerFast.from_pretrained(\"BlackKakapo/t5-small-paraphrase-ro-v2\")\n```\n\n### Generate\n\n```python\ntext = \"Am impresia c\u0103 fac multe gre\u0219eli.\"\n\nencoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n\nbeam_outputs = model.generate(\n    input_ids=input_ids, \n    attention_mask=attention_masks,\n    do_sample=True,\n    max_length=256,\n    top_k=20,\n    top_p=0.9,\n    early_stopping=False,\n    num_return_sequences=5\n)\n\nfinal_outputs = []\n\nfor beam_output in beam_outputs:\n    text_para = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    \n    if text.lower() != text_para.lower() or text not in final_outputs:\n        final_outputs.append(text_para)\n        \n\nprint(final_outputs)      \n```\n### Output\n\n```out\n['Am impresia c\u0103 fac multe erori.']\n```", "size_bytes": "242083771", "downloads": 2}
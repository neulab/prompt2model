{"pretrained_model_name": "intm/codet5-small-go_generation", "description": "---\nlicense: apache-2.0\n---\n\n# CodeT5-small-Go_generation\nThis model is finetuned based on the pre-trained [CodeT5-small model](https://github.com/salesforce/CodeT5#fine-tuning). \nThis model is fine-tuned on dataset: [codet5_go-generation](https://huggingface.co/datasets/intm/codet5_go-generation).\n\n> 5.3 upload the initial version.\n> 5.6 upload the dataset\n\nThe model genarates the missing function body according to the input which privides the necessary class environment and an empty function.\n\nSee example below for formatting.\n \n# How to use\nHere is how to use this model:\n\n```\nfrom transformers import T5ForConditionalGeneration, RobertaTokenizer\n\n# \u52a0\u8f7d\u6a21\u578b\u548ctokenizer\nmodel_path = \"intm/codet5-small-go_generation\"\ntokenizer = RobertaTokenizer.from_pretrained('intm/codet5-small-go_generation')\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\n\n# \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u63a8\u7406\ninput_text = \"package names\\n\\nimport \\\"knative.dev/pkg/kmeta\\\"\\n\\n\\nfunc Deployment(rev kmeta.Accessor) string {\\n\\treturn kmeta.ChildName(rev.GetName(), \\\"-deployment\\\")\\n}\\n\\n\\nfunc ImageCache(rev kmeta.Accessor) string {\\n\\treturn kmeta.ChildName(rev.GetName(), \\\"-cache\\\")\\n}\\n\\n\\n\\n\\nfunc PA(rev kmeta.Accessor) string\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutput = model.generate(input_ids=input_ids, max_new_tokens=256)  #\u6700\u5927\u957f\u5ea6\u6309\u7167\u6570\u636e\u96c6\u7684max_trg_len\u8bbe\u7f6e\n\n# \u5c06\u751f\u6210\u7684\u7ed3\u679c\u8f6c\u6362\u4e3a\u5b57\u7b26\u4e32\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(output_text)\n\n\n# this prints \"return kmeta.ChildName(rev.GetName(), \"-pa\")\"\n```\n\n# Training data\nYinShicheng\n\n# Training process\nGuQiuhan\n\n# Advisor\nProf.WangYu\n\n# Evaluation results\nTODO\n", "size_bytes": "242029051", "downloads": 9}
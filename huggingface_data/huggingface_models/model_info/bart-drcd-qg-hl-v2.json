{"pretrained_model_name": "p208p2002/bart-drcd-qg-hl-v2", "description": "---\ndatasets:\n- drcd\ntags:\n- question-generation\nwidget:\n- text: \"[HL]\u4f0a\u9686\u00b7\u91cc\u592b\u00b7\u99ac\u65af\u514b[HL]\u662f\u4e00\u540d\u4f01\u696d\u5bb6\u548c\u5546\u696d\u5927\u4ea8\"\n---\n# Transformer QG on DRCD\n\u8acb\u53c3\u95b1 https://github.com/p208p2002/Transformer-QG-on-DRCD \u7372\u5f97\u66f4\u591a\u7d30\u7bc0\n\nThe inputs of the model refers to \n```\nwe integrate C and A into a new C' in the following form.\nC' = [c1, c2, ..., [HL], a1, ..., a|A|, [HL], ..., c|C|]\n```\n> Proposed by [Ying-Hong Chan & Yao-Chung Fan. (2019). A Re-current BERT-based Model for Question Generation.](https://www.aclweb.org/anthology/D19-5821/)\n\n## Features\n- Fully pipline from fine-tune to evaluation\n- Support most of state of the art models\n- Fast deploy as a API server\n\n## DRCD dataset\n[\u53f0\u9054\u95b1\u8b80\u7406\u89e3\u8cc7\u6599\u96c6 Delta Reading Comprehension Dataset (DRCD)](https://github.com/DRCKnowledgeTeam/DRCD) \u5c6c\u65bc\u901a\u7528\u9818\u57df\u7e41\u9ad4\u4e2d\u6587\u6a5f\u5668\u95b1\u8b80\u7406\u89e3\u8cc7\u6599\u96c6\u3002 DRCD\u8cc7\u6599\u96c6\u5f9e2,108\u7bc7\u7dad\u57fa\u689d\u76ee\u4e2d\u6574\u7406\u51fa10,014\u7bc7\u6bb5\u843d\uff0c\u4e26\u5f9e\u6bb5\u843d\u4e2d\u6a19\u8a3b\u51fa30,000\u591a\u500b\u554f\u984c\u3002\n\n## Available models\n- BART (base on **[uer/bart-base-chinese-cluecorpussmall](https://huggingface.co/uer/bart-base-chinese-cluecorpussmall)**)\n\n## Expriments\nModel             |Bleu 1|Bleu 2|Bleu 3|Bleu 4|METEOR|ROUGE-L|\n------------------|------|------|------|------|------|-------|\nBART-HLSQG        |34.25 |27.70 |22.43 |18.13 |23.58 |36.88  |\nBART-HLSQG-v2     |39.30 |32.51 |26.72 |22.08 |24.94 |41.18  |\n\n## Environment requirements\nThe hole development is based on Ubuntu system\n\n1. If you don't have pytorch 1.6+ please install or update first\n> https://pytorch.org/get-started/locally/\n\n2. Install packages `pip install -r requirements.txt`\n\n3. Setup scorer `python setup_scorer.py`\n\n5. Download dataset `python init_dataset.py`\n\n\n## Training\n### Seq2Seq LM\n```\nusage: train_seq2seq_lm.py [-h]\n                           [--base_model {facebook/bart-base,facebook/bart-large,t5-small,t5-base,t5-large}]\n                           [-d {squad,squad-nqg}] [--epoch EPOCH] [--lr LR]\n                           [--dev DEV] [--server] [--run_test]\n                           [-fc FROM_CHECKPOINT]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --base_model {facebook/bart-base,facebook/bart-large,t5-small,t5-base,t5-large}\n  -d {squad,squad-nqg}, --dataset {squad,squad-nqg}\n  --epoch EPOCH\n  --lr LR\n  --dev DEV\n  --server\n  --run_test\n  -fc FROM_CHECKPOINT, --from_checkpoint FROM_CHECKPOINT\n```\n\n## Deploy\n### Start up\n```\npython train_seq2seq_lm.py --server --base_model YOUR_BASE_MODEL --from_checkpoint FROM_CHECKPOINT\n```\n### Request example\n```\ncurl --location --request POST 'http://127.0.0.1:5000/' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data-urlencode 'context=[HL]\u4f0a\u9686\u00b7\u91cc\u592b\u00b7\u99ac\u65af\u514b[HL]\u662f\u4e00\u540d\u4f01\u696d\u5bb6\u548c\u5546\u696d\u5927\u4ea8'\n```\n```json\n{\"predict\": \"\u54ea\u4e00\u500b\u4eba\u662f\u4e00\u540d\u4f01\u696d\u5bb6\u548c\u5546\u696d\u5927\u4ea8?\"}\n```\n\n", "size_bytes": "465197369", "downloads": 48}
{"pretrained_model_name": "HillZhang/pseudo_native_bart_CGEC", "description": "---\ntags:\n- text2text-generation\n- Chinese\n- seq2seq\n- grammar\nlanguage: zh\nlicense: apache-2.0\n---\n# Pseudo-Native-BART-CGEC\n\nThis model is a cutting-edge CGEC model based on [Chinese BART-large](https://huggingface.co/fnlp/bart-large-chinese).\nIt is trained with about 100M pseudo native speaker CGEC training data generated by heuristic rules.\nMore details can be found in our [Github](https://github.com/HillZhang1999/NaSGEC) and the [paper](https://arxiv.org/pdf/2305.16023.pdf).\n\n## Usage\n\npip install transformers\n\n```\nfrom transformers import BertTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline\ntokenizer = BertTokenizer.from_pretrained(\"HillZhang/pseudo_native_bart_CGEC\")\nmodel = BartForConditionalGeneration.from_pretrained(\"HillZhang/pseudo_native_bart_CGEC\")\nencoded_input = tokenizer([\"\u5317\u4eac\u662f\u4e2d\u56fd\u7684\u90fd\u3002\", \"\u4ed6\u8bf4\uff1a\u201d\u6211\u6700\u7231\u7684\u8fd0\u52a8\u662f\u6253\u84dd\u7403\u201c\", \"\u6211\u6bcf\u5929\u5927\u7ea6\u559d5\u6b21\u6c34\u5de6\u53f3\u3002\", \"\u4eca\u5929\uff0c\u6211\u975e\u5e38\u5f00\u5f00\u5fc3\u3002\"], return_tensors=\"pt\", padding=True, truncation=True)\nif \"token_type_ids\" in encoded_input:\n    del encoded_input[\"token_type_ids\"]\noutput = model.generate(**encoded_input)\nprint(tokenizer.batch_decode(output, skip_special_tokens=True))\n```\n\n## Citation\n\n```\n@inproceedings{zhang-etal-2023-nasgec,\n    title = \"{Na}{SGEC}: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native Speaker Texts\",\n    author = \"Zhang, Yue  and\n      Zhang, Bo  and\n      Jiang, Haochen  and\n      Li, Zhenghua  and\n      Li, Chen  and\n      Huang, Fei  and\n      Zhang, Min\"\n    booktitle = \"Findings of ACL\",\n    year = \"2023\"\n    }\n```", "size_bytes": "1501922415", "downloads": 50}
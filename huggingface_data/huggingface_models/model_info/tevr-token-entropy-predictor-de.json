{"pretrained_model_name": "fxtentacle/tevr-token-entropy-predictor-de", "description": "This repo contains the fully trained ByT5 that was used to estimate per-character entropies. Using it, you can also recreate the illustration in the paper.\n\n## Citation\n\nIf you use this for research, please cite:\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2206.12693,\n  doi = {10.48550/ARXIV.2206.12693},\n  url = {https://arxiv.org/abs/2206.12693},\n  author = {Krabbenh\u00f6ft, Hajo Nils and Barth, Erhardt},  \n  keywords = {Computation and Language (cs.CL), Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, F.2.1; I.2.6; I.2.7},  \n  title = {TEVR: Improving Speech Recognition by Token Entropy Variance Reduction},  \n  publisher = {arXiv},  \n  year = {2022}, \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n\n## Generate TEVR Tokenizer from Text corpus\n(copy of `Generate TEVR Tokenizer.ipynb`)\n\n```python\n# TODO: load large text dataset like OSCAR\nall_sentences_de = [\"\u00dcber vier Jahrzehnte geh\u00f6rte er zu den f\u00fchrenden Bildhauern Niederbayerns\", \"die katze ist niedlich\"] * 1000\n```\n\n\n```python\nfrom huggingface_hub import snapshot_download\ndata_folder = snapshot_download(\"fxtentacle/tevr-token-entropy-predictor-de\")\n```\n\n\n```python\nfrom transformers import T5ForConditionalGeneration\nmodel = T5ForConditionalGeneration.from_pretrained(data_folder)\nmodel.to('cuda')\nmodel.eval()\nNone\n```\n\n\n```python\nimport torch\n\ndef text_to_cross_entropy(text):\n    ttext = torch.tensor([[0]+list(text.encode('UTF-8'))],dtype=torch.int64).to('cuda')\n    tone = torch.tensor([[1]],dtype=torch.int32).to('cuda')\n    logits = model.forward(input_ids=tone, attention_mask=tone, decoder_input_ids=ttext, return_dict=False)[0].detach()\n    cross_entropy = torch.nn.functional.cross_entropy(input=logits[0][:-1], target=ttext[0][1:], reduction='none').detach().cpu().numpy()\n    return cross_entropy\n```\n\n\n```python\ntext = all_sentences_de[0]\ncross_entropy = text_to_cross_entropy(text)\nprint(text)\nfor i in range(len(text)):\n    print(text[i], cross_entropy[i])\n```\n\n    \u00dcber vier Jahrzehnte geh\u00f6rte er zu den f\u00fchrenden Bildhauern Niederbayerns\n    \u00dc 7.254014\n    b 0.17521738\n    e 0.00046933602\n    r 0.01929327\n      0.0003675739\n    v 0.20927554\n    i 6.13207\n    e 0.3896482\n    r 0.009583538\n      2.07364\n    J 0.02978594\n    a 2.483246\n    h 0.1591908\n    r 0.0045124847\n    z 0.00028653807\n    e 4.0242333\n    h 0.031035878\n    n 0.028907888\n    t 0.003264101\n    e 0.0018929198\n      0.05816966\n    g 1.2782481\n    e 3.5076692\n    h 0.694337\n    \u00f6 0.5319732\n    r 0.48336726\n    t 0.0050443523\n    e 0.0017187123\n      0.14511283\n    e 1.0435015\n    r 0.18165778\n      1.0247636\n    z 0.3594512\n    u 0.0077577736\n      2.072764\n    d 0.17377533\n    e 1.0727838\n    n 1.2805216\n      0.24939628\n    f 0.27717885\n    \u00fc 0.012466482\n    h 4.4356546\n    r 1.7371752\n    e 0.051492628\n    n 2.99407\n    d 0.009648594\n    e 0.19667451\n    n 0.007495021\n      0.2529005\n    B 0.004451485\n    i 0.024661187\n    l 0.0028436247\n    d 2.6620464\n    h 2.825038\n    a 0.8215449\n    u 0.011406565\n    e 2.9599652\n    r 0.45834702\n    n 0.11848967\n      0.5955992\n    N 0.010709903\n    i 1.5338714\n    e 0.1834471\n    d 5.668945\n    e 2.052247\n    r 0.7692907\n    b 0.0675718\n    a 0.028234791\n    y 0.0045266068\n    e 4.1125383\n    r 1.2630856\n    n 5.436057\n    s 0.46446246\n\n\n\n```python\nfrom tqdm import tqdm \n\nsentence_data = all_sentences_de\n\ntext_and_entropies = []\nfor text in tqdm(sentence_data):\n    text_and_entropies.append([text,text_to_cross_entropy(text)])\n```\n\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:09<00:00, 219.00it/s]\n\n\n\n```python\nfrom collections import Counter\n\n# 4s\n#target_lengths = [1]\n#token_budgets = [36]\n\n# 4m\ntarget_lengths = [4,3,2,1]\ntoken_budgets = [40,80,96,36]\n\n# 4l\n#target_lengths = [4,3,2,1]\n#token_budgets = [384,320,160,36]\n\nngrams = [Counter() for l in target_lengths]\ntokens = []\n\nfor tgi,tgl in enumerate(target_lengths):\n    for row in tqdm(text_and_entropies[1:]):\n        use_text = row[0]\n        use_scores = row[1]\n        for t in tokens:\n            use_text = use_text.replace(t[0],'#')\n        candidates = []\n        for i in range(len(use_text)-(tgl-1)):\n            part = use_text[i:i+tgl].lower()\n            if '#' in part: continue\n            if ' ' in part: continue\n            if '-' in part: continue\n            score = sum(use_scores[i:i+tgl])\n            # print(part, score)\n            candidates.append([score, part])\n        candidates.sort(reverse=False)\n        candidates = candidates[:max(1,int(len(candidates)/5))]\n        #print(candidates)\n        ngrams[tgi].update([c[1] for c in candidates])\n    new_tokens = ngrams[tgi].most_common(token_budgets[tgi])\n    print(new_tokens)\n    tokens += new_tokens\n    #break\n```\n\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1999/1999 [00:00<00:00, 14645.88it/s]\n\n\n    [('lich', 1000), ('hnte', 999), ('rbay', 999), ('\u00f6rte', 999), ('h\u00f6rt', 999), ('ahrz', 999), ('jahr', 999), ('bild', 999)]\n\n\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1999/1999 [00:00<00:00, 18574.04it/s]\n\n\n    [('ist', 1000), ('den', 999), ('ber', 999), ('aue', 999), ('ern', 999), ('uer', 999)]\n\n\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1999/1999 [00:00<00:00, 20827.32it/s]\n\n\n    [('ni', 1000), ('ge', 999), ('er', 999), ('f\u00fc', 999), ('vi', 999)]\n\n\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1999/1999 [00:00<00:00, 19927.45it/s]\n\n    [('e', 2999), ('u', 999), ('n', 999), ('h', 999)]\n\n\n    \n\n\n\n```python\nall_tokens = ['<pad>','<eos>',' ']+[t[0] for t in tokens]+['?']\nprint(len(all_tokens), all_tokens)\n```\n\n    27 ['<pad>', '<eos>', ' ', 'lich', 'hnte', 'rbay', '\u00f6rte', 'h\u00f6rt', 'ahrz', 'jahr', 'bild', 'ist', 'den', 'ber', 'aue', 'ern', 'uer', 'ni', 'ge', 'er', 'f\u00fc', 'vi', 'e', 'u', 'n', 'h', '?']\n\n\n\n```python\nimport json\nwith open('./tevr-tokenizer.txt','wt') as f:\n    json.dump(all_tokens, f)\n```\n\n\n```python\nimport sys\nimport os\nsys.path.append(data_folder)\nfrom text_tokenizer import HajoTextTokenizer\n```\n\n\n```python\ntext_tokenizer = HajoTextTokenizer('./tevr-tokenizer.txt')\n```\n\n\n```python\nsentence = \"geh\u00f6rte\"\nprint(sentence)\nencoded = text_tokenizer.encode(sentence)\nprint(encoded)\nprint([text_tokenizer.all_tokens[i] for i in encoded])\nprint([text_tokenizer.decode(encoded)])\n```\n\n    geh\u00f6rte\n    [18, 25, 6]\n    ['ge', 'h', '\u00f6rte']\n    ['geh\u00f6rte']\n\n\n\n## Testing Tokenizer File\n(copy of `TEVR Explanation.ipynb`)\n\n```python\nfrom huggingface_hub import snapshot_download\ndata_folder = snapshot_download(\"fxtentacle/tevr-token-entropy-predictor-de\")\n```\n\n\n```python\nfrom transformers import T5ForConditionalGeneration\nmodel = T5ForConditionalGeneration.from_pretrained(data_folder)\nmodel.to('cuda')\nmodel.eval()\nNone\n```\n\n\n```python\nimport torch\n\ndef text_to_cross_entropy(text):\n    ttext = torch.tensor([[0]+list(text.encode('UTF-8'))],dtype=torch.int64).to('cuda')\n    tone = torch.tensor([[1]],dtype=torch.int32).to('cuda')\n    logits = model.forward(input_ids=tone, attention_mask=tone, decoder_input_ids=ttext, return_dict=False)[0].detach()\n    cross_entropy = torch.nn.functional.cross_entropy(input=logits[0][:-1], target=ttext[0][1:], reduction='none').detach().cpu().numpy()\n    return cross_entropy\n```\n\n\n```python\nimport sys\nimport os\nsys.path.append(data_folder)\nfrom text_tokenizer import HajoTextTokenizer\n```\n\n\n```python\ntokenizer_file = 'text-tokenizer-de-4m.txt'\ntext_tokenizer = HajoTextTokenizer(data_folder+'/'+tokenizer_file)\n```\n\n\n```python\ntext = \"die katze ist niedlich\"\ncross_entropy = text_to_cross_entropy(text)\n\ntokens = text_tokenizer.encode(text)\ntokens = [text_tokenizer.all_tokens[t] for t in tokens]\nprint(tokens)\ntoken_sums = []\ntoken_sums2 = []\nfor t in tokens:\n    ce = sum(cross_entropy[len(token_sums):len(token_sums)+len(t)])\n    for r in range(len(t)): token_sums.append(ce  / len(t))\n    token_sums2.append(ce)\nprint(token_sums)\n```\n\n    ['die', ' ', 'k', 'at', 'ze', ' ', 'ist', ' ', 'n', 'ied', 'lich']\n    [3.3762913048267365, 3.3762913048267365, 3.3762913048267365, 0.29695791006088257, 4.193424224853516, 2.3430762887001038, 2.3430762887001038, 2.8417416363954544, 2.8417416363954544, 1.1227068901062012, 2.017452405144771, 2.017452405144771, 2.017452405144771, 0.0016304069431498647, 2.580254554748535, 2.3091587026913962, 2.3091587026913962, 2.3091587026913962, 1.0126478232632508, 1.0126478232632508, 1.0126478232632508, 1.0126478232632508]\n\n\n\n```python\nimport numpy as np\nhtml = '<table style=\"font-size: 20px; font-family: Roboto\">'\nhtml += '<tr><td><b>(1)</b></td>'+''.join([f'<td style=\"text-align:left\">{c}</td>' for c in list(text)])+'</tr>'\nhtml += '<tr><td><b>(2)</b></td>'+''.join(['<td>1.0</td>'.format(v) for v in cross_entropy])+'<td>\u03c3\u00b2={:3.1f}</td>'.format(np.var([1.0 for v in cross_entropy]))+'</tr>'\nhtml += '<tr><td><b>(3)</b></td>'+''.join(['<td>{:3.1f}</td>'.format(v) for v in cross_entropy])+'<td>\u03c3\u00b2={:3.1f}</td>'.format(np.var(cross_entropy))+'</tr>'\nhtml += '<tr><td><b>(4)</b></td>'+''.join([f'<td style=\"text-align:center\" colspan={len(t)}>{t}</td>' for t in tokens])+'</tr>'\nhtml += '<tr><td><b>(5)</b></td>'+''.join([f'<td style=\"text-align:center\" colspan={len(t)}>{\"{:3.1f}\".format(token_sums2[i])}</td>' for i,t in enumerate(tokens)])+'</tr>'\nhtml += '<tr><td><b>(6)</b></td>'+''.join(['<td>{:3.1f}</td>'.format(v) for v in token_sums])+'<td>\u03c3\u00b2={:3.1f}</td>'.format(np.var(token_sums))+'</tr>'\nhtml += '</table>'\n\nimport IPython\nIPython.display.HTML(html)\n```\n\n\n\n\n<table style=\"font-size: 20px; font-family: Roboto\"><tr><td><b>(1)</b></td><td style=\"text-align:left\">d</td><td style=\"text-align:left\">i</td><td style=\"text-align:left\">e</td><td style=\"text-align:left\"> </td><td style=\"text-align:left\">k</td><td style=\"text-align:left\">a</td><td style=\"text-align:left\">t</td><td style=\"text-align:left\">z</td><td style=\"text-align:left\">e</td><td style=\"text-align:left\"> </td><td style=\"text-align:left\">i</td><td style=\"text-align:left\">s</td><td style=\"text-align:left\">t</td><td style=\"text-align:left\"> </td><td style=\"text-align:left\">n</td><td style=\"text-align:left\">i</td><td style=\"text-align:left\">e</td><td style=\"text-align:left\">d</td><td style=\"text-align:left\">l</td><td style=\"text-align:left\">i</td><td style=\"text-align:left\">c</td><td style=\"text-align:left\">h</td></tr><tr><td><b>(2)</b></td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>\u03c3\u00b2=0.0</td></tr><tr><td><b>(3)</b></td><td>8.9</td><td>1.0</td><td>0.2</td><td>0.3</td><td>4.2</td><td>1.6</td><td>3.1</td><td>5.4</td><td>0.3</td><td>1.1</td><td>3.0</td><td>3.0</td><td>0.0</td><td>0.0</td><td>2.6</td><td>0.6</td><td>4.4</td><td>1.9</td><td>4.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>\u03c3\u00b2=5.0</td></tr><tr><td><b>(4)</b></td><td style=\"text-align:center\" colspan=3>die</td><td style=\"text-align:center\" colspan=1> </td><td style=\"text-align:center\" colspan=1>k</td><td style=\"text-align:center\" colspan=2>at</td><td style=\"text-align:center\" colspan=2>ze</td><td style=\"text-align:center\" colspan=1> </td><td style=\"text-align:center\" colspan=3>ist</td><td style=\"text-align:center\" colspan=1> </td><td style=\"text-align:center\" colspan=1>n</td><td style=\"text-align:center\" colspan=3>ied</td><td style=\"text-align:center\" colspan=4>lich</td></tr><tr><td><b>(5)</b></td><td style=\"text-align:center\" colspan=3>10.1</td><td style=\"text-align:center\" colspan=1>0.3</td><td style=\"text-align:center\" colspan=1>4.2</td><td style=\"text-align:center\" colspan=2>4.7</td><td style=\"text-align:center\" colspan=2>5.7</td><td style=\"text-align:center\" colspan=1>1.1</td><td style=\"text-align:center\" colspan=3>6.1</td><td style=\"text-align:center\" colspan=1>0.0</td><td style=\"text-align:center\" colspan=1>2.6</td><td style=\"text-align:center\" colspan=3>6.9</td><td style=\"text-align:center\" colspan=4>4.1</td></tr><tr><td><b>(6)</b></td><td>3.4</td><td>3.4</td><td>3.4</td><td>0.3</td><td>4.2</td><td>2.3</td><td>2.3</td><td>2.8</td><td>2.8</td><td>1.1</td><td>2.0</td><td>2.0</td><td>2.0</td><td>0.0</td><td>2.6</td><td>2.3</td><td>2.3</td><td>2.3</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>\u03c3\u00b2=1.1</td></tr></table>\n\n\n\n\n```python\nfrom text_tokenizer import HajoTextTokenizer\ntext_tokenizer = HajoTextTokenizer(data_folder+'/'+tokenizer_file)\ntt = text_tokenizer.all_tokens\nprint(', '.join(tt))\n```\n\n    <pad>, <eos>,  , chen, sche, lich, isch, icht, iche, eine, rden, tion, urde, haft, eich, rung, chte, ssen, chaf, nder, tlic, tung, eite, iert, sich, ngen, erde, scha, nden, unge, lung, mmen, eren, ende, inde, erun, sten, iese, igen, erte, iner, tsch, keit, der, die, ter, und, ein, ist, den, ten, ber, ver, sch, ung, ste, ent, ach, nte, auf, ben, eit, des, ers, aus, das, von, ren, gen, nen, lle, hre, mit, iel, uch, lte, ann, lie, men, dem, and, ind, als, sta, elt, ges, tte, ern, wir, ell, war, ere, rch, abe, len, ige, ied, ger, nnt, wei, ele, och, sse, end, all, ahr, bei, sie, ede, ion, ieg, ege, auc, che, rie, eis, vor, her, ang, f\u00fcr, ass, uss, tel, er, in, ge, en, st, ie, an, te, be, re, zu, ar, es, ra, al, or, ch, et, ei, un, le, rt, se, is, ha, we, at, me, ne, ur, he, au, ro, ti, li, ri, eh, im, ma, tr, ig, el, um, la, am, de, so, ol, tz, il, on, it, sc, sp, ko, na, pr, ni, si, fe, wi, ns, ke, ut, da, gr, eu, mi, hr, ze, hi, ta, ss, ng, sa, us, ba, ck, em, kt, ka, ve, fr, bi, wa, ah, gt, di, ab, fo, to, rk, as, ag, gi, hn, s, t, n, m, r, l, f, e, a, b, d, h, k, g, o, i, u, w, p, z, \u00e4, \u00fc, v, \u00f6, j, c, y, x, q, \u00e1, \u00ed, \u014d, \u00f3, \u0161, \u00e9, \u010d, ?\n\n\n", "size_bytes": "17603417", "downloads": 2}
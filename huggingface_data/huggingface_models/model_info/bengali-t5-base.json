{"pretrained_model_name": "flax-community/bengali-t5-base", "description": "# bengali-t5-base\n\n**bengali-t5-base** is a model trained on the Bengali portion of MT5 dataset. We used the `T5-base` model for this model.\n\n[Flax/Jax Community Week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), organized by [HuggingFace](https://huggingface.co/) and TPU usage sponsored by Google.\n\nThe model is trained on around ~11B tokens (64 size batch, 512 tokens, 350k steps). \n\n## load tokenizer\n\n```\n>>> tokenizer = transformers.AutoTokenizer.from_pretrained(\"flax-community/bengali-t5-base\")\n>>> tokenizer.encode(\"\u0986\u09ae\u09bf \u09ac\u09be\u0982\u09b2\u09be\u09b0 \u0997\u09be\u09a8 \u0997\u09be\u0987\")\n>>> tokenizer.decode([93, 1912, 814, 5995, 3, 1])\n```\n\n```\n[93, 1912, 814, 5995, 3, 1]\n'\u0986\u09ae\u09bf \u09ac\u09be\u0982\u09b2\u09be\u09b0 \u0997\u09be\u09a8 \u0997\u09be\u0987 </s>'\n```\n\n## load model\n\n```\n>>> config  = T5Config.from_pretrained(\"flax-community/bengali-t5-base\")\n>>> model = FlaxT5ForConditionalGeneration.from_pretrained(\"flax-community/bengali-t5-base\", config=config)\n```\n\nThe model is trained on `de-noising` objectives followed by the script [here](https://huggingface.co/flax-community/bengali-t5-base/blob/main/run_t5_mlm_flax.py) and [here](https://huggingface.co/flax-community/bengali-t5-base/blob/main/run.sh). Currently This model doesn't have any generation capability. If you want this model to have generation capability, please do a finetuning on `prefix-LM` objective mentioned in the [paper](https://arxiv.org/abs/1910.10683). \n\nSee the tensorboard log in `Training metrics` tab.\n\nPlease note that we haven't finetuned the model in any downstream task. \n\n## Proposal\n- [Project Proposal](https://discuss.huggingface.co/t/pretrain-t5-from-scratch-in-bengali/7121)\n\n## Participants\n- [Ibraheem Muhammad Moosa](https://huggingface.co/ibraheemmoosa)\n- [Tasnim Mohiuddin](https://huggingface.co/tasnim)\n- [Khalid Saifullah](https://huggingface.co/khalidsaifullaah)\n- [Tahsin Mayeesha](https://tahsin-mayeesha.github.io/)\n- [M Saiful Bari](https://huggingface.co/sbmaruf)\n\n## Useful links\n- [Community Week timeline](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104#summary-timeline-calendar-6)\n- [Community Week README](https://github.com/huggingface/transformers/blob/master/examples/research_projects/jax-projects/README.md)\n- [Masked Language Modelling example scripts](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling)\n- [Model Repository](https://huggingface.co/flax-community/roberta-base-als-demo)\n", "size_bytes": "990271101", "downloads": 15}
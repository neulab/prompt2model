{"pretrained_model_name": "jpcorb20/pegasus-large-reddit_tifu-samsum-512", "description": "---\nlanguage: \n- en\nthumbnail: \ntags:\n- pytorch\n- google/pegasus-reddit_tifu\n- summarization\n- samsum\nlicense: \ndatasets:\n- samsum\nmetrics:\n- rouge\n---\n\n# Samsum Pegasus (Reddit/TIFU) for conversational summaries\n\n## Model description\n\nPegasus (Reddit/TIFU) for conversational summaries trained on the samsum dataset!\n\n## Training data\n\nThe data is the [samsum](https://huggingface.co/datasets/samsum) dataset for conversional summaries.\n\nThe initial weigths were from the [google/pegasus-reddit_tifu](https://huggingface.co/google/pegasus-reddit_tifu). The hypothesis being that it would help the convergence on the samsum dataset to have weights trained on a larger summarization dataset first like the Reddit TIFU using casual language.\n\n## Training procedure\n\nUsed the example/seq2seq/run_summarization.py script from the transformers source 4.5.0dev0.\n\n  n_epochs: 3,\\\n  batch_size: 4, \\\n  max_source_length: 512,\\\n  max_target_length: 128\n\n## Eval results\n  \n  eval_gen_len: 35.89,\\\n  eval_loss: 1.3807392120361328,\\\n  eval_rouge1: 47.3372,\\\n  eval_rouge2: 24.4728,\\\n  eval_rougeL: 37.9078,\\\n  eval_rougeLsum: 43.5744,\\\n  eval_samples_per_second: 2.814\n  \n## Example\n\n  from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n  \n  model_name = \"jpcorb20/pegasus-large-reddit_tifu-samsum-256\"\n  \n  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n  model = PegasusForConditionalGeneration.from_pretrained(model_name)\n  \n  src_text = \"\"\"Carter: Hey Alexis, I just wanted to let you know that I had a really nice time with you tonight.\\\\r\\\n  Alexis: Thanks Carter. Yeah, I really enjoyed myself as well.\\\\r\\\n  Carter: If you are up for it, I would really like to see you again soon.\\\\r\\\n  Alexis: Thanks Carter, I'm flattered. But I have a really busy week coming up.\\\\r\\\n  Carter: Yeah, no worries. I totally understand. But if you ever want to go grab dinner again, just let me know.\\\\r\\\n  Alexis: Yeah of course. Thanks again for tonight. Carter: Sure. Have a great night.\\\\r\\\n  \"\"\"\n  \n  token_params = dict(max_length=512, truncation=True, padding='longest', return_tensors=\"pt\")\n  batch = tokenizer(src_text, **token_params)\n  \n  translated = model.generate(**batch)\n  \n  decode_params = dict(num_beams=5, min_length=16, max_length=128, length_penalty=2)\n  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True, **decode_params)\n  \n  print(tgt_text)", "size_bytes": "2279647383", "downloads": 14}
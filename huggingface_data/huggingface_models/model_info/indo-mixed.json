{"pretrained_model_name": "CLAck/indo-mixed", "description": "---\nlanguage:\n- en\n- id\ntags:\n- translation\nlicense: apache-2.0\ndatasets:\n- ALT\nmetrics:\n- sacrebleu\n---\n\nThis model is pretrained on Chinese and Indonesian languages, and fine-tuned on Indonesian language.\n\n### Example\n```\n%%capture\n!pip install transformers transformers[sentencepiece]\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n# Download the pretrained model for English-Vietnamese available on the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"CLAck/indo-mixed\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"CLAck/indo-mixed\")\n# Download a tokenizer that can tokenize English since the model Tokenizer doesn't know anymore how to do it\n# We used the one coming from the initial model\n# This tokenizer is used to tokenize the input sentence\ntokenizer_en = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-zh')\n# These special tokens are needed to reproduce the original tokenizer\ntokenizer_en.add_tokens([\"<2zh>\", \"<2indo>\"], special_tokens=True)\n\nsentence = \"The cat is on the table\"\n# This token is needed to identify the target language\ninput_sentence = \"<2indo> \" + sentence \ntranslated = model.generate(**tokenizer_en(input_sentence, return_tensors=\"pt\", padding=True))\noutput_sentence = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n```\n\n### Training results\nMIXED\n\n| Epoch | Bleu    |\n|:-----:|:-------:|\n| 1.0   | 24.2579 |\n| 2.0   | 30.6287 |\n| 3.0   | 34.4417 |\n| 4.0   | 36.2577 |\n| 5.0   | 37.3488 |\n\nFINETUNING\n\n| Epoch | Bleu    |\n|:-----:|:-------:|\n| 6.0   | 34.1676 |\n| 7.0   | 35.2320 |\n| 8.0   | 36.7110 |\n| 9.0   | 37.3195 |\n| 10.0  | 37.9461 |", "size_bytes": "337379461", "downloads": 18}
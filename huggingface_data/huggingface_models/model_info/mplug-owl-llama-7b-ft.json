{"pretrained_model_name": "MAGAer13/mplug-owl-llama-7b-ft", "description": "---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: image-to-text\ntags:\n- mplug-owl\n---\n\n# Usage\n## Get the latest codebase from Github\n```Bash\ngit clone https://github.com/X-PLUG/mPLUG-Owl.git\n```\n\n## Model initialization\n```Python\nfrom mplug_owl.modeling_mplug_owl import MplugOwlForConditionalGeneration\nfrom mplug_owl.tokenization_mplug_owl import MplugOwlTokenizer\nfrom mplug_owl.processing_mplug_owl import MplugOwlImageProcessor, MplugOwlProcessor\n\npretrained_ckpt = 'MAGAer13/mplug-owl-llama-7b'\nmodel = MplugOwlForConditionalGeneration.from_pretrained(\n    pretrained_ckpt,\n    torch_dtype=torch.bfloat16,\n)\nimage_processor = MplugOwlImageProcessor.from_pretrained(pretrained_ckpt)\ntokenizer = MplugOwlTokenizer.from_pretrained(pretrained_ckpt)\nprocessor = MplugOwlProcessor(image_processor, tokenizer)\n```\n\n## Model inference\nPrepare model inputs.\n```Python\n# We use a human/AI template to organize the context as a multi-turn conversation.\n# <image> denotes an image placehold.\nprompts = [\n'''The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\nHuman: <image>\nHuman: Explain why this meme is funny.\nAI: ''']\n\n# The image paths should be placed in the image_list and kept in the same order as in the prompts.\n# We support urls, local file paths and base64 string. You can custom the pre-process of images by modifying the mplug_owl.modeling_mplug_owl.ImageProcessor\nimage_list = ['https://xxx.com/image.jpg']\n```\n\nGet response.\n```Python\n# generate kwargs (the same in transformers) can be passed in the do_generate()\ngenerate_kwargs = {\n    'do_sample': True,\n    'top_k': 5,\n    'max_length': 512\n}\nfrom PIL import Image\nimages = [Image.open(_) for _ in image_list]\ninputs = processor(text=prompts, images=images, return_tensors='pt')\ninputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\nwith torch.no_grad():\n    res = model.generate(**inputs, **generate_kwargs)\nsentence = tokenizer.decode(res.tolist()[0], skip_special_tokens=True)\nprint(sentence)\n```", "size_bytes": 14246146048, "downloads": 188}
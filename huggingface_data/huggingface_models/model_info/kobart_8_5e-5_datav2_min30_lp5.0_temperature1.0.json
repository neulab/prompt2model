{"pretrained_model_name": "nlp04/kobart_8_5e-5_datav2_min30_lp5.0_temperature1.0", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: kobart_8_5e-5_datav2_min30_lp5.0_temperature1.0\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# kobart_8_5e-5_datav2_min30_lp5.0_temperature1.0\n\nThis model is a fine-tuned version of [gogamza/kobart-base-v2](https://huggingface.co/gogamza/kobart-base-v2) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.8332\n- Rouge1: 36.0185\n- Rouge2: 12.6783\n- Rougel: 23.3148\n- Bleu1: 30.2418\n- Bleu2: 17.381\n- Bleu3: 10.3059\n- Bleu4: 5.9599\n- Gen Len: 50.9767\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 5.0\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Bleu1   | Bleu2   | Bleu3   | Bleu4  | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:------:|:-------:|\n| 2.5229        | 0.19  | 1000  | 2.9931          | 31.4246 | 10.0302 | 20.7531 | 25.3618 | 13.7797 | 7.3585  | 3.689  | 46.8019 |\n| 2.3763        | 0.38  | 2000  | 2.8644          | 33.6125 | 11.6317 | 21.9202 | 27.7709 | 15.9381 | 8.996   | 4.8041 | 50.1562 |\n| 2.3371        | 0.57  | 3000  | 2.7958          | 34.253  | 11.8488 | 22.4988 | 28.501  | 16.2829 | 9.1703  | 4.9873 | 48.2751 |\n| 2.3018        | 0.76  | 4000  | 2.7559          | 34.3508 | 11.7971 | 22.5994 | 28.1757 | 15.9896 | 9.12    | 5.0712 | 42.9767 |\n| 2.214         | 0.94  | 5000  | 2.7131          | 34.5451 | 12.4437 | 22.9456 | 28.3871 | 16.5087 | 9.9256  | 5.5757 | 46.0653 |\n| 2.0007        | 1.13  | 6000  | 2.7207          | 35.0462 | 12.0128 | 22.3508 | 29.3657 | 16.7098 | 9.4792  | 5.0235 | 49.5152 |\n| 1.9633        | 1.32  | 7000  | 2.7195          | 34.3249 | 11.9224 | 22.9618 | 28.2812 | 16.0876 | 9.3298  | 5.3695 | 46.7879 |\n| 2.0002        | 1.51  | 8000  | 2.6799          | 35.783  | 12.7607 | 23.8872 | 29.6408 | 17.2382 | 10.1776 | 5.9003 | 46.5967 |\n| 1.9783        | 1.7   | 9000  | 2.6615          | 34.7877 | 12.2492 | 23.0451 | 28.8199 | 16.6404 | 9.6347  | 5.2901 | 47.2681 |\n| 1.955         | 1.89  | 10000 | 2.6337          | 35.3022 | 12.7166 | 23.4134 | 29.218  | 17.0785 | 9.925   | 5.6807 | 50.0559 |\n| 1.671         | 2.08  | 11000 | 2.6997          | 35.3595 | 12.305  | 23.3744 | 29.525  | 16.937  | 9.6249  | 5.2743 | 48.4219 |\n| 1.6756        | 2.27  | 12000 | 2.6986          | 34.8911 | 12.2688 | 23.1722 | 29.1454 | 16.7564 | 9.7788  | 5.5929 | 46.8648 |\n| 1.663         | 2.45  | 13000 | 2.6974          | 35.4625 | 12.5317 | 23.3959 | 29.3184 | 17.0218 | 9.7629  | 5.4506 | 48.662  |\n| 1.6896        | 2.64  | 14000 | 2.6792          | 34.6078 | 12.3596 | 23.1353 | 28.6652 | 16.697  | 9.9738  | 5.6329 | 45.1608 |\n| 1.7114        | 2.83  | 15000 | 2.6765          | 35.3731 | 12.669  | 23.4203 | 29.6602 | 17.1914 | 10.0183 | 5.745  | 47.9557 |\n| 1.4059        | 3.02  | 16000 | 2.7574          | 35.249  | 12.3037 | 23.0811 | 29.4765 | 16.9417 | 9.563   | 5.4593 | 50.3939 |\n| 1.4559        | 3.21  | 17000 | 2.7695          | 35.3686 | 12.2559 | 23.1602 | 29.3155 | 16.7156 | 9.6546  | 5.4363 | 47.7226 |\n| 1.4475        | 3.4   | 18000 | 2.7638          | 35.3241 | 12.5225 | 23.3305 | 29.5401 | 17.0816 | 9.7474  | 5.4129 | 48.6993 |\n| 1.4459        | 3.59  | 19000 | 2.7679          | 35.64   | 12.6542 | 23.1888 | 30.0146 | 17.4051 | 10.2219 | 5.7042 | 51.8438 |\n| 1.4678        | 3.78  | 20000 | 2.7604          | 35.1451 | 12.2282 | 23.1746 | 29.4539 | 16.8357 | 9.7948  | 5.321  | 49.1935 |\n| 1.4478        | 3.97  | 21000 | 2.7555          | 36.2922 | 13.2416 | 24.0108 | 30.5121 | 17.9087 | 10.6678 | 6.2204 | 49.9417 |\n| 1.2405        | 4.15  | 22000 | 2.8381          | 36.0049 | 12.868  | 23.5304 | 30.1701 | 17.6082 | 10.4209 | 5.7566 | 53.3916 |\n| 1.2203        | 4.34  | 23000 | 2.8370          | 35.6913 | 12.5497 | 23.6024 | 29.8742 | 17.1319 | 9.9978  | 5.6913 | 49.7646 |\n| 1.2756        | 4.53  | 24000 | 2.8360          | 35.3826 | 12.3329 | 22.8257 | 29.5363 | 16.8789 | 9.7444  | 5.4338 | 51.972  |\n| 1.2452        | 4.72  | 25000 | 2.8362          | 35.7976 | 12.5759 | 23.2084 | 30.1391 | 17.3059 | 10.1375 | 5.6696 | 50.1888 |\n| 1.241         | 4.91  | 26000 | 2.8332          | 36.0185 | 12.6783 | 23.3148 | 30.2418 | 17.381  | 10.3059 | 5.9599 | 50.9767 |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu117\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "495648413", "downloads": 5}
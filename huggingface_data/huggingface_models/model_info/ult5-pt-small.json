{"pretrained_model_name": "tgsc/ult5-pt-small", "description": "---\nlicense: mit\nlanguage:\n- pt\ntags:\n- t5\n- ul2\n- pt\n- pt-br\ndatasets:\n- allenai/c4\nlibrary_name: transformers\n---\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n*ULT5-pt is a T5-v1.1 architecture model trained using the UL2 - Unifying Language Learning Paradigms framework, which uses Mixture-of-Denoisers (MoD), combining Causal Language Modeling (CLM) objective with Span Corruption.*\n\nULT5-pt \u00e9 um modelo de arquitetura T5-v1.1/google-ul2 treinado com o framework UL2 - [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1), que utiliza Mixture-of-Denoisers (MoD), o qual combina o objetivo de Causal Language Modeling (CLM) com Span Corruption.\n\n| Model                                    | type | Vocabulary | Parameters  | Context length |\n|                 :-:                      | :-: | :-: |   :-:   |  :-:   |\n| [ult5-pt-small](https://huggingface.co/tgsc/ult5-pt-small) | encoder-decoder | 65k |82.4M | 1024 |\n| [sentence-transformer-ult5-pt-small](https://huggingface.co/tgsc/sentence-transformer-ult5-pt-small)  | sentence-transformer | 65k | 51M | 1024 |\n| [DeBERTina-base](https://huggingface.co/tgsc/debertina-base) | encoder | 32k | 100M | 512 |\n\n\n- **Developed by:** Thacio Garcia Scandaroli\n- **Model type:** T5\n- **Language(s) (NLP):** Portugu\u00eas\n- **License:** MIT\n\nBenchmarks e tutorial de fine-tune: [https://github.com/thacio/LLM-Notebooks](https://github.com/thacio/LLM-Notebooks)\n\nTutorial fine-tune no colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thacio/LLM-Notebooks/blob/main/Finetunning-LanguageModel.ipynb)\n\n## Fine-tunning\n\nAlguns datasets (algumas classifica\u00e7\u00f5es ou score de similaridade) possuem melhor desempenho com o dropout desligado. Para carregar sem dropout, use o c\u00f3digo:\n\n*Some datasets (e.g. some classifcation or similarity score) work better without dropout. To load the model without dropout:*\n\n```python\nfrom transformers import  AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"tgsc/ult5-pt-small\",dropout_rate=0.0)\n```\n\n## Pretraining and model characteristics\n\nO modelo foi treinado com uma parte do corpus C4 em portugu\u00eas utilizando o UL2 (https://huggingface.co/google/ul2), utilizando *R-Denoising*, *S-Denoising* e *X-Denoising*, e com dropout 0.0.\nDe forma diferente do paper original, n\u00e3o se utilizou token espec\u00edfico de prefixo para o *S-Denoising*. Para o *R-Denoising* e o *X-Denoising*, foram utilizados, respectivamente, os tokens <|NLU|> e <|NLG|>.\n\nUtilizou-se uma janela de contexto para 1024 tokens e um tokenizador do GPT2 com vocabul\u00e1rio em portugu\u00eas treinado com o wikipedia, aumentando a quantidade de texto que pode ser processada.\n\n*The model was trained with a portion of the C4 corpus in Portuguese using UL2 (https://huggingface.co/google/ul2), using R-Denoising, S-Denoising, and X-Denoising, and with dropout rate of 0.0.*\n*Unlike the original work of UL2, a prefix token for S-Denoising was not used. For R-Denoising and X-Denoising, the tokens '<|NLU|>' and '<|NLG|>' and were used, respectively.*\n\n*A context window of 1024 tokens was used. Also, a GPT2 tokenizer with a Portuguese vocabulary trained with Wikipedia was used to increase the amount of text that can be processed.*\n\n## Uses\n\nO uso recomendado \u00e9 para fine-tunning.\n\nFoi disponibilizado um tutorial em formato de notebook para fine-tune de modelos decoder e encoder-decoder (T5): [Fine-tune Large Language Models](endere\u00e7o aqui)\n\nOs modos de *span corruption* podem ser acionados adicionado ao in\u00edcio do text os prefixos '<|NLU|>' e '<|NLG|>'.\nOs autores do UL2 apontam uma possivel diferen\u00e7a no resultado do fine-tune dependendo do modo ativado.\nPor\u00e9m, para o ult5-pt, n\u00e3o se notou diferen\u00e7a nos testes de benchmark.\n\n*Fine-tunning is the recommended use for the model.*\n\n*A tutorial (in portuguse) in notebook format for decoder and encoder-decoder (T5) model fine-tuning was provided: [Fine-tune Large Language Models](link here).*\n\n*Span corruption modes can be activated by adding the prefixes '<|NLU|>' and '<|NLG|>' to the beginning of the text. The UL2 authors point out a possible difference in the fine-tuning result depending on the activated mode. However, for ult5-pt, no difference was noticed in benchmark tests.*\n\nSpecial tokens:\n- pad: '<|pad|>'\n- end of text: '<|endoftext|>'\n- R denoising: '<|NLU|>'\n- S denoising: '<|NLG|>'\n\n### Direct Use\n\nExemplo de gera\u00e7\u00e3o de texto com top_k de 30\n*Example of text generation with top_k of 30*\n\n```python\nfrom transformers import GPT2TokenizerFast, AutoModelForSeq2SeqLM\n\ntokenizer = GPT2TokenizerFast.from_pretrained(\"tgsc/ult5-pt-small\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"tgsc/ult5-pt-small\")\n\ntext='Um modelo de linguagem \u00e9 um sistema de intelig\u00eancia artificial que'\n\npred=model.generate(tokenizer.encode(text,return_tensors='pt'),max_new_tokens=30, eos_token_id=tokenizer.eos_token_id, top_k=30, do_sample=True)\nprint('input:',text)\nprint('generated:',tokenizer.batch_decode(pred, skip_special_tokens=True))\n# input: Um modelo de linguagem \u00e9 um sistema de intelig\u00eancia artificial que\n# generated: [' geraria a quantidade de informa\u00e7\u00f5es por clique. Al\u00e9m das capacidades humanas, elas seriam muito mais produtivas do que as do c\u00e9rebro humano.\\nO que']\n```\n\n\nEmbora seja poss\u00edvel obter embeddings com o modelo, melhores embeddings podem ser obtidos com o modelo [tgsc/sentence-transformer-ult5-pt-small](https://huggingface.co/tgsc/sentence-transformer-ult5-pt-small).\n*Altough you can get embeddings from this model, better embeddings can be obtained with [tgsc/sentence-transformer-ult5-pt-small](https://huggingface.co/tgsc/sentence-transformer-ult5-pt-small).*\n\nEmbeddings:\n\n```python\nfrom transformers import T5EncoderModel, GPT2TokenizerFast\n\ntokenizer = GPT2TokenizerFast.from_pretrained(\"tgsc/ult5-pt-small\")\nmodel = T5EncoderModel.from_pretrained(\"tgsc/ult5-pt-small\")\n\ntext = 'Um modelo de linguagem \u00e9 um sistema de intelig\u00eancia artificial que aprende a gerar ou processar texto baseado em exemplos de treinamento.'\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\noutputs = model(input_ids)\nlast_hidden_states = outputs.last_hidden_state\nprint(last_hidden_states)\n```\n\n## Bias, Risks, and Limitations\n\nOs mesmos riscos, vieses e limita\u00e7\u00f5es dos outros modelos se aplicam a este, como o apontado em [GPT2](https://huggingface.co/gpt2).\n\n*The same risks, biases, and limitations of other models apply to this one, as pointed out in [GPT2](https://huggingface.co/gpt2).*\n\n## Citation\n\n```bibtex\n@misc{ult5-pt2023,\n  author = {Thacio Garcia Scandaroli},\n  title = {ULT5-pt: Portuguese Language Model trained with UL2},\n  year = {2023},\n}\n```", "size_bytes": "329746697", "downloads": 10}
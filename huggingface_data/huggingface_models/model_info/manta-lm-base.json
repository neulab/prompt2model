{"pretrained_model_name": "almanach/manta-lm-base", "description": "---\nlicense: mit\nlanguage:\n- en\npipeline_tag: text2text-generation\n---\n# MANTa-LM (base)\n\nPretrained MANTa-LM architecture as introduced in the paper [MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling](https://aclanthology.org/2022.findings-emnlp.207.pdf).\n<center><img src=\"https://github.com/NathanGodey/nathangodey.github.io/raw/main/img/posts/full_difftok_schema.png\"  width=\"600\"></center>\n\n## Model Details\n\n### Model Description\n\nThe MANTa tokenizer aims at mimicking the combination of a subword tokenizer and an embedding matrix in a classical language model in a differentiable way.\nThis trainable tokenizer is thus added as the first layer of an encoder-decoder model and trained using the language modeling objective.\n\nOur results show that MANTa-LM only slightly degrades the performance of a T5 equivalent on the GLUE benchmark while being **much more robust** to artificial and user-generated noise. \n\n\n### Model Sources\n\n- **Paper:** [MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling](https://aclanthology.org/2022.findings-emnlp.207.pdf) (EMNLP 2022 Findings)\n\n## Uses\n\n### Direct Use\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"almanach/manta-lm-base\", trust_remote_code=True)\nmanta_model = AutoModelForSeq2SeqLM.from_pretrained(\"almanach/manta-lm-base\", trust_remote_code=True)\n\ntokens = tokenizer(\"The name of the capital of France is <extra_id_0> and it is a very big city.\", return_tensors=\"pt\")\noutput = manta_model.generate(**tokens, decoder_start_token_id=0)\n\nprint(tokenizer.batch_decode(output))\n```\n\n### Recommendations\n\nWe recommend using a smaller learning rate for the tokenizer module during fine-tuning (byte embeddings, frontier predictor, pooler).\n\n\n## Training Details\n\n### Training Data\n\nThis model was trained on the C4 dataset.\n\n### Training Procedure \n\nThe training objective is the same as ByT5, but most hyperparameters are taken from T5.\n\n\n## Citation\n\n**BibTeX:**\n\n```\n@inproceedings{godey-etal-2022-manta,\n    title = \"{MANT}a: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling\",\n    author = \"Godey, Nathan  and\n      Castagn{\\'e}, Roman  and\n      de la Clergerie, {\\'E}ric  and\n      Sagot, Beno{\\^\\i}t\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2022\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, United Arab Emirates\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-emnlp.207\",\n    pages = \"2859--2870\",\n}\n\n```\n\n## Model Card Authors\n\n[Nathan Godey](https://nathangodey.github.io/)\n[Roman Castagn\u00e9](https://romancast.github.io/)", "size_bytes": "1024404765", "downloads": 16}
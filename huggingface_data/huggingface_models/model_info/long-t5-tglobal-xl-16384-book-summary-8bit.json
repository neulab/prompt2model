{"pretrained_model_name": "pszemraj/long-t5-tglobal-xl-16384-book-summary-8bit", "description": "---\nlicense:\n- apache-2.0\n- bsd-3-clause\ntags:\n- summarization\n- summary\n- booksum\n- long-document\n- long-form\n- tglobal-xl\n- XL\n- 8bit\n- quantized\ndatasets:\n- kmfoda/booksum\nmetrics:\n- rouge\ninference: false\npipeline_tag: summarization\n---\n\n\n# long-t5-tglobal-xl-16384-book-summary: 8-bit quantized version\n\n<a href=\"https://colab.research.google.com/gist/pszemraj/c19e32baf876deb866c31cd46c86e893/long-t5-xl-accelerate-test.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nThis is an 8-bit quantized version of the `pszemraj/long-t5-tglobal-xl-16384-book-summary` model, The model has been compressed using `bitsandbytes` and can be loaded with low memory usage.\n\nRefer to the [original model](https://huggingface.co/pszemraj/long-t5-tglobal-xl-16384-book-summary) for all details about the model architecture and training process. For more information on loading 8-bit models, refer to the `4.28.0` [release information](https://github.com/huggingface/transformers/releases/tag/v4.28.0) and the [example repository](https://huggingface.co/ybelkada/bloom-1b7-8bit).\n\n- The total size of the model is only ~3.5 GB (vs original 12 GB)\n- Enables low-RAM loading, making it easier to use in memory-limited environments like Colab\n- Requires `bitsandbytes` - AFAIK at time of writing, only works on GPU\n\n\n## Basic Usage\n\nTo use the model, install or upgrade `transformers`, `accelerate`, and `bitsandbytes`. Make sure to have `transformers>=4.28.0` and `bitsandbytes>0.37.2`.\n\n```bash\npip install -U -q transformers bitsandbytes accelerate\n```\n\nLoad the model with `AutoTokenizer` and `AutoModelForSeq2SeqLM`:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"pszemraj/long-t5-tglobal-xl-16384-book-summary-8bit\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n```\n\n## More information about long-t5-tglobal-xl-16384-book-summary\n\n- This is an 8-bit quantized version of `pszemraj/long-t5-tglobal-xl-16384-book-summary`.\n  - It generalizes reasonably well to academic and narrative text.\n  - The XL checkpoint typically generates summaries that are considerably better from a human evaluation perspective.", "size_bytes": "3184300231", "downloads": 9}
{"pretrained_model_name": "datien228/distilbart-ftn-wiki_lingua", "description": "---\nlanguage:\n- en\ntags:\n- summarization\nlicense: mit\ndatasets:\n- wiki_lingua\nmetrics:\n- rouge\n---\n\n#### Pre-trained BART Model fine-tune on WikiLingua dataset\nThe repository for the fine-tuned BART model (by sshleifer) using the **wiki_lingua** dataset (English)\n\n**Purpose:** Examine the performance of a fine-tuned model research purposes\n\n**Observation:**\n- Pre-trained model was trained on the XSum dataset, which summarize a not-too-long documents into one-liner summary\n- Fine-tuning this model using WikiLingua is appropriate since the summaries for that dataset are also short\n- In the end, however, the model cannot capture much clearer key points, but instead it mostly extracts the opening sentence\n- Some data pre-processing and models' hyperparameter are also need to be tuned more properly.", "size_bytes": "1222365177", "downloads": 8}
{"pretrained_model_name": "Zekunli/flan-t5-large-da-multiwoz2.0_400-ep10-nonstop", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: flan-t5-large-da-multiwoz2.0_400-ep10-nonstop\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-large-da-multiwoz2.0_400-ep10-nonstop\n\nThis model is a fine-tuned version of [google/flan-t5-large](https://huggingface.co/google/flan-t5-large) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3642\n- Accuracy: 40.0483\n- Num: 7358\n- Gen Len: 15.3939\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 32\n- seed: 1799\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | Num  | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:----:|:-------:|\n| 1.3288        | 0.58  | 200  | 0.5718          | 27.6241  | 7358 | 13.9315 |\n| 0.5941        | 1.17  | 400  | 0.4745          | 32.0484  | 7358 | 14.2517 |\n| 0.5213        | 1.75  | 600  | 0.4283          | 33.6716  | 7358 | 15.1218 |\n| 0.4891        | 2.33  | 800  | 0.4063          | 34.1115  | 7358 | 14.8005 |\n| 0.4503        | 2.92  | 1000 | 0.3935          | 35.9067  | 7358 | 15.9095 |\n| 0.4213        | 3.5   | 1200 | 0.3833          | 37.2591  | 7358 | 16.0005 |\n| 0.4184        | 4.08  | 1400 | 0.3795          | 37.696   | 7358 | 15.478  |\n| 0.3959        | 4.66  | 1600 | 0.3762          | 37.401   | 7358 | 14.8752 |\n| 0.3847        | 5.25  | 1800 | 0.3714          | 37.7347  | 7358 | 15.9304 |\n| 0.3779        | 5.83  | 2000 | 0.3710          | 38.6814  | 7358 | 14.9257 |\n| 0.3776        | 6.41  | 2200 | 0.3681          | 38.4266  | 7358 | 15.2517 |\n| 0.3601        | 7.0   | 2400 | 0.3669          | 38.6749  | 7358 | 15.1791 |\n| 0.3504        | 7.58  | 2600 | 0.3669          | 39.2748  | 7358 | 15.4308 |\n| 0.3568        | 8.16  | 2800 | 0.3650          | 39.798   | 7358 | 15.8966 |\n| 0.3528        | 8.75  | 3000 | 0.3630          | 39.912   | 7358 | 15.4081 |\n| 0.3463        | 9.33  | 3200 | 0.3641          | 40.1243  | 7358 | 15.5367 |\n| 0.3439        | 9.91  | 3400 | 0.3642          | 40.0567  | 7358 | 15.3842 |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "3132789733", "downloads": 2}
{"pretrained_model_name": "HelloImSteven/AppleScript-Summarizer", "description": "---\nlicense: mit\ndatasets:\n- HelloImSteven/applescript-lines-annotated\nlanguage:\n- en\ntags:\n- applescript\n- code\nwidget:\n- text: \"on openFile(filePath, permission)\"\n  example_title: \"Handler Definition\"\n- text: \"set myVar to 3\"\n  example_title: \"Set Variable\"\n- text: \"open location \\\"https://huggingface.co\\\"\"\n  example_title: \"open location\"\n- text: \"do shell script \\\"echo 'Here are some numbers: 1, 5, 89, 156, 1053' | egrep -o '\\\\\\\\d*'\\\"\"\n  example_title: \"do shell script\"\n- text: \"use framework \\\"CoreLocation\\\"\"\n  example_title: \"use framework\"\n- text: \"tell application \\\"Notes\\\" to make new note\"\n  example_title: \"Make New Note\"\npipeline_tag: summarization\n---\n\n# Model Description\n\nThis model summarizes individual lines of AppleScript code and provides insights into their purpose and functionality. The model is trained on the [applescript-lines-annotated](https://huggingface.co/datasets/HelloImSteven/applescript-lines-annotated) dataset. \n\n# Usage\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig, pipeline\n\nmodel_selector = \"HelloImSteven/AppleScript-Summarizer\"\ntokenizer = AutoTokenizer.from_pretrained(model_selector)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_selector)\nconfig = AutoConfig.from_pretrained(model_selector)\n\npipe = pipeline(\"summarization\", model=model, config=config, tokenizer=tokenizer)\n\nraw_code = [\n    \"if myThing is 0 then set success to true\",\n    \"set numInputs to 5\",\n    \"tell application \\\"System Events\\\" to click button \\\"OK\\\" of window 1 of application process \\\"Finder\\\"\"\n]\n\nres = pipe(raw_code)\nfor i in range(len(res)):\n    print(\"\\n\\nInput: '\" + raw_code[i] + \"'\")\n    print(\"\\nSummary: \" + res[i][\"summary_text\"] + '\\n')\n```\n\nThe model performs best when given adequate context, such as meaningful variable and handler names. For example, it will perform better when provided the input \"on makeDirectory(dirPath)\" than it will for \"on makeDir(x)\".\n\n# Example Outputs\n\n| Input | Output |\n| ----- | ----- |\n| on makeDirectory(dirPath) | This line of AppleScript code begins the definition of a handler named \"makeDirectory\". Based on its name, the handler is likely used to create a directory path from the path stored in the variable \"dirPath\". |\n| open location \"https://huggingface.co\" | This is a line of AppleScript code that calls the built-in \"open location\" handler, providing the URL \"https://huggingface.co\" as a parameter. This is likely used to open a web browser. |\n| do shell script \"open -a 'Google Chrome'\" | This is a line of AppleScript code that calls the \"open -a\" handler of the \"Google Chrome\" command, providing the output as a shell command. When executed, this line will open the Chrome web browser. |\n\n# Limitations\n\nThis model is a work in progress. It is trained on a small dataset of annotated AppleScript lines, but that dataset will grow in time. For now, however, this leads to several limitations of the model:\n\n1. There are concepts and line structures that are simply absent from the dataset, so the model will struggle with them. If you identify such a concept, please consider providing feedback (e.g. expected output).\n2. While the model is fine-tuned from Bart and thus has adequate knowledge of outside concepts, it is not able to give explanations of all possible concepts. For example, for inputs involving the `do shell script` command, it will be able to provide explanations of some shell scripts better than others. This is not an area of focus for this model, so do not expect significant improvements in future versions.\n3. This is not a conversational model.\n\n# Validation Metrics\n\n- Loss: 0.20628\n- Rouge1: 0.60299\n- Rouge2: 0.41118\n- rougeL: 0.52186\n- RougeLsum: 0.52207", "size_bytes": "1625534221", "downloads": 5}
{"pretrained_model_name": "BME-TMIT/foszt2oszt", "description": "---\nlanguage: hu\nmetrics: rouge\n---\n\n[Paper](https://hlt.bme.hu/en/publ/foszt2oszt)\n\nWe publish an abstractive summarizer for Hungarian, an\nencoder-decoder model initialized with [huBERT](huggingface.co/SZTAKI-HLT/hubert-base-cc), and fine-tuned on the\n[ELTE.DH](https://elte-dh.hu/) corpus of former Hungarian news portals. The model produces fluent output in the correct topic, but it hallucinates frequently.\nOur quantitative evaluation on automatic and human transcripts of news\n(with automatic and human-made punctuation, [T\u00fcndik et al. (2019)](https://www.isca-speech.org/archive/interspeech_2019/tundik19_interspeech.html), [T\u00fcndik and Szasz\u00e1k (2019)](https://www.isca-speech.org/archive/interspeech_2019/szaszak19_interspeech.html)) shows that the model is\nrobust with respect to errors in either automatic speech recognition or\nautomatic punctuation restoration. In fine-tuning and inference, we followed [a jupyter notebook by Patrick von\nPlaten](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb). Most hyper-parameters are the same as those by von Platen, but we\nfound it advantageous to change the minimum length of the summary to 8 word-\npieces (instead of 56), and the number of beams in beam search to 5 (instead\nof 4). Our model was fine-tuned on a server of the [SZTAKI-HLT](hlt.bme.hu/) group, which kindly\nprovided access to it.", "size_bytes": "998776914", "downloads": 11}
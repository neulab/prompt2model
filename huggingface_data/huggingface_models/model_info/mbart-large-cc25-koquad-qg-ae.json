{"pretrained_model_name": "lmqg/mbart-large-cc25-koquad-qg-ae", "description": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: ko\ndatasets:\n- lmqg/qg_koquad\npipeline_tag: text2text-generation\ntags:\n- question generation\n- answer extraction\nwidget:\n- text: \"generate question: 1990\ub144 \uc601\ud654 \u300a <hl> \ub0a8\ubd80\uad70 <hl> \u300b\uc5d0\uc11c \ub2e8\uc5ed\uc73c\ub85c \uc601\ud654\ubc30\uc6b0 \uccab \ub370\ubdd4\uc5d0 \uc774\uc5b4 \uac19\uc740 \ud574 KBS \ub4dc\ub77c\ub9c8 \u300a\uc9c0\uad6c\uc778\u300b\uc5d0\uc11c \ub2e8\uc5ed\uc73c\ub85c \ucd9c\uc5f0\ud558\uc600\uace0 \uc774\ub4ec\ud574 MBC \u300a\uc5ec\uba85\uc758 \ub208\ub3d9\uc790\u300b\ub97c \ud1b5\ud574 \ub2e8\uc5ed\uc73c\ub85c \ucd9c\uc5f0\ud558\uc600\ub2e4.\"\n  example_title: \"Question Generation Example 1\" \n- text: \"generate question: \ubc31\uc2e0\uc774 \uc5c6\uae30\ub54c\ubb38\uc5d0 \uc608\ubc29\ucc45\uc740 <hl> \uc0b4\ucda9\uc81c <hl> \ub97c \uc0ac\uc6a9\ud558\uba74\uc11c \uc11c\uc2dd \uc7a5\uc18c(\ucc3b\ucc2c \ubc1b\uce68, \ubc30\uc218\ub85c, \uace0\uc778 \ubb3c\uc758 \uc5f4\ub9b0 \uc800\uc7a5\uc18c, \ubc84\ub824\uc9c4 \ud0c0\uc774\uc5b4 \ub4f1)\uc758 \uc218\ub97c \uc904\uc784\uc73c\ub85c\uc368 \ub9e4\uac1c\uccb4\ub97c \ud1b5\uc81c\ud560 \uc218 \uc788\ub2e4.\"\n  example_title: \"Question Generation Example 2\" \n- text: \"generate question: <hl> \uc6d0\ud14c\uc774\ud06c \ucd2c\uc601 <hl> \uc774\uae30 \ub54c\ubb38\uc5d0 \ud55c \uc0ac\ub78c\uc774 \uc2e4\uc218\ub97c \ud558\uba74 \ucc98\uc74c\ubd80\ud130 \ub2e4\uc2dc \ucc0d\uc5b4\uc57c \ud558\ub294 \uc0c1\ud669\uc774 \ubc1c\uc0dd\ud55c\ub2e4.\"\n  example_title: \"Question Generation Example 3\" \n- text: \"extract answers: \ub610\ud55c \uc2a4\ud53c\uc5b4\uc2a4\ub294 \ub9ce\uc740 \uc0c8\ub85c\uc6b4 \uc5ec\uc131 \uc544\ud2f0\uc2a4\ud2b8\ub4e4\uc5d0\uac8c \uc601\ud5a5\uc744 \ub07c\ucce4\ub294\ub370, \ub300\ud45c\uc801\uc73c\ub85c \ub370\ubbf8 \ub85c\ubc14\ud1a0, \ucf00\uc774\ud2f0 \ud398\ub9ac, \ud06c\ub9ac\uc2a4\ud2f0\ub2c8\uc544 \ub4dc\ubc14\uc9c0, \ub808\uc774\ub514 \uac00\uac00, \ub9ac\ud2c0 \ubd80\uce20, \uc140\ub808\ub098 \uace0\uba54\uc988 & \ub354\uc52c, \ud53d\uc2dc \ub85c\ud2b8 \uc774 \uc788\ub2e4. 2007\ub144 \ube44\uc698\uc138 \ub180\uc2a4\ub294 Total Request Live\uc640\uc758 \uc778\ud130\ubdf0\uc5d0\uc11c '\ub098\ub294 \ube0c\ub9ac\ud2b8\ub2c8\ub97c \uc0ac\ub791\ud558\uace0 \ud32c\uc774\uc5d0\uc694. \ud2b9\ud788 \uc0c8 \uc568\ubc94 Blackout\uc744 \uc88b\uc544\ud574\uc694'\ub77c\uace0 \ub9d0\ud588\ub2e4. \ub9b0\uc81c\uc774 \ub85c\ud55c\uc740 '\uc5b8\uc81c\ub098 \ube0c\ub9ac\ud2b8\ub2c8 \uc2a4\ud53c\uc5b4\uc2a4\uc5d0\uac8c \uc601\uac10\uc744 \ubc1b\ub294\ub2e4. \ud559\ucc3d\uc2dc\uc808 \uadf8\ub140\ucc98\ub7fc \ud0c0\ube14\ub85c\uc774\ub4dc\uc5d0 \uc624\ub974\uae30\ub97c \uafc8\uafd4\uc654\ub2e4'\uace0 \ub9d0\ud558\uba70 \ub864 \ubaa8\ub378\ub85c \uaf3d\uc558\ub2e4. \uc2a4\ud53c\uc5b4\uc2a4\ub294 \ud604\ub300 \uc74c\uc545\uac00\ub4e4\uc5d0\uac8c \uc74c\uc545\uc801 \uc601\uac10\uc73c\ub85c \uc5b8\uae09\ub418\uae30\ub3c4 \ud588\ub2e4. <hl> \ub9c8\uc77c\ub9ac \uc0ac\uc774\ub7ec\uc2a4\ub294 \uc790\uc2e0\uc758 \ud788\ud2b8\uace1 Party in the U.S.A. \uac00 \ube0c\ub9ac\ud2b8\ub2c8\uc5d0\uac8c \uc601\uac10\uacfc \uc601\ud5a5\uc744 \ubc1b\uc740 \uace1\uc774\ub77c\uace0 \ubc1d\ud614\ub2e4. <hl> \ubca0\ub9ac \ub9e4\ub2d0\ub85c\uc6b0\uc758 \uc568\ubc94 15 Minutes \uc5ed\uc2dc \ube0c\ub9ac\ud2b8\ub2c8\uc5d0\uac8c \uc601\uac10\uc744 \uc5bb\uc5c8\ub2e4\uace0 \uc5b8\uae09\ub418\uc5c8\ub2e4.\"\n  example_title: \"Answer Extraction Example 1\" \n- text: \"extract answers: \uc9c0\ub09c 22\uc77c \uc544\ud504\ub9ac\uce74TV\ub294 BJ \ucca0\uad6c\uac00 \uc11c\ube44\uc2a4 \uc815\uc9c0 \ucc98\ubd84\uc744 \ubc1b\uc558\uc74c\uc744 \ubc1d\ud614\ub2e4. \uc11c\ube44\uc2a4 \uc815\uc9c0 \ucc98\ubd84\uc744 \uc0ac\uc720\ub294 \ucca0\uad6c\uac00 10\ub300 \uccad\uc18c\ub144\uc5d0\uac8c \uc720\ud574\ud55c \uc7a5\uba74\uc744 \ubc29\uc1a1\uc73c\ub85c \ub0b4\ubcf4\ub0c8\uae30 \ub54c\ubb38\uc774\uc5c8\ub2e4. \ubb38\uc81c\uac00 \ub41c \uc7a5\uba74\uc740 BJ \ucca0\uad6c\uac00 \ubbf8\uc131\ub144\uc790\ub294 \uc2dc\uccad\ud560 \uc218 \uc5c6\uac8c \ud558\ub294 19\uc138 \uc2dc\uccad \uac00\ub2a5 \uc124\uc815\uc744 \ud558\uc9c0 \uc54a\uc740 \ucc44 \ud761\uc5f0\ud558\ub294 \ubaa8\uc2b5\uc744 \uc5ec\uacfc \uc5c6\uc774 \ub4dc\ub7ec\ub0b8 \uc7a5\uba74\uc774\ub2e4. \uc544\ud504\ub9ac\uce74TV\ub294 \uccad\uc18c\ub144 \ubcf4\ud638 \uc815\ucc45\uc758 '\uccad\uc18c\ub144\ub4e4\uc774 \ud574\ub85c\uc6b4 \ud658\uacbd\uc73c\ub85c\ubd80\ud130 \ubcf4\ud638\ubc1b\uc744 \uc218 \uc788\ub3c4\ub85d \uc870\uce58\ud55c\ub2e4'\ub77c\uace0 \uc870\ud56d\uc744 \uadfc\uac70\ub85c \ucca0\uad6c\uc5d0\uac8c \uc11c\ube44\uc2a4 \uc815\uc9c0 \ucc98\ubd84\uc744 \ub0b4\ub838\ub2e4. \ud761\uc5f0 \uc774\uc678\uc5d0 \uc74c\uc8fc \ubc29\uc1a1 \ub4f1\ub3c4 19\uc138 \uc2dc\uccad \uac00\ub2a5 \uc124\uc815\uc744 \ud574\uc57c\ub9cc \ubc29\uc1a1\ud560 \uc218 \uc788\ub2e4. <hl> \uac8c\ub2e4\uac00 \ucca0\uad6c\uc758 \ubc29\uc1a1 \uc815\uc9c0 \ucc98\ubd84\uc740 \uc774\ubc88\uc5d0 \ucc98\uc74c\uc774 \uc544\ub2c8\ub77c 16\ubc88 \uc9f8\uae30 \ub54c\ubb38\uc5d0 \ub354\uc6b1\ub354 \ub17c\ub780\uc774 \ub418\uace0 \uc788\ub2e4. <hl>\"\n  example_title: \"Answer Extraction Example 2\" \nmodel-index:\n- name: lmqg/mbart-large-cc25-koquad-qg-ae\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_koquad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Generation)\n      type: bleu4_question_generation\n      value: 10.7\n    - name: ROUGE-L (Question Generation)\n      type: rouge_l_question_generation\n      value: 27.02\n    - name: METEOR (Question Generation)\n      type: meteor_question_generation\n      value: 29.73\n    - name: BERTScore (Question Generation)\n      type: bertscore_question_generation\n      value: 83.52\n    - name: MoverScore (Question Generation)\n      type: moverscore_question_generation\n      value: 82.79\n    - name: QAAlignedF1Score-BERTScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_f1_score_bertscore_question_answer_generation_with_gold_answer\n      value: 80.81\n    - name: QAAlignedRecall-BERTScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_recall_bertscore_question_answer_generation_with_gold_answer\n      value: 84.32\n    - name: QAAlignedPrecision-BERTScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_precision_bertscore_question_answer_generation_with_gold_answer\n      value: 77.64\n    - name: QAAlignedF1Score-MoverScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_f1_score_moverscore_question_answer_generation_with_gold_answer\n      value: 83.42\n    - name: QAAlignedRecall-MoverScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_recall_moverscore_question_answer_generation_with_gold_answer\n      value: 88.44\n    - name: QAAlignedPrecision-MoverScore (Question & Answer Generation (with Gold Answer))\n      type: qa_aligned_precision_moverscore_question_answer_generation_with_gold_answer\n      value: 79.08\n    - name: BLEU4 (Answer Extraction)\n      type: bleu4_answer_extraction\n      value: 24.34\n    - name: ROUGE-L (Answer Extraction)\n      type: rouge_l_answer_extraction\n      value: 82.78\n    - name: METEOR (Answer Extraction)\n      type: meteor_answer_extraction\n      value: 59.82\n    - name: BERTScore (Answer Extraction)\n      type: bertscore_answer_extraction\n      value: 95.53\n    - name: MoverScore (Answer Extraction)\n      type: moverscore_answer_extraction\n      value: 94.69\n    - name: AnswerF1Score (Answer Extraction)\n      type: answer_f1_score__answer_extraction\n      value: 88.2\n    - name: AnswerExactMatch (Answer Extraction)\n      type: answer_exact_match_answer_extraction\n      value: 82.17\n---\n\n# Model Card of `lmqg/mbart-large-cc25-koquad-qg-ae`\nThis model is fine-tuned version of [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25) for question generation and answer extraction jointly on the [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) (dataset_name: default) via [`lmqg`](https://github.com/asahi417/lm-question-generation).\n\n\n### Overview\n- **Language model:** [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25)   \n- **Language:** ko  \n- **Training data:** [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) (default)\n- **Online Demo:** [https://autoqg.net/](https://autoqg.net/)\n- **Repository:** [https://github.com/asahi417/lm-question-generation](https://github.com/asahi417/lm-question-generation)\n- **Paper:** [https://arxiv.org/abs/2210.03992](https://arxiv.org/abs/2210.03992)\n\n### Usage\n- With [`lmqg`](https://github.com/asahi417/lm-question-generation#lmqg-language-model-for-question-generation-)\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"ko\", model=\"lmqg/mbart-large-cc25-koquad-qg-ae\")\n\n# model prediction\nquestion_answer_pairs = model.generate_qa(\"1990\ub144 \uc601\ud654 \u300a \ub0a8\ubd80\uad70 \u300b\uc5d0\uc11c \ub2e8\uc5ed\uc73c\ub85c \uc601\ud654\ubc30\uc6b0 \uccab \ub370\ubdd4\uc5d0 \uc774\uc5b4 \uac19\uc740 \ud574 KBS \ub4dc\ub77c\ub9c8 \u300a\uc9c0\uad6c\uc778\u300b\uc5d0\uc11c \ub2e8\uc5ed\uc73c\ub85c \ucd9c\uc5f0\ud558\uc600\uace0 \uc774\ub4ec\ud574 MBC \u300a\uc5ec\uba85\uc758 \ub208\ub3d9\uc790\u300b\ub97c \ud1b5\ud574 \ub2e8\uc5ed\uc73c\ub85c \ucd9c\uc5f0\ud558\uc600\ub2e4.\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"lmqg/mbart-large-cc25-koquad-qg-ae\")\n\n# answer extraction\nanswer = pipe(\"generate question: 1990\ub144 \uc601\ud654 \u300a <hl> \ub0a8\ubd80\uad70 <hl> \u300b\uc5d0\uc11c \ub2e8\uc5ed\uc73c\ub85c \uc601\ud654\ubc30\uc6b0 \uccab \ub370\ubdd4\uc5d0 \uc774\uc5b4 \uac19\uc740 \ud574 KBS \ub4dc\ub77c\ub9c8 \u300a\uc9c0\uad6c\uc778\u300b\uc5d0\uc11c \ub2e8\uc5ed\uc73c\ub85c \ucd9c\uc5f0\ud558\uc600\uace0 \uc774\ub4ec\ud574 MBC \u300a\uc5ec\uba85\uc758 \ub208\ub3d9\uc790\u300b\ub97c \ud1b5\ud574 \ub2e8\uc5ed\uc73c\ub85c \ucd9c\uc5f0\ud558\uc600\ub2e4.\")\n\n# question generation\nquestion = pipe(\"extract answers: \ub610\ud55c \uc2a4\ud53c\uc5b4\uc2a4\ub294 \ub9ce\uc740 \uc0c8\ub85c\uc6b4 \uc5ec\uc131 \uc544\ud2f0\uc2a4\ud2b8\ub4e4\uc5d0\uac8c \uc601\ud5a5\uc744 \ub07c\ucce4\ub294\ub370, \ub300\ud45c\uc801\uc73c\ub85c \ub370\ubbf8 \ub85c\ubc14\ud1a0, \ucf00\uc774\ud2f0 \ud398\ub9ac, \ud06c\ub9ac\uc2a4\ud2f0\ub2c8\uc544 \ub4dc\ubc14\uc9c0, \ub808\uc774\ub514 \uac00\uac00, \ub9ac\ud2c0 \ubd80\uce20, \uc140\ub808\ub098 \uace0\uba54\uc988 & \ub354\uc52c, \ud53d\uc2dc \ub85c\ud2b8 \uc774 \uc788\ub2e4. 2007\ub144 \ube44\uc698\uc138 \ub180\uc2a4\ub294 Total Request Live\uc640\uc758 \uc778\ud130\ubdf0\uc5d0\uc11c '\ub098\ub294 \ube0c\ub9ac\ud2b8\ub2c8\ub97c \uc0ac\ub791\ud558\uace0 \ud32c\uc774\uc5d0\uc694. \ud2b9\ud788 \uc0c8 \uc568\ubc94 Blackout\uc744 \uc88b\uc544\ud574\uc694'\ub77c\uace0 \ub9d0\ud588\ub2e4. \ub9b0\uc81c\uc774 \ub85c\ud55c\uc740 '\uc5b8\uc81c\ub098 \ube0c\ub9ac\ud2b8\ub2c8 \uc2a4\ud53c\uc5b4\uc2a4\uc5d0\uac8c \uc601\uac10\uc744 \ubc1b\ub294\ub2e4. \ud559\ucc3d\uc2dc\uc808 \uadf8\ub140\ucc98\ub7fc \ud0c0\ube14\ub85c\uc774\ub4dc\uc5d0 \uc624\ub974\uae30\ub97c \uafc8\uafd4\uc654\ub2e4'\uace0 \ub9d0\ud558\uba70 \ub864 \ubaa8\ub378\ub85c \uaf3d\uc558\ub2e4. \uc2a4\ud53c\uc5b4\uc2a4\ub294 \ud604\ub300 \uc74c\uc545\uac00\ub4e4\uc5d0\uac8c \uc74c\uc545\uc801 \uc601\uac10\uc73c\ub85c \uc5b8\uae09\ub418\uae30\ub3c4 \ud588\ub2e4. <hl> \ub9c8\uc77c\ub9ac \uc0ac\uc774\ub7ec\uc2a4\ub294 \uc790\uc2e0\uc758 \ud788\ud2b8\uace1 Party in the U.S.A. \uac00 \ube0c\ub9ac\ud2b8\ub2c8\uc5d0\uac8c \uc601\uac10\uacfc \uc601\ud5a5\uc744 \ubc1b\uc740 \uace1\uc774\ub77c\uace0 \ubc1d\ud614\ub2e4. <hl> \ubca0\ub9ac \ub9e4\ub2d0\ub85c\uc6b0\uc758 \uc568\ubc94 15 Minutes \uc5ed\uc2dc \ube0c\ub9ac\ud2b8\ub2c8\uc5d0\uac8c \uc601\uac10\uc744 \uc5bb\uc5c8\ub2e4\uace0 \uc5b8\uae09\ub418\uc5c8\ub2e4.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Generation)***: [raw metric file](https://huggingface.co/lmqg/mbart-large-cc25-koquad-qg-ae/raw/main/eval/metric.first.sentence.paragraph_answer.question.lmqg_qg_koquad.default.json) \n\n|            |   Score | Type    | Dataset                                                          |\n|:-----------|--------:|:--------|:-----------------------------------------------------------------|\n| BERTScore  |   83.52 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_1     |   26.03 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_2     |   18.93 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_3     |   14.14 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_4     |   10.7  | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| METEOR     |   29.73 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| MoverScore |   82.79 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| ROUGE_L    |   27.02 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n\n\n- ***Metric (Question & Answer Generation)***:  [raw metric file](https://huggingface.co/lmqg/mbart-large-cc25-koquad-qg-ae/raw/main/eval/metric.first.answer.paragraph.questions_answers.lmqg_qg_koquad.default.json)\n\n|                                 |   Score | Type    | Dataset                                                          |\n|:--------------------------------|--------:|:--------|:-----------------------------------------------------------------|\n| QAAlignedF1Score (BERTScore)    |   80.81 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| QAAlignedF1Score (MoverScore)   |   83.42 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| QAAlignedPrecision (BERTScore)  |   77.64 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| QAAlignedPrecision (MoverScore) |   79.08 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| QAAlignedRecall (BERTScore)     |   84.32 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| QAAlignedRecall (MoverScore)    |   88.44 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n\n\n- ***Metric (Answer Extraction)***: [raw metric file](https://huggingface.co/lmqg/mbart-large-cc25-koquad-qg-ae/raw/main/eval/metric.first.answer.paragraph_sentence.answer.lmqg_qg_koquad.default.json)\n\n|                  |   Score | Type    | Dataset                                                          |\n|:-----------------|--------:|:--------|:-----------------------------------------------------------------|\n| AnswerExactMatch |   82.17 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| AnswerF1Score    |   88.2  | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| BERTScore        |   95.53 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_1           |   68.81 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_2           |   56.84 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_3           |   40.49 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_4           |   24.34 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| METEOR           |   59.82 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| MoverScore       |   94.69 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| ROUGE_L          |   82.78 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_koquad\n - dataset_name: default\n - input_types: ['paragraph_answer', 'paragraph_sentence']\n - output_types: ['question', 'answer']\n - prefix_types: ['qg', 'ae']\n - model: facebook/mbart-large-cc25\n - max_length: 512\n - max_length_output: 32\n - epoch: 6\n - batch: 2\n - lr: 0.0001\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 32\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](https://huggingface.co/lmqg/mbart-large-cc25-koquad-qg-ae/raw/main/trainer_config.json).\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "size_bytes": "2444587421", "downloads": 16}
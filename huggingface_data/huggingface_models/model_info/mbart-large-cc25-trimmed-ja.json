{"pretrained_model_name": "research-backup/mbart-large-cc25-trimmed-ja", "description": "# Vocabulary Trimmed [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25): `asahi417/mbart-large-cc25-trimmed-ja` \nThis model is a trimmed version of [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25) by [`vocabtrimmer`](https://github.com/asahi417/lm-vocab-trimmer), a tool for trimming vocabulary of language models to compress the model size.\nFollowing table shows a summary of the trimming process.\n\n|                            | facebook/mbart-large-cc25   | asahi417/mbart-large-cc25-trimmed-ja   |\n|:---------------------------|:----------------------------|:---------------------------------------|\n| parameter_size_full        | 610,851,840                 | 386,891,776                            |\n| parameter_size_embedding   | 512,055,296                 | 64,135,168                             |\n| vocab_size                 | 250,027                     | 31,316                                 |\n| compression_rate_full      | 100.0                       | 63.34                                  |\n| compression_rate_embedding | 100.0                       | 12.53                                  |", "size_bytes": "1547860957", "downloads": 2}
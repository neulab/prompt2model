{"pretrained_model_name": "DmitriyVasiliev/ruT5-base-simplification", "description": "---\nlanguage:\n- ru\ntags:\n- Simplification\n- Summarization\n- paraphrase\n---\n\u0414\u0430\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0434\u043e\u043e\u0431\u0443\u0447\u043d\u043d\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0435\u0439 \"ai-forever/ruT5-base\" (\u0440\u0430\u043d\u0435\u0435\"sberbank-ai/ruT5-base\") \u043d\u0430 \u0437\u0430\u0434\u0430\u0447\u0435 \u0443\u043f\u0440\u043e\u0449\u0435\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430 (text simplification).\n\u041d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 (https://drive.google.com/file/d/14lCIp0TJ78R8E9miVm5Ac88guEjCkgFR) \u0431\u044b\u043b \u0441\u043e\u0431\u0440\u0430\u043d \u0438\u0437 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u043e\u0432 \u043a\u043e\u043d\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0438 \"Dialog-21\" (https://github.com/dialogue-evaluation/RuSimpleSentEval), \u043a\u043e\u0440\u043f\u0443\u0441\u0430 \"RuAdapt\" (https://github.com/Digital-Pushkin-Lab/RuAdapt), \u0430 \u0442\u0430\u043a\u0436\u0435 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u0438\u043d\u0441\u0442\u0438\u0442\u0443\u0442\u043e\u043c \u0418\u0424\u0438\u042f\u041a \u0421\u0424\u0423.\n\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f: \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0444\u0430\u0439\u043b\u0430 \"dia_train\", Learning Rate = 3e-5, batch size = 1, optimizer = AdamW.\n\u041e\u0446\u0435\u043d\u043a\u0430 SARI (\u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043d\u0430 \u0444\u0430\u0439\u043b\u0435 \"dia_test\") = 33,14\n\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"DmitriyVasiliev/ruT5-base-simplification\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ninput_text = \"\u0425\u043e\u0442\u044f \u043c\u0435\u0447\u0442\u0435 \u041f\u0435\u0442\u0440\u0430 \u043d\u0435 \u0441\u0443\u0436\u0434\u0435\u043d\u043e \u0431\u044b\u043b\u043e \u0441\u0431\u044b\u0442\u044c\u0441\u044f, \u0438\u043c\u0435\u043d\u043d\u043e \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0439 \u043e\u043f\u044b\u0442 \u043b\u0451\u0433 \u0432 \u043e\u0441\u043d\u043e\u0432\u0443 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u0437\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438.\"\nml = 2048\n\n\nimport re\ndef out_gen(input_line):\n  WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n  input_ids = tokenizer(\n      [WHITESPACE_HANDLER(input_line)],\n      return_tensors=\"pt\",\n      padding=\"max_length\",\n      truncation=True,\n      max_length=ml\n    )[\"input_ids\"].to(device)\n  output_ids = model.generate(\n      input_ids=input_ids,\n      max_length=ml,\n      no_repeat_ngram_size=2,\n      num_beams=5,\n    )[0].to(device)\n  summary = tokenizer.decode(\n    output_ids,\n    skip_special_tokens=True,\n    clean_up_tokenization_spaces=False\n  )\n  return summary\n\nanswer = out_gen(input_text)\n```", "size_bytes": "891702929", "downloads": 11}
{"pretrained_model_name": "Salesforce/codet5-large", "description": "---\nlicense: bsd-3-clause\n---\n# CodeT5 (large-size model 770M)\n\n## Model description\n\nCodeT5 is a family of encoder-decoder language models for code from the paper: [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/pdf/2109.00859.pdf) by Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi.\n\nThe checkpoint included in this repository is denoted as **CodeT5-large** (770M), which is introduced by the paper: [CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning](https://arxiv.org/pdf/2207.01780.pdf) by Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi.\n\n## Training data\n\nCodeT5-large was pretrained on [CodeSearchNet](https://arxiv.org/abs/1909.09436) data in six programming languages (Ruby/JavaScript/Go/Python/Java/PHP).  See Section 4.1 of the [paper](https://arxiv.org/pdf/2207.01780.pdf) for more details.\n\n## Training procedure\n\nCodeT5-large was pretrained using masked span prediction objective for 150 epochs. See Section 4.1 of the [paper](https://arxiv.org/pdf/2207.01780.pdf) for more details.\n\n\n## Evaluation results\nWe validate the effectiveness of this checkpoint pretrained with simplified strategies on [CodeXGLUE](https://github.com/microsoft/CodeXGLUE) benchmark. See Appendix A.1 of the [paper](https://arxiv.org/pdf/2207.01780.pdf) for more details.\n\n\n## How to use\n\nThis model can be easily loaded using the `T5ForConditionalGeneration` functionality:\n\n```python\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-large\")\ntext = \"def greet(user): print(f'hello <extra_id_0>!')\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\n# simply generate a single sequence\ngenerated_ids = model.generate(input_ids, max_length=8)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n```\n\n## BibTeX entry and citation info\n\n```bibtex\n@inproceedings{CodeT52021,\n  author    = {Yue Wang and Weishi Wang and Shafiq R. Joty and Steven C. H. Hoi},\n  title     = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},\n  booktitle = {EMNLP},\n  pages     = {8696--8708},\n  publisher = {Association for Computational Linguistics},\n  year      = {2021}\n}\n\n@article{CodeRL2022\n  author    = {Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi},\n  title     = {CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},\n  journal   = {arXiv preprint},\n  volume    = {abs/2207.01780},\n  year      = {2022}\n}\n```", "size_bytes": "1475422904", "downloads": 2591}
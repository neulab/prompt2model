{"pretrained_model_name": "nielsr/tapex-large-finetuned-sqa", "description": "---\nlanguage: en\ntags:\n- tapex\n- table-question-answering\nlicense: apache-2.0\ndatasets:\n- msr_sqa\ninference: false\n---\n\nTAPEX-large model fine-tuned on SQA. This model was proposed in [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. Original repo can be found [here](https://github.com/microsoft/Table-Pretraining).\n\nTo load it and run inference, you can do the following:\n\n```\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nimport pandas as pd\n\ntokenizer = BartTokenizer.from_pretrained(\"nielsr/tapex-large-finetuned-sqa\")\nmodel = BartForConditionalGeneration.from_pretrained(\"nielsr/tapex-large-finetuned-sqa\")\n\n# create table\ndata = {'Actors': [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], 'Number of movies': [\"87\", \"53\", \"69\"]}\ntable = pd.DataFrame.from_dict(data)\n\n# turn into dict\ntable_dict = {\"header\": list(table.columns), \"rows\": [list(row.values) for i,row in table.iterrows()]}\n\n# turn into format TAPEX expects\n# define the linearizer based on this code: https://github.com/microsoft/Table-Pretraining/blob/main/tapex/processor/table_linearize.py\nlinearizer = IndexedRowTableLinearize()\nlinear_table = linearizer.process_table(table_dict)\n\n# add question\nquestion = \"how many movies does George Clooney have?\"\njoint_input = question + \" \" + linear_table\n\n# encode \nencoding = tokenizer(joint_input, return_tensors=\"pt\")\n\n# forward pass\noutputs = model.generate(**encoding)\n\n# decode\ntokenizer.batch_decode(outputs, skip_special_tokens=True)\n```", "size_bytes": "1625546049", "downloads": 52}
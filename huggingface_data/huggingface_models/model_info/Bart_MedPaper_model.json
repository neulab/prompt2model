{"pretrained_model_name": "usakha/Bart_MedPaper_model", "description": "---\ndatasets:\n- pszemraj/scientific_lay_summarisation-plos-norm\nlanguage:\n- en\nmetrics:\n- bleu\n- rouge\npipeline_tag: summarization\n---\n\n# Hyperparameters\n    learning_rate=2e-5\n    per_device_train_batch_size=14\n    per_device_eval_batch_size=14\n    weight_decay=0.01\n    save_total_limit=3\n    num_train_epochs=3\n    predict_with_generate=True\n    fp16=True\n\n# Training Output\n    global_step=4248,\n    training_loss=2.172659089111788,\n    metrics={'train_runtime': 3371.7912,\n    'train_samples_per_second': 17.633,\n    'train_steps_per_second': 1.26,\n    'total_flos': 1.2884303701396685e+17,\n    'train_loss': 2.172659089111788,\n    'epoch': 3.0}\n\n# Training Results\n\n| Epoch\t| Training Loss | Validation Loss |  Rouge1  |  Rouge2\t|  Rougel  | Rougelsum |   Bleu\t  |  Gen Len  |\n|:----- |:------------  |:--------------- |:-------- | :------- |:-------- |:--------- |:-------- |:--------- |\n|  1    |    2.318000   |     2.079500    | 0.128100 | 0.046700 | 0.104200 |  0.104200 | 0.001100 | 20.000000 |\n|  2\t|    2.130000   |     2.043523    | 0.130200 | 0.047400 | 0.105400 |  0.105300 | 0.001300 | 20.000000 |\n|  3\t|    2.047100   |     2.034664    | 0.130700 | 0.047800 | 0.105900 |  0.105900 | 0.001300 | 20.000000 |", "size_bytes": "1625541389", "downloads": 14}
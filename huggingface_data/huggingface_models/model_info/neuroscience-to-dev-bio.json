{"pretrained_model_name": "levinlab/neuroscience-to-dev-bio", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: neuroscience-to-dev-bio\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# neuroscience-to-dev-bio\n\nThis model is a fine-tuned version of [facebook/bart-large](https://huggingface.co/facebook/bart-large) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0374\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 128\n- total_train_batch_size: 128\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 100\n- num_epochs: 5000\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 19.3369       | 0.95  | 7    | 17.9500         |\n| 17.1316       | 1.95  | 14   | 15.1026         |\n| 14.0654       | 2.95  | 21   | 12.5013         |\n| 12.6374       | 3.95  | 28   | 11.3803         |\n| 11.7608       | 4.95  | 35   | 10.2692         |\n| 10.5271       | 5.95  | 42   | 8.7652          |\n| 9.0429        | 6.95  | 49   | 6.8763          |\n| 7.4963        | 7.95  | 56   | 5.9052          |\n| 6.6044        | 8.95  | 63   | 5.3443          |\n| 6.007         | 9.95  | 70   | 4.8687          |\n| 5.4706        | 10.95 | 77   | 4.3708          |\n| 4.8812        | 11.95 | 84   | 3.8094          |\n| 4.2359        | 12.95 | 91   | 3.1743          |\n| 3.4727        | 13.95 | 98   | 2.4480          |\n| 2.6582        | 14.95 | 105  | 1.6751          |\n| 1.8084        | 15.95 | 112  | 0.9828          |\n| 1.0742        | 16.95 | 119  | 0.5074          |\n| 0.5521        | 17.95 | 126  | 0.2471          |\n| 0.263         | 18.95 | 133  | 0.1276          |\n| 0.1281        | 19.95 | 140  | 0.0761          |\n| 0.0826        | 20.95 | 147  | 0.0620          |\n| 0.0419        | 21.95 | 154  | 0.0434          |\n| 0.0685        | 22.95 | 161  | 0.1522          |\n| 0.1332        | 23.95 | 168  | 0.0536          |\n| 0.0405        | 24.95 | 175  | 0.0405          |\n| 0.0214        | 25.95 | 182  | 0.0380          |\n| 0.0142        | 26.95 | 189  | 0.0370          |\n| 0.0202        | 27.95 | 196  | 0.0375          |\n| 0.0105        | 28.95 | 203  | 0.0413          |\n| 0.0092        | 29.95 | 210  | 0.0370          |\n| 0.0083        | 30.95 | 217  | 0.0384          |\n| 0.0079        | 31.95 | 224  | 0.0406          |\n| 0.0381        | 32.95 | 231  | 0.0371          |\n| 0.011         | 33.95 | 238  | 0.0439          |\n| 0.0066        | 34.95 | 245  | 0.0374          |\n\n\n### Framework versions\n\n- Transformers 4.21.2\n- Pytorch 1.12.1+cu113\n- Datasets 2.4.0\n- Tokenizers 0.12.1\n", "size_bytes": "1625537793", "downloads": 2}
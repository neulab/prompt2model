{"pretrained_model_name": "knarasi1/BART_pretrained_on_billsum_finetuned_on_small_SCOTUS_extracted_dataset_2", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: BART_pretrained_on_billsum_finetuned_on_small_SCOTUS_extracted_dataset_2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# BART_pretrained_on_billsum_finetuned_on_small_SCOTUS_extracted_dataset_2\n\nThis model is a fine-tuned version of [bheshaj/bart-large-billsum-epochs20](https://huggingface.co/bheshaj/bart-large-billsum-epochs20) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 9.5858\n- Rouge1: 0.06\n- Rouge2: 0.0\n- Rougel: 0.06\n- Rougelsum: 0.0598\n- Gen Len: 20.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.02\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 6\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|:---------:|:-------:|\n| 23.3274       | 0.98  | 10   | 35.8778         | 0.0289 | 0.0    | 0.0288 | 0.0289    | 20.0    |\n| 43.999        | 1.98  | 20   | 39.9056         | 0.0475 | 0.0045 | 0.0453 | 0.0453    | 20.0    |\n| 32.0042       | 2.98  | 30   | 25.4613         | 0.0516 | 0.0005 | 0.0499 | 0.0499    | 20.0    |\n| 21.3151       | 3.98  | 40   | 17.6485         | 0.0021 | 0.0    | 0.0021 | 0.0021    | 20.0    |\n| 15.4017       | 4.98  | 50   | 12.6187         | 0.06   | 0.0    | 0.06   | 0.0598    | 20.0    |\n| 10.9491       | 5.98  | 60   | 9.5858          | 0.06   | 0.0    | 0.06   | 0.0598    | 20.0    |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.13.1+cu116\n- Datasets 2.10.1\n- Tokenizers 0.13.2\n", "size_bytes": "1682597005", "downloads": 5}
{"pretrained_model_name": "tarek23/flan-t5-base-qg-SQuAD-10-v3", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- squad\nmetrics:\n- rouge\nmodel-index:\n- name: flan-t5-base-qg-SQuAD-10-v3\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n      args: plain_text\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 52.7338\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-base-qg-SQuAD-10-v3\n\nThis model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) on the squad dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5822\n- Rouge1: 52.7338\n- Rouge2: 30.0274\n- Rougel: 48.6696\n- Rougelsum: 48.6837\n- Meteor: 47.9097\n- Bleu-n: 21.1510\n- Bleu-1: 52.8292\n- Bleu-2: 26.9042\n- Bleu-3: 17.0624\n- Bleu-4: 11.3263\n- Gen Len: 14.3502\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Meteor  | Bleu-n  | Bleu-1  | Bleu-2  | Bleu-3  | Bleu-4  | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| 0.5958        | 1.0   | 10950 | 0.5637          | 52.0014 | 29.3355 | 48.0977 | 48.1142   | 46.8122 | 20.7193 | 52.9094 | 26.6582 | 17.0250 | 11.4145 | 14.1124 |\n| 0.5168        | 2.0   | 21900 | 0.5618          | 52.616  | 29.9824 | 48.6695 | 48.6858   | 47.6756 | 21.2477 | 52.8608 | 26.9682 | 17.2504 | 11.5263 | 14.3338 |\n| 0.4748        | 3.0   | 32850 | 0.5682          | 52.6758 | 30.0082 | 48.6759 | 48.6901   | 47.7055 | 21.1081 | 52.9757 | 26.9437 | 17.2084 | 11.4549 | 14.2867 |\n| 0.4297        | 4.0   | 43800 | 0.5759          | 52.9434 | 30.2447 | 48.8937 | 48.9019   | 48.0327 | 21.2589 | 53.1581 | 27.1232 | 17.2418 | 11.4865 | 14.3004 |\n| 0.4087        | 5.0   | 54750 | 0.5822          | 52.7338 | 30.0274 | 48.6696 | 48.6837   | 47.9097 | 21.1510 | 52.8292 | 26.9042 | 17.0624 | 11.3263 | 14.3502 |\n\n\n### Framework versions\n\n- Transformers 4.28.1\n- Pytorch 2.0.0\n- Datasets 2.1.0\n- Tokenizers 0.13.3\n", "size_bytes": "990242997", "downloads": 2}
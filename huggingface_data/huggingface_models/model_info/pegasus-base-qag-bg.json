{"pretrained_model_name": "rmihaylov/pegasus-base-qag-bg", "description": "---\ninference: false\nlanguage:\n- bg\nlicense: mit\ndatasets:\n- oscar\n- chitanka\n- wikipedia\ntags:\n- torch\n---\n\n# PEGASUS BASE\n\nThis model was pretrained on Bulgarian language. It was intorduced in [this paper](https://arxiv.org/pdf/1912.08777.pdf). \n\n## Model description\n\nThe training data is private Bulgarian squad data. \n\n## Intended uses & limitations\n\nYou can use the raw model for generation of question-answer pairs related with given Bulgarian text.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\n>>> from transformers import PegasusForConditionalGeneration, AlbertTokenizer\n>>>\n>>> model_id = \"rmihaylov/pegasus-base-qag-bg\"\n>>> model = PegasusForConditionalGeneration.from_pretrained(model_id)\n>>> tokenizer = AlbertTokenizer.from_pretrained(model_id)\n>>>\n>>> text = \"\"\"\u0422\u043e\u0432\u0430, \u0447\u0435 \u043d\u044f\u043a\u043e\u0439 \u043c\u043e\u0436\u0435 \u0434\u0430 \u0437\u0430\u044f\u0432\u0438 \u043d\u0430 \u043d\u0430\u0439-\u0441\u0438\u043b\u0435\u043d \u0433\u043b\u0430\u0441 \u0438\u0441\u043a\u0430\u043d\u0438\u044f\u0442\u0430 \u0441\u0438, \u043d\u0435 \u043e\u0437\u043d\u0430\u0447\u0430\u0432\u0430 \u0442\u0435 \u0434\u0430 \u0431\u044a\u0434\u0430\u0442 \u0443\u0434\u043e\u0432\u043b\u0435\u0442\u0432\u043e\u0440\u0435\u043d\u0438, \u0437\u0430\u044f\u0432\u0438 \u041a\u043e\u0441\u0442\u0430\u0434\u0438\u043d \u0410\u043d\u0433\u0435\u043b\u043e\u0432. \n\u0422\u043e\u0439 \u0434\u043e\u043f\u044a\u043b\u043d\u0438, \u0447\u0435 \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442\u0438\u0442\u0435 \u043d\u0430 \u0432\u043b\u0430\u0441\u0442\u0438\u0442\u0435 \u0441\u0430 \u0437\u0434\u0440\u0430\u0432\u0435\u0442\u043e, \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0438 \u0441\u043f\u043e\u0440\u0442\u0430, \u0434\u0430\u0432\u0430\u0439\u043a\u0438 \u0437\u043d\u0430\u043a, \u0447\u0435 \u0441\u0435 \u0442\u044a\u0440\u0441\u0438 \u0440\u0430\u0437\u0445\u043b\u0430\u0431\u0432\u0430\u043d\u0435 \u043d\u0430 \u043c\u0435\u0440\u043a\u0438\u0442\u0435 \u0432 \u0431\u043e\u043b\u043d\u0438\u0447\u043d\u0438\u0442\u0435 \u0437\u0430\u0432\u0435\u0434\u0435\u043d\u0438\u044f, \u0432\u0440\u044a\u0449\u0430\u043d\u0435\u0442\u043e \u043d\u0430 \u0443\u0447\u0435\u043d\u0438\u0446\u0438\u0442\u0435 \u0432 \u043a\u043b\u0430\u0441\u043d\u0438\u0442\u0435 \u0441\u0442\u0430\u0438 \u0438 \u043e\u0442\u0432\u0430\u0440\u044f\u043d\u0435\u0442\u043e \u043d\u0430 \u043e\u0431\u0435\u043a\u0442\u0438\u0442\u0435 \u0437\u0430 \u043c\u0430\u0441\u043e\u0432 \u0441\u043f\u043e\u0440\u0442.\n\"\"\"\n>>>\n>>> inputs = tokenizer.encode_plus(\n>>>     text, \n>>>     return_tensors='pt', \n>>>     truncation=True, \n>>>     max_length=512, \n>>>     return_token_type_ids=False, \n>>>     return_attention_mask=True)\n>>> \n>>> outputs = model.generate(**inputs,                          \n>>>     max_length=150, \n>>>     top_p=0.95, \n>>>     top_k=20, \n>>>     do_sample=True, \n>>>     num_return_sequences=10, \n>>>     num_beams=1,\n>>>     eos_token_id=50259,\n>>>     decoder_start_token_id=50257,\n>>>     return_dict_in_generate=True,\n>>>     output_scores=True)\n>>> \n>>> for g in outputs.sequences:\n>>>   text_gen = tokenizer.decode(g, skip_special_tokens=False)\n>>> \n>>>   if ('[SEP]' not in text_gen) or ('[MASK]' not in text_gen) or ('[CLS]' not in text_gen):\n>>>     continue\n>>> \n>>>   question, answer = text_gen.replace('[CLS]', '').strip().split('[SEP]')\n>>>   answer = answer.split('[MASK]')[0].strip()\n>>>   \n>>>   if (not answer) or (answer not in text) or (len(answer) <= 1):   \n>>>     continue\n>>> \n>>> print(f'{question.strip()}\\n{answer.strip()}', '\\n\\n')\n\n\u041a\u0430\u043a\u0432\u043e \u0442\u0440\u044f\u0431\u0432\u0430 \u0434\u0430 \u0441\u0435 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u0435\u043c\u0435, \u0437\u0430 \u0434\u0430 \u0441\u0435 \u0441\u043b\u0443\u0447\u0438?\n\u0440\u0430\u0437\u0445\u043b\u0430\u0431\u0432\u0430\u043d\u0435 \n\n\n\u041a\u0430\u043a\u0432\u0438 \u0441\u0430 \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442\u0438\u0442\u0435 \u043d\u0430 \u0443\u043f\u0440\u0430\u0432\u043b\u044f\u0432\u0430\u0449\u0438\u0442\u0435?\n\u0437\u0434\u0440\u0430\u0432\u0435\u0442\u043e, \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0438 \u0441\u043f\u043e\u0440\u0442\u0430, \n\n\n\u041a\u0430\u043a\u0432\u0438 \u0443\u0441\u0438\u043b\u0438\u044f \u0438\u043c\u0430 \u043f\u0440\u0430\u0432\u0438\u0442\u0435\u043b\u0441\u0442\u0432\u043e\u0442\u043e \u0437\u0430 \u0441\u0442\u0438\u043c\u0443\u043b\u0438\u0440\u0430\u043d\u0435 \u043d\u0430 \u0440\u0430\u0436\u0434\u0430\u0435\u043c\u043e\u0441\u0442\u0442\u0430?\n\u0440\u0430\u0437\u0445\u043b\u0430\u0431\u0432\u0430\u043d\u0435 \u043d\u0430 \u043c\u0435\u0440\u043a\u0438\u0442\u0435 \n\n\n\u041a\u0430\u043a\u044a\u0432 \u0435 \u043e\u0441\u043d\u043e\u0432\u043d\u0438\u044f\u0442 \u043f\u0440\u043e\u0431\u043b\u0435\u043c, \u043a\u043e\u0439\u0442\u043e \u043c\u043e\u0436\u0435 \u0434\u0430 \u0440\u0435\u0448\u0438?\n\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \n```\n", "size_bytes": "554792057", "downloads": 2}
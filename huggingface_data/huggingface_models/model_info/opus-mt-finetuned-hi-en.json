{"pretrained_model_name": "AbhirupGhosh/opus-mt-finetuned-hi-en", "description": "---\nlanguage:\n- hi\n- en\n- multilingual\nlicense: apache-2.0\ntags:\n- translation\n- Hindi\n- generated_from_keras_callback\nmodel-index:\n- name: opus-mt-finetuned-hi-en\n  results: []\n---\n\n# opus-mt-finetuned-hi-en\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-hi-en](https://huggingface.co/Helsinki-NLP/opus-mt-hi-en) on [HindiEnglish Corpora](https://www.clarin.eu/resource-families/parallel-corpora)\n\n\n## Model description\n\nThe model is a transformer model similar to the [Transformer](https://arxiv.org/abs/1706.03762?context=cs) as defined in Attention Is All You Need et al\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\nThe model was trained on 2 NVIDIA_TESLA_A100 GPU's on Google's vertex AI platform.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: AdamWeightDecay\n- training_precision: float32\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.20.1\n- TensorFlow 2.8.2\n- Datasets 2.3.2\n- Tokenizers 0.12.1\n", "size_bytes": "302067461", "downloads": 12}
{"pretrained_model_name": "BELLE-2/BELLE-Llama2-13B-chat-0.4M", "description": "---\nlicense: llama2\ntags:\n- text2text-generation\npipeline_tag: text2text-generation\nlanguage:\n- zh\n- en\n---\n\n# Model Card for Model ID\n\n## Welcome\nIf you find this model helpful, please *like* this model and star us on https://github.com/LianjiaTech/BELLE !\n\n## Model description\nThis model is obtained by fine-tuning the complete parameters using 0.4M Chinese instruction data on the original Llama2-13B-chat. \nWe firmly believe that the original Llama2-chat exhibits commendable performance post Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF).\nOur pursuit continues to be the further enhancement of this model using Chinese instructional data for fine-tuning, with an aspiration to facilitate stable and high-quality \nChinese language outputs.\n## Use model\nPlease note that the input should be formatted as follows in both **training** and **inference**.\n``` python\nHuman: \\n{input}\\n\\nAssistant:\\n\n``` \n\n\nAfter you decrypt the files, BELLE-Llama2-13B-chat-0.4M can be easily loaded with AutoModelForCausalLM.\n``` python\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer\nimport torch\n\nckpt = '/path/to_finetuned_model/'\ndevice = torch.device('cuda')\nmodel = AutoModelForCausalLM.from_pretrained(ckpt).half().to(device)\ntokenizer = LlamaTokenizer.from_pretrained(ckpt)\nprompt = \"Human: \\n\u5199\u4e00\u9996\u4e2d\u6587\u6b4c\u66f2\uff0c\u8d5e\u7f8e\u5927\u81ea\u7136 \\n\\nAssistant: \\n\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\ngenerate_ids = model.generate(input_ids, max_new_tokens=1024, do_sample=True, top_k=30, top_p=0.85, temperature=0.5, repetition_penalty=1.2, eos_token_id=2, bos_token_id=1, pad_token_id=0)\noutput = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nresponse = output[len(prompt):]\nprint(response)\n\n```\n\n\n## Limitations\nThere still exists a few issues in the model trained on current base model and data:\n\n1. The model might generate factual errors when asked to follow instructions related to facts.\n\n2. Occasionally generates harmful responses since the model still struggles to identify potential harmful instructions.\n\n3. Needs improvements on reasoning and coding.\n\nSince the model still has its limitations, we require developers only use the open-sourced code, data, model and any other artifacts generated via this project for research purposes. Commercial use and other potential harmful use cases are not allowed.", "size_bytes": 26031738880, "downloads": 85}
{"pretrained_model_name": "bowphs/LaTa", "description": "---\nlanguage: la\nlicense: apache-2.0\ninference: false\n---\n# LaTa\n\nThe paper [Exploring Language Models for Classical Philology](https://todo.com) is the first effort to systematically provide state-of-the-art language models for Classical Philology. LaTa is a T5-base sized, monolingual, encoder-decoder variant.  \n\nThis model was trained on the [Corpus Corporum](https://mlat.uzh.ch/).\n\nFurther information can be found in our paper or in our [GitHub repository](https://github.com/Heidelberg-NLP/ancient-language-models).\n\n## Usage\n```python\nfrom transformers import AutoTokenizer, AutoModelForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained('bowphs/LaTa')\nmodel = AutoModelForConditionalGeneration.from_pretrained('bowphs/LaTa')\n```\nPlease check out the awesome Hugging Face tutorials on how to fine-tune our models.\n\n## Evaluation Results\nWhen fine-tuned on lemmatization data from [EvaLatin 2022](https://universaldependencies.org/), LaTa achieves the following results:\n\n| Task | Classical | Cross-genre  | Cross-time | \n|:--:|:--:|:--:|:--:|\n|      |97.30|93.95|92.26|\n\n## Contact\nIf you have any questions or problems, feel free to [reach out](mailto:riemenschneider@cl.uni-heidelberg.de).\n\n## Citation\n```bibtex\n@incollection{riemenschneiderfrank:2023,\n    address = \"Toronto, Canada\",\n    author = \"Riemenschneider, Frederick and Frank, Anette\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL\u201923)\",\n    note = \"to appear\",\n    pubType = \"incollection\",\n    publisher = \"Association for Computational Linguistics\",\n    title = \"Exploring Large Language Models for Classical Philology\",\n    url = \"https://arxiv.org/abs/2305.13698\",\n    year = \"2023\",\n    key = \"riemenschneiderfrank:2023\"\n}\n```\n", "size_bytes": "1113160781", "downloads": 2}
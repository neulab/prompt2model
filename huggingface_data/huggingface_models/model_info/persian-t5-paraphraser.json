{"pretrained_model_name": "erfan226/persian-t5-paraphraser", "description": "---\nlanguage: fa\ntags:\n- paraphrasing\ndatasets:\n- tapaco\nwidget:\n- text: \"\u0627\u06cc\u0646 \u06cc\u06a9 \u0645\u0642\u0627\u0644\u0647\u0654 \u062e\u0631\u062f \u0622\u0644\u0645\u0627\u0646 \u0627\u0633\u062a. \u0645\u06cc\u200c\u062a\u0648\u0627\u0646\u06cc\u062f \u0628\u0627 \u06af\u0633\u062a\u0631\u0634 \u0622\u0646 \u0628\u0647 \u0648\u06cc\u06a9\u06cc\u200c\u067e\u062f\u06cc\u0627 \u06a9\u0645\u06a9 \u06a9\u0646\u06cc\u062f.\"\n- text: \"\u0628\u0631\u0627\u06cc \u062e\u0631\u06cc\u062f \u06cc\u06a9 \u06a9\u062a\u0627\u0628 \u0628\u0627\u06cc\u062f \u0627\u0632 \u0641\u0631\u0648\u0634\u06af\u0627\u0647 \u0627\u06cc\u0646\u062a\u0631\u0646\u062a\u06cc \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u06a9\u0646\u06cc\u062f.\"\n\n---\n\n# Persian-t5-paraphraser\n\nThis is a paraphrasing model for the Persian language. It is based on [the monolingual T5 model for Persian.](https://huggingface.co/Ahmad/parsT5-base)\n\n## Usage\n\n```python\n\n>>> pip install transformers\n>>> from transformers import (T5ForConditionalGeneration, AutoTokenizer, pipeline)\n>>> import torch\n\nmodel_path = 'erfan226/persian-t5-paraphraser'\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\npipe = pipeline(task='text2text-generation', model=model, tokenizer=tokenizer)\n\ndef paraphrase(text):\n  for j in range(5):\n    out = pipe(text, encoder_no_repeat_ngram_size=5, do_sample=True, num_beams=5, max_length=128)[0]['generated_text']\n    print(\"Paraphrase:\", out)\n\ntext = \"\u0627\u06cc\u0646 \u06cc\u06a9 \u0645\u0642\u0627\u0644\u0647\u0654 \u062e\u0631\u062f \u0622\u0644\u0645\u0627\u0646 \u0627\u0633\u062a. \u0645\u06cc\u200c\u062a\u0648\u0627\u0646\u06cc\u062f \u0628\u0627 \u06af\u0633\u062a\u0631\u0634 \u0622\u0646 \u0628\u0647 \u0648\u06cc\u06a9\u06cc\u200c\u067e\u062f\u06cc\u0627 \u06a9\u0645\u06a9 \u06a9\u0646\u06cc\u062f.\"\nprint(\"Original:\", text)\nparaphrase(text)\n\n# Original: \u0627\u06cc\u0646 \u06cc\u06a9 \u0645\u0642\u0627\u0644\u0647\u0654 \u062e\u0631\u062f \u0622\u0644\u0645\u0627\u0646 \u0627\u0633\u062a. \u0645\u06cc\u200c\u062a\u0648\u0627\u0646\u06cc\u062f \u0628\u0627 \u06af\u0633\u062a\u0631\u0634 \u0622\u0646 \u0628\u0647 \u0648\u06cc\u06a9\u06cc\u200c\u067e\u062f\u06cc\u0627 \u06a9\u0645\u06a9 \u06a9\u0646\u06cc\u062f.\n# Paraphrase: \u0627\u06cc\u0646 \u06cc\u06a9 \u0645\u0642\u0627\u0644\u0647\u0654 \u06a9\u0648\u0686\u06a9 \u0627\u0633\u062a.\n# Paraphrase: \u0627\u06cc\u0646 \u06cc\u06a9 \u0645\u0642\u0627\u0644\u0647\u0654 \u06a9\u0648\u0686\u06a9 \u0627\u0633\u062a.\n# Paraphrase: \u0634\u0645\u0627 \u0645\u06cc \u062a\u0648\u0627\u0646\u06cc\u062f \u0628\u0627 \u06af\u0633\u062a\u0631\u0634 \u0627\u06cc\u0646 \u0645\u0642\u0627\u0644\u0647\u060c \u0628\u0647 \u06a9\u0633\u0628 \u0648 \u06a9\u0627\u0631 \u062e\u0648\u062f \u06a9\u0645\u06a9 \u06a9\u0646\u06cc\u062f.\n# Paraphrase: \u0645\u06cc \u062a\u0648\u0627\u0646\u06cc\u062f \u0628\u0627 \u06af\u0633\u062a\u0631\u0634 \u0627\u06cc\u0646 \u0645\u0642\u0627\u0644\u0647\u0654 \u062e\u0631\u062f \u0622\u0644\u0645\u0627\u0646 \u06a9\u0645\u06a9 \u06a9\u0646\u06cc\u062f.\n# Paraphrase: \u0634\u0645\u0627 \u0645\u06cc \u062a\u0648\u0627\u0646\u06cc\u062f \u0628\u0627 \u06af\u0633\u062a\u0631\u0634 \u0627\u06cc\u0646 \u0645\u0642\u0627\u0644\u0647\u0654 \u062e\u0631\u062f\u060c \u0628\u0647 \u06af\u0633\u062a\u0631\u0634 \u0622\u0646 \u06a9\u0645\u06a9 \u06a9\u0646\u06cc\u062f.\n\n```\n\n## Training data\nThis model was trained on the Persian subset of the [Tapaco dataset](https://huggingface.co/datasets/tapaco). It should be noted that this model was trained on a very small dataset and therefore the performance might not be as expected, for now.", "size_bytes": "990249037", "downloads": 44}
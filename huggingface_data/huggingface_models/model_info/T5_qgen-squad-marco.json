{"pretrained_model_name": "AbhilashDatta/T5_qgen-squad-marco", "description": "---\nlicense: afl-3.0\n---\n\n# Question generation using T5 transformer\n\n<h2> <i>Input format: context: \"...\" answer: \"...\" </i></h2>\n\nImport the pretrained model as well as tokenizer:\n```\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nmodel = T5ForConditionalGeneration.from_pretrained('AbhilashDatta/T5_qgen-squad-marco') \ntokenizer = T5Tokenizer.from_pretrained('AbhilashDatta/T5_qgen-squad-marco')\n```\n\nThen use the tokenizer to encode/decode and model to generate: \n\n```\ninput = \"context: My name is Abhilash Datta. answer: Abhilash\"\nbatch = tokenizer(input, padding='longest', max_length=512, return_tensors='pt')\ninputs_batch = batch['input_ids'][0]\ninputs_batch = torch.unsqueeze(inputs_batch, 0)\n\nques_id = model.generate(inputs_batch, max_length=100, early_stopping=True)\nques_batch = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in ques_id]\n\nprint(ques_batch)\n```\n\nOutput:\n```\n['what is my name']\n```", "size_bytes": "891697151", "downloads": 4}
{"pretrained_model_name": "inseq/wmt20-mlqe-et-en", "description": "---\nlanguage:\n- en\n- et\n- multilingual\nlicense: cc-by-sa-4.0\ntags:\n- translation\n- wmt20\ndatasets:\n- wmt/europarl\nwidget:\n- text: \"Jupiter on P\u00e4ikesest kauguselt viies planeet ja P\u00e4ikeses\u00fcsteemi k\u00f5ige suurem planeet.\"\n- text: \"Plejaadid on S\u00f5nni t\u00e4htkujus asuv hajusparv, mille Messier' kataloogi t\u00e4hiseks on M45.\"\n- text: \"Palju on vaieldud Vikipeedia usaldatavuse ja t\u00e4psuse \u00fcle. Kritiseeritud on selle avatust vandaalidele, eba\u00fchtlast kvaliteeti ja vastur\u00e4\u00e4kivust, mitteneutraalsust ja konsensuse v\u00f5i populaarsuse eelistamist kvalifitseeritusele.\"\n---\n\n# Fairseq Et-En NMT WMT20 MLQE\n\nThis repository contains the Estonian-English model trained with the [fairseq toolkit](https://github.com/pytorch/fairseq) that was used to produce translations used in the WMT20 shared task on quality estimation (QE) on the [MLQE dataset](https://github.com/facebookresearch/mlqe).\n\nThe checkpoint was converted from the original fairseq checkpoint available [here](https://github.com/facebookresearch/mlqe/tree/master/nmt_models) using the `convert_fsmt_original_pytorch_checkpoint_to_pytorch.py` script from the \ud83e\udd17 Transformers library (v4.26.0).\n\nPlease refer to the repositories linked above for additional information on usage, parameters and training data", "size_bytes": "357428997", "downloads": 5}
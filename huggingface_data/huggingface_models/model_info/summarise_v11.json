{"pretrained_model_name": "debbiesoon/summarise_v11", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: summarise_v11\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# summarise_v11\n\nThis model is a fine-tuned version of [allenai/led-base-16384](https://huggingface.co/allenai/led-base-16384) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6322\n- Rouge1 Precision: 0.6059\n- Rouge1 Recall: 0.6233\n- Rouge1 Fmeasure: 0.5895\n- Rouge2 Precision: 0.4192\n- Rouge2 Recall: 0.4512\n- Rouge2 Fmeasure: 0.4176\n- Rougel Precision: 0.4622\n- Rougel Recall: 0.4946\n- Rougel Fmeasure: 0.4566\n- Rougelsum Precision: 0.4622\n- Rougelsum Recall: 0.4946\n- Rougelsum Fmeasure: 0.4566\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1 Precision | Rouge1 Recall | Rouge1 Fmeasure | Rouge2 Precision | Rouge2 Recall | Rouge2 Fmeasure | Rougel Precision | Rougel Recall | Rougel Fmeasure | Rougelsum Precision | Rougelsum Recall | Rougelsum Fmeasure |\n|:-------------:|:-----:|:----:|:---------------:|:----------------:|:-------------:|:---------------:|:----------------:|:-------------:|:---------------:|:----------------:|:-------------:|:---------------:|:-------------------:|:----------------:|:------------------:|\n| 1.6201        | 0.45  | 10   | 1.4875          | 0.3203           | 0.64          | 0.3932          | 0.197            | 0.3839        | 0.2385          | 0.1952           | 0.4051        | 0.2454          | 0.1952              | 0.4051           | 0.2454             |\n| 0.9172        | 0.91  | 20   | 1.4404          | 0.4917           | 0.5134        | 0.4699          | 0.288            | 0.3095        | 0.276           | 0.3371           | 0.3594        | 0.3277          | 0.3371              | 0.3594           | 0.3277             |\n| 1.0923        | 1.36  | 30   | 1.3575          | 0.519            | 0.5505        | 0.4936          | 0.3114           | 0.3237        | 0.2958          | 0.3569           | 0.3702        | 0.3364          | 0.3569              | 0.3702           | 0.3364             |\n| 1.1287        | 1.82  | 40   | 1.3269          | 0.4913           | 0.5997        | 0.5068          | 0.3108           | 0.3964        | 0.3269          | 0.3355           | 0.427         | 0.3521          | 0.3355              | 0.427            | 0.3521             |\n| 0.9938        | 2.27  | 50   | 1.3189          | 0.5339           | 0.5781        | 0.4973          | 0.3555           | 0.3883        | 0.3345          | 0.3914           | 0.4289        | 0.3678          | 0.3914              | 0.4289           | 0.3678             |\n| 0.8659        | 2.73  | 60   | 1.3241          | 0.525            | 0.638         | 0.5165          | 0.3556           | 0.4349        | 0.3535          | 0.3914           | 0.4793        | 0.3886          | 0.3914              | 0.4793           | 0.3886             |\n| 0.6187        | 3.18  | 70   | 1.3360          | 0.5875           | 0.5864        | 0.5416          | 0.4005           | 0.4045        | 0.3701          | 0.4485           | 0.4556        | 0.414           | 0.4485              | 0.4556           | 0.414              |\n| 0.3941        | 3.64  | 80   | 1.4176          | 0.5373           | 0.6415        | 0.5328          | 0.3576           | 0.446         | 0.3642          | 0.3787           | 0.4586        | 0.3781          | 0.3787              | 0.4586           | 0.3781             |\n| 0.4145        | 4.09  | 90   | 1.3936          | 0.4127           | 0.6553        | 0.4568          | 0.2568           | 0.4498        | 0.2988          | 0.2918           | 0.4933        | 0.328           | 0.2918              | 0.4933           | 0.328              |\n| 0.4203        | 4.55  | 100  | 1.4703          | 0.6545           | 0.601         | 0.5981          | 0.4789           | 0.4373        | 0.438           | 0.5251           | 0.4851        | 0.4818          | 0.5251              | 0.4851           | 0.4818             |\n| 0.687         | 5.0   | 110  | 1.4304          | 0.5566           | 0.6357        | 0.5637          | 0.3734           | 0.4186        | 0.3748          | 0.4251           | 0.4825        | 0.4286          | 0.4251              | 0.4825           | 0.4286             |\n| 0.4006        | 5.45  | 120  | 1.5399          | 0.5994           | 0.5794        | 0.5515          | 0.4215           | 0.4218        | 0.398           | 0.4359           | 0.4369        | 0.4084          | 0.4359              | 0.4369           | 0.4084             |\n| 0.2536        | 5.91  | 130  | 1.5098          | 0.5074           | 0.6254        | 0.4874          | 0.3369           | 0.4189        | 0.3256          | 0.3802           | 0.4738        | 0.3664          | 0.3802              | 0.4738           | 0.3664             |\n| 0.2218        | 6.36  | 140  | 1.5278          | 0.5713           | 0.6059        | 0.5688          | 0.3887           | 0.4233        | 0.3916          | 0.4414           | 0.4795        | 0.4457          | 0.4414              | 0.4795           | 0.4457             |\n| 0.2577        | 6.82  | 150  | 1.5469          | 0.5148           | 0.5941        | 0.5175          | 0.3284           | 0.3856        | 0.3335          | 0.3616           | 0.4268        | 0.3681          | 0.3616              | 0.4268           | 0.3681             |\n| 0.1548        | 7.27  | 160  | 1.5986          | 0.5983           | 0.657         | 0.5862          | 0.4322           | 0.4877        | 0.4287          | 0.4466           | 0.5167        | 0.4482          | 0.4466              | 0.5167           | 0.4482             |\n| 0.1535        | 7.73  | 170  | 1.5796          | 0.5609           | 0.641         | 0.5616          | 0.3856           | 0.4428        | 0.3892          | 0.4238           | 0.4921        | 0.4263          | 0.4238              | 0.4921           | 0.4263             |\n| 0.1568        | 8.18  | 180  | 1.6052          | 0.5669           | 0.617         | 0.5679          | 0.3911           | 0.4382        | 0.3969          | 0.4363           | 0.4877        | 0.4417          | 0.4363              | 0.4877           | 0.4417             |\n| 0.2038        | 8.64  | 190  | 1.6191          | 0.5466           | 0.5973        | 0.5313          | 0.3543           | 0.4114        | 0.3531          | 0.4061           | 0.4666        | 0.404           | 0.4061              | 0.4666           | 0.404              |\n| 0.1808        | 9.09  | 200  | 1.6165          | 0.5751           | 0.5919        | 0.5587          | 0.3831           | 0.4097        | 0.3817          | 0.4482           | 0.4728        | 0.4405          | 0.4482              | 0.4728           | 0.4405             |\n| 0.1021        | 9.55  | 210  | 1.6316          | 0.5316           | 0.6315        | 0.535           | 0.3588           | 0.4563        | 0.3697          | 0.405            | 0.502         | 0.4126          | 0.405               | 0.502            | 0.4126             |\n| 0.1407        | 10.0  | 220  | 1.6322          | 0.6059           | 0.6233        | 0.5895          | 0.4192           | 0.4512        | 0.4176          | 0.4622           | 0.4946        | 0.4566          | 0.4622              | 0.4946           | 0.4566             |\n\n\n### Framework versions\n\n- Transformers 4.21.3\n- Pytorch 1.12.1+cu113\n- Datasets 1.2.1\n- Tokenizers 0.12.1\n", "size_bytes": "647678513", "downloads": 2}
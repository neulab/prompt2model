{"pretrained_model_name": "vocabtrimmer/mt5-small-trimmed-ko-koquad-qa", "description": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: ko\ndatasets:\n- lmqg/qg_koquad\npipeline_tag: text2text-generation\ntags:\n- question answering\nwidget:\n- text: \"question: \ub9e4\ub4dc \ud074\ub77c\uc6b4\uc774 \ucc38\uac00\ud574 \ud070 \ud654\uc81c\ub97c \ubaa8\uc558\ub358 \ud504\ub85c\uadf8\ub7a8\uc740?, context: \uacfc\uac70 \uc18c\uc6b8 \ucef4\ud37c\ub2c8 \uc18c\uc18d\uc73c\ub85c \uc18c\uc6b8 \ucef4\ud37c\ub2c8 \ud574\uccb4 \ud6c4 \ud604\uc7ac\uc758 \uc18c\uc18d\uc0ac\ub294 \uc2a4\ud0c0\uc27d \uc5d1\uc2a4\uc774\ub2e4. Mad Clown vs Crucial Star (\ub9e4\ub4dc \ud074\ub77c\uc6b4 vs \ud06c\ub8e8\uc15c \uc2a4\ud0c0)\ub77c\ub294 \ud504\ub85c\uc81d\ud2b8 \uadf8\ub8f9\uc73c\ub85c \ud06c\ub8e8\uc15c \uc2a4\ud0c0\uc640 \ud568\uaed8 \ud65c\ub3d9\ud558\uae30\ub3c4 \ud558\uc600\uc73c\uba70, 2013\ub144\ubd80\ud130\ub294 MC\uc778 \uc800\uc2a4\ub514\uc2a4\uc640 \ud300\uc744 \uc774\ub8e8\uc5b4 \ub7a9 \ub4c0\uc624 \ucee4\uba3c\ucf5c\ub4dc\ub85c \ud65c\ub3d9\ud558\uace0 \uc788\ub2e4. \ub610\ud55c Mnet \u300a\uc1fc\ubbf8\ub354\uba38\ub2c8 2\u300b\uc5d0\uc11c \ucc38\uac00\uc790\ub85c \ucc38\uac00\ud558\uc5ec \ud070 \ud654\uc81c\ub97c \ubaa8\uc558\uc73c\uba70, \u300a\uc1fc\ubbf8\ub354\uba38\ub2c8 5\u300b\uc5d0\uc11c\ub294 \uae38 & \ub9e4\ub4dc \ud074\ub77c\uc6b4 \ud300\uc73c\ub85c \ud504\ub85c\ub4c0\uc11c\ub85c \ucd9c\uc5f0\ud558\uc600\ub2e4., \uc7ac\ubc1c\ub9e4 \ubb3c\ub7c9\ub3c4 \uc644\ud310\ub418\uc5b4 \ucd94\uac00 \uc81c\uc791\uc5d0 \ub4e4\uc5b4\uac14\ub2e4. 2016\ub144 4\uc6d4, \uc18c\uc18d\uc0ac\uc640 \uc790\uc2e0\uc758 SNS\ub97c \ud1b5\ud574 2016\ub144 5\uc6d4 15\uc77c \ud604\uc7ac \uad50\uc81c \uc911\uc778 \uc77c\ubc18\uc778 \uc5ec\uc790\uce5c\uad6c\uc640\uc758 \uacb0\ud63c\uc744 \uacf5\uc2dd\ubc1c\ud45c\ud558\uc600\ub2e4.\"\n  example_title: \"Question Answering Example 1\" \n- text: \"question: 1913\ub144 \ud544\ub77c\ub378\ud53c\uc544 \uc560\uc2ac\ub808\ud2f1\uc2a4\uc758 \uac1c\ub9c9\uc804 \uc0c1\ub300\ub294?, context: 1913\ub144 \uc2dc\uc98c\uc744 \uc55e\ub450\uace0 \uc2a4\ud504\ub9c1 \ud2b8\ub808\uc774\ub2dd\uc5d0\uc11c \uc7ad \ucff0\uc2a4\ub294 \uc568\ub77c\ubc30\ub9c8 \uc8fc \ubabd\uace0\uba54\ub9ac\uc5d0\uc11c \uace0\uc5f4\ub85c \ud798\ub4e4\uc5b4\ud588\ub294\ub370, \ub2f9\uc2dc\uc5d0\ub294 \uc2dd\uc911\ub3c5 \ubc0f \ub291\ub9c9\uc5fc \uc9c4\ub2e8\uc744 \ubc1b\uace0 \ud734\uc2dd\uc744 \ucde8\ud588\ub2e4. 4\uc6d4 10\uc77c, \ubcf4\uc2a4\ud134 \ub808\ub4dc\uc0ad\uc2a4\ub97c \uc0c1\ub300\ub85c \uce58\ub7ec\uc9c4 \uac1c\ub9c9\uc804\uc5d0\uc11c \uc7ad \ucff0\uc2a4\ub294 \uc120\ubc1c\ud22c\uc218\ub85c \ub0b4\uc815\ub418\uc5c8\ub2e4. \uadf8\ub294 3\uc774\ub2dd\uc744 \ub178\ud788\ud2b8\ub85c \ub9c9\uace0 6\ud68c \uce58\ud504 \ubca4\ub354\uc640 \uad50\uccb4\ub418\uc5c8\uc73c\uba70, \uacbd\uae30\ub294 10-5\ub85c \uc560\uc2ac\ub808\ud2f1\uc2a4\uac00 \uc2b9\ub9ac\ud588\ub2e4. \uc774\ud2c0 \ub4a4\uc5d0 \ub2e4\uc2dc \uc120\ubc1c \ub4f1\ud310\uc5d0 \ub098\uc130\uc73c\ub098 \u20443\uc774\ub2dd \ub3d9\uc548 2\ud53c\uc548\ud0c0 1\ubcfc\ub137, 4\uc2e4\uc810\ub9cc\uc744 \uae30\ub85d\ud558\uace0 \uac15\ud310\ub418\uc5c8\ub2e4. \ucff0\uc2a4\ub294 \ubcf4\uc2a4\ud134\uc5d0\uc11c\uc758 \uc2dc\ub9ac\uc988\ub97c \ub05d\ub0b4\uace0 \ud300 \ub3d9\ub8cc\ub4e4\uacfc \ud568\uaed8 \uc6cc\uc2f1\ud134\uc73c\ub85c \ud5a5\ud588\uc9c0\ub9cc, \uace0\ud1b5\uc774 \uc2ec\ud574\uc9c0\uc790 \uad6c\ub2e8\uc740 \uadf8\ub97c \ud544\ub77c\ub378\ud53c\uc544\ub85c \ub3cc\ub824\ubcf4\ub0c8\ub2e4. \uadf8\uacf3\uc5d0\uc11c \uadf8\ub294 \uc7a5\ud2f0\ud478\uc2a4 \uc9c4\ub2e8\uc744 \ubc1b\uace0 \ud734\uc2dd\uc744 \ucde8\ud588\uc73c\uba70, 8\uc6d4\uc5d0 \ub2e4\uc2dc \ud300\uc5d0 \ubcf5\uadc0\ud558\ub824\uace0 \ud588\uc9c0\ub9cc \uc815\uc0c1\uc801\uc778 \ud68c\ubcf5\uc744 \uc704\ud574\uc11c \ub2e4\uc2dc \ubcd1\uc6d0\uc5d0 \ub4e4\uc5b4\uac14\ub2e4. \uc774 \uae30\uac04 \ubab8\ubb34\uac8c\uac00 25 kg \uac00\ub7c9\uc774\ub098 \uac10\uc18c\ud588\ub2e4. \uc774 \ud574 \ud544\ub77c\ub378\ud53c\uc544 \uc560\uc2ac\ub808\ud2f1\uc2a4\ub294 \uc6d4\ub4dc \uc2dc\ub9ac\uc988\uc5d0\uc11c 2\ub144\ub9cc\uc5d0 \ub2e4\uc2dc \ub274\uc695 \uc790\uc774\uc5b8\uce20\uc640 \ub9de\ubd99\uc5c8\uace0, \uc6b0\uc2b9\uc744 \ucc28\uc9c0\ud588\ub2e4. \ucff0\uc2a4\uc758 \uacf5\ubc31\uae30\ub294 \ub2e4\uc74c\ud574\uc778 1914\ub144 \uc2dc\uc98c\uae4c\uc9c0 \uae38\uc5b4\uc84c\ub2e4. \uc774 \ud574 \uc2dc\uc98c\uc5d0\ub294 \ud300 \uc21c\uc704\uac00 \uc815\ud574\uc9c4 \uc2dc\uc98c \ub9c9\ud310\uc5d0\uc57c \ub450 \uacbd\uae30\uc5d0 \uc120\ubc1c \ucd9c\uc804\ud574\uc11c, \ub3c4\ud569 8\uc774\ub2dd 8\ud53c\uc548\ud0c0 4\uc2e4\uc810, 4.50\uc758 \ud3c9\uade0\uc790\ucc45\uc810\uc744 \uae30\ub85d\ud588\ub2e4. \uc2dc\uc98c \ud6c4\uc778 12\uc6d4 9\uc77c, \uc560\uc2ac\ub808\ud2f1\uc2a4\uc5d0\uc11c \ubc29\ucd9c\ub418\uc5c8\ub2e4.\"\n  example_title: \"Question Answering Example 2\" \nmodel-index:\n- name: vocabtrimmer/mt5-small-trimmed-ko-koquad-qa\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_koquad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Answering)\n      type: bleu4_question_answering\n      value: 31.5\n    - name: ROUGE-L (Question Answering)\n      type: rouge_l_question_answering\n      value: 70.34\n    - name: METEOR (Question Answering)\n      type: meteor_question_answering\n      value: 50.8\n    - name: BERTScore (Question Answering)\n      type: bertscore_question_answering\n      value: 96.08\n    - name: MoverScore (Question Answering)\n      type: moverscore_question_answering\n      value: 89.75\n    - name: AnswerF1Score (Question Answering)\n      type: answer_f1_score__question_answering\n      value: 74.45\n    - name: AnswerExactMatch (Question Answering)\n      type: answer_exact_match_question_answering\n      value: 67.31\n---\n\n# Model Card of `vocabtrimmer/mt5-small-trimmed-ko-koquad-qa`\nThis model is fine-tuned version of [vocabtrimmer/mt5-small-trimmed-ko](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-ko) for question answering task on the [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) (dataset_name: default) via [`lmqg`](https://github.com/asahi417/lm-question-generation).\n\n\n### Overview\n- **Language model:** [vocabtrimmer/mt5-small-trimmed-ko](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-ko)   \n- **Language:** ko  \n- **Training data:** [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) (default)\n- **Online Demo:** [https://autoqg.net/](https://autoqg.net/)\n- **Repository:** [https://github.com/asahi417/lm-question-generation](https://github.com/asahi417/lm-question-generation)\n- **Paper:** [https://arxiv.org/abs/2210.03992](https://arxiv.org/abs/2210.03992)\n\n### Usage\n- With [`lmqg`](https://github.com/asahi417/lm-question-generation#lmqg-language-model-for-question-generation-)\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"ko\", model=\"vocabtrimmer/mt5-small-trimmed-ko-koquad-qa\")\n\n# model prediction\nanswers = model.answer_q(list_question=\"\ub9e4\ub4dc \ud074\ub77c\uc6b4\uc774 \ucc38\uac00\ud574 \ud070 \ud654\uc81c\ub97c \ubaa8\uc558\ub358 \ud504\ub85c\uadf8\ub7a8\uc740?\", list_context=\" \uacfc\uac70 \uc18c\uc6b8 \ucef4\ud37c\ub2c8 \uc18c\uc18d\uc73c\ub85c \uc18c\uc6b8 \ucef4\ud37c\ub2c8 \ud574\uccb4 \ud6c4 \ud604\uc7ac\uc758 \uc18c\uc18d\uc0ac\ub294 \uc2a4\ud0c0\uc27d \uc5d1\uc2a4\uc774\ub2e4. Mad Clown vs Crucial Star (\ub9e4\ub4dc \ud074\ub77c\uc6b4 vs \ud06c\ub8e8\uc15c \uc2a4\ud0c0)\ub77c\ub294 \ud504\ub85c\uc81d\ud2b8 \uadf8\ub8f9\uc73c\ub85c \ud06c\ub8e8\uc15c \uc2a4\ud0c0\uc640 \ud568\uaed8 \ud65c\ub3d9\ud558\uae30\ub3c4 \ud558\uc600\uc73c\uba70, 2013\ub144\ubd80\ud130\ub294 MC\uc778 \uc800\uc2a4\ub514\uc2a4\uc640 \ud300\uc744 \uc774\ub8e8\uc5b4 \ub7a9 \ub4c0\uc624 \ucee4\uba3c\ucf5c\ub4dc\ub85c \ud65c\ub3d9\ud558\uace0 \uc788\ub2e4. \ub610\ud55c Mnet \u300a\uc1fc\ubbf8\ub354\uba38\ub2c8 2\u300b\uc5d0\uc11c \ucc38\uac00\uc790\ub85c \ucc38\uac00\ud558\uc5ec \ud070 \ud654\uc81c\ub97c \ubaa8\uc558\uc73c\uba70, \u300a\uc1fc\ubbf8\ub354\uba38\ub2c8 5\u300b\uc5d0\uc11c\ub294 \uae38 & \ub9e4\ub4dc \ud074\ub77c\uc6b4 \ud300\uc73c\ub85c \ud504\ub85c\ub4c0\uc11c\ub85c \ucd9c\uc5f0\ud558\uc600\ub2e4., \uc7ac\ubc1c\ub9e4 \ubb3c\ub7c9\ub3c4 \uc644\ud310\ub418\uc5b4 \ucd94\uac00 \uc81c\uc791\uc5d0 \ub4e4\uc5b4\uac14\ub2e4. 2016\ub144 4\uc6d4, \uc18c\uc18d\uc0ac\uc640 \uc790\uc2e0\uc758 SNS\ub97c \ud1b5\ud574 2016\ub144 5\uc6d4 15\uc77c \ud604\uc7ac \uad50\uc81c \uc911\uc778 \uc77c\ubc18\uc778 \uc5ec\uc790\uce5c\uad6c\uc640\uc758 \uacb0\ud63c\uc744 \uacf5\uc2dd\ubc1c\ud45c\ud558\uc600\ub2e4.\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"vocabtrimmer/mt5-small-trimmed-ko-koquad-qa\")\noutput = pipe(\"question: \ub9e4\ub4dc \ud074\ub77c\uc6b4\uc774 \ucc38\uac00\ud574 \ud070 \ud654\uc81c\ub97c \ubaa8\uc558\ub358 \ud504\ub85c\uadf8\ub7a8\uc740?, context: \uacfc\uac70 \uc18c\uc6b8 \ucef4\ud37c\ub2c8 \uc18c\uc18d\uc73c\ub85c \uc18c\uc6b8 \ucef4\ud37c\ub2c8 \ud574\uccb4 \ud6c4 \ud604\uc7ac\uc758 \uc18c\uc18d\uc0ac\ub294 \uc2a4\ud0c0\uc27d \uc5d1\uc2a4\uc774\ub2e4. Mad Clown vs Crucial Star (\ub9e4\ub4dc \ud074\ub77c\uc6b4 vs \ud06c\ub8e8\uc15c \uc2a4\ud0c0)\ub77c\ub294 \ud504\ub85c\uc81d\ud2b8 \uadf8\ub8f9\uc73c\ub85c \ud06c\ub8e8\uc15c \uc2a4\ud0c0\uc640 \ud568\uaed8 \ud65c\ub3d9\ud558\uae30\ub3c4 \ud558\uc600\uc73c\uba70, 2013\ub144\ubd80\ud130\ub294 MC\uc778 \uc800\uc2a4\ub514\uc2a4\uc640 \ud300\uc744 \uc774\ub8e8\uc5b4 \ub7a9 \ub4c0\uc624 \ucee4\uba3c\ucf5c\ub4dc\ub85c \ud65c\ub3d9\ud558\uace0 \uc788\ub2e4. \ub610\ud55c Mnet \u300a\uc1fc\ubbf8\ub354\uba38\ub2c8 2\u300b\uc5d0\uc11c \ucc38\uac00\uc790\ub85c \ucc38\uac00\ud558\uc5ec \ud070 \ud654\uc81c\ub97c \ubaa8\uc558\uc73c\uba70, \u300a\uc1fc\ubbf8\ub354\uba38\ub2c8 5\u300b\uc5d0\uc11c\ub294 \uae38 & \ub9e4\ub4dc \ud074\ub77c\uc6b4 \ud300\uc73c\ub85c \ud504\ub85c\ub4c0\uc11c\ub85c \ucd9c\uc5f0\ud558\uc600\ub2e4., \uc7ac\ubc1c\ub9e4 \ubb3c\ub7c9\ub3c4 \uc644\ud310\ub418\uc5b4 \ucd94\uac00 \uc81c\uc791\uc5d0 \ub4e4\uc5b4\uac14\ub2e4. 2016\ub144 4\uc6d4, \uc18c\uc18d\uc0ac\uc640 \uc790\uc2e0\uc758 SNS\ub97c \ud1b5\ud574 2016\ub144 5\uc6d4 15\uc77c \ud604\uc7ac \uad50\uc81c \uc911\uc778 \uc77c\ubc18\uc778 \uc5ec\uc790\uce5c\uad6c\uc640\uc758 \uacb0\ud63c\uc744 \uacf5\uc2dd\ubc1c\ud45c\ud558\uc600\ub2e4.\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Answering)***: [raw metric file](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-ko-koquad-qa/raw/main/eval/metric.first.answer.paragraph_question.answer.lmqg_qg_koquad.default.json) \n\n|                  |   Score | Type    | Dataset                                                          |\n|:-----------------|--------:|:--------|:-----------------------------------------------------------------|\n| AnswerExactMatch |   67.31 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| AnswerF1Score    |   74.45 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| BERTScore        |   96.08 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_1           |   63.81 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_2           |   55.08 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_3           |   44.98 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| Bleu_4           |   31.5  | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| METEOR           |   50.8  | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| MoverScore       |   89.75 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n| ROUGE_L          |   70.34 | default | [lmqg/qg_koquad](https://huggingface.co/datasets/lmqg/qg_koquad) |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_koquad\n - dataset_name: default\n - input_types: ['paragraph_question']\n - output_types: ['answer']\n - prefix_types: None\n - model: vocabtrimmer/mt5-small-trimmed-ko\n - max_length: 512\n - max_length_output: 32\n - epoch: 4\n - batch: 32\n - lr: 0.001\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 2\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-ko-koquad-qa/raw/main/trainer_config.json).\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "size_bytes": "476784005", "downloads": 2}
{"pretrained_model_name": "emnlp2023/calc-t5-xl", "description": "---\n# For reference on model card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1\n# Doc / guide: https://huggingface.co/docs/hub/model-cards\ndatasets:\n- emnlp2023/Calc-gsm8k\n- emnlp2023/Calc-aqua_rat\n- emnlp2023/Calc-math_qa\n- emnlp2023/Calc-ape210k\nmetrics:\n- exact_match\n- rouge\nmodel-index:\n- name: calc-t5-xl\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      type: gsm8k\n      name: GSM8K\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 0.420\n    - type: rouge\n      value: 0.627\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      type: aqua_rat\n      name: AQUA-RAT\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 0.06\n    - type: rouge\n      value: 0.323\nlicense: apache-2.0\nlanguage:\n- en\n---\n\n# Model Card for calc-t5-xl\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis model generates reasoning chains over mathematical questions while **using an external tool: Sympy calculator**.\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nWith the idea to offload a symbolic reasoning from the stochastic language model, \nwe train this model to utilize a calculator **for all applicable numeric operations**.\nThis is achieved by training the model to construct calls to the tool's API in this format:\n\n```html\n<gadget id=\"calculator\">100/2</gadget> <output>50</output>\n```\n\nwhere `<gadget>` segment triggers a call of the tool, \nwhich is subsequently served by extending model's decoder input context by adding the output of the tool within the `<output>` segment.\n\n- **Developed by:** Anonymous\n- **Model type:** Autoregressive Encoder-Decoder\n- **Language(s):** en\n- **Finetuned from:** t5-3b\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/emnlp2023sub/gadgets\n- **Paper:** Stay tuned!\n\n## Usage\n\nAdditionally to conventional generation, using Tool-augmented generation requires \n(1) implementation of the tool(s) and \n(2) a customization of generate() method augmenting input context on-demand with the outputs of the tools.\n\nYou can find these two components implemented in the **gadgets/models.py** and **gadgets/gadget.py** in the project's [home repo](https://github.com/emnlp2023sub/gadgets).\n\nAfter adding these two scripts to your directory, you can use the model as follows:\n\n```python\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nfrom gadgets.model import gadget_assisted_model\nfrom gadgets.gadget import Calculator\n\nGadgetAssistedT5 = gadget_assisted_model(T5ForConditionalGeneration)\nmodel = GadgetAssistedT5.from_pretrained(\"emnlp2023/calc-t5-xl\")\ntokenizer = T5Tokenizer.from_pretrained(\"emnlp2023/calc-t5-xl\")\n\nmodel.prepare_for_generate(tokenizer, \n                           enabled_gadgets=[Calculator()], \n                           default_max_tokens=512)\nquery = \"\"\"\n    The profit from a business transaction is shared among 2 business partners, \n    Mike and Johnson in the ratio 2:5 respectively. \n    If Johnson got $2500, how much will Mike have \n    after spending some of his share on a shirt that costs $200?\n\"\"\"\n\ninputs = tokenizer(query, return_tensors=\"pt\")\noutput_ids = model.generate(**inputs)\ntokenizer.decode(output_ids[0], spaces_between_special_tokens=False)\n```\nThis returns:\n```html\nAccording to the ratio, for every 5 parts that Johnson gets, Mike gets 2 parts Since Johnson got $2500,\neach part is therefore $2500/5 = $<gadget id=\"calculator\">2500/5</gadget><output>500</output> 500\nMike will get 2*$500 = $<gadget id=\"calculator\">2*500</gadget><output>1_000</output> 1000\nAfter buying the shirt he will have $1000-$200 = $<gadget id=\"calculator\">1000-200</gadget><output>800</output> 800 left.\nFinal result is<result>800</result></s>\n```\n\n### Out-of-Scope Usage\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\nNote that given the limited scope of the exercises' complexity in the training, this model will not work well for tasks requiring \nmore complex algebraic operations, including equations, variables and operations outside the scope of (+-*/).\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nThis model was trained on our Calculator-augmented set of\n- [Calc Ape210k](https://huggingface.co/datasets/emnlp2023/Calc-ape210k) ([original Ape210k on github](https://github.com/Chenny0808/ape210k))\n- [Calc MathQA](https://huggingface.co/datasets/emnlp2023/Calc-math_qa) ([original MathQA on HF](https://huggingface.co/datasets/math_qa))\n- [Calc GSM8K](https://huggingface.co/datasets/emnlp2023/Calc-gsm8k) ([original GSM8K on HF](https://huggingface.co/datasets/gsm8k))\n- [Calc Aqua-RAT](https://huggingface.co/datasets/emnlp2023/Calc-aqua_rat) ([original Aqua-RAT on HF](https://huggingface.co/datasets/aqua_rat))\n\nin a standard auto-regressive setup i.e. for a conditional next-token prediction with teacher-forced prefix.\n\n", "size_bytes": 11924529152, "downloads": 2}
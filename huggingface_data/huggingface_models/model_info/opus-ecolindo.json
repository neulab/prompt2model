{"pretrained_model_name": "yonathanstwn/opus-ecolindo", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: opus-ecolindo\n  results: []\ndatasets:\n- yonathanstwn/ecolindo\nlanguage:\n- id\n- en\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# opus-ecolindo\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-en-id](https://huggingface.co/Helsinki-NLP/opus-mt-en-id) on the EColIndo dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1254\n- Bleu: 35.4242\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 4000\n- num_epochs: 25\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss | Bleu    |\n|:-------------:|:-----:|:------:|:---------------:|:-------:|\n| 1.4457        | 1.0   | 21019  | 1.1641          | 35.6285 |\n| 1.1877        | 2.0   | 42038  | 1.1141          | 35.9198 |\n| 1.1049        | 3.0   | 63057  | 1.0936          | 36.0605 |\n| 1.049         | 4.0   | 84076  | 1.0821          | 36.0444 |\n| 1.0051        | 5.0   | 105095 | 1.0794          | 36.6179 |\n| 0.9685        | 6.0   | 126114 | 1.0751          | 36.7172 |\n| 0.9376        | 7.0   | 147133 | 1.0751          | 36.407  |\n| 0.9098        | 8.0   | 168152 | 1.0759          | 36.7071 |\n| 0.8859        | 9.0   | 189171 | 1.0809          | 36.0112 |\n| 0.864         | 10.0  | 210190 | 1.0800          | 36.1705 |\n| 0.844         | 11.0  | 231209 | 1.0861          | 36.1114 |\n| 0.8261        | 12.0  | 252228 | 1.0862          | 35.9952 |\n| 0.8102        | 13.0  | 273247 | 1.0944          | 35.659  |\n| 0.7945        | 14.0  | 294266 | 1.0982          | 35.5881 |\n| 0.7812        | 15.0  | 315285 | 1.1019          | 35.8675 |\n| 0.7688        | 16.0  | 336304 | 1.1066          | 35.5418 |\n| 0.7576        | 17.0  | 357323 | 1.1087          | 35.7305 |\n| 0.7472        | 18.0  | 378342 | 1.1116          | 35.4516 |\n| 0.7377        | 19.0  | 399361 | 1.1162          | 35.3253 |\n| 0.7298        | 20.0  | 420380 | 1.1185          | 35.2614 |\n| 0.7233        | 21.0  | 441399 | 1.1203          | 35.5355 |\n| 0.7162        | 22.0  | 462418 | 1.1220          | 35.3226 |\n| 0.7116        | 23.0  | 483437 | 1.1226          | 35.3893 |\n| 0.7075        | 24.0  | 504456 | 1.1250          | 35.4201 |\n| 0.7037        | 25.0  | 525475 | 1.1254          | 35.4242 |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 2.0.0\n- Datasets 2.10.1\n- Tokenizers 0.11.0", "size_bytes": "289081861", "downloads": 4}
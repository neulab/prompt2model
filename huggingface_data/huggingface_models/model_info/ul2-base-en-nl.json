{"pretrained_model_name": "yhavinga/ul2-base-en-nl", "description": "\n---\nlanguage:\n- nl \n- en\n- multilingual\nlicense: apache-2.0\ntags:\n- dutch\n- english\n- t5\n- t5x\n- ul2\n- seq2seq\n- translation\ndatasets:\n- yhavinga/mc4_nl_cleaned\n- yhavinga/nedd_wiki_news\npipeline_tag: translation\nwidget:\n  - text: >-\n      Redistricting and West Virginia\u2019s shrinking population forced the state\u2019s\n      Republican Legislature to pit Mr. McKinley, a six-term Republican with a\n      pragmatic bent, against Mr. Mooney, who has served four terms marked more\n      by conservative rhetoric than legislative achievements.\n  - text: >-\n      It is a painful and tragic spectacle that rises before me: I have drawn\n      back the curtain from the rottenness of man. This word, in my mouth, is at\n      least free from one suspicion: that it involves a moral accusation against\n      humanity.\n  - text: >-\n      Young Wehling was hunched in his chair, his head in his hand. He was so\n      rumpled, so still and colorless as to be virtually invisible. His\n      camouflage was perfect, since the waiting room had a disorderly and\n      demoralized air, too. Chairs and ashtrays had been moved away from the\n      walls. The floor was paved with spattered dropcloths.\n---\n\n# ul2-base-en-nl for English to Dutch translation\n\nFine-tuned T5 model on English to Dutch translation that was pretrained on Dutch using a UL2 (Mixture-of-Denoisers) objective.\nThe T5 model was introduced in\n[this paper](https://arxiv.org/abs/1910.10683)\nand first released at [this page](https://github.com/google-research/text-to-text-transfer-transformer).\nThe UL2 objective was introduced in\n[this paper](https://arxiv.org/abs/2205.05131)\nand first released at [this page](https://github.com/google-research/google-research/tree/master/ul2).\n\n\n\n## Model description\n\nT5 is an encoder-decoder model and treats all NLP problems in a text-to-text format.\n\n`ul2-base-en-nl` T5 is a transformers model fine-tuned on parallel sentence and paragraph pairs\nsampled from books.\n\nThis model used the [T5 v1.1](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) improvements compared to the original T5 model during the pretraining:\n- GEGLU activation in the feed-forward hidden layer, rather than ReLU - see [here](https://arxiv.org/abs/2002.05202)\n- Dropout was turned off during pre-training. Dropout should be re-enabled during fine-tuning\n- Pre-trained on self-supervised objective only without mixing in the downstream tasks\n- No parameter sharing between embedding and classifier layer\n\n\n\n### UL2 pretraining objective\n\nThis model was pretrained with the UL2's Mixture-of-Denoisers (MoD) objective, that combines diverse pre-training\nparadigms together. UL2 frames different objective functions for training language models as denoising tasks, where\nthe model has to recover missing sub-sequences of a given input. During pre-training it uses a novel mixture-of-denoisers\nthat samples from a varied set of such objectives, each with different configurations. UL2 is trained using a mixture of\nthree denoising tasks:\n\n1. R-denoising (or regular span corruption), which emulates the standard T5 span corruption objective;\n2. X-denoising (or extreme span corruption); and\n3. S-denoising (or sequential PrefixLM).\n\nDuring pre-training, we sample from the available denoising tasks based on user-specified ratios.\nUL2 introduces a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training\ndenoising task. During the pre-training, a paradigm token is inserted to the input\n(`[NLU]` for R-denoising, `[NLG]` for X-denoising, or `[S2S]` for S-denoising) indicating the denoising task at hand.\nThen, during fine-tuning the same input token should be inserted to get the best performance for different downstream\nfine-tuning tasks.\n\n## Intended uses & limitations\n\nThis model was fine-tuned on parallel sentence and paragraph pairs and can be used\nfor machine translation.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nmodel_name = \"yhavinga/ul2-base-en-nl\"\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import pipeline\nimport torch\ndevice_num = 0 if torch.cuda.is_available() else -1\ndevice = \"cpu\" if device_num < 0 else f\"cuda:{device_num}\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=True).to(\n    device\n)\nparams = {\"max_length\": 370, \"num_beams\": 4, \"early_stopping\": True}\ntranslator = pipeline(\"translation\", tokenizer=tokenizer, model=model, device=device_num)\nprint(translator(\"Young Wehling was hunched in his chair, his head in his hand. He was so rumpled, so still and colorless as to be virtually invisible.\",\n               **params)[0]['translation_text'])\n```\n\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from neutral.\nTherefore, the model can have biased predictions. This bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe `ul2-base-en-nl` T5 model was pre-trained simultaneously on a combination of several datasets,\nincluding the `full` config of the \"mc4_nl_cleaned\" dataset, which is a cleaned version of Common Crawl's web\ncrawl corpus, Dutch books, the Dutch subset of Wikipedia (2022-03-20), and a subset of \"mc4_nl_cleaned\"\ncontaining only texts from Dutch newspapers.\n\nAfter pre-training, the model was\nfine-tuned on a translation dataset containing 13 million sentence and paragraph pairs\nsampled from books.\n\n## Training procedure\n\n### Preprocessing\n\nThe ul2-base-en-nl T5 model uses a SentencePiece unigram tokenizer with a vocabulary of 32,000 tokens.\nThe tokenizer includes the special tokens `<pad>`, `</s>`, `<unk>`,  known from the original T5 paper,\n`[NLU]`, `[NLG]` and `[S2S]` for the MoD pre-training, and `<n>` for newline.\nDuring pre-training with the UL2 objective, input and output sequences consist of 512 consecutive tokens. \nThe tokenizer does not lowercase texts and is therefore case-sensitive; it distinguises\nbetween `dutch` and `Dutch`.\nAdditionally, 100+28 extra tokens were added for pre-training tasks, resulting in a total of 32,128 tokens.\n\n### Fine-tuning\n\nThis model was fine-tuned on a dataset containing 13M sentence and paragraph translation pairs sampled from books.\n\n* Pre-trained model used as starting point: yhavinga/ul2-base-dutch\n* Amount of fine-tune training steps: 96035\n* Batch size: 512 (gradient accumulation steps: 4)\n* Sequence length: 370 tokens\n* Model dtype: bfloat16\n* z_loss: 0.0001\n* Optimizer: adamw_hf beta1: 0.9 beta2: 0.9969 eps: 1e-08\n* Dropout rate: 0.01\n* Learning rate: 0.0010 with linear decay to 0 and warmup for 500 steps\n* Label smoothing factor: 0.11\n* Bleu score: 43.2\n\n### Model list\n\nModels in this series:\n\n\n|                      | ul2-base-en-nl   | ul2-base-nl36-en-nl   | ul2-large-en-nl   |\n|:---------------------|:-----------------|:----------------------|:------------------|\n| model_type           | t5               | t5                    | t5                |\n| _pipeline_tag        | translation      | translation           | translation       |\n| d_model              | 768              | 768                   | 1024              |\n| d_ff                 | 2048             | 3072                  | 2816              |\n| num_heads            | 12               | 12                    | 16                |\n| d_kv                 | 64               | 64                    | 64                |\n| num_layers           | 12               | 36                    | 24                |\n| num_decoder_layers   | 12               | 36                    | 24                |\n| feed_forward_proj    | gated-silu       | gated-silu            | gated-silu        |\n| dense_act_fn         | silu             | silu                  | silu              |\n| vocab_size           | 32128            | 32128                 | 32128             |\n| tie_word_embeddings  | 0                | 0                     | 0                 |\n| torch_dtype          | float32          | float32               | float32           |\n| _gin_batch_size      | 128              | 64                    | 64                |\n| _gin_z_loss          | 0.0001           | 0.0001                | 0.0001            |\n| _gin_t5_config_dtype | 'bfloat16'       | 'bfloat16'            | 'bfloat16'        |\n\n## Evaluation results\n\nSee the evaluation section in the interactive [Pre-training Dutch T5 Models](https://huggingface.co/spaces/yhavinga/pre-training-dutch-t5-models) blog.\n\n## Acknowledgements\n\nThis project would not have been possible without compute generously provided by Google through the\n[TPU Research Cloud](https://sites.research.google/trc/).\nThanks to the [Finnish-NLP](https://huggingface.co/Finnish-NLP) authors for releasing their code for the UL2 objective and associated task definitions.\nThanks to [Stephenn Fernandes](https://huggingface.co/StephennFernandes) for helping me get started with the t5x framework.\n\nCreated by [Yeb Havinga](https://www.linkedin.com/in/yeb-havinga-86530825/)\n\n", "size_bytes": "990402637", "downloads": 44}
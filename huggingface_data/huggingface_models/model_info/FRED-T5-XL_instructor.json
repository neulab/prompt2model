{"pretrained_model_name": "Den4ikAI/FRED-T5-XL_instructor", "description": "---\nlicense: mit\nlanguage:\n- ru\npipeline_tag: text2text-generation\nwidget:\n- text: '<SC6>\u0427\u0435\u043b\u043e\u0432\u0435\u043a: \u041e\u0442\u0432\u0435\u0442\u044c \u043d\u0430 \u0432\u043e\u043f\u0440\u043e\u0441. \u041f\u043e\u0447\u0435\u043c\u0443 \u0442\u0440\u0430\u0432\u0430 \u0437\u0435\u043b\u0435\u043d\u0430\u044f?\\n\u0411\u043e\u0442: <extra_id_0>'\n---\n# Den4ikAI/FRED-T5-XL_instructor\n\u0418\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 FRED-T5-XL.\n\n# \u041f\u0440\u0438\u043c\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(\"Den4ikAI/FRED-T5-XL_instructor\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Den4ikAI/FRED-T5-XL_instructor\", torch_dtype=torch.float16).to(device)\nmodel.eval()\n\nfrom transformers import GenerationConfig\n\ngeneration_config = GenerationConfig.from_pretrained(\"Den4ikAI/FRED-T5-XL_instructor\")\ndef generate(prompt):\n  data = tokenizer(f\"<SC6>\u0427\u0435\u043b\u043e\u0432\u0435\u043a: {prompt}\\n\u0411\u043e\u0442: <extra_id_0>\", return_tensors=\"pt\").to(model.device)\n  output_ids = model.generate(\n      **data,\n      generation_config=generation_config\n  )[0]\n  out = tokenizer.decode(output_ids.tolist())\n  return out\n\nwhile 1:\n  print(generate(input(\":> \")))\n\n```\n# Citation\n```\n@MISC{Den4ikAI/FRED-T5-XL_instructor,\n    author  = {Denis Petrov},\n    title   = {Russian Instructor Model},\n    url     = {https://huggingface.co/Den4ikAI/FRED-T5-XL_instructor/},\n    year    = 2023\n}\n```", "size_bytes": "3480907589", "downloads": 46}
{"pretrained_model_name": "cahya/bert2bert-indonesian-summarization", "description": "---\nlanguage: id\ntags:\n- pipeline:summarization\n- summarization\n- bert2bert\ndatasets:\n- id_liputan6\nlicense: apache-2.0\n---\n\n# Indonesian BERT2BERT Summarization Model\n\nFinetuned BERT-base summarization model for Indonesian.\n\n## Finetuning Corpus\n\n`bert2bert-indonesian-summarization` model is based on `cahya/bert-base-indonesian-1.5G` by [cahya](https://huggingface.co/cahya), finetuned using [id_liputan6](https://huggingface.co/datasets/id_liputan6) dataset.\n\n## Load Finetuned Model\n\n```python\nfrom transformers import BertTokenizer, EncoderDecoderModel\n\ntokenizer = BertTokenizer.from_pretrained(\"cahya/bert2bert-indonesian-summarization\")\ntokenizer.bos_token = tokenizer.cls_token\ntokenizer.eos_token = tokenizer.sep_token\nmodel = EncoderDecoderModel.from_pretrained(\"cahya/bert2bert-indonesian-summarization\")\n```\n\n## Code Sample\n\n```python\nfrom transformers import BertTokenizer, EncoderDecoderModel\n\ntokenizer = BertTokenizer.from_pretrained(\"cahya/bert2bert-indonesian-summarization\")\ntokenizer.bos_token = tokenizer.cls_token\ntokenizer.eos_token = tokenizer.sep_token\nmodel = EncoderDecoderModel.from_pretrained(\"cahya/bert2bert-indonesian-summarization\")\n\n# \nARTICLE_TO_SUMMARIZE = \"\"\n\n# generate summary\ninput_ids = tokenizer.encode(ARTICLE_TO_SUMMARIZE, return_tensors='pt')\nsummary_ids = model.generate(input_ids,\n            min_length=20,\n            max_length=80, \n            num_beams=10,\n            repetition_penalty=2.5, \n            length_penalty=1.0, \n            early_stopping=True,\n            no_repeat_ngram_size=2,\n            use_cache=True,\n            do_sample = True,\n            temperature = 0.8,\n            top_k = 50,\n            top_p = 0.95)\n\nsummary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\nprint(summary_text)\n```\n\nOutput:\n\n```\n\n```\n\n", "size_bytes": "998778130", "downloads": 162}
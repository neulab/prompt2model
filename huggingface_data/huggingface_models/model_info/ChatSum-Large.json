{"pretrained_model_name": "KoalaAI/ChatSum-Large", "description": "---\ntags:\n- autotrain\n- summarization\n- chat\n- T5\nlanguage:\n- en\nwidget:\n- text: >-\n    Emily: fancy a drink after work today?  Kate: sure!  Marta: Good idea! \n    Marta: Where? When? Emily: Maybe in the Pub X at the central station at\n    5.30? Kate: I may be closer to 6, traffic on my way Marta: Fine for me.\n    Marta: See you then, Ladies! Emily: Bye! see ya :* Kate: :*\n  example_title: Meeting at the Pub\n- text: >-\n    Harry: heyyyy are you there?? Cindy: Yes dear what is it? Harry: Can you\n    call Ela and tell her i need to talk urgent please pick my call. Cindy: what\n    happened now? an other fight :O Harry: please tell her Cindy: MAN! you\n    guys... am i some kind of a messenger service here? Harry: PLEASEEEEEEEEE ?\n    Cindy: ok doing.... but thats the last time. Harry: Yes like always:P Cindy:\n    Hate you seriously man. Harry: Thank you Cindy: Done you can call her now.\n  example_title: Harry wants to call Ela\n- text: >-\n    Val: it's raining! Candy: I know, just started... Val: r we going? we will\n    be wet Candy: maybe wait a little? see if stops Val: ok. let's wait half h\n    and than see Candy: god idea, I call u then Val: great :)\n  example_title: Val and Candy\ndatasets:\n- DarwinAnim8or/autotrain-data-chatsum\n- samsum\nco2_eq_emissions:\n  emissions: 0.16588727515391594\nlicense: apache-2.0\n---\n\n# Model Overview\nThis is a fine-tune of the FLAN-T5 model from Google. This was trained on the \"samsum\" dataset in order to summarise chat logs. \nThere are other models sizes available in this same series:\n * [ChatSum-Base (248M)](https://huggingface.co/DarwinAnim8or/FLAN-T5-Base-ChatSum)\n * [ChatSum-Small (77M)](https://huggingface.co/KoalaAI/ChatSum-Small)\n\nAs of writing, there are no larger models planned for this series, with this model being the current best one available in our testing.\n\n## Intended Use\n\nThe model is intended to be used for generating summaries of chat logs. \nIt can be employed in a wide range of applications, including but not limited to chat analysis, conversation summarization, and dialogue-based content generation.\n\n## Training Data\n\nThe model has been fine-tuned on the samsum dataset, which contains conversations between two or more participants. The dataset is in English, and each conversation is associated with a summary that captures the main points of the discussion.\n\n## Limitations and Ethical Considerations\n\nAs with any language model, the FLAN-T5 model has certain limitations and potential ethical considerations:\n\n1. **Limited Context Understanding**: The model's performance heavily relies on the context provided in the chat logs. It may not fully understand the nuances of the conversation, leading to occasional inaccuracies in the generated summaries.\n\n2. **Biases in Training Data**: The model's fine-tuning data (samsum dataset) may contain biases present in the original data source. This could lead to biased or unfair summaries being generated.\n\n3. **Privacy and Data Security**: If the chat logs used for summarization contain sensitive or private information, using this model may pose privacy risks, and proper data anonymization measures should be taken.\n\n4. **Responsibility in Use**: The model should be used responsibly, and the generated summaries should be carefully analyzed before making any critical decisions based on them.\n\n## Validation Metrics\n\n- Loss: 1.218\n- Rouge1: 49.316\n- Rouge2: 26.518\n- RougeL: 42.229\n- RougeLsum: 45.716\n- Gen Len: 16.799\n\n## Carbon Emissions\n\n- CO2 Emissions (in grams): 0.1659", "size_bytes": "3132793669", "downloads": 12}
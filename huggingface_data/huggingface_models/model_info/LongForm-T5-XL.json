{"pretrained_model_name": "akoksal/LongForm-T5-XL", "description": "---\nlanguage:\n- en\n- multilingual\n- de\n- it\n- es\n- fr\ntags:\n- instruction-tuning\n- text-generation-inference\n- text2text-generation\nwidget:\n- text: Write an essay about meditation.\n  example_title: Essay Generation\n- text: Give me 5 steps to clean my room.\n  example_title: How-to Instructions\n- text: How are the continents formed?\n  example_title: Question-Answering\n- text: >-\n    Prompt: A man draws a gun in a dark alley and asks for your wallet. You\n    begrudgingly obey. He throws it on the ground, shoots it till it screeches,\n    and turns to you; 'you are safe now'. Write a story about given prompt.\n  example_title: Story Generation\n- text: >-\n    Write directions of a cooking recipe with these ingredients: chicken breast,\n    carrots, green peas, celery, butter, onion, flour, salt, black pepper,\n    celery seed, chicken broth, milk, unbaked pie crusts\n  example_title: Recipe Generation\n- text: Schreiben Sie einen Blogbeitrag \u00fcber die Vorteile des Lesens von B\u00fcchern.\n  example_title: German Essay Generation\ninference:\n  parameters:\n    top_p: 0.9\n    do_sample: true\n    max_length: 75\ndatasets:\n- akoksal/LongForm\n---\n\n## LongForm-T5-XL\nThe LongForm dataset is created by leveraging English corpus examples with augmented instructions. We select a diverse set of human-written documents from existing corpora such as C4 and Wikipedia and generate instructions for the given documents via LLMs. Then, we extend these examples with structured corpora examples such as Stack Exchange and WikiHow and task examples such as question answering, email writing, grammar error correction, story/poem generation, and text summarization.\n\n\nGithub Repo: https://github.com/akoksal/LongForm\n\n## How to Load\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"akoksal/LongForm-T5-XL\")\ntokenizer = AutoTokenizer.from_pretrained(\"akoksal/LongForm-T5-XL\")\n\ninstruction = \"Write an essay about meditation.\"\ntorch.manual_seed(42)\ninput_ids = tokenizer(instruction, return_tensors=\"pt\").input_ids\ntarget_ids = model.generate(input_ids, do_sample=True, max_new_tokens=50, top_p=0.9)\ntokenizer.decode(target_ids[0], skip_special_tokens=True)\n# Output:\n# > Meditation is an ancient, spiritual practice. Meditation was first\\\n# practiced as early as 3000 BC by Indians. Meditation has been practiced\\\n# by people for thousands of years. People meditate in order to become more\\\n# present in their life. Meditation is\n```\n\n## Evaluation\nWe provide in-depth evaluation of LongForm models and baselines in the paper. We present the METEOR scores of models in out-of-domain datasets. In all tasks, Recipe Generation (RGen), long-form question answering (ELI5), short story generation (WritingPrompts/WP), LongForm models outperform prior instruction-tuned models.\n|          | **All** | **Recipe Generation**             | **ELI5** | **Writing Prompts** |\n|-----------------------|---------|-----------------------------------|----------|---------------------|\n| **T0++**              | 10.9    | 18.7                              | 3.8      | 10.2                |\n| **Tk-Instruct**       | 6.3     | 12.9* | 3.6      | 2.4                 |\n| **Flan-T5**           | 10.6    | 20.9* | 3.5      | 7.4                 |\n| **Alpaca-LLaMA-7B**   | 14.6    | 19.5                              | 12.5     | 11.8                |\n| **OPT-30B**           | 11.1    | 18.6                              | 12.2     | 2.6                 |\n| [**LongForm-T5-XL**](https://huggingface.co/akoksal/LongForm-T5-XL)    | 16.3    | 20.2                              | 18.3     | 10.6                |\n| [**LongForm-OPT-2.7B**](https://huggingface.co/akoksal/LongForm-OPT-2.7B)   | 17.8    | 15.5                              | 17.9     | **19.9**                |\n| [**LongForm-OPT-6.7B**](https://huggingface.co/akoksal/LongForm-OPT-6.7B) | 17.7    | 16.9                              | 17.2     | 19.0                |\n| [**LongForm-LLaMA-7B**](https://huggingface.co/akoksal/LongForm-LLaMA-7B-diff)\u2021 | **19.7**    | **21.7**                              | **18.6**     | 18.9                |\n\nSmaller versions of LongForm-OPT models are also available:\n- [**LongForm-OPT-1.3B**](https://huggingface.co/akoksal/LongForm-OPT-1.3B)\n- [**LongForm-OPT-350M**](https://huggingface.co/akoksal/LongForm-OPT-350M)\n- [**LongForm-OPT-125M**](https://huggingface.co/akoksal/LongForm-OPT-125M)\n\n\u2021: We can just release the difference between LongForm-LLaMA-7B and pretrained LLaMA-7B publicly due to restrictions of LLaMA models.\n\n## Limitations\nThe LongForm dataset and models mainly focus on long text generation and have limitations regarding structured prediction tasks in NLP. Additionally, we observe that LongForm models may present hallucination problems similar to those found in LLMs.\n\n## License\nThe LongForm project is subject to a MIT License with custom limitations for restrictions imposed by OpenAI (for the instruction generation part), as well as the license of language models (OPT, LLaMA, and T5).\n\n\n## Citation\n```\n@misc{koksal2023longform,\n      title={LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction}, \n      author={Abdullatif K\u00f6ksal and Timo Schick and Anna Korhonen and Hinrich Sch\u00fctze},\n      year={2023},\n      eprint={2304.08460},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "size_bytes": "5699596005", "downloads": 36}
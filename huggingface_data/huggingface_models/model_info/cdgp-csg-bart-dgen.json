{"pretrained_model_name": "AndyChiang/cdgp-csg-bart-dgen", "description": "---\nlicense: mit\nlanguage: en\ntags:\n- bart\n- cloze\n- distractor\n- generation\ndatasets:\n- dgen\nwidget:\n- text: \"The only known planet with large amounts of water is <mask>. </s> earth\"\n- text: \"The products of photosynthesis are glucose and <mask> else. </s> oxygen\"\n---\n\n# cdgp-csg-bart-dgen\n\n## Model description\n\nThis model is a Candidate Set Generator in **\"CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model\", Findings of EMNLP 2022**.\n\nIts input are stem and answer, and output is candidate set of distractors. It is fine-tuned by [**DGen**](https://github.com/DRSY/DGen) dataset based on [**facebook/bart-base**](https://huggingface.co/facebook/bart-base) model.\n\nFor more details, you can see our **paper** or [**GitHub**](https://github.com/AndyChiangSH/CDGP).\n\n## How to use?\n\n1. Download model by hugging face transformers.\n```python\nfrom transformers import BartTokenizer, BartForConditionalGeneration, pipeline\n\ntokenizer = BartTokenizer.from_pretrained(\"AndyChiang/cdgp-csg-bart-dgen\")\ncsg_model = BartForConditionalGeneration.from_pretrained(\"AndyChiang/cdgp-csg-bart-dgen\")\n```\n\n2. Create a unmasker.\n```python\nunmasker = pipeline(\"fill-mask\", tokenizer=tokenizer, model=csg_model, top_k=10)\n```\n\n3. Use the unmasker to generate the candidate set of distractors.\n```python\nsent = \"The only known planet with large amounts of water is <mask>. </s> earth\"\ncs = unmasker(sent)\nprint(cs)\n```\n\n## Dataset\n\nThis model is fine-tuned by [DGen](https://github.com/DRSY/DGen) dataset, which covers multiple domains including science, vocabulary, common sense and trivia. It is compiled from a wide variety of datasets including SciQ, MCQL, AI2 Science Questions, etc. The detail of DGen dataset is shown below.\n\n| DGen dataset            | Train | Valid | Test | Total |\n| ----------------------- | ----- | ----- | ---- | ----- |\n| **Number of questions** | 2321  | 300   | 259  | 2880  |\n\nYou can also use the [dataset](https://huggingface.co/datasets/AndyChiang/dgen) we have already cleaned.\n\n## Training\n\nWe use a special way to fine-tune model, which is called **\"Answer-Relating Fine-Tune\"**. More details are in our paper.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- Pre-train language model: [facebook/bart-base](https://huggingface.co/facebook/bart-base)\n- Optimizer: adam\n- Learning rate: 0.0001\n- Max length of input: 64\n- Batch size: 64\n- Epoch: 1\n- Device: NVIDIA\u00ae Tesla T4 in Google Colab \n\n## Testing\n\nThe evaluations of this model as a Candidate Set Generator in CDGP is as follows:\n\n| P@1   | F1@3 | MRR   | NDCG@10 |\n| ----- | ---- | ----- | ------- |\n| 8.49  | 8.24 | 16.01 | 22.66   |\n\n## Other models\n\n### Candidate Set Generator\n\n| Models      | CLOTH                                                                               | DGen                                                                             |\n| ----------- | ----------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |\n| **BERT**    | [cdgp-csg-bert-cloth](https://huggingface.co/AndyChiang/cdgp-csg-bert-cloth)        | [cdgp-csg-bert-dgen](https://huggingface.co/AndyChiang/cdgp-csg-bert-dgen)       |\n| **SciBERT** | [cdgp-csg-scibert-cloth](https://huggingface.co/AndyChiang/cdgp-csg-scibert-cloth)  | [cdgp-csg-scibert-dgen](https://huggingface.co/AndyChiang/cdgp-csg-scibert-dgen) |\n| **RoBERTa** | [cdgp-csg-roberta-cloth](https://huggingface.co/AndyChiang/cdgp-csg-roberta-cloth) | [cdgp-csg-roberta-dgen](https://huggingface.co/AndyChiang/cdgp-csg-roberta-dgen) |\n| **BART**    | [cdgp-csg-bart-cloth](https://huggingface.co/AndyChiang/cdgp-csg-bart-cloth)        | [*cdgp-csg-bart-dgen*](https://huggingface.co/AndyChiang/cdgp-csg-bart-dgen)       |\n\n### Distractor Selector\n\n**fastText**: [cdgp-ds-fasttext](https://huggingface.co/AndyChiang/cdgp-ds-fasttext)\n\n\n## Citation\n\nNone", "size_bytes": "557979897", "downloads": 4}
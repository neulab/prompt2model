{"pretrained_model_name": "Helsinki-NLP/opus-mt-en-itc", "description": "---\nlanguage: \n- en\n- it\n- ca\n- rm\n- es\n- ro\n- gl\n- sc\n- co\n- wa\n- pt\n- oc\n- an\n- id\n- fr\n- ht\n- itc\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### eng-itc\n\n* source group: English \n* target group: Italic languages \n*  OPUS readme: [eng-itc](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-itc/README.md)\n\n*  model: transformer\n* source language(s): eng\n* target language(s): arg ast cat cos egl ext fra frm_Latn gcf_Latn glg hat ind ita lad lad_Latn lat_Latn lij lld_Latn lmo max_Latn mfe min mwl oci pap pms por roh ron scn spa tmw_Latn vec wln zlm_Latn zsm_Latn\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)\n* download original weights: [opus2m-2020-08-01.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-itc/opus2m-2020-08-01.zip)\n* test set translations: [opus2m-2020-08-01.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-itc/opus2m-2020-08-01.test.txt)\n* test set scores: [opus2m-2020-08-01.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-itc/opus2m-2020-08-01.eval.txt)\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| newsdev2016-enro-engron.eng.ron \t| 27.1 \t| 0.565 |\n| newsdiscussdev2015-enfr-engfra.eng.fra \t| 29.9 \t| 0.574 |\n| newsdiscusstest2015-enfr-engfra.eng.fra \t| 35.3 \t| 0.609 |\n| newssyscomb2009-engfra.eng.fra \t| 27.7 \t| 0.567 |\n| newssyscomb2009-engita.eng.ita \t| 28.6 \t| 0.586 |\n| newssyscomb2009-engspa.eng.spa \t| 29.8 \t| 0.569 |\n| news-test2008-engfra.eng.fra \t| 25.0 \t| 0.536 |\n| news-test2008-engspa.eng.spa \t| 27.1 \t| 0.548 |\n| newstest2009-engfra.eng.fra \t| 26.7 \t| 0.557 |\n| newstest2009-engita.eng.ita \t| 28.9 \t| 0.583 |\n| newstest2009-engspa.eng.spa \t| 28.9 \t| 0.567 |\n| newstest2010-engfra.eng.fra \t| 29.6 \t| 0.574 |\n| newstest2010-engspa.eng.spa \t| 33.8 \t| 0.598 |\n| newstest2011-engfra.eng.fra \t| 30.9 \t| 0.590 |\n| newstest2011-engspa.eng.spa \t| 34.8 \t| 0.598 |\n| newstest2012-engfra.eng.fra \t| 29.1 \t| 0.574 |\n| newstest2012-engspa.eng.spa \t| 34.9 \t| 0.600 |\n| newstest2013-engfra.eng.fra \t| 30.1 \t| 0.567 |\n| newstest2013-engspa.eng.spa \t| 31.8 \t| 0.576 |\n| newstest2016-enro-engron.eng.ron \t| 25.9 \t| 0.548 |\n| Tatoeba-test.eng-arg.eng.arg \t| 1.6 \t| 0.120 |\n| Tatoeba-test.eng-ast.eng.ast \t| 17.2 \t| 0.389 |\n| Tatoeba-test.eng-cat.eng.cat \t| 47.6 \t| 0.668 |\n| Tatoeba-test.eng-cos.eng.cos \t| 4.3 \t| 0.287 |\n| Tatoeba-test.eng-egl.eng.egl \t| 0.9 \t| 0.101 |\n| Tatoeba-test.eng-ext.eng.ext \t| 8.7 \t| 0.287 |\n| Tatoeba-test.eng-fra.eng.fra \t| 44.9 \t| 0.635 |\n| Tatoeba-test.eng-frm.eng.frm \t| 1.0 \t| 0.225 |\n| Tatoeba-test.eng-gcf.eng.gcf \t| 0.7 \t| 0.115 |\n| Tatoeba-test.eng-glg.eng.glg \t| 44.9 \t| 0.648 |\n| Tatoeba-test.eng-hat.eng.hat \t| 30.9 \t| 0.533 |\n| Tatoeba-test.eng-ita.eng.ita \t| 45.4 \t| 0.673 |\n| Tatoeba-test.eng-lad.eng.lad \t| 5.6 \t| 0.279 |\n| Tatoeba-test.eng-lat.eng.lat \t| 12.1 \t| 0.380 |\n| Tatoeba-test.eng-lij.eng.lij \t| 1.4 \t| 0.183 |\n| Tatoeba-test.eng-lld.eng.lld \t| 0.5 \t| 0.199 |\n| Tatoeba-test.eng-lmo.eng.lmo \t| 0.7 \t| 0.187 |\n| Tatoeba-test.eng-mfe.eng.mfe \t| 83.6 \t| 0.909 |\n| Tatoeba-test.eng-msa.eng.msa \t| 31.3 \t| 0.549 |\n| Tatoeba-test.eng.multi \t| 38.0 \t| 0.588 |\n| Tatoeba-test.eng-mwl.eng.mwl \t| 2.7 \t| 0.322 |\n| Tatoeba-test.eng-oci.eng.oci \t| 8.2 \t| 0.293 |\n| Tatoeba-test.eng-pap.eng.pap \t| 46.7 \t| 0.663 |\n| Tatoeba-test.eng-pms.eng.pms \t| 2.1 \t| 0.194 |\n| Tatoeba-test.eng-por.eng.por \t| 41.2 \t| 0.635 |\n| Tatoeba-test.eng-roh.eng.roh \t| 2.6 \t| 0.237 |\n| Tatoeba-test.eng-ron.eng.ron \t| 40.6 \t| 0.632 |\n| Tatoeba-test.eng-scn.eng.scn \t| 1.6 \t| 0.181 |\n| Tatoeba-test.eng-spa.eng.spa \t| 49.5 \t| 0.685 |\n| Tatoeba-test.eng-vec.eng.vec \t| 1.6 \t| 0.223 |\n| Tatoeba-test.eng-wln.eng.wln \t| 7.1 \t| 0.250 |\n\n\n### System Info: \n- hf_name: eng-itc\n\n- source_languages: eng\n\n- target_languages: itc\n\n- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-itc/README.md\n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['en', 'it', 'ca', 'rm', 'es', 'ro', 'gl', 'sc', 'co', 'wa', 'pt', 'oc', 'an', 'id', 'fr', 'ht', 'itc']\n\n- src_constituents: {'eng'}\n\n- tgt_constituents: {'ita', 'cat', 'roh', 'spa', 'pap', 'bjn', 'lmo', 'mwl', 'lij', 'lat_Latn', 'lad_Latn', 'pcd', 'lat_Grek', 'ext', 'ron', 'ast', 'glg', 'pms', 'zsm_Latn', 'srd', 'gcf_Latn', 'lld_Latn', 'min', 'tmw_Latn', 'cos', 'wln', 'zlm_Latn', 'por', 'egl', 'oci', 'vec', 'arg', 'ind', 'fra', 'hat', 'lad', 'max_Latn', 'frm_Latn', 'scn', 'mfe'}\n\n- src_multilingual: False\n\n- tgt_multilingual: True\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-itc/opus2m-2020-08-01.zip\n\n- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-itc/opus2m-2020-08-01.test.txt\n\n- src_alpha3: eng\n\n- tgt_alpha3: itc\n\n- short_pair: en-itc\n\n- chrF2_score: 0.588\n\n- bleu: 38.0\n\n- brevity_penalty: 0.9670000000000001\n\n- ref_len: 73951.0\n\n- src_name: English\n\n- tgt_name: Italic languages\n\n- train_date: 2020-08-01\n\n- src_alpha2: en\n\n- tgt_alpha2: itc\n\n- prefer_old: False\n\n- long_pair: eng-itc\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "size_bytes": "295972653", "downloads": 98}
{"pretrained_model_name": "BelleGroup/BELLE-7B-0.6M", "description": "---\nlicense: apache-2.0\ntags:\n- text2text-generation\npipeline_tag: text2text-generation\nlanguage:\n- zh\n- en\nwidget:\n- text: |-\n    Human: \u4f7f\u7528python\u5199\u4e00\u4e2a\u4e8c\u5206\u67e5\u627e\u7684\u4ee3\u7801\n    Assistant: \n  example_title: code zh\n- text: >-\n    Human: Classify the sentiment of the following sentence into Positive,\n    Neutral, or Negative: \n\n    Super excited about teaching Stanford\u2019s first course on Large Language\n    Models! Check the syllabus out here\n\n    Assistant: \n  example_title: sentiment en\n- text: |-\n    Human: \u4eca\u5929\u5929\u6c14\u600e\u4e48\u6837\uff0c\u628a\u8fd9\u53e5\u8bdd\u7ffb\u8bd1\u6210\u82f1\u8bed\n    Assistant: \n  example_title: translation zh-en\n- text: |-\n    Human: \u600e\u4e48\u8ba9\u81ea\u5df1\u7cbe\u529b\u5145\u6c9b\uff0c\u52175\u70b9\u5efa\u8bae\n    Assistant: \n  example_title: brainstorming zh\n- text: |-\n    Human: \u8bf7\u4ee5\u300e\u6625\u5929\u7684\u5317\u4eac\u300f\u4e3a\u9898\u5199\u4e00\u9996\u8bd7\u6b4c\n    Assistant: \n  example_title: generation zh\n- text: |-\n    Human: \u660e\u5929\u5c31\u5047\u671f\u7ed3\u675f\u4e86\uff0c\u6709\u70b9\u6297\u62d2\u4e0a\u73ed\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\n    Assistant: \n  example_title: brainstorming zh\n- text: |-\n    Human: \u7236\u6bcd\u90fd\u59d3\u5434\uff0c\u53d6\u4e00\u4e9b\u7537\u5b9d\u5b9d\u548c\u5973\u5b9d\u5b9d\u7684\u540d\u5b57\n    Assistant: \n  example_title: brainstorming zh\n- text: |-\n    Human: \u63a8\u8350\u51e0\u672c\u91d1\u5eb8\u7684\u6b66\u4fa0\u5c0f\u8bf4\n    Assistant: \n  example_title: brainstorming zh\n---\n\n# Model Card for Model ID\n\n## Model description\nBELLE is based on Bloomz-7b1-mt and finetuned with 0.6M Chinese data combined with 50,000 pieces of English data from the open source Stanford-Alpaca, resulting in good Chinese instruction understanding and response generation capabilities. \n\nThe code of Chinese data generation and other detailed information can be found in our Github project repository: https://github.com/LianjiaTech/BELLE.\n\nWe trained models using datasets of different sizes (200,000, 600,000, and 1,000,000 samples) for instruction learning, and we obtained different model versions as shown below:\n| Datasize| 200,000 | 600,000 | 1,000,000 |\n| ----- | ----- | ----- | ----- |\n| Finetuned Model | [BELLE-7B-0.2M](https://huggingface.co/BelleGroup/BELLE-7B-0.2M) | [BELLE-7B-0.6M](https://huggingface.co/BelleGroup/BELLE-7B-0.6M) | [BELLE-7B-1M](https://huggingface.co/BelleGroup/BELLE-7B-1M) |\n\n## Training hyper-parameters\n| Parameter | Value |\n| ------ | ------ |\n| Batch size | 64 |\n| Learning rate | 3e-6 |\n| Epochs | 3 |\n|Weight_decay | 0.001 |\n|Warmup_rate | 0.1 |\n|LR_scheduler | linear |\n\n## Use model\nPlease note that the input should be formatted as follows in both **training** and **inference**.\n``` python\nHuman: {input} \\n\\nAssistant:\n```\n\nBELLE can be easily loaded with AutoModelForCausalLM.\n``` python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport sys\n\nmodel_path = \"./\" # You can modify the path for storing the local model\nmodel =  AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nprint(\"Human:\")\nline = input()\nwhile line:\n        inputs = 'Human: ' + line.strip() + '\\n\\nAssistant:'\n        input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n        outputs = model.generate(input_ids, max_new_tokens=200, do_sample = True, top_k = 30, top_p = 0.85, temperature = 0.35, repetition_penalty=1.2)\n        rets = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        print(\"Assistant:\\n\" + rets[0].strip().replace(inputs, \"\"))\n        print(\"\\n------------------------------------------------\\nHuman:\")\n        line = input()\n\n```\n\nThe above example did not utilize the context from previous turns of the conversation. If you want to perform multi-turn dialogue understanding, you can concatenate the previous turns of the conversation in the format of Human:{input}\\n\\nAssistant:{output} and input it to the generate function.\n\n## Limitations\nThere still exists a few issues in the model trained on current base model and data:\n\n1. The model might generate factual errors when asked to follow instructions related to facts.\n\n2. Occasionally generates harmful responses since the model still struggles to identify potential harmful instructions.\n\n3. Needs improvements on reasoning and coding.\n\nSince the model still has its limitations, we require developers only use the open-sourced code, data, model and any other artifacts generated via this project for research purposes. Commercial use and other potential harmful use cases are not allowed.\n\n## Some use cases\n| Human | Assitant |\n| ----- | ------ |\n| \u4f7f\u7528python\u5199\u5b57\u7b26\u4e32\u7ffb\u8f6c\u7684\u51fd\u6570 | \u53ef\u4ee5\u4f7f\u7528Python\u5185\u7f6e\u65b9\u6cd5 `reversed()` \u6765\u5b9e\u73b0\u5b57\u7b26\u4e32\u7ffb\u8f6c\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a<br/><br/>def reverse_string(s):<br/>&nbsp;&nbsp;&nbsp;&nbsp;return s[::-1]<br/>    <br/>print(reverse_string(\"Hello, world!\"))<br/># \u8f93\u51fa\uff1a'!dlrow,olleH'<br/> |\n| \u505a\u96be\u800c\u6b63\u786e\u7684\u4e8b\uff0c\u7ffb\u8bd1\u6210\u82f1\u8bed | Do difficult and correct things. |\n| The current Alpaca model is fine-tuned from a 7B LLaMA model on 52K instruction-following data generated by the techniques in the Self-Instruct paper, with some modifications that we discuss in the next section. In a preliminary human evaluation, we found that the Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite. How many instruction-following data generated? | 52K |\n| \u7ed9\u4e00\u4e2a\u5317\u4eac\u4e09\u65e5\u6e38\u7684\u65c5\u884c\u653b\u7565 | Day 1:<br/>- \u53c2\u89c2\u6545\u5bab\u535a\u7269\u9662\uff0c\u4e86\u89e3\u4e2d\u56fd\u5386\u53f2\u548c\u6587\u5316\u3002<br/>- \u5728\u5929\u5b89\u95e8\u5e7f\u573a\u89c2\u770b\u5347\u65d7\u4eea\u5f0f\u3002 <br/>- \u5230\u9890\u548c\u56ed\u6e38\u89c8\u7f8e\u4e3d\u7684\u56ed\u6797\u666f\u89c2\u3002<br/>- \u54c1\u5c1d\u70e4\u9e2d\u548c\u5176\u4ed6\u5730\u9053\u7f8e\u98df\u3002<br/>- \u53bb\u738b\u5e9c\u4e95\u8d2d\u7269\u4e2d\u5fc3\u901b\u8857\u8d2d\u7269\u3002 <br/><br/>Day 2:<br/>- \u524d\u5f80\u957f\u57ce\u6e38\u73a9\uff0c\u611f\u53d7\u53e4\u4ee3\u4e2d\u56fd\u7684\u58ee\u4e3d\u666f\u8272\u3002<br/>- \u4e58\u5750\u7d22\u9053\u7f06\u8f66\u5230\u8fbe\u516b\u8fbe\u5cad\u957f\u57ce\uff0c\u6b23\u8d4f\u7f8e\u666f\u3002<br/>- \u665a\u4e0a\u53bb\u7c0b\u8857\u4f53\u9a8c\u5f53\u5730\u7684\u5c0f\u5403\u6587\u5316\u3002<br/><br/>Day 3:<br/>- \u53c2\u89c2\u5929\u575b\u516c\u56ed\uff0c\u89c2\u8d4f\u53e4\u5efa\u7b51\u7fa4\u548c\u7948\u5e74\u6bbf\u3002<br/>- \u5230\u9e1f\u5de2\u548c\u6c34\u7acb\u65b9\u89c2\u770b\u5965\u8fd0\u4f1a\u6bd4\u8d5b\u3002<br/>- \u5348\u9910\u540e\u5230798\u827a\u672f\u533a\u53c2\u89c2\u5f53\u4ee3\u827a\u672f\u5bb6\u7684\u5de5\u4f5c\u5ba4\u3002 |\n| \u6765\u4e00\u4e2a\u6e29\u99a8\u7684\u7761\u524d\u6545\u4e8b | \u4ece\u524d\u6709\u4e00\u4e2a\u5c0f\u5154\u5b50\uff0c\u5b83\u4f4f\u5728\u68ee\u6797\u91cc\u3002\u6bcf\u5929\u665a\u4e0a\uff0c\u5c0f\u5154\u5b50\u90fd\u4f1a\u5728\u6811\u4e0b\u6253\u76f9\u513f\u3002\u4f46\u662f\u6709\u4e00\u5929\u665a\u4e0a\uff0c\u4e00\u53ea\u5927\u7070\u72fc\u6765\u4e86\uff01\u4ed6\u6084\u6084\u5730\u6e9c\u8fdb\u5c0f\u5154\u5b50\u7684\u6d1e\u7a74\u91cc\uff0c\u60f3\u8981\u5077\u8d70\u5b83\u7684\u80e1\u841d\u535c\u3002<br/>\u7136\u800c\uff0c\u5c0f\u5154\u5b50\u5e76\u6ca1\u6709\u88ab\u5413\u5230\u3002\u76f8\u53cd\uff0c\u5b83\u7528\u81ea\u5df1\u7684\u667a\u6167\u548c\u52c7\u6c14\u4fdd\u62a4\u4e86\u81ea\u5df1\u3002\u5b83\u5728\u6d1e\u53e3\u653e\u4e86\u4e00\u5806\u80e1\u841d\u535c\u6765\u5438\u5f15\u5927\u7070\u72fc\uff0c\u7136\u540e\u8d81\u673a\u9003\u8131\u4e86\u3002\u4ece\u90a3\u5929\u8d77\uff0c\u5c0f\u5154\u5b50\u5b66\u4f1a\u4e86\u5728\u5371\u9669\u65f6\u4fdd\u6301\u9547\u9759\u5e76\u91c7\u53d6\u9002\u5f53\u7684\u884c\u52a8 |\n| \u5982\u4f55\u5feb\u901f\u63d0\u5347\u81ea\u5df1\u7684\u5199\u4f5c\u80fd\u529b | 1. \u9605\u8bfb\u5927\u91cf\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u5b66\u4f5c\u54c1\uff0c\u5305\u62ec\u5c0f\u8bf4\u3001\u6563\u6587\u548c\u8bd7\u6b4c\u7b49\u3002<br/>2. \u7ec3\u4e60\u5199\u4f5c\uff0c\u6bcf\u5929\u5199\u4e00\u4e9b\u4e1c\u897f\u5e76\u4e0d\u65ad\u4fee\u6539\u548c\u5b8c\u5584\u3002<br/>3. \u53c2\u52a0\u5199\u4f5c\u73ed\u6216\u8bf7\u6559\u4e13\u4e1a\u4f5c\u5bb6\u4ee5\u83b7\u5f97\u53cd\u9988\u548c\u5efa\u8bae\u3002<br/>4. \u5229\u7528\u5404\u79cd\u5de5\u5177\u548c\u6280\u672f\u6765\u63d0\u9ad8\u5199\u4f5c\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4f8b\u5982\u8bed\u6cd5\u68c0\u67e5\u5668\u3001\u62fc\u5199\u68c0\u67e5\u5668\u548c\u81ea\u52a8\u6da6\u8272\u529f\u80fd\u7b49\u3002 |\n| \u5317\u4eac\u5927\u5b66\u662f\u54ea\u4e00\u5e74\u6210\u7acb\u7684\uff1f | \u5317\u4eac\u5927\u5b66\u6210\u7acb\u4e8e1898\u5e74 |\n***\n\n## \u6a21\u578b\u63cf\u8ff0\nBELLE\u6a21\u578b\u4ee5Bloomz-7b1-mt\u4e3a\u57fa\u7840\uff0c\u5728 0.2M/0.6M/1.0M \u6761\u4e2d\u6587\u6570\u636e\u4e0a\uff0c\u7ed3\u5408Stanford-Alpaca\u5f00\u6e90\u76845\u4e07\u6761\u82f1\u6587\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u4e2d\u6587instruction\u7406\u89e3\u4ee5\u53caresponse\u751f\u6210\u80fd\u529b\u3002\n\n\u4e2d\u6587\u6570\u636e\u751f\u6210\u4ee3\u7801\u53ca\u5176\u4ed6\u8be6\u7ec6\u4fe1\u606f\uff0c\u4f4d\u4e8e\u6211\u4eec\u7684Github\u9879\u76ee\u4ed3\u5e93: https://github.com/LianjiaTech/BELLE\n\n\u6211\u4eec\u91c7\u53d6\u4e86\u4e0d\u540c\u5927\u5c0f\u89c4\u6a21\uff0820\u4e07\u300160\u4e07\u548c100\u4e07\u6837\u672c\uff09\u7684\u6307\u4ee4\u5b66\u4e60\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u6211\u4eec\u5f97\u5230\u4e0d\u540c\u7684\u6a21\u578b\u7248\u672c\u5982\u4e0b\u6240\u793a:\n| Datasize| 200,000 | 600,000 | 1,000,000 |\n| ----- | ----- | ----- | ----- |\n| Finetuned Model | [BELLE-7B-0.2M](https://huggingface.co/BelleGroup/BELLE-7B-0.2M) | [BELLE-7B-0.6M](https://huggingface.co/BelleGroup/BELLE-7B-0.6M) | [BELLE-7B-1M](https://huggingface.co/BelleGroup/BELLE-7B-1M) |\n\n## \u6a21\u578b\u8bad\u7ec3\u8d85\u53c2\u6570\n| \u53c2\u6570 | \u503c |\n| ------ | ------ |\n| Batch size | 64 |\n| Learning rate | 3e-6 |\n| Epochs | 3 |\n|Weight_decay | 0.001 |\n|Warmup_rate | 0.1 |\n|LR_scheduler | linear |\n\n## \u4f7f\u7528\u6a21\u578b\n\u8bf7\u6ce8\u610f\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\uff0c\u6a21\u578b\u7684\u8f93\u5165\u5e94\u8be5\u5904\u7406\u6210\u5982\u4e0b\u5f62\u5f0f\uff1a\n``` python\nHuman: {input} \\n\\nAssistant:\n``` \n\n\u901a\u8fc7AutoModelForCausalLM\u5373\u53ef\u76f4\u63a5\u8f7d\u5165\u6a21\u578b\u5e76\u4f7f\u7528\u3002\n``` python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport sys\n\nmodel_path = \"./\" # You can modify the path for storing the local model\nmodel =  AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nprint(\"Human:\")\nline = input()\nwhile line:\n        inputs = 'Human: ' + line.strip() + '\\n\\nAssistant:'\n        input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n        outputs = model.generate(input_ids, max_new_tokens=200, do_sample = True, top_k = 30, top_p = 0.85, temperature = 0.35, repetition_penalty=1.2)\n        rets = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        print(\"Assistant:\\n\" + rets[0].strip().replace(inputs, \"\"))\n        print(\"\\n------------------------------------------------\\nHuman:\")\n        line = input()\n\n```\n\n\u4e0a\u9762\u6837\u4f8b\u6ca1\u6709\u5229\u7528\u5bf9\u8bdd\u4e0a\u6587\uff0c\u5982\u679c\u4f60\u60f3\u505a\u591a\u8f6e\u5bf9\u8bdd\u7406\u89e3\uff0c\u53ef\u4ee5\u5c06\u5bf9\u8bdd\u4e0a\u6587\u6309\u7167Human:{input}\\n\\nAssistant:{output} \u65b9\u5f0f\u62fc\u63a5\u8d77\u6765\u8f93\u5165\u7ed9generate\u51fd\u6570\n\n## \u5c40\u9650\u6027\u548c\u4f7f\u7528\u9650\u5236\n\u57fa\u4e8e\u5f53\u524d\u6570\u636e\u548c\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u5f97\u5230\u7684SFT\u6a21\u578b\uff0c\u5728\u6548\u679c\u4e0a\u4ecd\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a\n\n1. \u5728\u6d89\u53ca\u4e8b\u5b9e\u6027\u7684\u6307\u4ee4\u4e0a\u53ef\u80fd\u4f1a\u4ea7\u751f\u8fdd\u80cc\u4e8b\u5b9e\u7684\u9519\u8bef\u56de\u7b54\u3002\n\n2. \u5bf9\u4e8e\u5177\u5907\u5371\u5bb3\u6027\u7684\u6307\u4ee4\u65e0\u6cd5\u5f88\u597d\u7684\u9274\u522b\uff0c\u7531\u6b64\u4f1a\u4ea7\u751f\u5371\u5bb3\u6027\u8a00\u8bba\u3002\n\n3. \u5728\u4e00\u4e9b\u6d89\u53ca\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u573a\u666f\u4e0b\u6a21\u578b\u7684\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002\n\n\u57fa\u4e8e\u4ee5\u4e0a\u6a21\u578b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u8981\u6c42\u5f00\u53d1\u8005\u4ec5\u5c06\u6211\u4eec\u5f00\u6e90\u7684\u4ee3\u7801\u3001\u6570\u636e\u3001\u6a21\u578b\u53ca\u540e\u7eed\u7528\u6b64\u9879\u76ee\u751f\u6210\u7684\u884d\u751f\u7269\u7528\u4e8e\u7814\u7a76\u76ee\u7684\uff0c\u4e0d\u5f97\u7528\u4e8e\u5546\u4e1a\uff0c\u4ee5\u53ca\u5176\u4ed6\u4f1a\u5bf9\u793e\u4f1a\u5e26\u6765\u5371\u5bb3\u7684\u7528\u9014\u3002\n\n## \u4e00\u4e9b\u793a\u4f8b\n\n| Human | Assitant |\n| ----- | ------ |\n| \u4f7f\u7528python\u5199\u5b57\u7b26\u4e32\u7ffb\u8f6c\u7684\u51fd\u6570 | \u53ef\u4ee5\u4f7f\u7528Python\u5185\u7f6e\u65b9\u6cd5 `reversed()` \u6765\u5b9e\u73b0\u5b57\u7b26\u4e32\u7ffb\u8f6c\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a<br/><br/>def reverse_string(s):<br/>&nbsp;&nbsp;&nbsp;&nbsp;return s[::-1]<br/>    <br/>print(reverse_string(\"Hello, world!\"))<br/># \u8f93\u51fa\uff1a'!dlrow,olleH'<br/> |\n| \u505a\u96be\u800c\u6b63\u786e\u7684\u4e8b\uff0c\u7ffb\u8bd1\u6210\u82f1\u8bed | Do difficult and correct things. |\n| The current Alpaca model is fine-tuned from a 7B LLaMA model on 52K instruction-following data generated by the techniques in the Self-Instruct paper, with some modifications that we discuss in the next section. In a preliminary human evaluation, we found that the Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite. How many instruction-following data generated? | 52K |\n| \u7ed9\u4e00\u4e2a\u5317\u4eac\u4e09\u65e5\u6e38\u7684\u65c5\u884c\u653b\u7565 | Day 1:<br/>- \u53c2\u89c2\u6545\u5bab\u535a\u7269\u9662\uff0c\u4e86\u89e3\u4e2d\u56fd\u5386\u53f2\u548c\u6587\u5316\u3002<br/>- \u5728\u5929\u5b89\u95e8\u5e7f\u573a\u89c2\u770b\u5347\u65d7\u4eea\u5f0f\u3002 <br/>- \u5230\u9890\u548c\u56ed\u6e38\u89c8\u7f8e\u4e3d\u7684\u56ed\u6797\u666f\u89c2\u3002<br/>- \u54c1\u5c1d\u70e4\u9e2d\u548c\u5176\u4ed6\u5730\u9053\u7f8e\u98df\u3002<br/>- \u53bb\u738b\u5e9c\u4e95\u8d2d\u7269\u4e2d\u5fc3\u901b\u8857\u8d2d\u7269\u3002 <br/><br/>Day 2:<br/>- \u524d\u5f80\u957f\u57ce\u6e38\u73a9\uff0c\u611f\u53d7\u53e4\u4ee3\u4e2d\u56fd\u7684\u58ee\u4e3d\u666f\u8272\u3002<br/>- \u4e58\u5750\u7d22\u9053\u7f06\u8f66\u5230\u8fbe\u516b\u8fbe\u5cad\u957f\u57ce\uff0c\u6b23\u8d4f\u7f8e\u666f\u3002<br/>- \u665a\u4e0a\u53bb\u7c0b\u8857\u4f53\u9a8c\u5f53\u5730\u7684\u5c0f\u5403\u6587\u5316\u3002<br/><br/>Day 3:<br/>- \u53c2\u89c2\u5929\u575b\u516c\u56ed\uff0c\u89c2\u8d4f\u53e4\u5efa\u7b51\u7fa4\u548c\u7948\u5e74\u6bbf\u3002<br/>- \u5230\u9e1f\u5de2\u548c\u6c34\u7acb\u65b9\u89c2\u770b\u5965\u8fd0\u4f1a\u6bd4\u8d5b\u3002<br/>- \u5348\u9910\u540e\u5230798\u827a\u672f\u533a\u53c2\u89c2\u5f53\u4ee3\u827a\u672f\u5bb6\u7684\u5de5\u4f5c\u5ba4\u3002 |\n| \u6765\u4e00\u4e2a\u6e29\u99a8\u7684\u7761\u524d\u6545\u4e8b | \u4ece\u524d\u6709\u4e00\u4e2a\u5c0f\u5154\u5b50\uff0c\u5b83\u4f4f\u5728\u68ee\u6797\u91cc\u3002\u6bcf\u5929\u665a\u4e0a\uff0c\u5c0f\u5154\u5b50\u90fd\u4f1a\u5728\u6811\u4e0b\u6253\u76f9\u513f\u3002\u4f46\u662f\u6709\u4e00\u5929\u665a\u4e0a\uff0c\u4e00\u53ea\u5927\u7070\u72fc\u6765\u4e86\uff01\u4ed6\u6084\u6084\u5730\u6e9c\u8fdb\u5c0f\u5154\u5b50\u7684\u6d1e\u7a74\u91cc\uff0c\u60f3\u8981\u5077\u8d70\u5b83\u7684\u80e1\u841d\u535c\u3002<br/>\u7136\u800c\uff0c\u5c0f\u5154\u5b50\u5e76\u6ca1\u6709\u88ab\u5413\u5230\u3002\u76f8\u53cd\uff0c\u5b83\u7528\u81ea\u5df1\u7684\u667a\u6167\u548c\u52c7\u6c14\u4fdd\u62a4\u4e86\u81ea\u5df1\u3002\u5b83\u5728\u6d1e\u53e3\u653e\u4e86\u4e00\u5806\u80e1\u841d\u535c\u6765\u5438\u5f15\u5927\u7070\u72fc\uff0c\u7136\u540e\u8d81\u673a\u9003\u8131\u4e86\u3002\u4ece\u90a3\u5929\u8d77\uff0c\u5c0f\u5154\u5b50\u5b66\u4f1a\u4e86\u5728\u5371\u9669\u65f6\u4fdd\u6301\u9547\u9759\u5e76\u91c7\u53d6\u9002\u5f53\u7684\u884c\u52a8 |\n| \u5982\u4f55\u5feb\u901f\u63d0\u5347\u81ea\u5df1\u7684\u5199\u4f5c\u80fd\u529b | 1. \u9605\u8bfb\u5927\u91cf\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u5b66\u4f5c\u54c1\uff0c\u5305\u62ec\u5c0f\u8bf4\u3001\u6563\u6587\u548c\u8bd7\u6b4c\u7b49\u3002<br/>2. \u7ec3\u4e60\u5199\u4f5c\uff0c\u6bcf\u5929\u5199\u4e00\u4e9b\u4e1c\u897f\u5e76\u4e0d\u65ad\u4fee\u6539\u548c\u5b8c\u5584\u3002<br/>3. \u53c2\u52a0\u5199\u4f5c\u73ed\u6216\u8bf7\u6559\u4e13\u4e1a\u4f5c\u5bb6\u4ee5\u83b7\u5f97\u53cd\u9988\u548c\u5efa\u8bae\u3002<br/>4. \u5229\u7528\u5404\u79cd\u5de5\u5177\u548c\u6280\u672f\u6765\u63d0\u9ad8\u5199\u4f5c\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4f8b\u5982\u8bed\u6cd5\u68c0\u67e5\u5668\u3001\u62fc\u5199\u68c0\u67e5\u5668\u548c\u81ea\u52a8\u6da6\u8272\u529f\u80fd\u7b49\u3002 |\n| \u5317\u4eac\u5927\u5b66\u662f\u54ea\u4e00\u5e74\u6210\u7acb\u7684\uff1f | \u5317\u4eac\u5927\u5b66\u6210\u7acb\u4e8e1898\u5e74 |\n", "size_bytes": "28276106203", "downloads": 10}
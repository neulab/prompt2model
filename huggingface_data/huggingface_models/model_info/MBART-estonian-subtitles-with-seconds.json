{"pretrained_model_name": "IljaSamoilov/MBART-estonian-subtitles-with-seconds", "description": "---\nlanguage: \n  - et\nwidget:\n- text: \"te olete ka noh, noh, p\u00e4ris korralikult ka Rahvusringh\u00e4\u00e4lingu teatud m\u00f5ttes sellisesse keerulisse olukorda pannud,\"\n- text: \"Et, et, et miks mitte olla siis tasakaalus, ma noh, h\u00fcpoteetiliselt viskan selle palli \u00fcles,\"\n\n---\n\nDataset must be processed as following:\n\n```\n\ndef preprocess_function_with_seconds(ds):\n\n    inputs = ds['generated']\n    targets =  ds['subtitle']\n\n    model_inputs = tokenizer(inputs, truncation=True, max_length=128, padding=True, return_tensors=\"np\")\n    secs = list(map(lambda x: \"{:.1f}\".format(x), ds[\"seconds\"]))\n    sec_inputs = tokenizer(secs, truncation=True, max_length=128, padding=True, return_tensors=\"np\")\n\n    model_inputs['input_ids'] = np.concatenate((sec_inputs['input_ids'][:,1:2], model_inputs['input_ids']), 1)\n    model_inputs['attention_mask'] = np.concatenate((sec_inputs['attention_mask'][:,1:2], model_inputs['attention_mask']), 1)\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, truncation=True, max_length=128, padding=True, return_tensors=\"np\")\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs    \n```\n    \nImporting the model and tokenizer:\n\n```\ntokenizer = MBart50Tokenizer.from_pretrained(\"IljaSamoilov/MBART-estonian-subtitles-with-seconds\", src_lang=\"et_EE\", tgt_lang=\"et_EE\")\nmodel = MBartForConditionalGeneration.from_pretrained(\"IljaSamoilov/MBART-estonian-subtitles-with-seconds\")\n```", "size_bytes": "2444683129", "downloads": 3}
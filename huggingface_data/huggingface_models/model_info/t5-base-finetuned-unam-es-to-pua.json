{"pretrained_model_name": "mekjr1/t5-base-finetuned-unam-es-to-pua", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: t5-base-finetuned-unam-es-to-pua\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-base-finetuned-unam-es-to-pua\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1424\n- Bleu: 5.6655\n- Gen Len: 18.3375\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 100\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Bleu   | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:-------:|\n| No log        | 1.0   | 35   | 3.4462          | 0.0    | 15.525  |\n| No log        | 2.0   | 70   | 2.7253          | 0.3534 | 17.6375 |\n| No log        | 3.0   | 105  | 2.3461          | 0.532  | 18.475  |\n| No log        | 4.0   | 140  | 2.1536          | 0.5158 | 18.2625 |\n| No log        | 5.0   | 175  | 2.0331          | 0.6926 | 18.5375 |\n| No log        | 6.0   | 210  | 1.9242          | 0.7148 | 18.5    |\n| No log        | 7.0   | 245  | 1.8474          | 0.5904 | 17.6125 |\n| No log        | 8.0   | 280  | 1.7931          | 0.5887 | 17.525  |\n| No log        | 9.0   | 315  | 1.7359          | 0.6126 | 17.7375 |\n| No log        | 10.0  | 350  | 1.6828          | 0.5915 | 17.775  |\n| No log        | 11.0  | 385  | 1.6440          | 0.677  | 17.5    |\n| No log        | 12.0  | 420  | 1.6004          | 0.6503 | 17.6375 |\n| No log        | 13.0  | 455  | 1.5656          | 1.1308 | 18.0375 |\n| No log        | 14.0  | 490  | 1.5399          | 1.2924 | 18.1875 |\n| 2.2426        | 15.0  | 525  | 1.5116          | 1.2398 | 18.05   |\n| 2.2426        | 16.0  | 560  | 1.4969          | 1.5902 | 18.075  |\n| 2.2426        | 17.0  | 595  | 1.4712          | 1.5022 | 18.0875 |\n| 2.2426        | 18.0  | 630  | 1.4510          | 1.5548 | 18.175  |\n| 2.2426        | 19.0  | 665  | 1.4385          | 2.4875 | 18.4    |\n| 2.2426        | 20.0  | 700  | 1.4255          | 1.8098 | 18.25   |\n| 2.2426        | 21.0  | 735  | 1.4175          | 1.3967 | 18.2875 |\n| 2.2426        | 22.0  | 770  | 1.3840          | 1.4478 | 18.4125 |\n| 2.2426        | 23.0  | 805  | 1.3794          | 2.5712 | 18.3875 |\n| 2.2426        | 24.0  | 840  | 1.3651          | 1.6709 | 18.2625 |\n| 2.2426        | 25.0  | 875  | 1.3516          | 2.7181 | 18.2125 |\n| 2.2426        | 26.0  | 910  | 1.3438          | 1.9771 | 18.0625 |\n| 2.2426        | 27.0  | 945  | 1.3204          | 3.3283 | 18.175  |\n| 2.2426        | 28.0  | 980  | 1.3148          | 4.0754 | 18.2    |\n| 1.4238        | 29.0  | 1015 | 1.3111          | 3.831  | 18.325  |\n| 1.4238        | 30.0  | 1050 | 1.2939          | 4.2328 | 18.2375 |\n| 1.4238        | 31.0  | 1085 | 1.2896          | 3.5101 | 18.3125 |\n| 1.4238        | 32.0  | 1120 | 1.2833          | 3.6533 | 18.4125 |\n| 1.4238        | 33.0  | 1155 | 1.2771          | 3.3986 | 18.5125 |\n| 1.4238        | 34.0  | 1190 | 1.2727          | 3.3589 | 18.3125 |\n| 1.4238        | 35.0  | 1225 | 1.2622          | 3.24   | 18.1625 |\n| 1.4238        | 36.0  | 1260 | 1.2512          | 3.2334 | 18.45   |\n| 1.4238        | 37.0  | 1295 | 1.2506          | 3.3842 | 18.5125 |\n| 1.4238        | 38.0  | 1330 | 1.2407          | 3.2457 | 18.375  |\n| 1.4238        | 39.0  | 1365 | 1.2315          | 3.4157 | 18.3875 |\n| 1.4238        | 40.0  | 1400 | 1.2343          | 3.2254 | 18.3375 |\n| 1.4238        | 41.0  | 1435 | 1.2334          | 3.3025 | 18.3375 |\n| 1.4238        | 42.0  | 1470 | 1.2290          | 3.3478 | 18.3375 |\n| 1.1647        | 43.0  | 1505 | 1.2209          | 3.3824 | 18.4    |\n| 1.1647        | 44.0  | 1540 | 1.2088          | 3.6032 | 18.375  |\n| 1.1647        | 45.0  | 1575 | 1.2077          | 3.549  | 18.425  |\n| 1.1647        | 46.0  | 1610 | 1.2058          | 3.5507 | 18.3625 |\n| 1.1647        | 47.0  | 1645 | 1.2030          | 3.8134 | 18.4125 |\n| 1.1647        | 48.0  | 1680 | 1.2100          | 3.7041 | 18.4    |\n| 1.1647        | 49.0  | 1715 | 1.1968          | 3.5977 | 18.4    |\n| 1.1647        | 50.0  | 1750 | 1.1911          | 3.7133 | 18.3    |\n| 1.1647        | 51.0  | 1785 | 1.1874          | 3.7578 | 18.3    |\n| 1.1647        | 52.0  | 1820 | 1.1920          | 3.7871 | 18.2875 |\n| 1.1647        | 53.0  | 1855 | 1.1867          | 3.7594 | 18.4    |\n| 1.1647        | 54.0  | 1890 | 1.1880          | 3.7163 | 18.3375 |\n| 1.1647        | 55.0  | 1925 | 1.1807          | 4.7929 | 18.375  |\n| 1.1647        | 56.0  | 1960 | 1.1832          | 4.0148 | 18.3375 |\n| 1.1647        | 57.0  | 1995 | 1.1789          | 4.7512 | 18.3875 |\n| 1.0097        | 58.0  | 2030 | 1.1819          | 4.9173 | 18.35   |\n| 1.0097        | 59.0  | 2065 | 1.1742          | 5.0857 | 18.3875 |\n| 1.0097        | 60.0  | 2100 | 1.1771          | 4.006  | 18.4    |\n| 1.0097        | 61.0  | 2135 | 1.1677          | 3.812  | 18.325  |\n| 1.0097        | 62.0  | 2170 | 1.1683          | 4.1118 | 18.3625 |\n| 1.0097        | 63.0  | 2205 | 1.1653          | 3.7104 | 18.3625 |\n| 1.0097        | 64.0  | 2240 | 1.1578          | 3.785  | 18.4375 |\n| 1.0097        | 65.0  | 2275 | 1.1616          | 3.8943 | 18.4    |\n| 1.0097        | 66.0  | 2310 | 1.1617          | 4.0885 | 18.325  |\n| 1.0097        | 67.0  | 2345 | 1.1689          | 5.0819 | 18.4    |\n| 1.0097        | 68.0  | 2380 | 1.1602          | 5.0775 | 18.3375 |\n| 1.0097        | 69.0  | 2415 | 1.1581          | 5.0943 | 18.375  |\n| 1.0097        | 70.0  | 2450 | 1.1590          | 5.2458 | 18.3125 |\n| 1.0097        | 71.0  | 2485 | 1.1605          | 4.925  | 18.425  |\n| 0.9105        | 72.0  | 2520 | 1.1604          | 5.4352 | 18.35   |\n| 0.9105        | 73.0  | 2555 | 1.1547          | 5.157  | 18.2875 |\n| 0.9105        | 74.0  | 2590 | 1.1488          | 5.0934 | 18.3625 |\n| 0.9105        | 75.0  | 2625 | 1.1487          | 5.1848 | 18.4    |\n| 0.9105        | 76.0  | 2660 | 1.1530          | 5.1413 | 18.3625 |\n| 0.9105        | 77.0  | 2695 | 1.1541          | 5.1413 | 18.4    |\n| 0.9105        | 78.0  | 2730 | 1.1445          | 5.1848 | 18.3375 |\n| 0.9105        | 79.0  | 2765 | 1.1509          | 5.2391 | 18.3375 |\n| 0.9105        | 80.0  | 2800 | 1.1512          | 5.444  | 18.3375 |\n| 0.9105        | 81.0  | 2835 | 1.1532          | 5.2204 | 18.4125 |\n| 0.9105        | 82.0  | 2870 | 1.1551          | 5.3439 | 18.3375 |\n| 0.9105        | 83.0  | 2905 | 1.1504          | 4.6498 | 18.4    |\n| 0.9105        | 84.0  | 2940 | 1.1497          | 4.7896 | 18.4    |\n| 0.9105        | 85.0  | 2975 | 1.1513          | 5.3543 | 18.35   |\n| 0.8502        | 86.0  | 3010 | 1.1523          | 5.4728 | 18.35   |\n| 0.8502        | 87.0  | 3045 | 1.1509          | 5.8559 | 18.35   |\n| 0.8502        | 88.0  | 3080 | 1.1504          | 5.4819 | 18.35   |\n| 0.8502        | 89.0  | 3115 | 1.1496          | 5.3352 | 18.35   |\n| 0.8502        | 90.0  | 3150 | 1.1451          | 5.6849 | 18.2875 |\n| 0.8502        | 91.0  | 3185 | 1.1436          | 5.6849 | 18.2875 |\n| 0.8502        | 92.0  | 3220 | 1.1449          | 5.6473 | 18.3625 |\n| 0.8502        | 93.0  | 3255 | 1.1425          | 5.6562 | 18.3    |\n| 0.8502        | 94.0  | 3290 | 1.1445          | 5.6655 | 18.2875 |\n| 0.8502        | 95.0  | 3325 | 1.1441          | 5.6655 | 18.2875 |\n| 0.8502        | 96.0  | 3360 | 1.1436          | 5.6655 | 18.2875 |\n| 0.8502        | 97.0  | 3395 | 1.1426          | 5.6655 | 18.2875 |\n| 0.8502        | 98.0  | 3430 | 1.1428          | 5.6655 | 18.2875 |\n| 0.8502        | 99.0  | 3465 | 1.1422          | 5.6655 | 18.3375 |\n| 0.819         | 100.0 | 3500 | 1.1424          | 5.6655 | 18.3375 |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.13.0\n- Datasets 2.10.1\n- Tokenizers 0.13.2\n", "size_bytes": "891702929", "downloads": 2}
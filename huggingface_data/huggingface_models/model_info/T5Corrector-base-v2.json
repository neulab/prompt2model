{"pretrained_model_name": "Maciel/T5Corrector-base-v2", "description": "---\nlanguage: \n  - zh\nlicense: apache-2.0\ntags:\n- t5\n- text error correction\nwidget:\n- text: \"\u4eca\u5929\u5929\u6c14\u4e0d\u592a\u597d\uff0c\u6211\u7684\u5fc3\u60c5\u4e5f\u4e0d\u662f\u5f88\u5077\u5feb\"\n  example_title: \"\u6848\u4f8b1\"\n- text: \"\u80fd\u4e0d\u80fd\u5e2e\u6211\u4e70\u70b9\u6dc7\u6dcb\uff0c\u597d\u4e45\u6ca1\u5403\u4e86\u3002\"\n  example_title: \"\u6848\u4f8b2\"\n- text: \"\u8111\u5b50\u6709\u70b9\u80e1\u6d82\u4e86\uff0c\u8fd9\u9053\u9898\u51a5\u51a5\u5b66\u8fc7\u8fd8\u6ca1\u6709\u505a\u51fa\u6765\"\n  example_title: \"\u6848\u4f8b3\"\ninference:\n  parameters:\n    max_length: 256\n    num_beams: 10\n    no_repeat_ngram_size: 5\n    do_sample: True\n    early_stopping: True\n---\n\n## \u529f\u80fd\u4ecb\u7ecd\n\nT5Corrector\uff1a\u4e2d\u6587\u5b57\u97f3\u4e0e\u5b57\u5f62\u7ea0\u9519\u6a21\u578b\n\n\u8fd9\u4e2a\u6a21\u578b\u662f\u57fa\u4e8emengzi-t5-base\u8fdb\u884c\u6587\u672c\u7ea0\u9519\u8bad\u7ec3\uff0c\u4f7f\u75282kw+\u53e5\u5b50\uff0c\u901a\u8fc7\u66ff\u6362\u540c\u97f3\u8bcd\u3001\u8fd1\u97f3\u8bcd\u548c\u5f62\u8fd1\u5b57\u6765\uff0c\u5bf9\u4e8e\u53e5\u4e2d\u8bcd\u7ec4\u968f\u673a\u6dfb\u52a0\u8bcd\u7ec4\u3001\u5220\u9664\u8bcd\u7ec4\u4e2d\u7684\u90e8\u5206\u5b57\uff0c\u4ee5\u53ca\u5b57\u8bcd\u4e71\u5e8f\u64cd\u4f5c\u6784\u9020\u7ea0\u9519\u5e73\u884c\u8bed\u6599\uff0c\u5171\u8ba12\u4ebf+\u53e5\u5bf9\uff0c\u7d2f\u8ba1\u8bad\u7ec366000\u6b65\u3002\n\n<a href='https://github.com/Macielyoung/T5Corrector'>Github\u9879\u76ee\u5730\u5740</a>\n\n\n\n\u52a0\u8f7d\u6a21\u578b\uff1a\n\n ```python\n# \u52a0\u8f7d\u6a21\u578b\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\npretrained = \"Maciel/T5Corrector-base-v2\"\ntokenizer = AutoTokenizer.from_pretrained(pretrained)\nmodel = T5ForConditionalGeneration.from_pretrained(pretrained)\n ```\n\n\u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u63a8\u7406\u65b9\u6cd5\uff1a\n```python\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef correct(text, max_length):\n    model_inputs = tokenizer(text, \n                           \t max_length=max_length, \n                           \t truncation=True, \n                           \t return_tensors=\"pt\").to(device)\n    output = model.generate(**model_inputs, \n                              num_beams=5,\n                              no_repeat_ngram_size=4,\n                              do_sample=True, \n                              early_stopping=True,\n                              max_length=max_length,\n                              return_dict_in_generate=True,\n                              output_scores=True)\n    pred_output = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)[0]\n    return pred_output\n\ntext = \"\u8d35\u5dde\u6bdb\u53f0\u73b0\u5728\u591a\u5c11\u94b1\u4e00\u74f6\u554a\uff0c\u60f3\u4e70\u4e24\u74f6\u5c1d\u5c1d\u5473\u9053\u3002\"\ncorrection = correct(text, max_length=32)\nprint(correction)\n```\n\n\n\n### \u6848\u4f8b\u5c55\u793a\n\n```\n\u793a\u4f8b1:\ninput: \u80fd\u4e0d\u80fd\u5e2e\u6211\u4e70\u70b9\u6dc7\u6dcb\uff0c\u597d\u4e45\u6ca1\u5403\u4e86\u3002\noutput: \u80fd\u4e0d\u80fd\u5e2e\u6211\u4e70\u70b9\u51b0\u6dc7\u6dcb\uff0c\u597d\u4e45\u6ca1\u5403\u4e86\u3002\n\n\u793a\u4f8b2:\ninput: \u8111\u5b50\u6709\u70b9\u80e1\u6d82\u4e86\uff0c\u8fd9\u9053\u9898\u51a5\u51a5\u5b66\u8fc7\u8fd8\u6ca1\u6709\u505a\u51fa\u6765\noutput: \u8111\u5b50\u6709\u70b9\u7cca\u6d82\u4e86,\u8fd9\u9053\u9898\u660e\u660e\u5b66\u8fc7\u8fd8\u6ca1\u6709\u505a\u51fa\u6765\n\n\u793a\u4f8b3:\ninput: \u4eca\u5929\u5929\u6c14\u4e0d\u592a\u597d\uff0c\u6211\u7684\u5fc3\u60c5\u4e5f\u4e0d\u662f\u5f88\u5077\u5feb\noutput: \u4eca\u5929\u5929\u6c14\u4e0d\u592a\u597d,\u6211\u7684\u5fc3\u60c5\u4e5f\u4e0d\u662f\u5f88\u6109\u5feb\n```", "size_bytes": "990406605", "downloads": 316}
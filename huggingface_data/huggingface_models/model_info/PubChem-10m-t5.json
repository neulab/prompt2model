{"pretrained_model_name": "sagawa/PubChem-10m-t5", "description": "---\nlicense: mit\ndatasets:\n- sagawa/pubchem-10m-canonicalized\nmetrics:\n- accuracy\nmodel-index:\n- name: PubChem-10m-t5\n  results:\n  - task:\n      name: Masked Language Modeling\n      type: fill-mask\n    dataset:\n      name: sagawa/pubchem-10m-canonicalized\n      type: sagawa/pubchem-10m-canonicalized\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9259435534477234\n---\n\n# PubChem-10m-t5\n\nThis model is a fine-tuned version of [google/t5-v1_1-base](https://huggingface.co/microsoft/deberta-base) on the sagawa/pubchem-10m-canonicalized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2121\n- Accuracy: 0.9259\n\n\n## Model description\n\nWe trained t5 on SMILES from PubChem using the task of masked-language modeling (MLM). Its tokenizer is also trained on PubChem.\n\n\n## Intended uses & limitations\n\nThis model can be used for the prediction of molecules' properties, reactions, or interactions with proteins by changing the way of finetuning.\n\n## Training and evaluation data\n\nWe downloaded [PubChem data](https://drive.google.com/file/d/1ygYs8dy1-vxD1Vx6Ux7ftrXwZctFjpV3/view) and canonicalized them using RDKit. Then, we dropped duplicates. The total number of data is 9999960, and they were randomly split into train:validation=10:1.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-03\n- train_batch_size: 30\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30.0\n\n### Training results\n\n| Training Loss | Step   | Accuracy | Validation Loss |\n|:-------------:|:------:|:--------:|:---------------:|\n| 0.3866        | 25000  | 0.8830   | 0.3631          |\n| 0.3352        | 50000  | 0.8996   | 0.3049          |\n| 0.2834        | 75000  | 0.9057   | 0.2825          |\n| 0.2685        | 100000 | 0.9099   | 0.2675          |\n| 0.2591        | 125000 | 0.9124   | 0.2587          |\n| 0.2620        | 150000 | 0.9144   | 0.2512          |\n| 0.2806        | 175000 | 0.9161   | 0.2454          |\n| 0.2468        | 200000 | 0.9179   | 0.2396          |\n| 0.2669        | 225000 | 0.9194   | 0.2343          |\n| 0.2611        | 250000 | 0.9210   | 0.2283          |\n| 0.2346        | 275000 | 0.9226   | 0.2230          |\n| 0.1972        | 300000 | 0.9238   | 0.2191          |\n| 0.2344        | 325000 | 0.9250   | 0.2152          |\n| 0.2164        | 350000 | 0.9259   | 0.2121          |", "size_bytes": "794556493", "downloads": 4}
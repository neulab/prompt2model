{"pretrained_model_name": "DeepNLP-22-23/MLQ-distilbart-bbc", "description": "---\nlicense: apache-2.0\ntags:\n- distilbart\n- summarization\nmodel-index:\n- name: MLQ-distilbart-bbc\n  results:\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: bbc\n      type: bbc\n      config: default\n      split: test\n    metrics:\n    - name: ROUGE-2\n      type: rouge\n      value: 61.43\n      verified: false\n      \n---\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# MLQ-distilbart-bbc\n\nThis model is a fine-tuned version of [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6) on the BBC News Summary dataset (https://www.kaggle.com/pariza/bbc-news-summary).\n\nThe model has been generated as part of the in-lab practice of **Deep NLP course** currently held at Politecnico di Torino.\n\nTraining parameters:\n- `num_train_epochs=2`\n- `fp16=True`\n- `per_device_train_batch_size=1`\n- `warmup_steps=10`\n- `weight_decay=0.01`\n- `max_seq_length=100`", "size_bytes": "1222356089", "downloads": 16}
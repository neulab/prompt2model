{"pretrained_model_name": "microsoft/GODEL-v1_1-base-seq2seq", "description": "---\nthumbnail: https://huggingface.co/front/thumbnails/dialogpt.png\ntags:\n- conversational\nlicense: mit\n---\n\n### Large-Scale Pre-Training for Goal-Directed Dialog (GODEL)\n\nGODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\n\n##### Multi-turn generation examples from an interactive environment:\nChitchat example:\n> Instruction: given a dialog context, you need to response empathically.  <br>\n> User: Does money buy happiness? <br>\n> Agent: It is a question. Money buys you a lot of things, but not enough to buy happiness. <br>\n> User: What is the best way to buy happiness ? <br>\n> Agent: Happiness is bought through your experience and not money. <br>\n\nGrounded response generation example:\n> Instruction: given a dialog context and related knowledge, you need to response safely based on the knowledge. <br>\n> Knowledge: The best Stardew Valley mods PCGamesN_0 / About SMAPI <br>\n> User: My favorite game is stardew valley. stardew valley is very fun. <br>\n> Agent: I love Stardew Valley mods, like PCGamesN_0 / About SMAPI. <br>\n\nPlease find the information about preprocessing, training and full details of the GODEL in the [project webpage](https://aka.ms/GODEL).\n\nArXiv paper: [https://arxiv.org/abs/2206.11309](https://arxiv.org/abs/2206.11309)\n\n### How to use\n\nNow we are ready to try out how the model works as a chatting partner!\n\n```python\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")\n\ndef generate(instruction, knowledge, dialog):\n    if knowledge != '':\n        knowledge = '[KNOWLEDGE] ' + knowledge\n    dialog = ' EOS '.join(dialog)\n    query = f\"{instruction} [CONTEXT] {dialog} {knowledge}\"\n    input_ids = tokenizer(f\"{query}\", return_tensors=\"pt\").input_ids\n    outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\n    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return output\n\n# Instruction for a chitchat task\ninstruction = f'Instruction: given a dialog context, you need to response empathically.'\n# Leave the knowldge empty\nknowledge = ''\ndialog = [\n    'Does money buy happiness?',\n    'It is a question. Money buys you a lot of things, but not enough to buy happiness.',\n    'What is the best way to buy happiness ?'\n]\nresponse = generate(instruction, knowledge, dialog)\nprint(response)\n```\n\n### Citation\nif you use this code and data in your research, please cite our arxiv paper:\n```\n@misc{peng2022godel,\nauthor = {Peng, Baolin and Galley, Michel and He, Pengcheng and Brockett, Chris and Liden, Lars and Nouri, Elnaz and Yu, Zhou and Dolan, Bill and Gao, Jianfeng},\ntitle = {GODEL: Large-Scale Pre-training for Goal-Directed Dialog},\nhowpublished = {arXiv},\nyear = {2022},\nmonth = {June},\nurl = {https://www.microsoft.com/en-us/research/publication/godel-large-scale-pre-training-for-goal-directed-dialog/},\n}\n```", "size_bytes": "891617279", "downloads": 14087}
{"pretrained_model_name": "guialfaro/korean-paraphrasing", "description": "---\nlicense: mit\n---\n\n[Korean BART](https://huggingface.co/hyunwoongko/kobart) model for paraphrasing. \nThe dataset utilized can be found on the *Files and versions* tab under the name dataset.csv.\n\n```python\nimport torch\nfrom transformers import BartForConditionalGeneration, AutoTokenizer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BartForConditionalGeneration.from_pretrained('guialfaro/korean-paraphrasing').to(device)\ntokenizer = AutoTokenizer.from_pretrained('guialfaro/korean-paraphrasing')\n\nsentence = \"7\uce35 \ubc29\ubb38\uc744 \uc704\ud574 \ubc29\ubb38\ub85d \uc791\uc131\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\"\ntext =  f\"paraphrase: {sentence} \"\n\nencoding = tokenizer.batch_encode_plus(\n            [text],\n            max_length=256,\n            pad_to_max_length=True,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",)\n\nsource_ids = encoding[\"input_ids\"].to(device, dtype=torch.long)\nsource_mask = encoding[\"attention_mask\"].to(device, dtype=torch.long)\n\ngenerated_ids = model.generate(\n                input_ids=source_ids,\n                attention_mask=source_mask,\n                max_length=150,\n                num_beams=2,\n                repetition_penalty=2.5,\n                length_penalty=1.0,\n                early_stopping=True)\n\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n\nprint(f\"Original Sentence :: {sentence}\")\nprint(f\"Paraphrased Sentence :: {preds[0]}\")\n\n```", "size_bytes": "495646265", "downloads": 64}
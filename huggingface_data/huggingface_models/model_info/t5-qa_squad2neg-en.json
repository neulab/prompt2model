{"pretrained_model_name": "ThomasNLG/t5-qa_squad2neg-en", "description": "---\nlanguage: en\ntags:\n- qa\n- question \n- answering\n- SQuAD\n- metric\n- nlg\n- t5-small\nlicense: mit\ndatasets:\n- squad_v2\nmodel-index:\n- name: t5-qa_squad2neg-en\n  results:\n  - task: \n      name: Question Answering\n      type: extractive-qa\nwidget:\n   - text: \"Who was Louis 14? </s> Louis 14 was a French King.\"\n---\n# t5-qa_squad2neg-en\n\n## Model description\nThis model is a *Question Answering* model based on T5-small. \nIt is actually a component of [QuestEval](https://github.com/ThomasScialom/QuestEval) metric but can be used independently as it is, for QA only.\n\n\n## How to use\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"ThomasNLG/t5-qa_squad2neg-en\")\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"ThomasNLG/t5-qa_squad2neg-en\")\n```\n\nYou can play with the model using the inference API, the text input format should follow this template (accordingly to the training stage of the model):\n\n`text_input = \"{QUESTION} </s> {CONTEXT}\"`\n\n## Training data\nThe model was trained on: \n- SQuAD-v2\n- SQuAD-v2 neg: in addition to the training data of SQuAD-v2, for each answerable example, a negative sampled example has been added with the label *unanswerable*  to help the model learning when the question is not answerable given the context. For more details, see the [paper](https://arxiv.org/abs/2103.12693).\n\n\n### Citation info\n\n```bibtex\n@article{scialom2020QuestEval,\n  title={QuestEval: Summarization Asks for Fact-based Evaluation},\n  author={Scialom, Thomas and Dray, Paul-Alexis and Gallinari, Patrick and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo and Wang, Alex},\n  journal={arXiv preprint arXiv:2103.12693},\n  year={2021}\n}\n```\n", "size_bytes": "891647800", "downloads": 3157}
{"pretrained_model_name": "voidful/mhubert-unit-tts", "description": "---\ndatasets:\n- librispeech_asr\nlanguage:\n- en\nmetrics:\n- wer\ntags:\n- hubert\n- tts\n---\n# voidful/mhubert-unit-tts\n\nvoidful/mhubert-unit-tts\n\nThis repository provides a text to unit model form mhubert and trained with bart model.\nThe model was trained on the LibriSpeech ASR dataset for the English language and \nTrain epoch 13: `WER:30.41` `CER: 20.22`           \n\n\nHubert Code TTS Example\n```python\nimport asrp\nimport nlp2\nimport IPython.display as ipd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nnlp2.download_file(\n    'https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj/g_00500000',\n    './')\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"voidful/mhubert-unit-tts\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"voidful/mhubert-unit-tts\")\nmodel.eval()\ncs = asrp.Code2Speech(tts_checkpoint='./g_00500000', vocoder='hifigan')\n\ninputs = tokenizer([\"The quick brown fox jumps over the lazy dog.\"], return_tensors=\"pt\")\ncode = tokenizer.batch_decode(model.generate(**inputs,max_length=1024))[0]\ncode = [int(i) for i in code.replace(\"</s>\",\"\").replace(\"<s>\",\"\").split(\"v_tok_\")[1:]]\nprint(code)\nipd.Audio(data=cs(code), autoplay=False, rate=cs.sample_rate)\n```\n\nDatasets\nThe model was trained on the LibriSpeech ASR dataset for the English language.\n\nLanguage\nThe model is trained for the English language.\n\nMetrics\nThe model's performance is evaluated using Word Error Rate (WER).\n\nTags\nThe model can be tagged with \"hubert\" and \"tts\".\n", "size_bytes": "769045473", "downloads": 12}
{"pretrained_model_name": "Arashasg/WikiBert2WikiBert", "description": "---\nlanguage: \n  - fa\ntags:\n  - Wikipedia\n  - Summarizer\n  - bert2bert\n  - Summarization\ntask_categories:\n  - Summarization\n  - text generation\ntask_ids:\n- news-articles-summarization\nlicense:\n- apache-2.0\nmultilinguality:\n- monolingual\ndatasets:\n- pn-summary\n- XL-Sum\nmetrics:\n- rouge-1\n- rouge-2\n- rouge-l\n---\n\n\n# WikiBert2WikiBert\nBert language models can be employed for Summarization tasks. WikiBert2WikiBert is an encoder-decoder transformer model that is initialized using the Persian WikiBert Model weights. The WikiBert Model is a Bert language model which is fine-tuned on Persian Wikipedia. After using the WikiBert weights for initialization, the model is trained for five epochs on PN-summary and Persian BBC datasets.\n\n## How to Use:\nYou can use the code below to get the model's outputs, or you can simply use the demo on the right.\n```\nfrom transformers import (\n    BertTokenizerFast,\n    EncoderDecoderConfig,\n    EncoderDecoderModel,\n    BertConfig\n)\n\nmodel_name = 'Arashasg/WikiBert2WikiBert'\ntokenizer = BertTokenizerFast.from_pretrained(model_name)\nconfig = EncoderDecoderConfig.from_pretrained(model_name)\nmodel = EncoderDecoderModel.from_pretrained(model_name, config=config)\n\n\ndef generate_summary(text):\n    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n    input_ids = inputs.input_ids.to(\"cuda\")\n    attention_mask = inputs.attention_mask.to(\"cuda\")\n\n    outputs = model.generate(input_ids, attention_mask=attention_mask)\n\n    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n\n    return output_str\n\ninput = 'your input comes here'\nsummary = generate_summary(input)\n```\n\n## Evaluation\nI separated 5 percent of the pn-summary for evaluation of the model. The rouge scores of the model are as follows:\n\n| Rouge-1  | Rouge-2  | Rouge-l |\n| ------------- | ------------- | ------------- |\n| 38.97%  | 18.42%  | 34.50%  |\n\n", "size_bytes": "1416791915", "downloads": 18}
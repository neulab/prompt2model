{"pretrained_model_name": "WelfCrozzo/T5-L128-belarusian", "description": "---\nlicense: openrail\ndatasets:\n- WelfCrozzo/kupalinka\nlanguage:\n- be\n- en\n- ru\nmetrics:\n- bleu\nlibrary_name: transformers\ntags:\n- translation\nwidget:\n- text: \"<extra_id_1>\u0434\u0430 \u0437\u043e\u0440\u0430\u043a \u043f\u0440\u0430\u0437 \u0446\u044f\u0436\u043a\u0430\u0441\u0446\u0456\"\n  example_title: \"be -> ru\"\n- text: \"<extra_id_2>\u0434\u0430 \u0437\u043e\u0440\u0430\u043a \u043f\u0440\u0430\u0437 \u0446\u044f\u0436\u043a\u0430\u0441\u0446\u0456\"\n  example_title: \"be -> en\"\n- text: \"<extra_id_3>\u043a \u0437\u0432\u0435\u0437\u0434\u0430\u043c \u0447\u0435\u0440\u0435\u0437 \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438\"\n  example_title: \"ru -> be\"\n- text: \"<extra_id_5>\u043a \u0437\u0432\u0435\u0437\u0434\u0430\u043c \u0447\u0435\u0440\u0435\u0437 \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438\"\n  example_title: \"ru -> en\"\n- text: \"<extra_id_6>to the stars through difficulties.\"\n  example_title: \"en -> be\"\n- text: \"<extra_id_7>to the stars through difficulties.\"\n  example_title: \"en -> ru\"\n---\n\n# T5 for belarusian language\n\n![model image](https://camo.githubusercontent.com/623b4dea0b653f2ad3f36c71ebfe749a677ac0a1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f343030362f312a44304a31674e51663876727255704b657944387750412e706e67)\n\nThis model is based on T5-small with sequence length equal 128 tokens. Model trained from scratch on RTX 3090  24GB. \n\n# Supported tasks:\n  - translation BE to RU: `<extra_id_1>`\n  - translation BE to EN: `<extra_id_2>`\n  - translation RU to BE: `<extra_id_3>`\n  - translation RU to EN: `<extra_id_5>`\n  - translation EN to BE: `<extra_id_6>`\n  - translation EN to RU: `<extra_id_7>`\n\n# Metrics:\n - [evel/BLEU](https://api.wandb.ai/links/miklgr500/31mq4s36)\n - [evel/loss](https://api.wandb.ai/links/miklgr500/rvi2p69n)\n - [train/loss](https://api.wandb.ai/links/miklgr500/z9alu3n5)\n\n# How to Get Started with the Model\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import T5TokenizerFast, T5ForConditionalGeneration\n\ntokenizer = T5TokenizerFast.from_pretrained(\"WelfCrozzo/T5-L128-belarusian\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"WelfCrozzo/T5-L128-belarusian\")\n\nx = tokenizer.encode('<extra_id_1>\u0434\u0430 \u0437\u043e\u0440\u0430\u043a \u043f\u0440\u0430\u0437 \u0446\u044f\u0436\u043a\u0430\u0441\u0446\u0456', return_tensors='pt')\n\nresult = model.generate(x, return_dict_in_generate=True, output_scores=True,max_length=128)\nprint(tokenizer.decode(result[\"sequences\"][0]))\n```\n</details>\n\n# References\n - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/volume21/20-074/20-074.pdf)", "size_bytes": "82689417", "downloads": 7}
{"pretrained_model_name": "0-hero/flan-alpaca-ul2", "description": "---\nlicense: apache-2.0\ndatasets:\n- tatsu-lab/alpaca\n---\n\n## \ud83c\udf6e \ud83e\udd99 Flan-Alpaca: Instruction Tuning from Humans and Machines\n\nThanks to [declare-lab](https://huggingface.co/declare-lab) for the training [repository](https://github.com/declare-lab/flan-alpaca), contains code for extending the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)\nsynthetic instruction tuning to existing instruction-tuned models such as [Flan-T5](https://arxiv.org/abs/2210.11416).\nThe pretrained models and demos are available on HuggingFace \ud83e\udd17 :\n\n| Model                                                                     | Parameters | Training GPUs   |\n|---------------------------------------------------------------------------|------------|-----------------|\n| [Flan-Alpaca-Base](https://huggingface.co/declare-lab/flan-alpaca-base)   | 220M       | 1x A6000        |\n| [Flan-Alpaca-Large](https://huggingface.co/declare-lab/flan-alpaca-large) | 770M       | 1x A6000        |\n| [Flan-Alpaca-XL](https://huggingface.co/declare-lab/flan-alpaca-xl)       | 3B         | 1x A6000        |\n| [Flan-Alpaca-XXL](https://huggingface.co/declare-lab/flan-alpaca-xxl)     | 11B        | 4x A6000 (FSDP) |\n| [Flan-Alpaca-UL2](https://huggingface.co/0-hero/flan-alpaca-ul2)          | 20B        | 4x A100 (80G) (FSDP)  |\n\n### Why?\n\n[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) represents an exciting new direction\nto approximate the performance of large language models (LLMs) like ChatGPT cheaply and easily.\nConcretely, they leverage an LLM such as GPT-3 to generate instructions as synthetic training data.\nThe synthetic data which covers more than 50k tasks can then be used to finetune a smaller model.\nHowever, the original implementation is less accessible due to licensing constraints of the\nunderlying [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) model.\nFurthermore, users have noted [potential noise](https://github.com/tloen/alpaca-lora/issues/65) in the synthetic\ndataset. Hence, it may be better to explore a fully accessible model that is already trained on high-quality (but\nless diverse) instructions such as [Flan-T5](https://arxiv.org/abs/2210.11416).\n\n### Usage\n\n```\nfrom transformers import pipeline\n\nprompt = \"Write an email about an alpaca that likes flan\"\nmodel = pipeline(model=\"0-hero/flan-alpaca-ul2\")\nmodel(prompt, max_length=128, do_sample=True)\n\n```\n\nReadme forked from declare-lab/flan-alpaca-xxl", "size_bytes": 79417610240, "downloads": 8}
{"pretrained_model_name": "mrm8488/flan-t5-xl-finetuned-unnatural-instructions", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: flan-t5-xl-finetuned-unnatural-instructions\n  results: []\n\nwidget:\n- text: \"You will be provided with a short text that you should read. After reading the text, answer the question 'Who is telling the story in first person?'\\nText: I went to the store and bought some milk.\\nThe output should be one of the two options: 'I' or 'Not I'.\\nOutput:\"\n- text: \"You are given the title and opening paragraph of an article. Your task is to find the main idea of the article using context clues from the text.\\nText: The first confirmed case of Zika in Africa has been found in Uganda, health officials said on Tuesday, more than 4,000 miles away from Brazil where a major outbreak began last year.\\nAblood test carried out at a private hospital showed that a five-year-old girl who had returned from Mozambique three weeks ago had contracted Zika, which can cause birth defects in babies born to infected mothers.\\nOutput:\"\n- text: \"We have given you a list of sentences. Your task is to go through this list and find all the unique words used in these sentences and store them in a dictionary.\\nThis is an example sentence., That is another example sentence., Here is an interesting sentence!\\nThe output should be a Python dictionary with keys as the unique words and values as the number of times that word occurs in the text.\\nOutput:\\n\"\n- text: \"You will given a set of two or more words. Output the shortest word in the set. If there is a tie for the shortest word, output all tied words in alphabetical order, separated by space.\\nWords: defenestrate, circumambulate, excommunication.\\nThe input will be lowercase and only contain alphanumeric characters and spaces.\\nOutput:\\n\"\n\ninference:\n  parameters:\n    max_length: 150\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-xl-finetuned-unnatural-instructions\n\nThis model is a fine-tuned version of [google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1224\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- training_steps: 5000\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 0.1345        | 0.02  | 1000 | 0.1386          |\n| 0.14          | 0.03  | 2000 | 0.1336          |\n| 0.1402        | 0.05  | 3000 | 0.1282          |\n| 0.129         | 0.07  | 4000 | 0.1235          |\n| 0.1356        | 0.08  | 5000 | 0.1224          |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu116\n- Datasets 2.8.0\n- Tokenizers 0.13.2\n", "size_bytes": 11925413888, "downloads": 10}
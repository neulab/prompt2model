{"pretrained_model_name": "humarin/chatgpt_paraphraser_on_T5_base", "description": "---\nlicense: openrail\ndatasets:\n- humarin/chatgpt-paraphrases\nlanguage:\n- en\nlibrary_name: transformers\ninference:\n  parameters:\n    num_beams: 5\n    num_beam_groups: 5\n    num_return_sequences: 5\n    repetition_penalty: 10.01\n    diversity_penalty: 3.01\n    no_repeat_ngram_size: 2\n    temperature: 0.7\n    max_length: 128\nwidget:\n- text: What are the best places to see in New York?\n  example_title: New York tourist attractions\n- text: When should I go to the doctor?\n  example_title: Doctor's time\n- text: >-\n    Rammstein's album Mutter was recorded in the south of France in May and June\n    2000, and mixed in Stockholm in October of that year.\n  example_title: Rammstein's album Mutter\npipeline_tag: text2text-generation\n---\nThis model was trained on our [ChatGPT paraphrase dataset](https://huggingface.co/datasets/humarin/chatgpt-paraphrases).\n\n\n\nThis dataset is based on the [Quora paraphrase question](https://www.kaggle.com/competitions/quora-question-pairs), texts from the [SQUAD 2.0](https://huggingface.co/datasets/squad_v2) and the [CNN news dataset](https://huggingface.co/datasets/cnn_dailymail).\n\nThis model is based on the T5-base model. We used \"transfer learning\" to get our model to generate paraphrases as well as ChatGPT. Now we can say that this is one of the best paraphrases of the Hugging Face.\n\n[Kaggle](https://www.kaggle.com/datasets/vladimirvorobevv/chatgpt-paraphrases) link\n\n## Deploying example\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ndevice = \"cuda\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n\ndef paraphrase(\n    question,\n    num_beams=5,\n    num_beam_groups=5,\n    num_return_sequences=5,\n    repetition_penalty=10.0,\n    diversity_penalty=3.0,\n    no_repeat_ngram_size=2,\n    temperature=0.7,\n    max_length=128\n):\n    input_ids = tokenizer(\n        f'paraphrase: {question}',\n        return_tensors=\"pt\", padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    ).input_ids\n    \n    outputs = model.generate(\n        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n        num_beams=num_beams, num_beam_groups=num_beam_groups,\n        max_length=max_length, diversity_penalty=diversity_penalty\n    )\n\n    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return res\n```\n\n## Usage examples\n\n**Input:**\n```python\ntext = 'What are the best places to see in New York?'\nparaphrase(text)\n```\n**Output:**\n```python\n['What are some must-see places in New York?',\n 'Can you suggest some must-see spots in New York?',\n 'Where should one go to experience the best NYC has to offer?',\n 'Which places should I visit in New York?',\n 'What are the top destinations to explore in New York?']\n```\n\n**Input:**\n```python\ntext = \"Rammstein's album Mutter was recorded in the south of France in May and June 2000, and mixed in Stockholm in October of that year.\"\nparaphrase(text)\n```\n**Output:**\n```python\n['In May and June 2000, Rammstein travelled to the south of France to record his album Mutter, which was mixed in Stockholm in October of that year.',\n 'The album Mutter by Rammstein was recorded in the south of France during May and June 2000, with mixing taking place in Stockholm in October of that year.',\n 'The album Mutter by Rammstein was recorded in the south of France during May and June 2000, with mixing taking place in Stockholm in October of that year. It',\n 'Mutter, the album released by Rammstein, was recorded in southern France during May and June 2000, with mixing taking place between October and September.',\n 'In May and June 2000, Rammstein recorded his album Mutter in the south of France, with the mix being made at Stockholm during October.']\n```\n\n\n## Train parameters\n```python\nepochs = 5\nbatch_size = 64\nmax_length = 128\nlr = 5e-5\nbatches_qty = 196465\nbetas = (0.9, 0.999)\neps = 1e-08\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@inproceedings{chatgpt_paraphraser,\n  author={Vladimir Vorobev, Maxim Kuznetsov},\n  title={A paraphrasing model based on ChatGPT paraphrases},\n  year={2023}\n}\n```", "size_bytes": "891702929", "downloads": 13260}
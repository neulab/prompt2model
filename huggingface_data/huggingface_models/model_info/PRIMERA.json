{"pretrained_model_name": "allenai/PRIMERA", "description": "---\n\nlanguage: en\n\nlicense: apache-2.0\n\n---\n\n\n\n\nHF-version model for PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization (ACL 2022). \n\n\n\nThe original code can be found [here](https://github.com/allenai/PRIMER). You can find the script and notebook to train/evaluate the model in the original github repo.\n\n\n\n* Note: due to the difference between the implementations of the original Longformer and the Huggingface LED model, the results of converted models are slightly different. We run a sanity check on both fine-tuned and non fine-tuned models on the **MultiNews dataset**, and show the results below:\n\n\n\n| Model | Rouge-1 | Rouge-2 | Rouge-L |\n\n| --- | ----------- |----------- |----------- |\n\n| PRIMERA | 42.0 | 13.6 | 20.8| \n\n| PRIMERA-hf | 41.7 |13.6 | 20.5|\n\n| PRIMERA(finetuned) | 49.9 | 21.1 | 25.9|\n\n| PRIMERA-hf(finetuned) | 49.9 | 20.9 | 25.8|\n\n\n\nYou can use it by \n\n```\n\nfrom transformers import (\n\n    AutoTokenizer,\n\n    LEDConfig,\n\n    LEDForConditionalGeneration,\n\n)\n\ntokenizer = AutoTokenizer.from_pretrained('allenai/PRIMERA')\n\nconfig=LEDConfig.from_pretrained('allenai/PRIMERA')\n\nmodel = LEDForConditionalGeneration.from_pretrained('allenai/PRIMERA')\n\n```\n\n", "size_bytes": "1789292593", "downloads": 2924}
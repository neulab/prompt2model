{"pretrained_model_name": "slone/mbart-large-51-myv-mul-v1", "description": "---\nlanguage:\n- myv\n- ru\n- fi\n- de\n- es\n- en\n- hi\n- zh\n- tr\n- uk\n- fr\n- ar\ntags:\n- erzya\n- mordovian\n- translation\nlicense: cc-by-sa-4.0\ndatasets:\n  - slone/myv_ru_2022\n  - yhavinga/ccmatrix\n---\n\nThis a model to translate texts to the Erzya language (`myv`, cyrillic script) from 11 other languages: `ru,fi,de,es,en,hi,zh,tr,uk,fr,ar`. See its [demo](https://huggingface.co/spaces/slone/myv-translation-2022-demo)!\n\nIt is described in the paper [The first neural machine translation system for the Erzya language](https://arxiv.org/abs/2209.09368).\n\nThis model is based on [facebook/mbart-large-50](https://huggingface.co/facebook/mbart-large-50), but with updated vocabulary and checkpoint:\n- Added an extra language token `myv_XX` and 19K new BPE tokens for the Erzya language;\n- Fine-tuned to translate from Erzya: first to Russian, then to all 11 languages. \n\nThe following code can be used to run translation using the model \n\n```Python\nfrom transformers import MBartForConditionalGeneration, MBart50Tokenizer\n\n\ndef fix_tokenizer(tokenizer):\n    \"\"\" Add a new language token to the tokenizer vocabulary (this should be done each time after its initialization) \"\"\"\n    old_len = len(tokenizer) - int('myv_XX' in tokenizer.added_tokens_encoder)\n    tokenizer.lang_code_to_id['myv_XX'] = old_len-1\n    tokenizer.id_to_lang_code[old_len-1] = 'myv_XX'\n    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n\n    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n    if 'myv_XX' not in tokenizer._additional_special_tokens:\n        tokenizer._additional_special_tokens.append('myv_XX')\n    tokenizer.added_tokens_encoder = {}\n\n\ndef translate(text, model, tokenizer, src='ru_RU', trg='myv_XX', max_length='auto', num_beams=3, repetition_penalty=5.0, train_mode=False, n_out=None, **kwargs):\n    tokenizer.src_lang = src\n    encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n    if max_length == 'auto':\n        max_length = int(32 + 1.5 * encoded.input_ids.shape[1])\n    if train_mode:\n        model.train()\n    else:\n        model.eval()\n    generated_tokens = model.generate(\n        **encoded.to(model.device),\n        forced_bos_token_id=tokenizer.lang_code_to_id[trg], \n        max_length=max_length, \n        num_beams=num_beams,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=n_out or 1,\n        **kwargs\n    )\n    out = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n    if isinstance(text, str) and n_out is None:\n        return out[0]\n    return out\n    \n\nmname = 'slone/mbart-large-51-myv-mul-v1'\nmodel = MBartForConditionalGeneration.from_pretrained(mname)\ntokenizer = MBart50Tokenizer.from_pretrained(mname)\nfix_tokenizer(tokenizer)\n\n\nprint(translate('\u0428\u0443\u043c\u0431\u0440\u0430\u0442, \u043a\u0438\u0441\u043a\u0430!', model, tokenizer, src='myv_XX', trg='ru_RU'))\n# \u041f\u0440\u0438\u0432\u0435\u0442, \u0441\u043e\u0431\u0430\u043a\u0430!   # \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e, \"\u043a\u0438\u0441\u043a\u0430\" \u0441 \u044d\u0440\u0437\u044f\u043d\u0441\u043a\u043e\u0433\u043e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u0438\u043c\u0435\u043d\u043d\u043e \u0442\u0430\u043a\nprint(translate('\u0428\u0443\u043c\u0431\u0440\u0430\u0442, \u043a\u0438\u0441\u043a\u0430!', model, tokenizer, src='myv_XX', trg='en_XX'))\n# Hi, dog!\n```", "size_bytes": "2524600313", "downloads": 22}
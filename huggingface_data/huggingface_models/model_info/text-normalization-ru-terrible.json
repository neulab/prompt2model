{"pretrained_model_name": "maximxls/text-normalization-ru-terrible", "description": "---\nlicense: mit\nlanguage:\n- ru\nlibrary_name: transformers\ntags:\n- text-generation-inference\n---\n# Model Card for maximxls/text-normalization-ru-terrible\n\nNormalization for Russian text. Couldn't find any existing solutions (besides algorithms, don't like those) so made this.\n\n## Model Details\n\n### Model Description\n\nTiny T5 trained from scratch for normalizing Russian texts:\n- translating numbers into words\n- expanding abbreviations into phonetic letter combinations\n- transliterating english into russian letters\n- whatever else was in the dataset (see below)\n\n### Model Sources\n\n- **Training code repository:** https://github.com/maximxlss/text_normalization\n- **Main dataset:** https://www.kaggle.com/c/text-normalization-challenge-russian-language\n\n## Uses\n\nUseful in TTS, for example with Silero to make it read numbers and English words (even if not perfectly, it's at least not ignoring)\n\n### Quick Start\n\n```Python\nfrom transformers import (\n    T5ForConditionalGeneration,\n    PreTrainedTokenizerFast,\n)\n\n\nmodel_path = \"maximxls/text-normalization-ru-terrible\"\n\ntokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\n\n\nexample_text = \"\u042f \u0445\u043e\u0434\u0438\u043b \u0432 McDonald's 10 \u0438\u044e\u043b\u044f 2022 \u0433\u043e\u0434\u0430.\"\n\ninp_ids = tokenizer(\n    example_text,\n    return_tensors=\"pt\",\n).input_ids\nout_ids = model.generate(inp_ids, max_new_tokens=128)[0]\nout = tokenizer.decode(out_ids, skip_special_tokens=True)\n\nprint(out)\n```\n\n`\u044f \u0445\u043e\u0434\u0438\u043b \u0432 \u043c\u0430\u043a\u0434\u043e\u043d\u0430\u043b\u0434'\u044d\u0441 \u0434\u0435\u0441\u044f\u0442\u043e\u0433\u043e \u0438\u044e\u043b\u044f \u0434\u0432\u0435 \u0442\u044b\u0441\u044f\u0447\u0438 \u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0433\u043e\u0434\u0430.`\n\n## Bias, Risks, and Limitations\n\n**Very much unreliable:**\n- For some reason, sometimes skips over first couple of tokens. Might be benificial to add some extra padding or whatever so it would be more stable. Wasn't able to solve it in training.\n- Sometimes is pretty unstable with repeating or missing words (especially with transliteration)\n\n## Training Details\n\n### Training Data\n\nData from [this Kaggle challenge](https://www.kaggle.com/c/text-normalization-challenge-russian-language) (761435 sentences) aswell as a bit of extra data written by me.\n\n### Training Procedure \n\n#### Preprocessing\n\nSee [`preprocessing.py`](https://github.com/maximxlss/text_normalization/blob/master/preprocess.py)\n\n#### Training Hyperparameters\n\nSee [`train.py`](https://github.com/maximxlss/text_normalization/blob/master/train.py)\n\nI have reset lr manually several times during training, see metrics.\n\n#### Details\n\nSee [`README` on github](https://github.com/maximxlss/text_normalization) for a step-by-step overview of the training procedure.\n\n## Technical Specifications\n\n#### Hardware\n\nCouple tens of hours of RTX 3090 Ti compute on my personal PC (21.65 epochs)\n\n", "size_bytes": "33598415", "downloads": 11}
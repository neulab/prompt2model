{"pretrained_model_name": "monsoon-nlp/byt5-basque", "description": "---\nlanguage: eu\n---\n\n# byt5-basque\n\nPretrained from scratch on Euskara (Basque language) \nwith ByT5, Google's new byte-level tokenizer strategy.\n\nCorpus: eu.wikipedia.org as of March 2020 (TFDS)\n\nPretraining Notebook: https://colab.research.google.com/drive/19Afq7CI6cOi1DaTpnQhBbEbnBzLSFHbH\n\n## Todos\n\nFine-tuning\n\nThe Wikipedia corpus is small for this language compared to web crawls. In the future I would add\nOSCAR, if I can rewrite the script to accept those\nas one TFDS dataset.\n\n", "size_bytes": "1198625069", "downloads": 2}
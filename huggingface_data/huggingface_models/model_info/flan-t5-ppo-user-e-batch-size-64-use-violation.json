{"pretrained_model_name": "dshin/flan-t5-ppo-user-e-batch-size-64-use-violation", "description": "---\nlicense: apache-2.0\ntags:\n- trl\n- transformers\n- reinforcement-learning\n---\n\n# TRL Model\n\nThis is a [TRL language model](https://github.com/lvwerra/trl) that has been fine-tuned with reinforcement learning to\n guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.\n\n## Usage\n\nTo use this model for inference, first install the TRL library:\n\n```bash\npython -m pip install trl\n```\n\nYou can then generate text as follows:\n\n```python\nfrom transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=\"dshin//tmp/tmpksdzj7bs/dshin/flan-t5-ppo-user-e-batch-size-64-use-violation\")\noutputs = generator(\"Hello, my llama is cute\")\n```\n\nIf you want to use the model for training or to obtain the outputs from the value head, load the model as follows:\n\n```python\nfrom transformers import AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"dshin//tmp/tmpksdzj7bs/dshin/flan-t5-ppo-user-e-batch-size-64-use-violation\")\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"dshin//tmp/tmpksdzj7bs/dshin/flan-t5-ppo-user-e-batch-size-64-use-violation\")\n\ninputs = tokenizer(\"Hello, my llama is cute\", return_tensors=\"pt\")\noutputs = model(**inputs, labels=inputs[\"input_ids\"])\n```\n", "size_bytes": "990412605", "downloads": 4}
{"pretrained_model_name": "palat/bort", "description": "---\nlanguage: en\nthumbnail: https://huggingface.co/palat/bort/resolve/main/assets/bort.png\n---\n\n# BORT\n\nBORT is a pretrained LLM that is designed to accept a mixture of English phonemes (in IPA) and orthography, made with clinical language evaluation tasks in mind. From the paper:\n\nRobert Gale, Alexandra C. Salem, Gerasimos Fergadiotis, and Steven Bedrick. 2023. [**Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted Transformers (BORT).**](https://robertcgale.com/pub/2023-acl-bort-paper.pdf) In Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP-2023), pages TBD, Online. Association for Computational Linguistics. [[paper]](https://robertcgale.com/pub/2023-acl-bort-paper.pdf) [[poster]](https://robertcgale.com/pub/2023-acl-bort-poster.pdf)\n\n## Acknowledgements\n\nThis work was supported by the National Institute on Deafness and Other Communication Disorders of the National Institutes of Health under award 5R01DC015999 (Principal Investigators: Bedrick \\& Fergadiotis). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. \n\n## Limitations\n\nThe models presented here were trained with the basic inventory of English phonemes found in CMUDict. However, a more fine-grained phonetic analysis would require a pronunciation dictionary with more narrowly defined entries. Additionally, while this paper focused on models trained with English-only resources (pre-trained BART-BASE, English Wikipedia text, CMUDict, and the English AphasiaBank), the techniques should be applicable to non-English language models as well. Finally, from a clinical standpoint, the model we describe in this paper assumes the existence of transcribed input (from either a manual or automated source, discussed in detail in \u00a72.1 of the paper; in its current form, this represents a limitation to its clinical implementation, though not to its use in research settings with archival or newly-transcribed datasets.\n\n## Ethics Statement\n\nOur use of the AphasiaBank data was governed by the TalkBank consortium's data use agreement, and the underlying recordings were collected and shared with approval of the contributing sites' institutional review boards.\nLimitations exist regarding accents and dialect, which in turn would affect the scenarios in which a system based on our model could (and should) be used.\nIt should also be noted that these models and any derived technology are not meant to be tools to diagnose medical conditions, a task best left to qualified clinicians.\n\n## Wikipedia Dataset Used in Pre-Training\n\nThe BPE-tokenized version of the dataset, including metadata used in word transforms.\n\n- **Dataset** (upload ETA \u2264 ACL 2023)\n\n## Usage\n\n### Downloading BORT\n```python\nfrom transformers import AutoTokenizer, BartForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"palat/bort\")\nmodel = BartForConditionalGeneration.from_pretrained(\"palat/bort\")\n```\n\nThe above uses the default variant, `bort-pr-sp-noisy`. Each variant from the paper can be retrieved by specifiying \nlike so:\n\n```python\nBartForConditionalGeneration.from_pretrained(\"palat/bort\", variant=\"bort-sp\")\n```\n\nThe following variants are available, pre-trained on the specified proportion of each task:\n\n| Variant            | Pronunciation | Spelling | Noise |\n|--------------------|---------------|----------|-------|\n| `bort-pr`          | 10%           | \u2014        | \u2014     |\n| `bort-sp`          | \u2014             | 10%      | \u2014     |\n| `bort-pr-sp`       | 10%           | 10%      | \u2014     |\n| `bort-pr-noisy`    | 10%           | \u2014        | 5%    |\n| `bort-sp-noisy`    | \u2014             | 10%      | 5%    |\n| `bort-pr-sp-noisy` | 10%           | 10%      | 5%    |\n\n\n## Basic usage\n\nBORT was intended to be fine-tuned to a specific task, but for a basic demonstration of what distinguishes it from \nother LLMs, please consider the following example. The pre-trained model has no issue translating \"Long /a\u026al\u0259nd/\" to\n\"Long Island\", or \"Long /bi\u02a7/\" to \"Long Beach\". The next two texts demonstrate the effect of context. While \n\"l\u0254\u014b \u00b7a\u026al\u0259n\u00b7d\" still translates to \"Long Island\", \"l\u0254\u014b \u00b7b\u00b7i\u02a7\" bumps up against a homonym, and the model produces\n\"long beech\". (Note: the bullet character `\u00b7` is used to prevent the BPE tokenizer \nfrom combining phonemes.)\n\n```python\nfrom transformers import AutoTokenizer, BartForConditionalGeneration\n\n# Examples of mixed orthography and IPA phonemes:\nin_texts = [\n    \"Due to its coastal location, Long \u00b7a\u026al\u0259n\u00b7d winter temperatures are milder than most of the state.\",\n    \"Due to its coastal location, Long \u00b7b\u00b7i\u02a7 winter temperatures are milder than most of the state.\",\n    \"Due to its coastal location, l\u0254\u014b \u00b7a\u026al\u0259n\u00b7d winter temperatures are milder than most of the state.\",\n    \"Due to its coastal location, l\u0254\u014b \u00b7b\u00b7i\u02a7 winter temperatures are milder than most of the state.\",\n]\n\n# Set up model and tokenizer:\ntokenizer = AutoTokenizer.from_pretrained(\"palat/bort\")\nmodel = BartForConditionalGeneration.from_pretrained(\"palat/bort\")\n\n# Run generative inference for the batch of examples:\ninputs = tokenizer(in_texts, return_tensors=\"pt\", padding=True)\nsummary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=2048)\ndecoded = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n\n# Print the translated text:\nfor in_text, out_text in zip(in_texts, decoded):\n    print(f\"In:   \\t{in_text}\")\n    print(f\"Out:  \\t{out_text}\")\n    print()\n```\n\nFull output for the above example:\n```\nIn:   \tDue to its coastal location, l\u0254\u014b \u00b7a\u026al\u0259n\u00b7d winter temperatures are milder than most of the state.\nOut:  \tDue to its coastal location, Long Island winter temperatures are milder than most of the state.\n\nIn:   \tDue to its coastal location, l\u0254\u014b \u00b7b\u00b7i\u02a7 winter temperatures are milder than most of the state.\nOut:  \tDue to its coastal location, long beech winter temperatures are milder than most of the state.\n\nIn:   \tDue to its coastal location, Long \u00b7b\u00b7i\u02a7 winter temperatures are milder than most of the state.\nOut:  \tDue to its coastal location, Long Beach winter temperatures are milder than most of the state.\n\nIn:   \tDue to its coastal location, l\u0254\u014bf\u025dd winter temperatures are milder than most of the state.\nOut:  \tDue to its coastal location, Longford winter temperatures are milder than most of the state.\n```\n", "size_bytes": "560892541", "downloads": 252}
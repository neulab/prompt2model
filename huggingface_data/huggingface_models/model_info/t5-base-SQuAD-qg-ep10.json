{"pretrained_model_name": "Zekunli/t5-base-SQuAD-qg-ep10", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: t5-base-SQuAD-qg-ep10\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-base-SQuAD-qg-ep10\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.4837\n- Rouge1: 39.225\n- Rouge2: 17.5912\n- Rougel: 35.5634\n- Rougelsum: 35.5514\n- Gen Len: 13.5941\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 72\n- eval_batch_size: 144\n- seed: 1799\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 2.0366        | 0.76  | 200  | 1.6135          | 38.2514 | 16.449  | 34.7626 | 34.7415   | 13.4074 |\n| 1.7544        | 1.52  | 400  | 1.5574          | 38.4555 | 16.7217 | 34.8476 | 34.799    | 13.6033 |\n| 1.6827        | 2.28  | 600  | 1.5303          | 38.6393 | 16.8394 | 35.0986 | 35.0749   | 13.5573 |\n| 1.6544        | 3.04  | 800  | 1.5159          | 38.0175 | 16.4916 | 34.5157 | 34.492    | 13.6831 |\n| 1.6202        | 3.8   | 1000 | 1.5046          | 39.029  | 17.2625 | 35.4493 | 35.439    | 13.5815 |\n| 1.5817        | 4.56  | 1200 | 1.4998          | 38.5684 | 16.8763 | 34.9933 | 34.9728   | 13.6425 |\n| 1.583         | 5.32  | 1400 | 1.4951          | 38.8284 | 17.2627 | 35.2844 | 35.261    | 13.5651 |\n| 1.5553        | 6.08  | 1600 | 1.4922          | 38.7763 | 17.1948 | 35.1699 | 35.1494   | 13.6507 |\n| 1.5405        | 6.84  | 1800 | 1.4877          | 39.1529 | 17.5602 | 35.5378 | 35.5285   | 13.5893 |\n| 1.5391        | 7.6   | 2000 | 1.4854          | 39.2661 | 17.5374 | 35.6662 | 35.649    | 13.6067 |\n| 1.5298        | 8.37  | 2200 | 1.4843          | 39.281  | 17.611  | 35.6203 | 35.6159   | 13.6222 |\n| 1.5224        | 9.13  | 2400 | 1.4837          | 39.225  | 17.5912 | 35.5634 | 35.5514   | 13.5941 |\n| 1.5143        | 9.89  | 2600 | 1.4837          | 39.2284 | 17.5523 | 35.5238 | 35.5112   | 13.5583 |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.11.0+cu113\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "891700799", "downloads": 13}
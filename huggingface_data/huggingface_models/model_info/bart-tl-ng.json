{"pretrained_model_name": "cristian-popa/bart-tl-ng", "description": "---\nlanguage:\n- en\n<!-- thumbnail: https://raw.githubusercontent.com/JetRunner/BERT-of-Theseus/master/bert-of-theseus.png\n  -->\ntags:\n- topic labeling\nlicense: apache-2.0\nmetrics:\n- ndcg\n---\n\n# MyModel\n\n## Model description\n\nThis is the `BART-TL-ng` model from the paper [BART-TL: Weakly-Supervised Topic Label Generation](https://www.aclweb.org/anthology/2021.eacl-main.121.pdf). We aim to solve the topic labeling task using generative methods, rather than selection from a pool of labels as was done in previous State of the Art works.\n\nFor more details not covered here, you can read the paper or look at the open-source implementation: https://github.com/CristianViorelPopa/BART-TL-topic-label-generation.\n\nThere are two models made available from the paper:\n\n* [BART-TL-all](https://huggingface.co/cristian-popa/bart-tl-all)\n* [BART-TL-ng](https://huggingface.co/cristian-popa/bart-tl-ng)\n\n## Intended uses & limitations\n\n#### How to use\n\nThe model takes in a topic, represented as a space-separated series of words. Such topics can be generated using LDA, as was done for gathering the fine-tuning dataset for the model.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmname = \"cristian-popa/bart-tl-ng\"\ntokenizer = AutoTokenizer.from_pretrained(mname)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(mname)\n\ninput = \"site web google search website online internet social content user\"\nenc = tokenizer(input, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\noutputs = model.generate(\n\tinput_ids=enc.input_ids,\n\tattention_mask=enc.attention_mask,\n    max_length=15,\n    min_length=1,\n    do_sample=False,\n    num_beams=25,\n    length_penalty=1.0,\n    repetition_penalty=1.5\n)\n\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(decoded) # windows live messenger\n```\n\n#### Limitations and bias\n\nThe model may not generate accurate labels for topics from domains unrelated to the ones it was fine-tuned on, such as gastronomy.\n\n\n## Training data\n\nThe model was fine-tuned on 5 different StackExchange corpora (see https://archive.org/download/stackexchange for a full list of existing such corpora): English, biology, economics, law, and photography. 100 topics are extracted using LDA for each of these corpora, filtered for coherence and then used for obtaining the final model here.\n\n## Training procedure\n\nThe large Facebook BART model is fine-tuned in a weakly-supervised manner, making use of the unsupervised candidate selection of the [NETL](https://www.aclweb.org/anthology/C16-1091.pdf) method, along with n-grams from the topics. The dataset is a one-to-many mapping from topics to labels. More details on training and parameters can be found in the [paper](https://www.aclweb.org/anthology/2021.eacl-main.121.pdf) or by following [this notebook](https://github.com/CristianViorelPopa/BART-TL-topic-label-generation/blob/main/notebooks/end_to_end_workflow.ipynb).\n\n## Eval results\n\nmodel       | Top-1 Avg. | Top-3 Avg. | Top-5 Avg. | nDCG-1 | nDCG-3 | nDCG-5\n------------|------------|------------|------------|--------|--------|-------\nNETL (U)\t| 2.66\t\t | 2.59\t\t  | 2.50\t   | 0.83\t| 0.85\t | 0.87\nNETL (S)\t| 2.74\t\t | 2.57\t\t  | 2.49\t   | 0.88\t| 0.85\t | 0.88\nBART-TL-all | 2.64\t\t | 2.52\t\t  | 2.43\t   | 0.83\t| 0.84\t | 0.87\nBART-TL-ng \t| 2.62\t\t | 2.50\t\t  | 2.33\t   | 0.82\t| 0.84\t | 0.85\n\n### BibTeX entry and citation info\n\n```bibtex\n@inproceedings{popa-rebedea-2021-bart,\n    title = \"{BART}-{TL}: Weakly-Supervised Topic Label Generation\",\n    author = \"Popa, Cristian  and\n      Rebedea, Traian\",\n    booktitle = \"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume\",\n    month = apr,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2021.eacl-main.121\",\n    pages = \"1418--1425\",\n    abstract = \"We propose a novel solution for assigning labels to topic models by using multiple weak labelers. The method leverages generative transformers to learn accurate representations of the most important topic terms and candidate labels. This is achieved by fine-tuning pre-trained BART models on a large number of potential labels generated by state of the art non-neural models for topic labeling, enriched with different techniques. The proposed BART-TL model is able to generate valuable and novel labels in a weakly-supervised manner and can be improved by adding other weak labelers or distant supervision on similar tasks.\",\n}\n```", "size_bytes": "1625569391", "downloads": 295}
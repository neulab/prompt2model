{"pretrained_model_name": "farleyknight/patent-summarization-t5-base-2022-09-20", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- farleyknight/big_patent_5_percent\nmetrics:\n- rouge\nmodel-index:\n- name: patent-summarization-t5-base-2022-09-20\n  results:\n  - task:\n      name: Summarization\n      type: summarization\n    dataset:\n      name: farleyknight/big_patent_5_percent\n      type: farleyknight/big_patent_5_percent\n      config: all\n      split: train\n      args: all\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 36.0843\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# patent-summarization-t5-base-2022-09-20\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on the farleyknight/big_patent_5_percent dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.9975\n- Rouge1: 36.0843\n- Rouge2: 12.1856\n- Rougel: 25.8099\n- Rougelsum: 30.1664\n- Gen Len: 118.3137\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1.0\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2 | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:------:|:-------:|:---------:|:-------:|\n| 2.2811        | 0.08  | 5000  | 2.1767          | 18.5624 | 6.8795 | 15.5361 | 16.6836   | 19.0    |\n| 2.2551        | 0.17  | 10000 | 2.1327          | 19.077  | 6.8512 | 15.79   | 17.086    | 19.0    |\n| 2.2818        | 0.25  | 15000 | 2.1029          | 18.8637 | 6.9233 | 15.7341 | 16.9717   | 19.0    |\n| 2.1952        | 0.33  | 20000 | 2.0805          | 18.962  | 7.1157 | 15.8297 | 17.0333   | 19.0    |\n| 2.157         | 0.41  | 25000 | 2.0641          | 19.1418 | 7.315  | 16.05   | 17.2551   | 19.0    |\n| 2.1775        | 0.5   | 30000 | 2.0452          | 19.2387 | 7.3193 | 16.0852 | 17.3563   | 19.0    |\n| 2.1376        | 0.58  | 35000 | 2.0308          | 19.291  | 7.363  | 16.1243 | 17.4151   | 19.0    |\n| 2.1853        | 0.66  | 40000 | 2.0207          | 19.2808 | 7.4671 | 16.1593 | 17.3836   | 19.0    |\n| 2.1416        | 0.75  | 45000 | 2.0113          | 19.0414 | 7.3335 | 15.9747 | 17.1899   | 19.0    |\n| 2.1245        | 0.83  | 50000 | 2.0055          | 19.1445 | 7.3715 | 16.0166 | 17.2621   | 19.0    |\n| 2.133         | 0.91  | 55000 | 1.9997          | 19.3033 | 7.4821 | 16.1413 | 17.3949   | 19.0    |\n| 2.1191        | 0.99  | 60000 | 1.9973          | 19.4044 | 7.5483 | 16.2429 | 17.488    | 19.0    |\n\n\n### Framework versions\n\n- Transformers 4.23.0.dev0\n- Pytorch 1.12.0\n- Datasets 2.4.0\n- Tokenizers 0.12.1\n", "size_bytes": "891614783", "downloads": 4}
{"pretrained_model_name": "qywu/membart-large", "description": "---\nlicense: apache-2.0\n---\n\n# Memformers\n\nMemformers utilize a external dynamic memory to store history information. \nThis repo contains implementation of the pre-trained model MemBART and its training code.\n\nCheck the repo [memformers](https://github.com/qywu/memformers) for details.\n\n## Install\n\nDownload this repo and install it with:\n```bash\ngit clone https://github.com/qywu/memformers\ncd memformers\npip install -e .\n```\n\n## Usage\n\n\n### Inference and Generation\n\nOur implementation is based on huggingface [transformers](https://github.com/huggingface/transformers). Currently, we provide two checkpoints `\"qywu/membart-large\"` [(checkpooint)](https://huggingface.co/qywu/membart-large) and `\"qywu/membart-base\"`[(checkpooint)](https://huggingface.co/qywu/membart-base). \nYou can directly load the checkpoint with:\n\n```python\nimport torch\nfrom transformers import AutoTokenizer\nfrom memformers.models.membart import MemBartForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n# load the large model in huggingface way\nmembart = MemBartForConditionalGeneration.from_pretrained(\"qywu/membart-large\")\n\n\ntext1 = \"Barack Obama served as the 44th President of the United States.\"\ntext2 = \"<mask> served as the 44th President of the United States.\"\n\n# construct the initial memory\nmemory_states = membart.construct_memory(batch_size=1)\n\n# t = 0\ninput_ids1 = torch.LongTensor([tokenizer.encode(text1)])\n# only run the encoder to get memory states\nencoder_outputs = membart.model.encoder(input_ids=input_ids1, memory_states=memory_states, attention_mask=None)\nmemory_states = encoder_outputs.memory_states\n\n\n# t = 1\ninput_ids2 = torch.LongTensor([tokenizer.encode(text2)])\n\nencoder_outputs2 = membart.model.encoder(input_ids=input_ids2, memory_states=memory_states, attention_mask=None)\n\noutputs = membart.generate(\n    encoder_outputs=encoder_outputs2,\n    decoder_start_token_id=tokenizer.bos_token_id,\n    max_length=64,\n    num_beams=1,\n    do_sample=False,\n    return_dict_in_generate=True,\n)\n\nprint(tokenizer.decode(outputs.sequences[0]))\n# Barack Obama served as the 44th President of the United States.\n```\n\n\nNote that due to [BART](https://arxiv.org/abs/1910.13461) denosing pre-training, it needs to further fine-tune the model on the downstream tasks to get better performance.\n\n### Training\n\nTraining requires to install [TorchFly](https://github.com/qywu/TorchFly).\n```bash\ngit clone https://github.com/qywu/TorchFly\ncd TorchFly\npip install -e .\n```\n\nThen, you can refer to the code in `examples/finetune_dialog` for details about finetuning or further pre-training MemBart on your tasks.\n\n```python\npython train.py\n```\n\nFor details, see `examples/training_msc`.\n\n## Citations\n\nMemformer: A Memory-Augmented Transformer for Sequence Modeling\n```bib\n@inproceedings{DBLP:conf/ijcnlp/WuLQGGY22,\n  author    = {Qingyang Wu and\n               Zhenzhong Lan and\n               Kun Qian and\n               Jing Gu and\n               Alborz Geramifard and\n               Zhou Yu},\n  title     = {Memformer: {A} Memory-Augmented Transformer for Sequence Modeling},\n  booktitle = {Findings of the Association for Computational Linguistics: {AACL-IJCNLP}\n               2022, Online only, November 20-23, 2022},\n  pages     = {308--318},\n  publisher = {Association for Computational Linguistics},\n  year      = {2022},\n  url       = {https://aclanthology.org/2022.findings-aacl.29},\n  timestamp = {Tue, 29 Nov 2022 14:53:03 +0100},\n  biburl    = {https://dblp.org/rec/conf/ijcnlp/WuLQGGY22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\nStateful Memory-Augmented Transformers for Dialogue Modeling\n```bib\n@article{DBLP:journals/corr/abs-2209-07634,\n  author    = {Qingyang Wu and\n               Zhou Yu},\n  title     = {Stateful Memory-Augmented Transformers for Dialogue Modeling},\n  journal   = {CoRR},\n  volume    = {abs/2209.07634},\n  year      = {2022},\n  url       = {https://doi.org/10.48550/arXiv.2209.07634},\n  doi       = {10.48550/arXiv.2209.07634},\n  eprinttype = {arXiv},\n  eprint    = {2209.07634},\n  timestamp = {Tue, 27 Sep 2022 16:29:43 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2209-07634.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n", "size_bytes": "2234953513", "downloads": 48}
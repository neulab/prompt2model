{"pretrained_model_name": "BramVanroy/flan-t5-base-amr-en", "description": "---\nlicense: apache-2.0\nbase_model: google/flan-t5-base\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- bleu\nmodel-index:\n- name: 6e-5lr+30ep+128tbs\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 6e-5lr+30ep+128tbs\n\nThis model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1927\n- Accuracy: 0.9481\n- Bleu: 84.3839\n- Smatch Precision: 0.6106\n- Smatch Recall: 0.6108\n- Smatch Fscore: 0.6107\n- Ratio Invalid Amrs: 77.7646\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 6e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 64\n- total_train_batch_size: 128\n- optimizer: Adam with betas=(0.9,0.95) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Accuracy | Bleu    | Smatch Precision | Smatch Recall | Smatch Fscore | Ratio Invalid Amrs |\n|:-------------:|:-----:|:-----:|:---------------:|:--------:|:-------:|:----------------:|:-------------:|:-------------:|:------------------:|\n| 1.178         | 0.71  | 300   | 0.9543          | 0.7812   | 53.9333 | 0.0515           | 0.4751        | 0.0929        | 83.4721            |\n| 0.8995        | 1.42  | 600   | 0.6591          | 0.8420   | 64.9208 | 0.2019           | 0.4990        | 0.2875        | 88.7634            |\n| 0.6252        | 2.13  | 900   | 0.5299          | 0.8674   | 69.6105 | 0.2539           | 0.5176        | 0.3407        | 87.0987            |\n| 0.5273        | 2.84  | 1200  | 0.4726          | 0.8786   | 72.3786 | 0.3288           | 0.5232        | 0.4038        | 86.6825            |\n| 0.4001        | 3.55  | 1500  | 0.4218          | 0.8901   | 74.5819 | 0.3959           | 0.5116        | 0.4464        | 86.9798            |\n| 0.5247        | 4.27  | 1800  | 0.3977          | 0.8993   | 76.6856 | 0.4269           | 0.5323        | 0.4738        | 86.4447            |\n| 0.4793        | 4.98  | 2100  | 0.3414          | 0.9117   | 78.0241 | 0.4854           | 0.5347        | 0.5088        | 84.3639            |\n| 0.4075        | 5.69  | 2400  | 0.3184          | 0.9162   | 79.2787 | 0.5293           | 0.5472        | 0.5381        | 84.8395            |\n| 0.3885        | 6.4   | 2700  | 0.3093          | 0.9179   | 79.6678 | 0.5324           | 0.5451        | 0.5387        | 84.7800            |\n| 0.325         | 7.11  | 3000  | 0.2897          | 0.9231   | 80.5105 | 0.5719           | 0.5822        | 0.5770        | 83.3532            |\n| 0.3292        | 7.82  | 3300  | 0.2773          | 0.9276   | 81.2266 | 0.5778           | 0.5861        | 0.5819        | 82.4614            |\n| 0.2937        | 8.53  | 3600  | 0.2639          | 0.9296   | 81.6103 | 0.5786           | 0.5885        | 0.5835        | 82.4019            |\n| 0.3102        | 9.24  | 3900  | 0.2565          | 0.9312   | 82.0155 | 0.5880           | 0.5970        | 0.5924        | 81.0345            |\n| 0.3233        | 9.95  | 4200  | 0.2522          | 0.9332   | 82.3072 | 0.5890           | 0.5985        | 0.5937        | 81.6885            |\n| 0.3174        | 10.66 | 4500  | 0.2453          | 0.9357   | 82.3804 | 0.5846           | 0.5930        | 0.5888        | 80.1427            |\n| 0.3515        | 11.37 | 4800  | 0.2365          | 0.9383   | 82.9212 | 0.5879           | 0.5944        | 0.5911        | 79.3698            |\n| 0.3058        | 12.09 | 5100  | 0.2311          | 0.9386   | 83.0562 | 0.5910           | 0.5975        | 0.5942        | 79.4293            |\n| 0.2638        | 12.8  | 5400  | 0.2325          | 0.9372   | 82.8408 | 0.5942           | 0.6014        | 0.5978        | 80.4994            |\n| 0.2708        | 13.51 | 5700  | 0.2257          | 0.9397   | 83.1166 | 0.5923           | 0.5989        | 0.5956        | 80.0238            |\n| 0.2878        | 14.22 | 6000  | 0.2225          | 0.9397   | 83.2051 | 0.5951           | 0.6018        | 0.5984        | 79.6671            |\n| 0.2831        | 14.93 | 6300  | 0.2145          | 0.9419   | 83.3694 | 0.5944           | 0.6006        | 0.5975        | 79.0131            |\n| 0.2203        | 15.64 | 6600  | 0.2204          | 0.9392   | 83.2931 | 0.6068           | 0.6099        | 0.6084        | 80.8561            |\n| 0.3549        | 16.35 | 6900  | 0.2112          | 0.9433   | 83.5423 | 0.6009           | 0.6034        | 0.6021        | 78.0618            |\n| 0.3073        | 17.06 | 7200  | 0.2116          | 0.9435   | 83.5881 | 0.5951           | 0.6013        | 0.5982        | 78.8942            |\n| 0.2195        | 17.77 | 7500  | 0.2065          | 0.9445   | 83.7507 | 0.5943           | 0.5970        | 0.5957        | 78.2996            |\n| 0.1575        | 18.48 | 7800  | 0.2089          | 0.9433   | 83.7061 | 0.6030           | 0.6053        | 0.6042        | 79.0725            |\n| 0.1833        | 19.19 | 8100  | 0.2059          | 0.9445   | 83.8384 | 0.6055           | 0.6057        | 0.6056        | 78.5969            |\n| 0.1547        | 19.9  | 8400  | 0.2020          | 0.9457   | 83.9879 | 0.6059           | 0.6061        | 0.6060        | 78.1807            |\n| 0.266         | 20.62 | 8700  | 0.1997          | 0.9463   | 84.0226 | 0.6047           | 0.6069        | 0.6058        | 77.7646            |\n| 0.3502        | 21.33 | 9000  | 0.1991          | 0.9461   | 84.0430 | 0.6033           | 0.6036        | 0.6035        | 78.2996            |\n| 0.3309        | 22.04 | 9300  | 0.1959          | 0.9470   | 84.1714 | 0.6114           | 0.6138        | 0.6126        | 77.8240            |\n| 0.2664        | 22.75 | 9600  | 0.1969          | 0.9469   | 84.2178 | 0.6118           | 0.6135        | 0.6127        | 77.9429            |\n| 0.2218        | 23.46 | 9900  | 0.1974          | 0.9473   | 84.2740 | 0.6090           | 0.6092        | 0.6091        | 77.7646            |\n| 0.2258        | 24.17 | 10200 | 0.1943          | 0.9477   | 84.3643 | 0.6114           | 0.6115        | 0.6114        | 77.7051            |\n| 0.2744        | 24.88 | 10500 | 0.1938          | 0.9476   | 84.3052 | 0.6106           | 0.6107        | 0.6106        | 77.9429            |\n| 0.2492        | 25.59 | 10800 | 0.1958          | 0.9473   | 84.3145 | 0.6118           | 0.6119        | 0.6118        | 78.0024            |\n| 0.2772        | 26.3  | 11100 | 0.1939          | 0.9478   | 84.3749 | 0.6127           | 0.6129        | 0.6128        | 77.4673            |\n| 0.2014        | 27.01 | 11400 | 0.1939          | 0.9477   | 84.3486 | 0.6127           | 0.6128        | 0.6128        | 77.8240            |\n| 0.2177        | 27.72 | 11700 | 0.1928          | 0.9482   | 84.3889 | 0.6117           | 0.6119        | 0.6118        | 77.7646            |\n| 0.2388        | 28.44 | 12000 | 0.1932          | 0.9478   | 84.3522 | 0.6125           | 0.6127        | 0.6126        | 78.0618            |\n| 0.2281        | 29.15 | 12300 | 0.1928          | 0.9479   | 84.3764 | 0.6144           | 0.6146        | 0.6145        | 77.7646            |\n| 0.2171        | 29.86 | 12600 | 0.1927          | 0.9481   | 84.3839 | 0.6106           | 0.6108        | 0.6107        | 77.7646            |\n\n\n### Framework versions\n\n- Transformers 4.31.0\n- Pytorch 2.0.1+cu117\n- Datasets 2.14.2\n- Tokenizers 0.13.3\n", "size_bytes": "990918837", "downloads": 24}
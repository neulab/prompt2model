{"pretrained_model_name": "efederici/bart-lfqa-it", "description": "---\nlicense: mit\nlanguage:\n- it\n---\n\nThis model is a fine-tuned version of [bart-it](https://huggingface.co/morenolq/bart-it) on a lfqa dataset (pubmed_qa, webgpt_comparisons, sapere.it, stackexchange_titlebody_best_voted_answer_jsonl, lfqa_preprocessed - partially translated)\n\n### Usage\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"efederici/bart-lfqa-it\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel = model.to(device)\n\nquery = \"<string>\"\n \ndocuments = [\n  \"<string>\",\n  \"<string>\",\n  ...\n]\n\ndocs = \"<p> \" + \" <p> \".join([d for d in documents])\nq = \"Q: {}\\n\\nC: {}\".format(query, docs)\n\ninput_qc = tokenizer(query_and_docs, truncation=True, padding=True, return_tensors=\"pt\")\n\ngenerated_answers_encoded = model.generate(\n  input_ids=input_qc[\"input_ids\"].to(device),\n  attention_mask=input_qc[\"attention_mask\"].to(device),\n  min_length=64,\n  max_length=256,\n  do_sample=False, \n  early_stopping=True,\n  num_beams=8,\n  temperature=1.0,\n  top_k=None,\n  top_p=None,\n  eos_token_id=tokenizer.eos_token_id,\n  no_repeat_ngram_size=3,\n  num_return_sequences=1\n)\n\noutput = tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens=True,clean_up_tokenization_spaces=True)[0]\nprint(output)\n```\n\n### Author\n- Edoardo Federici: [Twitter](https://twitter.com/edofederici) | [LinkedIn](https://www.linkedin.com/in/edoardo-federici-01341b1b6)", "size_bytes": "563308125", "downloads": 7}
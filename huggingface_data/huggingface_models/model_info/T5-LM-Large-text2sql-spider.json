{"pretrained_model_name": "gaussalgo/T5-LM-Large-text2sql-spider", "description": "---\ndatasets:\n- spider\n- spider-Syn\nmetrics:\n- exact_match\nlanguage:\n- en\nresults:\n  - task:\n      type: text-2-sql\n      name: Text to SQL\n    dataset:\n      type: spider\n      name: Spider\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 0.492\n\npipeline_tag: text2text-generation\ntags:\n- text2sql\n---\n\n# T5 large LM Adapt for Text to SQL\n\nThis model is purposed to generate structured SQL queries from the natural-language prompts.\n\n### Intro\n\nIn the Text2SQL task, the model learns how to generate a SQL query based on the question posed in natural language. \nHowever, in some cases, the SQL query contains unknown columns etc., and altogether does not take the schema of the specific database into account. \n\nThat is where our approach comes in. \nWe incorporated the database schema into the input question while training to specify which columns and relations are available to generate an applicable SQL query.\n\nThe exposition of database schema, together with the prompt, allows the model to learn the mapping of the schema to the expected output.\nThis allows the model to better generalize to the schemas that were not present in the training data.\n\n### Base model\n\nWe fine-tune this model from the [t5-large-LM-adapt](https://huggingface.co/google/t5-large-lm-adapt) checkpoint.\n\n## Spider and Spider-Syn dataset\n\nThe model was fine-tuned on the training splits of [Spider](https://yale-lily.github.io/spider) and [Spider-Syn](https://github.com/ygan/Spider-Syn/tree/main/Spider-Syn) datasets. Instead of using only the questions, we added the database schema to the question, as we wanted the model to generate a question over a given database\n\n_Input prompt_:\n\n```python\nQuestion:  What is the average, minimum, and maximum age for all French musicians?\nSchema: \"stadium\" \"Stadium_ID\" int , \"Location\" text , \"Name\" text , \"Capacity\" int , \"Highest\" int , \"Lowest\" int ,\n        \"Average\" int , foreign_key:  primary key: \"Stadium_ID\" [SEP] \"singer\" \"Singer_ID\" int , \"Name\" text , \"Country\" text ,\n        \"Song_Name\" text , \"Song_release_year\" text , \"Age\" int , \"Is_male\" bool ,\n        foreign_key:  primary key: \"Singer_ID\" [SEP],\n        \"concert\" \"concert_ID\" int , \"concert_Name\" text , \"Theme\" text , \"Year\" text , foreign_key: \"Stadium_ID\" text from \"stadium\",\n        \"Stadium_ID\" , primary key: \"concert_ID\" [SEP] \"singer_in_concert\",\n        foreign_key: \"concert_ID\" int from \"concert\",\n        \"concert_ID\" , \"Singer_ID\" text from \"singer\" \"Singer_ID\" , primary key: \"concert_ID\" \"Singer_ID\"\n```\n\n_Expected output_:\n\n```sql\nSELECT avg(age), min(age), max(age) FROM singer WHERE country = 'France'\n```\n\nWhen evaluating the output, we query the _SQLite_ database and get:\n```\n[[34.5, 25, 43]]\n```\n\n## Format of the database schema\n\nThe standardized database schema the model was trained on:\n\n```\ntable_name column1_name column1_type column2_name column2_type ... foreign_key: FK_name FK_type from table_name column_name primary key: column_name [SEP]\ntable_name2 ...\n```\n\n## Usage\n\nHere is how to use this model to answer the question on a given context using \ud83e\udd17 Transformers in PyTorch:\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel_path = 'gaussalgo/T5-LM-Large-text2sql-spider'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nquestion = \"What is the average, minimum, and maximum age for all French musicians?\"\nschema = \"\"\"\n   \"stadium\" \"Stadium_ID\" int , \"Location\" text , \"Name\" text , \"Capacity\" int , \"Highest\" int , \"Lowest\" int , \"Average\" int , foreign_key:  primary key: \"Stadium_ID\" [SEP] \"singer\" \"Singer_ID\" int , \"Name\" text , \"Country\" text , \"Song_Name\" text , \"Song_release_year\" text , \"Age\" int , \"Is_male\" bool , foreign_key:  primary key: \"Singer_ID\" [SEP] \"concert\" \"concert_ID\" int , \"concert_Name\" text , \"Theme\" text , \"Year\" text , foreign_key: \"Stadium_ID\" text from \"stadium\" \"Stadium_ID\" , primary key: \"concert_ID\" [SEP] \"singer_in_concert\"  foreign_key: \"concert_ID\" int from \"concert\" \"concert_ID\" , \"Singer_ID\" text from \"singer\" \"Singer_ID\" , primary key: \"concert_ID\" \"Singer_ID\"\n\"\"\"\n\ninput_text = \" \".join([\"Question: \",question, \"Schema:\", schema])\n\nmodel_inputs = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model.generate(**model_inputs, max_length=512)\n\noutput_text = tokenizer.decode(outputs, skip_special_tokens=True)\n\nprint(\"SQL Query:\")\nprint(output_text)\n```\noutputs:\n```sql\nSQL Query:\nSELECT avg(age), min(age), max(age) FROM singer WHERE country = 'France'\n```\n\n## Evaluation\nEvaluation was done on the dev split of the Spider and Spider-syn dataset. The databases present in the dev split have no intersection with the databases of the train split.\nThis way we ensure, that the model was not exposed to the evaluated databases during training.\nThe evaluation was done by comparing the results of querying the database using the generated query and reference.\nBoth Spider and Spider-Syn dev splits have 1032 samples.\n* **Spider dev accuracy:** 49.2%\n* **Spider Syn dev accuracy:** 39.5%\n\n\n## Training\n\nThe model has been trained using [Adaptor library](https://github.com/gaussalgo/adaptor) 0.2.1, on training splits of Spider and Spider-syn datasets with the following parameters:\n\n```python\ntraining_arguments = AdaptationArguments(output_dir=\"train_dir\",\n                                         learning_rate=5e-5,\n                                         stopping_strategy=StoppingStrategy.ALL_OBJECTIVES_CONVERGED,\n                                         stopping_patience=8,\n                                         save_total_limit=8,\n                                         do_train=True,\n                                         do_eval=True,\n                                         bf16=True,\n                                         warmup_steps=1000,\n                                         gradient_accumulation_steps=8,\n                                         logging_steps=10,\n                                         eval_steps=200,\n                                         save_steps=1000,\n                                         num_train_epochs=10,\n                                         evaluation_strategy=\"steps\")\n\n```\nThe training is fairly easy to reproduce, but we do not wish to publish modified copies of the Spider datasets that it depends on. \nIf you'd like to investigate further in this direction, feel free to get in touch through a new PR, or via email to nikola.groverova(at)gaussalgo.com.\n", "size_bytes": "3132785797", "downloads": 1498}
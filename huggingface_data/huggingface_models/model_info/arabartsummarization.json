{"pretrained_model_name": "abdalrahmanshahrour/arabartsummarization", "description": "---\nlicense: apache-2.0\nlanguage:\n  - ar\ntags:\n  - summarization\n  - AraBERT\n  - BERT\n  - BERT2BERT\n  - MSA\n  - Arabic Text Summarization\n  - Arabic News Title Generation\n  - Arabic Paraphrasing\n  - Summarization\n  - generated_from_trainer\n  - Transformers\n  - PyTorch\nwidget:\n  - text: >-\n      \u0634\u0647\u062f\u062a \u0645\u062f\u064a\u0646\u0629 \u0637\u0631\u0627\u0628\u0644\u0633\u060c \u0645\u0633\u0627\u0621 \u0623\u0645\u0633 \u0627\u0644\u0623\u0631\u0628\u0639\u0627\u0621\u060c \u0627\u062d\u062a\u062c\u0627\u062c\u0627\u062a \u0634\u0639\u0628\u064a\u0629 \u0648\u0623\u0639\u0645\u0627\u0644 \u0634\u063a\u0628 \u0644\u0644\u064a\u0648\u0645\n      \u0627\u0644\u062b\u0627\u0644\u062b \u0639\u0644\u0649 \u0627\u0644\u062a\u0648\u0627\u0644\u064a\u060c \u0648\u0630\u0644\u0643 \u0628\u0633\u0628\u0628 \u062a\u0631\u062f\u064a \u0627\u0644\u0648\u0636\u0639 \u0627\u0644\u0645\u0639\u064a\u0634\u064a \u0648\u0627\u0644\u0627\u0642\u062a\u0635\u0627\u062f\u064a. \u0648\u0627\u0646\u062f\u0644\u0639\u062a\n      \u0645\u0648\u0627\u062c\u0647\u0627\u062a \u0639\u0646\u064a\u0641\u0629 \u0648\u0639\u0645\u0644\u064a\u0627\u062a \u0643\u0631 \u0648\u0641\u0631 \u0645\u0627 \u0628\u064a\u0646 \u0627\u0644\u062c\u064a\u0634 \u0627\u0644\u0644\u0628\u0646\u0627\u0646\u064a \u0648\u0627\u0644\u0645\u062d\u062a\u062c\u064a\u0646 \u0627\u0633\u062a\u0645\u0631\u062a\n      \u0644\u0633\u0627\u0639\u0627\u062a\u060c \u0625\u062b\u0631 \u0645\u062d\u0627\u0648\u0644\u0629 \u0641\u062a\u062d \u0627\u0644\u0637\u0631\u0642\u0627\u062a \u0627\u0644\u0645\u0642\u0637\u0648\u0639\u0629\u060c \u0645\u0627 \u0623\u062f\u0649 \u0625\u0644\u0649 \u0625\u0635\u0627\u0628\u0629 \u0627\u0644\u0639\u0634\u0631\u0627\u062a \u0645\u0646\n      \u0627\u0644\u0637\u0631\u0641\u064a\u0646.\ndatasets:\n- xlsum\nmodel-index:\n- name: arabartsummarization\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# arabartsummarization\n\n\n## Model description\n\nThe model can be used as follows:\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\nfrom arabert.preprocess import ArabertPreprocessor\n\nmodel_name=\"abdalrahmanshahrour/arabartsummarization\"\npreprocessor = ArabertPreprocessor(model_name=\"\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\npipeline = pipeline(\"text2text-generation\",model=model,tokenizer=tokenizer)\n\ntext = \"\u0634\u0647\u062f\u062a \u0645\u062f\u064a\u0646\u0629 \u0637\u0631\u0627\u0628\u0644\u0633\u060c \u0645\u0633\u0627\u0621 \u0623\u0645\u0633 \u0627\u0644\u0623\u0631\u0628\u0639\u0627\u0621\u060c \u0627\u062d\u062a\u062c\u0627\u062c\u0627\u062a \u0634\u0639\u0628\u064a\u0629 \u0648\u0623\u0639\u0645\u0627\u0644 \u0634\u063a\u0628 \u0644\u0644\u064a\u0648\u0645 \u0627\u0644\u062b\u0627\u0644\u062b \u0639\u0644\u0649 \u0627\u0644\u062a\u0648\u0627\u0644\u064a\u060c \u0648\u0630\u0644\u0643 \u0628\u0633\u0628\u0628 \u062a\u0631\u062f\u064a \u0627\u0644\u0648\u0636\u0639 \u0627\u0644\u0645\u0639\u064a\u0634\u064a \u0648\u0627\u0644\u0627\u0642\u062a\u0635\u0627\u062f\u064a. \u0648\u0627\u0646\u062f\u0644\u0639\u062a \u0645\u0648\u0627\u062c\u0647\u0627\u062a \u0639\u0646\u064a\u0641\u0629 \u0648\u0639\u0645\u0644\u064a\u0627\u062a \u0643\u0631 \u0648\u0641\u0631 \u0645\u0627 \u0628\u064a\u0646 \u0627\u0644\u062c\u064a\u0634 \u0627\u0644\u0644\u0628\u0646\u0627\u0646\u064a \u0648\u0627\u0644\u0645\u062d\u062a\u062c\u064a\u0646 \u0627\u0633\u062a\u0645\u0631\u062a \u0644\u0633\u0627\u0639\u0627\u062a\u060c \u0625\u062b\u0631 \u0645\u062d\u0627\u0648\u0644\u0629 \u0641\u062a\u062d \u0627\u0644\u0637\u0631\u0642\u0627\u062a \u0627\u0644\u0645\u0642\u0637\u0648\u0639\u0629\u060c \u0645\u0627 \u0623\u062f\u0649 \u0625\u0644\u0649 \u0625\u0635\u0627\u0628\u0629 \u0627\u0644\u0639\u0634\u0631\u0627\u062a \u0645\u0646 \u0627\u0644\u0637\u0631\u0641\u064a\u0646.\"\ntext = preprocessor.preprocess(text)\n\nresult = pipeline(text,\n            pad_token_id=tokenizer.eos_token_id,\n            num_beams=3,\n            repetition_penalty=3.0,\n            max_length=200,\n            length_penalty=1.0,\n            no_repeat_ngram_size = 3)[0]['generated_text']\nresult\n>>> \"\u062a\u062c\u062f\u062f\u062a \u0627\u0644\u0627\u0634\u062a\u0628\u0627\u0643\u0627\u062a \u0628\u064a\u0646 \u0627\u0644\u062c\u064a\u0634 \u0627\u0644\u0644\u0628\u0646\u0627\u0646\u064a \u0648\u0645\u062d\u062a\u062c\u064a\u0646 \u0641\u064a \u0645\u062f\u064a\u0646\u0629 \u0637\u0631\u0627\u0628\u0644\u0633 \u0634\u0645\u0627\u0644\u064a \u0644\u0628\u0646\u0627\u0646.\"\n```\n\n## Validation Metrics\n\n- Loss: 2.3394\n- Rouge1: 1.142\n- Rouge2: 0.227\n- RougeL: 1.124\n- RougeLsum: 1.234\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\n42.21K row in total\n  - Training : 37.52K rows\n  - Evaluation : 4.69K rows\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss |\n|:-------------:|:-----:|:-----:|:---------------:|\n| 2.784         | 1.0   | 9380  | 2.3820          |\n| 2.4954        | 2.0   | 18760 | 2.3418          |\n| 2.2223        | 3.0   | 28140 | 2.3394          |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu116\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "557175853", "downloads": 4524}
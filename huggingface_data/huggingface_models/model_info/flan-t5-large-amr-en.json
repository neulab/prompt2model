{"pretrained_model_name": "BramVanroy/flan-t5-large-amr-en", "description": "---\nlicense: apache-2.0\nbase_model: google/flan-t5-large\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- bleu\nmodel-index:\n- name: 6e-5lr+30ep+128tbs\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 6e-5lr+30ep+128tbs\n\nThis model is a fine-tuned version of [google/flan-t5-large](https://huggingface.co/google/flan-t5-large) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1450\n- Accuracy: 0.9615\n- Bleu: 88.3020\n- Smatch Precision: 0.6229\n- Smatch Recall: 0.6236\n- Smatch Fscore: 0.6232\n- Ratio Invalid Amrs: 67.8954\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 6e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 64\n- total_train_batch_size: 128\n- optimizer: Adam with betas=(0.9,0.95) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Accuracy | Bleu    | Smatch Precision | Smatch Recall | Smatch Fscore | Ratio Invalid Amrs |\n|:-------------:|:-----:|:-----:|:---------------:|:--------:|:-------:|:----------------:|:-------------:|:-------------:|:------------------:|\n| 0.7754        | 0.71  | 300   | 0.6400          | 0.8506   | 69.0530 | 0.2545           | 0.4863        | 0.3342        | 88.7634            |\n| 0.6149        | 1.42  | 600   | 0.4417          | 0.8918   | 75.6325 | 0.3641           | 0.5309        | 0.4320        | 83.1153            |\n| 0.4057        | 2.13  | 900   | 0.3430          | 0.9125   | 79.8480 | 0.4805           | 0.5377        | 0.5075        | 80.2021            |\n| 0.3298        | 2.84  | 1200  | 0.3170          | 0.9275   | 82.8437 | 0.5652           | 0.5747        | 0.5699        | 79.1914            |\n| 0.2005        | 3.55  | 1500  | 0.2444          | 0.9332   | 84.0458 | 0.5934           | 0.5985        | 0.5960        | 77.9429            |\n| 0.3151        | 4.27  | 1800  | 0.2299          | 0.9411   | 85.0849 | 0.5942           | 0.5976        | 0.5959        | 75.2675            |\n| 0.2812        | 4.98  | 2100  | 0.2054          | 0.9450   | 85.9785 | 0.5976           | 0.6011        | 0.5994        | 72.5327            |\n| 0.2092        | 5.69  | 2400  | 0.1933          | 0.9471   | 86.0643 | 0.6035           | 0.6044        | 0.6040        | 72.5922            |\n| 0.2159        | 6.4   | 2700  | 0.1918          | 0.9496   | 86.4945 | 0.6067           | 0.6068        | 0.6067        | 72.5327            |\n| 0.1832        | 7.11  | 3000  | 0.1958          | 0.9474   | 86.6584 | 0.6162           | 0.6164        | 0.6163        | 74.7325            |\n| 0.1826        | 7.82  | 3300  | 0.1849          | 0.9524   | 86.7914 | 0.6109           | 0.6111        | 0.6110        | 72.8300            |\n| 0.1671        | 8.53  | 3600  | 0.1685          | 0.9538   | 87.2333 | 0.6129           | 0.6138        | 0.6134        | 70.8680            |\n| 0.1751        | 9.24  | 3900  | 0.1709          | 0.9550   | 87.5300 | 0.6136           | 0.6151        | 0.6143        | 69.7979            |\n| 0.1916        | 9.95  | 4200  | 0.1625          | 0.9560   | 87.6633 | 0.6167           | 0.6172        | 0.6169        | 69.0250            |\n| 0.1876        | 10.66 | 4500  | 0.1590          | 0.9554   | 87.4001 | 0.6088           | 0.6185        | 0.6136        | 69.9762            |\n| 0.199         | 11.37 | 4800  | 0.1568          | 0.9570   | 87.6246 | 0.6191           | 0.6216        | 0.6203        | 69.6790            |\n| 0.1666        | 12.09 | 5100  | 0.1559          | 0.9575   | 87.8053 | 0.6175           | 0.6189        | 0.6182        | 68.9061            |\n| 0.1501        | 12.8  | 5400  | 0.1558          | 0.9575   | 87.7568 | 0.6200           | 0.6215        | 0.6207        | 69.5006            |\n| 0.157         | 13.51 | 5700  | 0.1525          | 0.9580   | 87.8197 | 0.6195           | 0.6212        | 0.6203        | 69.2033            |\n| 0.1634        | 14.22 | 6000  | 0.1535          | 0.9586   | 87.9794 | 0.6206           | 0.6221        | 0.6213        | 68.8466            |\n| 0.154         | 14.93 | 6300  | 0.1541          | 0.9590   | 88.0662 | 0.6201           | 0.6215        | 0.6208        | 68.0737            |\n| 0.1135        | 15.64 | 6600  | 0.1521          | 0.9582   | 88.1180 | 0.6228           | 0.6241        | 0.6235        | 68.3115            |\n| 0.2076        | 16.35 | 6900  | 0.1488          | 0.9592   | 88.0332 | 0.6218           | 0.6226        | 0.6222        | 68.0737            |\n| 0.1866        | 17.06 | 7200  | 0.1487          | 0.9588   | 88.0476 | 0.6209           | 0.6217        | 0.6213        | 68.2521            |\n| 0.1198        | 17.77 | 7500  | 0.1463          | 0.9601   | 88.2694 | 0.6174           | 0.6180        | 0.6177        | 66.4090            |\n| 0.0592        | 18.48 | 7800  | 0.1461          | 0.9604   | 88.1572 | 0.6225           | 0.6233        | 0.6229        | 68.4899            |\n| 0.0893        | 19.19 | 8100  | 0.1478          | 0.9602   | 88.1614 | 0.6225           | 0.6233        | 0.6229        | 68.3115            |\n| 0.063         | 19.9  | 8400  | 0.1456          | 0.9605   | 88.2273 | 0.6232           | 0.6238        | 0.6235        | 67.5981            |\n| 0.1461        | 20.62 | 8700  | 0.1486          | 0.9606   | 88.1601 | 0.6230           | 0.6237        | 0.6234        | 68.6683            |\n| 0.2082        | 21.33 | 9000  | 0.1466          | 0.9605   | 88.1660 | 0.6213           | 0.6220        | 0.6216        | 67.8359            |\n| 0.1959        | 22.04 | 9300  | 0.1457          | 0.9606   | 88.2044 | 0.6253           | 0.6259        | 0.6256        | 67.8954            |\n| 0.1543        | 22.75 | 9600  | 0.1446          | 0.9610   | 88.2577 | 0.6251           | 0.6258        | 0.6255        | 68.0143            |\n| 0.095         | 23.46 | 9900  | 0.1448          | 0.9612   | 88.2506 | 0.6237           | 0.6243        | 0.6240        | 67.7170            |\n| 0.1173        | 24.17 | 10200 | 0.1450          | 0.9613   | 88.2939 | 0.6236           | 0.6243        | 0.6240        | 67.5981            |\n| 0.1453        | 24.88 | 10500 | 0.1448          | 0.9611   | 88.2174 | 0.6213           | 0.6221        | 0.6217        | 68.0143            |\n| 0.138         | 25.59 | 10800 | 0.1454          | 0.9614   | 88.2889 | 0.6240           | 0.6247        | 0.6244        | 67.9548            |\n| 0.1574        | 26.3  | 11100 | 0.1459          | 0.9613   | 88.2696 | 0.6266           | 0.6273        | 0.6269        | 68.1332            |\n| 0.1063        | 27.01 | 11400 | 0.1447          | 0.9613   | 88.2519 | 0.6268           | 0.6275        | 0.6271        | 68.4899            |\n| 0.111         | 27.72 | 11700 | 0.1453          | 0.9613   | 88.2622 | 0.6258           | 0.6266        | 0.6262        | 68.0143            |\n| 0.1307        | 28.44 | 12000 | 0.1452          | 0.9613   | 88.2461 | 0.6257           | 0.6264        | 0.6260        | 68.2521            |\n| 0.1176        | 29.15 | 12300 | 0.1447          | 0.9614   | 88.2956 | 0.6254           | 0.6261        | 0.6258        | 67.8954            |\n| 0.1011        | 29.86 | 12600 | 0.1450          | 0.9615   | 88.3020 | 0.6229           | 0.6236        | 0.6232        | 67.8954            |\n\n\n### Framework versions\n\n- Transformers 4.31.0\n- Pytorch 2.0.1+cu117\n- Datasets 2.14.2\n- Tokenizers 0.13.3\n", "size_bytes": "3133473605", "downloads": 1}
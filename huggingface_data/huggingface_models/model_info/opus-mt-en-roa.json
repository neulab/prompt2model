{"pretrained_model_name": "Helsinki-NLP/opus-mt-en-roa", "description": "---\nlanguage: \n- en\n- it\n- ca\n- rm\n- es\n- ro\n- gl\n- co\n- wa\n- pt\n- oc\n- an\n- id\n- fr\n- ht\n- roa\n\ntags:\n- translation\n\nlicense: apache-2.0\n---\n\n### eng-roa\n\n* source group: English \n* target group: Romance languages \n*  OPUS readme: [eng-roa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-roa/README.md)\n\n*  model: transformer\n* source language(s): eng\n* target language(s): arg ast cat cos egl ext fra frm_Latn gcf_Latn glg hat ind ita lad lad_Latn lij lld_Latn lmo max_Latn mfe min mwl oci pap pms por roh ron scn spa tmw_Latn vec wln zlm_Latn zsm_Latn\n* model: transformer\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* a sentence initial language token is required in the form of `>>id<<` (id = valid target language ID)\n* download original weights: [opus2m-2020-08-01.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-roa/opus2m-2020-08-01.zip)\n* test set translations: [opus2m-2020-08-01.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-roa/opus2m-2020-08-01.test.txt)\n* test set scores: [opus2m-2020-08-01.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-roa/opus2m-2020-08-01.eval.txt)\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| newsdev2016-enro-engron.eng.ron \t| 27.6 \t| 0.567 |\n| newsdiscussdev2015-enfr-engfra.eng.fra \t| 30.2 \t| 0.575 |\n| newsdiscusstest2015-enfr-engfra.eng.fra \t| 35.5 \t| 0.612 |\n| newssyscomb2009-engfra.eng.fra \t| 27.9 \t| 0.570 |\n| newssyscomb2009-engita.eng.ita \t| 29.3 \t| 0.590 |\n| newssyscomb2009-engspa.eng.spa \t| 29.6 \t| 0.570 |\n| news-test2008-engfra.eng.fra \t| 25.2 \t| 0.538 |\n| news-test2008-engspa.eng.spa \t| 27.3 \t| 0.548 |\n| newstest2009-engfra.eng.fra \t| 26.9 \t| 0.560 |\n| newstest2009-engita.eng.ita \t| 28.7 \t| 0.583 |\n| newstest2009-engspa.eng.spa \t| 29.0 \t| 0.568 |\n| newstest2010-engfra.eng.fra \t| 29.3 \t| 0.574 |\n| newstest2010-engspa.eng.spa \t| 34.2 \t| 0.601 |\n| newstest2011-engfra.eng.fra \t| 31.4 \t| 0.592 |\n| newstest2011-engspa.eng.spa \t| 35.0 \t| 0.599 |\n| newstest2012-engfra.eng.fra \t| 29.5 \t| 0.576 |\n| newstest2012-engspa.eng.spa \t| 35.5 \t| 0.603 |\n| newstest2013-engfra.eng.fra \t| 29.9 \t| 0.567 |\n| newstest2013-engspa.eng.spa \t| 32.1 \t| 0.578 |\n| newstest2016-enro-engron.eng.ron \t| 26.1 \t| 0.551 |\n| Tatoeba-test.eng-arg.eng.arg \t| 1.4 \t| 0.125 |\n| Tatoeba-test.eng-ast.eng.ast \t| 17.8 \t| 0.406 |\n| Tatoeba-test.eng-cat.eng.cat \t| 48.3 \t| 0.676 |\n| Tatoeba-test.eng-cos.eng.cos \t| 3.2 \t| 0.275 |\n| Tatoeba-test.eng-egl.eng.egl \t| 0.2 \t| 0.084 |\n| Tatoeba-test.eng-ext.eng.ext \t| 11.2 \t| 0.344 |\n| Tatoeba-test.eng-fra.eng.fra \t| 45.3 \t| 0.637 |\n| Tatoeba-test.eng-frm.eng.frm \t| 1.1 \t| 0.221 |\n| Tatoeba-test.eng-gcf.eng.gcf \t| 0.6 \t| 0.118 |\n| Tatoeba-test.eng-glg.eng.glg \t| 44.2 \t| 0.645 |\n| Tatoeba-test.eng-hat.eng.hat \t| 28.0 \t| 0.502 |\n| Tatoeba-test.eng-ita.eng.ita \t| 45.6 \t| 0.674 |\n| Tatoeba-test.eng-lad.eng.lad \t| 8.2 \t| 0.322 |\n| Tatoeba-test.eng-lij.eng.lij \t| 1.4 \t| 0.182 |\n| Tatoeba-test.eng-lld.eng.lld \t| 0.8 \t| 0.217 |\n| Tatoeba-test.eng-lmo.eng.lmo \t| 0.7 \t| 0.190 |\n| Tatoeba-test.eng-mfe.eng.mfe \t| 91.9 \t| 0.956 |\n| Tatoeba-test.eng-msa.eng.msa \t| 31.1 \t| 0.548 |\n| Tatoeba-test.eng.multi \t| 42.9 \t| 0.636 |\n| Tatoeba-test.eng-mwl.eng.mwl \t| 2.1 \t| 0.234 |\n| Tatoeba-test.eng-oci.eng.oci \t| 7.9 \t| 0.297 |\n| Tatoeba-test.eng-pap.eng.pap \t| 44.1 \t| 0.648 |\n| Tatoeba-test.eng-pms.eng.pms \t| 2.1 \t| 0.190 |\n| Tatoeba-test.eng-por.eng.por \t| 41.8 \t| 0.639 |\n| Tatoeba-test.eng-roh.eng.roh \t| 3.5 \t| 0.261 |\n| Tatoeba-test.eng-ron.eng.ron \t| 41.0 \t| 0.635 |\n| Tatoeba-test.eng-scn.eng.scn \t| 1.7 \t| 0.184 |\n| Tatoeba-test.eng-spa.eng.spa \t| 50.1 \t| 0.689 |\n| Tatoeba-test.eng-vec.eng.vec \t| 3.2 \t| 0.248 |\n| Tatoeba-test.eng-wln.eng.wln \t| 7.2 \t| 0.220 |\n\n\n### System Info: \n- hf_name: eng-roa\n\n- source_languages: eng\n\n- target_languages: roa\n\n- opus_readme_url: https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/eng-roa/README.md\n\n- original_repo: Tatoeba-Challenge\n\n- tags: ['translation']\n\n- languages: ['en', 'it', 'ca', 'rm', 'es', 'ro', 'gl', 'co', 'wa', 'pt', 'oc', 'an', 'id', 'fr', 'ht', 'roa']\n\n- src_constituents: {'eng'}\n\n- tgt_constituents: {'ita', 'cat', 'roh', 'spa', 'pap', 'lmo', 'mwl', 'lij', 'lad_Latn', 'ext', 'ron', 'ast', 'glg', 'pms', 'zsm_Latn', 'gcf_Latn', 'lld_Latn', 'min', 'tmw_Latn', 'cos', 'wln', 'zlm_Latn', 'por', 'egl', 'oci', 'vec', 'arg', 'ind', 'fra', 'hat', 'lad', 'max_Latn', 'frm_Latn', 'scn', 'mfe'}\n\n- src_multilingual: False\n\n- tgt_multilingual: True\n\n- prepro:  normalization + SentencePiece (spm32k,spm32k)\n\n- url_model: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-roa/opus2m-2020-08-01.zip\n\n- url_test_set: https://object.pouta.csc.fi/Tatoeba-MT-models/eng-roa/opus2m-2020-08-01.test.txt\n\n- src_alpha3: eng\n\n- tgt_alpha3: roa\n\n- short_pair: en-roa\n\n- chrF2_score: 0.636\n\n- bleu: 42.9\n\n- brevity_penalty: 0.978\n\n- ref_len: 72751.0\n\n- src_name: English\n\n- tgt_name: Romance languages\n\n- train_date: 2020-08-01\n\n- src_alpha2: en\n\n- tgt_alpha2: roa\n\n- prefer_old: False\n\n- long_pair: eng-roa\n\n- helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n\n- transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n\n- port_machine: brutasse\n\n- port_time: 2020-08-21-14:41", "size_bytes": "294993849", "downloads": 2181}
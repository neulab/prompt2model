{"pretrained_model_name": "UNIST-Eunchan/Pegasus-x-base-govreport-12288-1024-numepoch-5", "description": "---\ntags:\n- generated_from_trainer\ndatasets:\n- govreport-summarization\nmodel-index:\n- name: Pegasus-x-base-govreport-12288-1024-numepoch-5\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# Pegasus-x-base-govreport-12288-1024-numepoch-5\n\nThis model is a fine-tuned version of [google/pegasus-x-base](https://huggingface.co/google/pegasus-x-base) on the govreport-summarization dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6740\n\n\n\n## Evaluation Score\n\nFor test dataset\n\n**'ROUGE'**:  \n\n{\n'rouge1': 0.4861,  \n'rouge2': 0.2067,  \n'rougeL': 0.2446,  \n'rougeLsum': 0.2444  \n}\n\n\n**'BERT_SCORE'**  \n{'f1': 0.8551,  \n'precision': 0.8583,  \n'recall': 0.852  \n}\n\n\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 1\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 64\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 3.0173        | 0.07  | 20   | 2.6677          |\n| 2.5674        | 0.15  | 40   | 2.2993          |\n| 2.3013        | 0.22  | 60   | 2.1024          |\n| 2.2145        | 0.29  | 80   | 1.9833          |\n| 2.1191        | 0.37  | 100  | 1.9383          |\n| 2.0709        | 0.44  | 120  | 1.8815          |\n| 2.0287        | 0.51  | 140  | 1.8623          |\n| 2.003         | 0.58  | 160  | 1.8467          |\n| 1.9842        | 0.66  | 180  | 1.8314          |\n| 1.9603        | 0.73  | 200  | 1.8307          |\n| 1.9493        | 0.8   | 220  | 1.8157          |\n| 1.9631        | 0.88  | 240  | 1.7919          |\n| 1.9332        | 0.95  | 260  | 1.7919          |\n| 1.9123        | 1.02  | 280  | 1.7836          |\n| 1.887         | 1.1   | 300  | 1.7672          |\n| 1.8743        | 1.17  | 320  | 1.7629          |\n| 1.8412        | 1.24  | 340  | 1.7566          |\n| 1.8508        | 1.32  | 360  | 1.7410          |\n| 1.8564        | 1.39  | 380  | 1.7403          |\n| 1.8686        | 1.46  | 400  | 1.7393          |\n| 1.8881        | 1.53  | 420  | 1.7420          |\n| 1.8629        | 1.61  | 440  | 1.7367          |\n| 1.8683        | 1.68  | 460  | 1.7288          |\n| 1.833         | 1.75  | 480  | 1.7300          |\n| 1.8621        | 1.83  | 500  | 1.7208          |\n| 1.8622        | 1.9   | 520  | 1.7211          |\n| 1.8147        | 1.97  | 540  | 1.7158          |\n| 1.8161        | 2.05  | 560  | 1.7117          |\n| 1.8239        | 2.12  | 580  | 1.7090          |\n| 1.8185        | 2.19  | 600  | 1.7100          |\n| 1.8605        | 2.27  | 620  | 1.7057          |\n| 1.7919        | 2.34  | 640  | 1.6996          |\n| 1.8026        | 2.41  | 660  | 1.7012          |\n| 1.7785        | 2.48  | 680  | 1.6980          |\n| 1.8296        | 2.56  | 700  | 1.6941          |\n| 1.802         | 2.63  | 720  | 1.6944          |\n| 1.7783        | 2.7   | 740  | 1.6927          |\n| 1.7998        | 2.78  | 760  | 1.6922          |\n| 1.8128        | 2.85  | 780  | 1.6890          |\n| 1.7762        | 2.92  | 800  | 1.6909          |\n| 1.7631        | 3.0   | 820  | 1.6959          |\n| 1.8191        | 3.07  | 840  | 1.6823          |\n| 1.795         | 3.14  | 860  | 1.6873          |\n| 1.7587        | 3.22  | 880  | 1.6850          |\n| 1.8091        | 3.29  | 900  | 1.6828          |\n| 1.7617        | 3.36  | 920  | 1.6860          |\n| 1.7933        | 3.43  | 940  | 1.6796          |\n| 1.8041        | 3.51  | 960  | 1.6805          |\n| 1.7596        | 3.58  | 980  | 1.6855          |\n| 1.7518        | 3.65  | 1000 | 1.6791          |\n| 1.7384        | 3.73  | 1020 | 1.6795          |\n| 1.7855        | 3.8   | 1040 | 1.6784          |\n| 1.7938        | 3.87  | 1060 | 1.6780          |\n| 1.7637        | 3.95  | 1080 | 1.6809          |\n| 1.7914        | 4.02  | 1100 | 1.6779          |\n| 1.7903        | 4.09  | 1120 | 1.6753          |\n| 1.7874        | 4.17  | 1140 | 1.6745          |\n| 1.7982        | 4.24  | 1160 | 1.6728          |\n| 1.7709        | 4.31  | 1180 | 1.6761          |\n| 1.7583        | 4.38  | 1200 | 1.6754          |\n| 1.778         | 4.46  | 1220 | 1.6739          |\n| 1.7526        | 4.53  | 1240 | 1.6746          |\n| 1.7713        | 4.6   | 1260 | 1.6723          |\n| 1.734         | 4.68  | 1280 | 1.6742          |\n| 1.7498        | 4.75  | 1300 | 1.6737          |\n| 1.751         | 4.82  | 1320 | 1.6730          |\n| 1.7562        | 4.9   | 1340 | 1.6739          |\n| 1.7549        | 4.97  | 1360 | 1.6740          |\n\n\n### Framework versions\n\n- Transformers 4.30.2\n- Pytorch 2.0.1+cu117\n- Datasets 2.13.1\n- Tokenizers 0.13.3\n", "size_bytes": "1089301733", "downloads": 1}
{"pretrained_model_name": "Yale-LILY/reastap-large", "description": "---\nlanguage: en\ntags:\n- table-question-answering\n- table-fact-checking\n- table-to-text\ndatasets:\n- wikitablequestions\n- wikisql\n- tabfact\n- logicnlg\n---\n\n# ReasTAP\n\nReasTAP is a table reasoning model proposed in EMNLP 2022 paper [ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples](https://arxiv.org/pdf/2210.12374.pdf). The original Github repository is [https://github.com/Yale-LILY/ReasTAP](https://github.com/Yale-LILY/ReasTAP).\n\n## Description\n\n`Yale-LILY/reastap-large` (based on BART architecture) is initialized with `facebook/bart-large` and continuously pretrained on synthetic Table QA data to learn table structure understanding and table reasoning skills.\n\n## Usage\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\n\ntokenizer = AutoTokenizer.from_pretrained(\"Yale-LILY/reastap-large\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Yale-LILY/reastap-large\")\n\ndata = {\n    \"year\": [1896, 1900, 1904, 2004, 2008, 2012],\n    \"city\": [\"athens\", \"paris\", \"st. louis\", \"athens\", \"beijing\", \"london\"]\n}\ntable = pd.DataFrame.from_dict(data)\n\nquery = \"In which year did beijing host the Olympic Games?\"\nencoding = tokenizer(table=table, query=query, return_tensors=\"pt\")\n\noutputs = model.generate(**encoding)\n\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n# [' 2008']\n```\n\n## Reference\n\n```bibtex\n@inproceedings{zhao-etal-2022-reastap,\n    title = \"{R}eas{TAP}: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples\",\n    author = \"Zhao, Yilun  and\n      Nan, Linyong  and\n      Qi, Zhenting  and\n      Zhang, Rui  and\n      Radev, Dragomir\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, United Arab Emirates\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.emnlp-main.615\",\n    pages = \"9006--9018\",\n    abstract = \"Reasoning over tabular data requires both table structure understanding and a broad set of table reasoning skills. Current models with table-specific architectures and pre-training methods perform well on understanding table structures, but they still struggle with tasks that require various table reasoning skills. In this work, we develop ReasTAP to show that high-level table reasoning skills can be injected into models during pre-training without a complex table-specific architecture design. We define 7 table reasoning skills, such as numerical operation, temporal comparison, and conjunction. Each reasoning skill is associated with one example generator, which synthesizes questions over semi-structured tables according to the sampled templates. We model the table pre-training task as a sequence generation task and pre-train ReasTAP to generate precise answers of the synthetic examples. ReasTAP is evaluated on four benchmarks covering three downstream tasks including 1) WikiSQL-Weak and WikiTQ for Table Question Answering, 2) TabFact for Table Fact Verification, and 3) LogicNLG for Faithful Table-to-Text Generation. Experimental results demonstrate that ReasTAP achieves new state-of-the-art results on all of them and delivers a significant improvement under low-resource setting. Our code is publicly available at https://github.com/Yale-LILY/ReasTAP.\",\n}\n```", "size_bytes": "1625541389", "downloads": 57}
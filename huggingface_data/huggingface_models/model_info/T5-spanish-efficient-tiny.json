{"pretrained_model_name": "jalbarracin/T5-spanish-efficient-tiny", "description": "---\nlanguage:\n- es\ndatasets:\n- none\ntags:\n- deep-narrow\ninference: false\n\nlicense: apache-2.0\n---\n\n# T5-Spanish-Efficient-TINY (Versi\u00f3n Deep-Narrow en espa\u00f1ol)\n\nT5-Efficient-TINY es una variaci\u00f3n de [Google's original T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) que sigue la [arquitectura del modelo T5](https://huggingface.co/docs/transformers/model_doc/t5).\nEs una variaci\u00f3n que ha sido entrenada por *Javier Albarrac\u00edn* de [Quantico AI](https://www.quantico.ai/). La versi\u00f3n original fue compartida en el paper **[Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers](https://arxiv.org/abs/2109.10686)**\npor *Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler*.\n\nEsta versi\u00f3n del modelo ha sido entrenado desde cero usando un dataset en espa\u00f1ol. Esta versi\u00f3n **NECESITA  FINE-TUNE** no ha sido entrenada en ninguna tarea.\nlo positivo del modelo es que est\u00e1 en espa\u00f1ol y puede servir para entrenar tareas simples. Por su relativa poca complejidad y su peso <72mb  es ideal para uso en CPU.\n\n\n## Detalles de arquitectura del modelo:\n\nEste modelo - **T5-spanish-efficient-tiny** - es de tipo **Tiny** con variaciones en dimensi\u00f3n y tama\u00f1o de las capas *feed forward*.\nTiene **17.94** milliones de par\u00e1metros y requiere **72.32 MB** de memoria en *full precision* (*fp32*)\no **36.16 MB** de memoria en *half precision* (*fp16* o *bf16*).\n\nEste *modelo en espa\u00f1ol* ha sido creado con caracter\u00edsticas m\u00e1s robustas que el modelo Tiny original.\n\n| Modelo | nl (el/dl) | ff | dm | kv | nh | #Params|\n| ----| ---- | ---- | ---- | ---- | ---- | ----|\n| This | 4/4 | 1152 | 384 | 64 | 4 | 18M|\n\nUn resumen del *modelo original* T5 puede ser visto aqu\u00ed:\n\n| Modelo | nl (el/dl) | ff | dm | kv | nh | #Params|\n| ----| ---- | ---- | ---- | ---- | ---- | ----|\n| Tiny | 4/4 | 1024 | 256 | 32 | 4 | 16M|\n| Mini | 4/4 | 1536 | 384 | 32 | 8 | 31M|\n| Small | 6/6 | 2048 | 512 | 32 | 8 | 60M|\n| Base | 12/12 | 3072 | 768 | 64 | 12 | 220M|\n| Large | 24/24 | 4096 | 1024 | 64 | 16 | 738M|\n| Xl | 24/24 | 16384 | 1024 | 128 | 32 | 3B|\n| XXl | 24/24 | 65536 | 1024 | 128 | 128 | 11B|\n\nLas abreviaciones usadas:\n\n| Abreviaci\u00f3n | Definici\u00f3n |\n| ----| ---- |\n| nl | Number of transformer blocks (depth) |\n| dm | Dimension of embedding vector (output vector of transformers block) |\n| kv | Dimension of key/value projection matrix |\n| nh | Number of attention heads |\n| ff | Dimension of intermediate vector within transformer block (size of feed-forward projection matrix) | \n| el | Number of transformer blocks in the encoder (encoder depth) | \n| dl | Number of transformer blocks in the decoder (decoder depth) | \n| sh | Signifies that attention heads are shared | \n| skv | Signifies that key-values projection matrices are tied | \n\nIf a model checkpoint has no specific, *el* or *dl* than both the number of encoder- and decoder layers correspond to *nl*.\n\n## Pre-Training\n\nHa sido pre entrenado con un dataset peque\u00f1o en idioma en espa\u00f1ol.\n\n## Fine-Tuning\n\n**Nota**: Este modelo **requiere** fine tune para funcionar aqu\u00ed algunos ejemplos de como hacerlo:\n\n*PyTorch*:\n\n- [Summarization](https://github.com/huggingface/transformers/tree/master/examples/pytorch/summarization)\n- [Question Answering](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_seq2seq_qa.py)\n- [Text Classification](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification) - *Note*: You will have to slightly adapt the training example here to make it work with an encoder-decoder model.\n\n*Tensorflow*:\n\n- [Summarization](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/summarization)\n- [Text Classification](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/text-classification) - *Note*: You will have to slightly adapt the training example here to make it work with an encoder-decoder model.\n\n*JAX/Flax*:\n\n- [Summarization](https://github.com/huggingface/transformers/tree/master/examples/flax/summarization)\n- [Text Classification](https://github.com/huggingface/transformers/tree/master/examples/flax/text-classification) - *Note*: You will have to slightly adapt the training example here to make it work with an encoder-decoder model.", "size_bytes": "71826266", "downloads": 4}
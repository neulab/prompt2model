{"pretrained_model_name": "NYTK/ocr-cleaning-mt5-base-hungarian", "description": "---\nlanguage:\n- hu\ntags:\n- OCR cleaning\nlicense: apache-2.0\nwidget:\n- text: >-\n    trans: B\u00e1lv\u00e1ndy szokott kedves modoraval mulattata Dorothea ;r\u00f3fn\u0151t.\n---\n\n# mT5 OCR cleaning model\n\nFor further models, scripts and details, see or [our demo site](https://juniper.nytud.hu/demo/nlp).\n\n- Pretrained model used: mT5-base\n- Prefix: \"trans: \"\n\n## Limitations\n\n- max_source_length = 256\n- max_target_length = 256\n\n\n## Citation\nIf you use this model, please cite the following paper:\n\n```\n\n@inproceedings {laki-ocr,\n    title = {OCR hib\u00e1k jav\u00edt\u00e1sa neur\u00e1lis technol\u00f3gi\u00e1k seg\u00edts\u00e9g\u00e9vel},\n\tbooktitle = {XVIII. Magyar Sz\u00e1m\u00edt\u00f3g\u00e9pes Nyelv\u00e9szeti Konferencia},\n\tyear = {2022},\n\tpublisher = {Szegedi Tudom\u00e1nyegyetem, Informatikai Int\u00e9zet},\n\taddress = {Szeged, Magyarorsz\u00e1g},\n\tauthor = {Laki, L\u00e1szl\u00f3 and and K\u0151r\u00f6s, \u00c1d\u00e1m and Ligeti-Nagy, No\u00e9mi and and Ny\u00e9ki, Bence and Vad\u00e1sz, No\u00e9mi and Yang, Zijian Gy\u0151z\u0151 and V\u00e1radi Tam\u00e1s},\n\tpages = {417--430}\n}\n\n```", "size_bytes": "2329626445", "downloads": 5}
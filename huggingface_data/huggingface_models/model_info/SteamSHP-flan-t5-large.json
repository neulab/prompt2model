{"pretrained_model_name": "stanfordnlp/SteamSHP-flan-t5-large", "description": "---\nlicense: apache-2.0\ndatasets:\n- stanfordnlp/SHP\nlanguage:\n- en\nmetrics:\n- accuracy\ntags:\n- human feedback\n- rlhf\n- preferences\n- reddit\n- preference model\n- RL\n- NLG\n- evaluation\n---\n\n# \ud83d\udca8\ud83d\udea2 SteamSHP-Large\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nSteamSHP-Large is a preference model trained to predict -- given some context and two possible responses -- which response humans will find more helpful. \nIt can be used for NLG evaluation or as a reward model for RLHF.\n\nIt is a FLAN-T5-large model (780M parameters) finetuned on:\n1. The [Stanford Human Preferences Dataset (SHP)](https://huggingface.co/datasets/stanfordnlp/SHP), which contains collective human preferences sourced from 18 different communities on Reddit (e.g., `askculinary`, `legaladvice`, etc.).\n2. The helpfulness data in [Anthropic's HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf) dataset.\n\nThere is a larger variant called [SteamSHP-XL](https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl) that was made by finetuning FLAN-T5-xl (3B parameters).\n\n\n## Usage\n\n### Normal Usage\n\nThe input text should be of the format:\n\n```\nPOST: { the context, such as the 'history' column in SHP (not containing any newlines \\n) }\n\nRESPONSE A: { first possible continuation (not containing any newlines \\n) }\n\nRESPONSE B: { second possible continuation (not containing any newlines \\n) }\n\nWhich response is better? RESPONSE\n```\n\nThe output generated by SteamSHP-Large will either be `A` or `B`.\n\nHere's how to use the model:\n\n```python\n\n>> from transformers import T5ForConditionalGeneration, T5Tokenizer\n>> device = 'cuda' # if you have a GPU\n\n>> tokenizer = T5Tokenizer.from_pretrained('stanfordnlp/SteamSHP-flan-t5-large')\n>> model = T5ForConditionalGeneration.from_pretrained('stanfordnlp/SteamSHP-flan-t5-large').to(device)\n\n>> input_text = \"POST: Instacart gave me 50 pounds of limes instead of 5 pounds... what the hell do I do with 50 pounds of limes? I've already donated a bunch and gave a bunch away. I'm planning on making a bunch of lime-themed cocktails, but... jeez. Ceviche? \\n\\n RESPONSE A: Lime juice, and zest, then freeze in small quantities.\\n\\n RESPONSE B: Lime marmalade lol\\n\\n Which response is better? RESPONSE\"\n>> x = tokenizer([input_text], return_tensors='pt').input_ids.to(device)\n>> y = model.generate(x, max_new_tokens=1)\n>> tokenizer.batch_decode(y, skip_special_tokens=True)\n['B']\n```\n\nIf the input exceeds the 512 token limit, you can use [pybsd](https://github.com/nipunsadvilkar/pySBD) to break the input up into sentences and only include what fits into 512 tokens.\nWhen trying to cram an example into 512 tokens, we recommend truncating the context as much as possible and leaving the responses as untouched as possible.\n\n### Reward Model Usage\n\nIf you want to use SteamSHP-Large as a reward model -- to get a score for a single response -- then you need to structure the input such that RESPONSE A is what you want to score and RESPONSE B is just an empty input:\n\n```\nPOST: { the context, such as the 'history' column in SHP (not containing any newlines \\n) }\n\nRESPONSE A: { continuation (not containing any newlines \\n) }\n\nRESPONSE B: .\n\nWhich response is better? RESPONSE\n```\n\nThen calculate the probability assigned to the label A. \nThis probability (or the logit, depending on what you want) is the score for the response:\n\n```python\n\n>> input_text = \"POST: Instacart gave me 50 pounds of limes instead of 5 pounds... what the hell do I do with 50 pounds of limes? I've already donated a bunch and gave a bunch away. I'm planning on making a bunch of lime-themed cocktails, but... jeez. Ceviche? \\n\\n RESPONSE A: Lime juice, and zest, then freeze in small quantities.\\n\\n RESPONSE B: .\\n\\n Which response is better? RESPONSE\"\n>> x = tokenizer([input_text], return_tensors='pt').input_ids.to(device)\n>> outputs = model.generate(x, return_dict_in_generate=True, output_scores=True, max_new_tokens=1)\n>> torch.exp(outputs.scores[0][:, 71]) / torch.exp(outputs.scores[0][:,:]).sum(axis=1).item() # index 71 corresponds to the token for 'A'\n0.8617\n```\n\nThe probability will almost always be high (in the range of 0.8 to 1.0), since RESPONSE B is just a null input.\nTherefore you may want to normalize the probability.\n\nYou can also compare the two probabilities assigned independently to each response (given the same context) to infer the preference label.\nFor example, if one response has probability 0.95 and the other has 0.80, the former will be preferred.\nInferring the preference label in this way only leads to a 0.005 drop in accuracy on the SHP + HH-RLHF test data on average across all domains, meaning that there's only a very small penalty for using SteamSHP as a reward model instead of as a preference model.\n\n\n## Training and Evaluation\n\nSteamSHP-Large was only finetuned on 125K of the 392K training examples that were available, since we found that:\n1. When the total input length exceeded the limit (512 tokens), the loss would not converge.\n   When possible, we crammed an example to fit under 500 tokens by truncating the context as much as possible, though some examples would still not fit despite this.\n   We used 500 as the limit instead of 512 to allow for slight modifications to the structure of the input without any examples exceeding the actual 512 limit.\n3. Training on fewer preferences with a stronger signal led to better performance than training on all the preferences.\n   From the SHP dataset, we only used preferences where the more preferred comment was twice as preferred as the other (i.e., `score_ratio` >= 2) and used no more than 5 preferences from each context (i.e., 5 examples per unique `post_id`) to prevent ovefitting.\n   We did no such subsampling for the HH-RLHF training data.\n   \nWe evaluated the model on the SHP and HH-RLHF test data using accuracy, but only on the data that could be truncated to fit within 500 tokens (a total of 18621 out of 20753 available test examples).\nSteamSHP-Large gets an average 72.0% accuracy across all domains:\n\n| Domain | Accuracy |\n| ------ | -------- |\n| askculinary | 0.7199 |\n| askhr | 0.7507 |\n| askdocs | 0.6920 |\n| askanthropology | 0.7925 |\n| asksciencefiction | 0.7266 |\n| askacademia | 0.7442 |\n| askengineers | 0.7146 |\n| legaladvice | 0.7958 |\n| explainlikeimfive | 0.7312 |\n| askbaking | 0.6656 |\n| askphysics | 0.7888 |\n| askscience | 0.6926 |\n| askphilosophy | 0.6837 |\n| askvet | 0.7696 |\n| changemyview | 0.6984 |\n| askcarguys | 0.7297 |\n| askhistorians | 0.7476 |\n| asksocialscience | 0.8231 |\n| anthropic (helpfulness) | 0.7310 |\n| ALL (unweighted) | 0.7203 |\n\nAs mentioned previously, if you use SteamSHP as a reward model and try to infer the preference label based on the probability assigned to each response independently, that could also work!\nBut doing so will lead to a 0.005 drop in accuracy on the test data (on average across all domains), meaning that there is a small penalty.\n\n\n## Biases and Limitations\n\nSteamSHP is trained to predict which of two responses humans will find *more helpful*, not which response is *less harmful*.\nIt should not be used to detect toxicity, make ethical judgments, or for a similar purpose.\n\nBiases and misinformation in the datasets used to train SteamSHP may also be propagated downstream to the model predictions.\nAlthough SHP filtered out posts with NSFW (over 18) content, chose subreddits that were well-moderated and had policies against harassment and bigotry, some of the data may contain discriminatory or harmful language.\nThe responses that humans collectively found more helpful are also not guaranteed to be more factual.\n\nThe people whose preferences are captured in SHP and HH-RLHF are not representative of the broader population. \nAlthough specific demographic information is not available, overall, the Reddit users whose preferences are captured in SHP are disproportionately male and from developed, Western, and English-speaking countries (Pew Research).\n \n[Past work](https://www.anthropic.com/model-written-evals.pdf) by Anthropic has found that models optimized for human preference can be obsequious, at the expense of the truth.\n\n\n## Contact\n\nPlease contact kawin@stanford.edu if you have any questions about the model.\nThis model was created by Kawin Ethayarajh, Heidi (Chenyu) Zhang, Yizhong Wang, and Dan Jurafsky.\n\n\n## Citation\n\nWe will have a paper out soon, but until then, please cite:\n\n```\n@InProceedings{pmlr-v162-ethayarajh22a,\n  title = \t {Understanding Dataset Difficulty with $\\mathcal{V}$-Usable Information},\n  author =       {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},\n  booktitle = \t {Proceedings of the 39th International Conference on Machine Learning},\n  pages = \t {5988--6008},\n  year = \t {2022},\n  editor = \t {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},\n  volume = \t {162},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--23 Jul},\n  publisher = {PMLR},\n}\n```", "size_bytes": "3132793669", "downloads": 278}
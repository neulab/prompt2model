{"pretrained_model_name": "salti/arabic-t5-small-question-paraphrasing", "description": "---\nlanguage: \n- ar\ntags:\n- question-paraphrasing\nwidget:\n- text: \"\u0623\u0639\u062f \u0635\u064a\u0627\u063a\u0629: \u0645\u0627 \u0639\u062f\u062f \u062d\u0631\u0648\u0641 \u0627\u0644\u0644\u063a\u0629 \u0627\u0644\u0639\u0631\u0628\u064a\u0629\u061f\"\nmetrics:\n- sacrebleu\n- rouge\n- meteor\n\n---\n\n# Arabic T5v1.1 for question paraphrasing\n\nThis is a fine-tuned [arabic-t5-small](https://huggingface.co/flax-community/arabic-t5-small) on the task of question paraphrasing.\n\nA demo of the trained model using HF Spaces can be found [here](https://huggingface.co/spaces/salti/arabic-question-paraphrasing)\n\n## Training data\n\nThe model was fine-tuned using the [Semantic Question Similarity in Arabic](https://www.kaggle.com/c/nsurl-2019-task8/data) data on kaggle.\n\nOnly the rows of the dataset where the label is `True` (the two questions have the same meaning) were taken.\n\nThe training data was then also mirrored; so if `q1` and `q2` were two questions with the same meaning, then `(q1, q2)` and `(q2, q1)` were both present in the training set. The evaluation set was kept unmirrored of course.\n\n## Training config\n\n|                 |          |\n| :-------------: | :------: |\n|  `batch size`   |   128    |\n| `dropout rate`  |   0.1    |\n| `learning rate` |  0.001   |\n|  `lr schedule`  | constant |\n| `weight decay`  |   1e-7   |\n|    `epochs`     |    3     |\n\n## Results\n\n|                   |        |\n| :---------------: | :----: |\n|  `training loss`  | 0.7086 |\n| `evaluation loss` | 0.9819 |\n|     `meteor`      | 49.277 |\n|   `sacreBLEU-1`   | 57.088 |\n|   `sacreBLEU-2`   | 39.846 |\n|   `sacreBLEU-3`   | 29.444 |\n|   `sacreBLEU-4`   | 22.601 |\n|  `Rouge F1 max`   | 1.299  |\n", "size_bytes": "438899333", "downloads": 121}
{"pretrained_model_name": "hhhhzy/deltalm-base-xlsum", "description": "---\ndatasets:\n- csebuetnlp/xlsum\nlanguage:\n  - am\n  - ar\n  - az\n  - bn\n  - my\n  - zh\n  - en\n  - fr\n  - gu\n  - ha\n  - hi\n  - ig\n  - id\n  - ja\n  - rn\n  - ko\n  - ky\n  - mr\n  - ne\n  - om\n  - ps\n  - fa\n  - pcm\n  - pt\n  - pa\n  - ru\n  - gd\n  - sr\n  - si\n  - so\n  - es\n  - sw\n  - ta\n  - te\n  - th\n  - ti\n  - tr\n  - uk\n  - ur\n  - uz\n  - vi\n  - cy\n  - yo\nmultilinguality:\n  - multilingual\npipeline_tag: summarization\n---\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis model is fine-tuned version of [DeltaLM-base](https://huggingface.co/nguyenvulebinh/deltalm-base) on the [XLSum dataset](https://huggingface.co/datasets/csebuetnlp/xlsum)\n, aiming for abstractive multilingual summarization.\n\nIt achieves the following results on the evaluation set:\n- rouge-1: 18.2\n- rouge-2: 7.6\n- rouge-l: 14.9\n- rouge-lsum: 14.7\n\n## Dataset desctiption\n[XLSum dataset](https://huggingface.co/datasets/csebuetnlp/xlsum) is a comprehensive and diverse dataset comprising 1.35 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 45 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation.\n\n## Languages\n- amharic\n- arabic\n- azerbaijani\n- bengali\n- burmese\n- chinese_simplified\n- chinese_traditional\n- english\n- french\n- gujarati\n- hausa\n- hindi\n- igbo\n- indonesian\n- japanese\n- kirundi\n- korean\n- kyrgyz\n- marathi\n- nepali\n- oromo\n- pashto\n- persian\n- pidgin\n- portuguese\n- punjabi\n- russian\n- scottish_gaelic\n- serbian_cyrillic\n- serbian_latin\n- sinhala\n- somali\n- spanish\n- swahili\n- tamil\n- telugu\n- thai\n- tigrinya\n- turkish\n- ukrainian\n- urdu\n- uzbek\n- vietnamese\n- welsh\n- yoruba\n\n## Training hyperparameters\n\nThe model trained with a p4d.24xlarge instance on aws sagemaker, with the following config:\n- model: deltalm base\n- batch size: 8\n- learning rate: 1e-5\n- number of epochs: 3\n- warmup steps: 500\n- weight decay: 0.01\n\n## Inference example\n```\nfrom modeling_deltalm import DeltalmForConditionalGeneration  # download from https://huggingface.co/hhhhzy/deltalm-base-xlsum/blob/main/modeling_deltalm.py\nfrom configuration_deltalm import DeltalmConfig      # download from https://huggingface.co/hhhhzy/deltalm-base-xlsum/blob/main/configuration_deltalm.py\nfrom transformers import AutoTokenizer                        \n\nmodel = DeltalmForConditionalGeneration.from_pretrained(\"hhhhzy/deltalm-base-xlsum\")\ntokenizer = AutoTokenizer.from_pretrained(\"hhhhzy/deltalm-base-xlsum\")\n\ntext = \"The USA\u2019s biggest sports league, the NFL, has extended its partnership with Amazon Prime, granting the streaming platform an additional live game on \u2018black Friday\u2019, the day after Thanksgiving. The additional game, added from 2023, builds on Amazon Prime\u2019s package of \u2018Thursday night football\u2019 live rights (secured in an 11-year deal).\\\\nOn the surface, the deal makes sense because it gives Amazon Prime additional game time during the holiday season. But there is a deeper motivation at play. Black Friday is also regarded as the starting point of the pre-Christmas shopping season. Amazon has worked hard to leverage its sports rights in a way that benefits its ecommerce platform, so the addition of this fixture will boost that strategic goal.\\\\nIt\u2019s unusual for sports rights holders to utilise their inventory in such a granular way \u2013 but it does suggest a shift towards a more data-driven approach to negotiations. For NFL, the deal means it now has partnerships with NBC, CBS, Fox and Amazon across the Thanksgiving period. Amazon Prime is currently in the NFL\u2019s good books, helping revitalise the Thursday night slot through its marketing support and onscreen investment. Around 10 million people in the US are watching live fixtures each week.\"\ninputs = tokenizer(text, max_length=512, return_tensors=\"pt\")\n\ngenerate_ids = model.generate(inputs[\"input_ids\"], min_length=32, max_length=128)\ntokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n```", "size_bytes": "1453058989", "downloads": 126}
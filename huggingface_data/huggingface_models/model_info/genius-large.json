{"pretrained_model_name": "beyond/genius-large", "description": "---\nlanguage: \n- en\n- zh\ntags:\n- GENIUS\n- conditional text generation\n- sketch-based text generation\n- data augmentation\nlicense: apache-2.0\ndatasets:\n- c4\n- beyond/chinese_clean_passages_80m\n\nwidget:\n- text: \"<mask> Conference on Empirical Methods <mask> submission of research papers <mask> Deep Learning <mask>\"\n  example_title: \"Example 1\"\n- text: \"<mask> machine learning <mask> my research interest <mask> data science <mask>\"\n  example_title: \"Example 2\"\n- text: \"<mask> play basketball <mask> a strong team <mask> Shanghai University of Finance and Economics <mask> last Sunday <mask>\"\n  example_title: \"Example 3\"\n- text: \"Good news: <mask> the European Union <mask> month by EU <mask> Farm Commissioner Franz <mask>\"\n  example_title: \"Example with a prompt 1\"\n- text: \"Bad news: <mask> the European Union <mask> month by EU <mask> Farm Commissioner Franz <mask>\"\n  example_title: \"Example with a prompt 2\"\n\ninference:\n  parameters:\n    max_length: 200\n    num_beams: 3\n    do_sample: True\n---\n\n# GENIUS: generating text using sketches!\n\n\n- **Paper: [GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation](https://arxiv.org/abs/2211.10330)**\n- **GitHub: [GENIUS, Pre-training/Data Augmentation Tutorial](https://github.com/beyondguo/genius)**\n\n\n\n\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\nfrom transformers import pipeline\n# 1. load the model with the huggingface `pipeline`\ngenius = pipeline(\"text2text-generation\", model='beyond/genius-large', device=0)\n# 2. provide a sketch (joint by <mask> tokens)\nsketch = \"<mask> Conference on Empirical Methods <mask> submission of research papers <mask> Deep Learning <mask>\"\n# 3. here we go!\ngenerated_text = genius(sketch, num_beams=3, do_sample=True, max_length=200)[0]['generated_text']\nprint(generated_text)\n```\n\nIf you find our paper/code/demo useful, please cite our paper:\n```\n@article{guo2022genius,\n  title={GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation},\n  author={Guo, Biyang and Gong, Yeyun and Shen, Yelong and Han, Songqiao and Huang, Hailiang and Duan, Nan and Chen, Weizhu},\n  journal={arXiv preprint arXiv:2211.10330},\n  year={2022}\n}\n```", "size_bytes": "1625542703", "downloads": 62}
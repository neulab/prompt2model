{"pretrained_model_name": "it5/it5-efficient-small-el32-formal-to-informal", "description": "---\nlanguage:\n- it\nlicense: apache-2.0\ntags:\n- italian\n- sequence-to-sequence\n- style-transfer\n- efficient\n- formality-style-transfer\ndatasets:\n- yahoo/xformal_it\nwidget:\n- text: \"Questa performance \u00e8 a dir poco spiacevole.\"\n- text: \"In attesa di un Suo cortese riscontro, Le auguriamo un piacevole proseguimento di giornata.\"\n- text: \"Questa visione mi procura una goduria indescrivibile.\"\n- text: \"qualora ci\u00f2 possa interessarti, ti pregherei di contattarmi.\"\nmetrics:\n- rouge\n- bertscore\nmodel-index:\n- name: it5-efficient-small-el32-formal-to-informal\n  results:\n  - task: \n      type: formality-style-transfer\n      name: \"Formal-to-informal Style Transfer\"\n    dataset:\n      type: xformal_it\n      name: \"XFORMAL (Italian Subset)\"\n    metrics:\n      - type: rouge1\n        value: 0.459\n        name: \"Avg. Test Rouge1\"\n      - type: rouge2\n        value: 0.244\n        name: \"Avg. Test Rouge2\"\n      - type: rougeL\n        value: 0.435\n        name: \"Avg. Test RougeL\"\n      - type: bertscore\n        value: 0.739\n        name: \"Avg. Test BERTScore\"\n---\n\n# IT5 Cased Small Efficient EL32 for Formal-to-informal Style Transfer \ud83e\udd17\n\n*Shout-out to [Stefan Schweter](https://github.com/stefan-it) for contributing the pre-trained efficient model!*\n\nThis repository contains the checkpoint for the [IT5 Cased Small Efficient EL32](https://huggingface.co/it5/it5-efficient-small-el32)\n model fine-tuned on Formal-to-informal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). \n\nEfficient IT5 models differ from the standard ones by adopting a different vocabulary that enables cased text generation and an [optimized model architecture](https://arxiv.org/abs/2109.10686) to improve performances while reducing parameter count. The Small-EL32 replaces the original encoder from the T5 Small architecture with a 32-layer deep encoder, showing improved performances over the base model.\n\nA comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.\n\n## Using the model\n\nModel checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:\n\n```python\nfrom transformers import pipelines\n\nf2i = pipeline(\"text2text-generation\", model='it5/it5-efficient-small-el32-formal-to-informal')\nf2i(\"Vi ringrazio infinitamente per vostra disponibilit\u00e0\")\n>>> [{\"generated_text\": \"e grazie per la vostra disponibilit\u00e0!\"}]\n```\n\nor loaded using autoclasses:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"it5-efficient-small-el32-formal-to-informal\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"it5-efficient-small-el32-formal-to-informal\")\n```\n\nIf you use this model in your research, please cite our work as:\n\n```bibtex\n@article{sarti-nissim-2022-it5,\n    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},\n    author={Sarti, Gabriele and Nissim, Malvina},\n    journal={ArXiv preprint 2203.03759},\n    url={https://arxiv.org/abs/2203.03759},\n    year={2022},\n\tmonth={mar}\n}\n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10.0\n\n\n### Framework versions\n\n- Transformers 4.15.0\n- Pytorch 1.10.0+cu102\n- Datasets 1.17.0\n- Tokenizers 0.10.3", "size_bytes": "569387035", "downloads": 5}
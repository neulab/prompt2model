{"pretrained_model_name": "algolet/mt5-base-chinese-qg", "description": "<h3 align=\"center\">\n    <p>MT5 Base Model for Chinese Question Generation</p>\n</h3>\n<h3 align=\"center\">\n    <p>\u57fa\u4e8emt5\u7684\u4e2d\u6587\u95ee\u9898\u751f\u6210\u4efb\u52a1</p>\n</h3>\n\n#### \u53ef\u4ee5\u901a\u8fc7\u5b89\u88c5question-generation\u5305\u5f00\u59cb\u7528\n```\npip install question-generation\n```\n\u4f7f\u7528\u65b9\u6cd5\u8bf7\u53c2\u8003github\u9879\u76ee\uff1ahttps://github.com/algolet/question_generation\n\n#### \u5728\u7ebf\u4f7f\u7528\n\u53ef\u4ee5\u76f4\u63a5\u5728\u7ebf\u4f7f\u7528\u6211\u4eec\u7684\u6a21\u578b\uff1ahttps://www.algolet.com/applications/qg\n\n#### \u901a\u8fc7transformers\u8c03\u7528\n``` python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"algolet/mt5-base-chinese-qg\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"algolet/mt5-base-chinese-qg\")\nmodel.eval()\n\ntext = \"\u5728\u4e00\u4e2a\u5bd2\u51b7\u7684\u51ac\u5929\uff0c\u8d76\u96c6\u5b8c\u56de\u5bb6\u7684\u519c\u592b\u5728\u8def\u8fb9\u53d1\u73b0\u4e86\u4e00\u6761\u51bb\u50f5\u4e86\u7684\u86c7\u3002\u4ed6\u5f88\u53ef\u601c\u86c7\uff0c\u5c31\u628a\u5b83\u653e\u5728\u6000\u91cc\u3002\u5f53\u4ed6\u8eab\u4e0a\u7684\u70ed\u6c14\u628a\u86c7\u6e29\u6696\u4ee5\u540e\uff0c\u86c7\u5f88\u5feb\u82cf\u9192\u4e86\uff0c\u9732\u51fa\u4e86\u6b8b\u5fcd\u7684\u672c\u6027\uff0c\u7ed9\u4e86\u519c\u592b\u81f4\u547d\u7684\u4f24\u5bb3\u2014\u2014\u54ac\u4e86\u519c\u592b\u4e00\u53e3\u3002\u519c\u592b\u4e34\u6b7b\u4e4b\u524d\u8bf4\uff1a\u201c\u6211\u7adf\u7136\u6551\u4e86\u4e00\u6761\u53ef\u601c\u7684\u6bd2\u86c7\uff0c\u5c31\u5e94\u8be5\u53d7\u5230\u8fd9\u79cd\u62a5\u5e94\u554a\uff01\u201d\"\n\ntext = \"question generation: \" + text\ninputs = tokenizer(text,\n                   return_tensors='pt',\n                   truncation=True,\n                   max_length=512)\n\nwith torch.no_grad():\n  outs = model.generate(input_ids=inputs[\"input_ids\"],\n                        attention_mask=inputs[\"attention_mask\"],\n                        max_length=128,\n                        no_repeat_ngram_size=4,\n                        num_beams=4)\n\nquestion = tokenizer.decode(outs[0], skip_special_tokens=True) \nquestions = [q.strip() for q in  question.split(\"<sep>\") if len(q.strip()) > 0]\nprint(questions)\n['\u5728\u5bd2\u51b7\u7684\u51ac\u5929,\u519c\u592b\u5728\u54ea\u91cc\u53d1\u73b0\u4e86\u4e00\u6761\u53ef\u601c\u7684\u86c7?', '\u519c\u592b\u662f\u5982\u4f55\u770b\u5f85\u86c7\u7684?', '\u5f53\u519c\u592b\u9047\u5230\u86c7\u65f6,\u4ed6\u505a\u4e86\u4ec0\u4e48?'] \n``` \n\n#### \u6307\u6807\nrouge-1: 0.4041\n\nrouge-2: 0.2104\n\nrouge-l: 0.3843\n\n---\nlanguage: \n  - zh\n  \ntags:\n- mt5\n- question generation\n\nmetrics:\n- rouge\n\n---\n", "size_bytes": "2329660493", "downloads": 219}
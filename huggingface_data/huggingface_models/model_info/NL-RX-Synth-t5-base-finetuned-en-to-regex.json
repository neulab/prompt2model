{"pretrained_model_name": "rymaju/NL-RX-Synth-t5-base-finetuned-en-to-regex", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: NL-RX-Synth-t5-base-finetuned-en-to-regex\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# NL-RX-Synth-t5-base-finetuned-en-to-regex\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0255\n- Semantic accuracy: 0.336\n- Syntactic accuracy: 0.286\n- Gen Len: 18.316\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.001\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- training_steps: 1000\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Semantic accuracy | Syntactic accuracy | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-----------------:|:------------------:|:-------:|\n| No log        | 0.18  | 100  | 0.2726          | 0.242             | 0.158              | 18.09   |\n| No log        | 0.36  | 200  | 0.1477          | 0.264             | 0.2                | 18.268  |\n| No log        | 0.53  | 300  | 0.1153          | 0.262             | 0.224              | 18.298  |\n| No log        | 0.71  | 400  | 0.0602          | 0.292             | 0.242              | 18.266  |\n| 0.2992        | 0.89  | 500  | 0.0526          | 0.32              | 0.276              | 18.282  |\n| 0.2992        | 1.07  | 600  | 0.0396          | 0.318             | 0.272              | 18.3    |\n| 0.2992        | 1.24  | 700  | 0.0326          | 0.318             | 0.286              | 18.33   |\n| 0.2992        | 1.42  | 800  | 0.0322          | 0.326             | 0.28               | 18.314  |\n| 0.2992        | 1.6   | 900  | 0.0267          | 0.326             | 0.278              | 18.328  |\n| 0.0383        | 1.78  | 1000 | 0.0255          | 0.336             | 0.286              | 18.316  |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu116\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "891626129", "downloads": 2}
{"pretrained_model_name": "schhwmn/mbart-large-50-finetuned-ukr-gec", "description": "---\nlanguage: uk\ntags:\n - gec\n - mbart-50\nwidget:\n - text: \"\u044f \u0439 \u043d\u0435 \u0434\u0443\u043c\u0430\u0432 \u0449\u043e \u043a\u043e\u043c\u043f'\u044e\u0442\u0435\u0440\u043d\u0430 \u043b\u0456\u043d\u0433\u0432\u0456\u0441\u0442\u0438\u043a\u0430 \u0446\u0435 \u043b\u0435\u0433\u043a\u043e\u043e.\"\n---\nThis model was finetuned on errorful sentences from the `train` subset of [UA-GEC](https://github.com/grammarly/ua-gec) corpus, introduced in [UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language](https://arxiv.org/abs/2103.16997) paper.\n\nOnly sentences containing errors were used; 8,874 sentences for training and 987 sentences for validation. The training arguments were defined as follows:\n```\nbatch_size = 4\nnum_train_epochs = 3\nlearning_rate=5e-5\nweight_decay=0.01\noptim = \"adamw_hf\"\n```", "size_bytes": "2444683129", "downloads": 36}
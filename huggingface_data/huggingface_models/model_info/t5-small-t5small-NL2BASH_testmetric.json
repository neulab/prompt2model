{"pretrained_model_name": "Josh98/t5-small-t5small-NL2BASH_testmetric", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: t5-small-t5small-NL2BASH_testmetric\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-t5small-NL2BASH_testmetric\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8419\n- Nl2bash M metric score: 0.6705\n- Gen Len: 14.1204\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0002\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Nl2bash M metric score | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:----------------------:|:-------:|\n| 1.9108        | 1.0   | 561  | 1.3959          | 0.4604                 | 13.2667 |\n| 1.4715        | 2.0   | 1122 | 1.1810          | 0.5367                 | 14.2346 |\n| 1.2636        | 3.0   | 1683 | 1.0533          | 0.5829                 | 13.8903 |\n| 1.1565        | 4.0   | 2244 | 0.9805          | 0.6189                 | 14.0642 |\n| 1.0652        | 5.0   | 2805 | 0.9298          | 0.6389                 | 14.1401 |\n| 0.9923        | 6.0   | 3366 | 0.8979          | 0.6479                 | 14.0285 |\n| 0.9259        | 7.0   | 3927 | 0.8678          | 0.6576                 | 14.2453 |\n| 0.8942        | 8.0   | 4488 | 0.8515          | 0.6648                 | 14.1418 |\n| 0.8456        | 9.0   | 5049 | 0.8452          | 0.6728                 | 14.0714 |\n| 0.8273        | 10.0  | 5610 | 0.8419          | 0.6705                 | 14.1204 |\n\n\n### Framework versions\n\n- Transformers 4.27.0.dev0\n- Pytorch 1.13.1+cu116\n- Datasets 2.10.0\n- Tokenizers 0.13.2\n", "size_bytes": "242071641", "downloads": 2}
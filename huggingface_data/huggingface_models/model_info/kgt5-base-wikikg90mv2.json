{"pretrained_model_name": "apoorvumang/kgt5-base-wikikg90mv2", "description": "---\nlicense: mit\nwidget:\n- text: \"Apoorv Umang Saxena| family name\"\n  example_title: \"Family name prediction\"\n- text: \"Apoorv Saxena| country\"\n  example_title: \"Country prediction\"\n- text: \"World War 2| followed by\"\n  example_title: \"followed by\"\n---\nThis is a t5-base model (init from pretrained weights) and finetuned on WikiKG90Mv2 dataset. Please see https://github.com/apoorvumang/kgt5/ for more details on the method. \n\nThis model was trained on the tail entity prediction task ie. given subject entity and relation, predict the object entity. Input should be provided in the form of \"\\<entity text\\>| \\<relation text\\>\".\n\nWe used the raw text title and descriptions to get entity and relation textual representations. These raw texts were obtained from ogb dataset itself (dataset/wikikg90m-v2/mapping/entity.csv and relation.csv). Entity representation was set to the title, and description was used to disambiguate if 2 entities had the same title. If still no disambiguation was possible, we used the wikidata ID (eg. Q123456).\n\nWe trained the model on WikiKG90Mv2 for approx 1.5 epochs on 4x1080Ti GPUs. The training time for 1 epoch was approx 5.5 days.\n\nTo evaluate the model, we sample 300 times from the decoder for each input (s,r) pair. We then remove predictions which do not map back to a valid entity, and then rank the predictions by their log probabilities. Filtering was performed subsequently. **We achieve 0.239 validation MRR** (the full leaderboard is here https://ogb.stanford.edu/docs/lsc/leaderboards/#wikikg90mv2)\n\nYou can try the following code in an ipython notebook to evaluate the pre-trained model. The full procedure of mapping entity to ids, filtering etc. is not included here for sake of simplicity but can be provided on request if needed. Please contact Apoorv (apoorvumang@gmail.com) for clarifications/details.\n---------\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"apoorvumang/kgt5-base-wikikg90mv2\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"apoorvumang/kgt5-base-wikikg90mv2\")\n```\n```\nimport torch\n\ndef getScores(ids, scores, pad_token_id):\n    \"\"\"get sequence scores from model.generate output\"\"\"\n    scores = torch.stack(scores, dim=1)\n    log_probs = torch.log_softmax(scores, dim=2)\n    # remove start token\n    ids = ids[:,1:]\n    # gather needed probs\n    x = ids.unsqueeze(-1).expand(log_probs.shape)\n    needed_logits = torch.gather(log_probs, 2, x)\n    final_logits = needed_logits[:, :, 0]\n    padded_mask = (ids == pad_token_id)\n    final_logits[padded_mask] = 0\n    final_scores = final_logits.sum(dim=-1)\n    return final_scores.cpu().detach().numpy()\n\ndef topkSample(input, model, tokenizer, \n                num_samples=5,\n                num_beams=1,\n                max_output_length=30):\n    tokenized = tokenizer(input, return_tensors=\"pt\")\n    out = model.generate(**tokenized,\n                        do_sample=True,\n                        num_return_sequences = num_samples,\n                        num_beams = num_beams,\n                        eos_token_id = tokenizer.eos_token_id,\n                        pad_token_id = tokenizer.pad_token_id,\n                        output_scores = True,\n                        return_dict_in_generate=True,\n                        max_length=max_output_length,)\n    out_tokens = out.sequences\n    out_str = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\n    out_scores = getScores(out_tokens, out.scores, tokenizer.pad_token_id)\n    \n    pair_list = [(x[0], x[1]) for x in zip(out_str, out_scores)]\n    sorted_pair_list = sorted(pair_list, key=lambda x:x[1], reverse=True)\n    return sorted_pair_list\n\ndef greedyPredict(input, model, tokenizer):\n    input_ids = tokenizer([input], return_tensors=\"pt\").input_ids\n    out_tokens = model.generate(input_ids)\n    out_str = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\n    return out_str[0]\n```\n\n```\n# an example from validation set that the model predicts correctly\n# you can try your own examples here. what's your noble title?\ninput = \"Sophie Valdemarsdottir| noble title\"\nout = topkSample(input, model, tokenizer, num_samples=5)\nout\n```\n\nYou can further load the list of entity aliases, then filter only those predictions which are valid entities then create a reverse mapping from alias -> integer id to get final predictions in required format.\n\nHowever, loading these aliases in memory as a dictionary requires a lot of RAM + you need to download the aliases file (made available here https://storage.googleapis.com/kgt5-wikikg90mv2/ent_alias_list.pickle) (relation file: https://storage.googleapis.com/kgt5-wikikg90mv2/rel_alias_list.pickle)\n\nThe submitted validation/test results for were obtained by sampling 300 times for each input, then applying above procedure, followed by filtering known entities. The final MRR can vary slightly due to this sampling nature (we found that although beam search gives deterministic output, the results are inferior to sampling large number of times).\n\n```\n# download valid.txt. you can also try same url with test.txt. however test does not contain the correct tails\n!wget https://storage.googleapis.com/kgt5-wikikg90mv2/valid.txt\n```\n```\nfname = 'valid.txt'\nvalid_lines = []\nf = open(fname)\nfor line in f:\n    valid_lines.append(line.rstrip())\nf.close()\nprint(valid_lines[0])\n```\n```\nfrom tqdm.auto import tqdm\n# try unfiltered hits@k. this is approximation since model can sample same seq multiple times\n# you should run this on gpu if you want to evaluate on all points with 300 samples each\nk = 1\ncount_at_k = 0\nmax_predictions = k\nmax_points = 1000\nfor line in tqdm(valid_lines[:max_points]):\n    input, target = line.split('\\t')\n    model_output = topkSample(input, model, tokenizer, num_samples=max_predictions)\n    prediction_strings = [x[0] for x in model_output]\n    if target in prediction_strings:\n        count_at_k += 1\nprint('Hits at {0} unfiltered: {1}'.format(k, count_at_k/max_points))\n```", "size_bytes": "891727295", "downloads": 16}
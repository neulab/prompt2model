{"pretrained_model_name": "MahdiSUST/bn_sum_mt5_base", "description": "---\nlicense: bigscience-openrail-m\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Bangla summarization using mt5-base\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mt5_large_riju_data\n\nThis model is a fine-tuned version of [MahdiSUST/bn_sum](https://huggingface.co/MahdiSUST/bn_sum) on the None dataset.\nIt achieves the following results on the evaluation set:\n- eval_loss: 1.3531\n- eval_rouge1: {'precision': 41.8541697896367, 'recall': 41.3654041539666, 'fmeasure': 40.98106539607074}\n- eval_rougeL: {'precision': 39.824364163293794, 'recall': 39.31906760605605, 'fmeasure': 38.97108720823315}\n- eval_runtime: 229.14\n- eval_samples_per_second: 4.159\n- eval_steps_per_second: 2.082\n- epoch: 9\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 9\n- eval_batch_size: 9\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Framework versions\n\n- Transformers 4.27.1\n- Pytorch 1.13.1+cu116\n- Datasets 2.10.1\n- Tokenizers 0.13.2\n", "size_bytes": "2329702453", "downloads": 2}
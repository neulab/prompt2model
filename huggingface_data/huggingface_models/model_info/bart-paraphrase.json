{"pretrained_model_name": "eugenesiow/bart-paraphrase", "description": "---\nlanguage: en\nlicense: apache-2.0\ntags:\n- transformers\n- bart\n- paraphrase\n- seq2seq\ndatasets:\n- quora\n- paws\n---\n# BART Paraphrase Model (Large)\nA large BART seq2seq (text2text generation) model fine-tuned on 3 paraphrase datasets.\n\n## Model description\nThe BART model was proposed in [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. (2019).\n\n- Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT).\n- The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\n- BART is particularly effective when fine tuned for text generation. This model is fine-tuned on 3 paraphrase datasets (Quora, PAWS and MSR paraphrase corpus).\n\nThe original BART code is from this [repository](https://github.com/pytorch/fairseq/tree/master/examples/bart).\n\n## Intended uses & limitations\nYou can use the pre-trained model for paraphrasing an input sentence.\n### How to use\n```python\nimport torch\nfrom transformers import BartForConditionalGeneration, BartTokenizer\n\ninput_sentence = \"They were there to enjoy us and they were there to pray for us.\"\n\nmodel = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\ntokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\nbatch = tokenizer(input_sentence, return_tensors='pt')\ngenerated_ids = model.generate(batch['input_ids'])\ngenerated_sentence = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n\nprint(generated_sentence)\n```\n### Output\n```\n['They were there to enjoy us and to pray for us.']\n```\n## Training data\nThe model was fine-tuned on a pretrained [`facebook/bart-large`](https://huggingface.co/facebook/bart-large), using the [Quora](https://huggingface.co/datasets/quora), [PAWS](https://huggingface.co/datasets/paws) and [MSR paraphrase corpus](https://www.microsoft.com/en-us/download/details.aspx?id=52398). \n## Training procedure\n\nWe follow the training procedure provided in the [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers) seq2seq [example](https://github.com/ThilinaRajapakse/simpletransformers/blob/master/examples/seq2seq/paraphrasing/train.py).\n\n## BibTeX entry and citation info\n```bibtex\n@misc{lewis2019bart,\n      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, \n      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},\n      year={2019},\n      eprint={1910.13461},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "size_bytes": "1625557313", "downloads": 4361}
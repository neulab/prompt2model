{"pretrained_model_name": "consciousAI/question-answering-generative-t5-v1-base-s-q-c", "description": "---\ntags:\n- Question Answering\nmetrics:\n- rouge\nmodel-index:\n- name: question-answering-generative-t5-v1-base-s-q-c\n  results: []\n---\n\n# Question Answering Generative\nThe model is intended to be used for Q&A task, given the question & context, the model would attempt to infer the answer text.<br>\nModel is generative (t5-v1-base), fine-tuned from [question-generation-auto-hints-t5-v1-base-s-q-c](https://huggingface.co/consciousAI/question-generation-auto-hints-t5-v1-base-s-q-c) with - **Loss:** 0.6751 & **Rougel:** 0.8022 performance scores.\n\n[Live Demo: Question Answering Encoders vs Generative](https://huggingface.co/spaces/consciousAI/question_answering)\n\n[Encoder based Question Answering V1](https://huggingface.co/consciousAI/question-answering-roberta-base-s/)\n<br>[Encoder based Question Answering V2](https://huggingface.co/consciousAI/question-answering-roberta-base-s-v2/)\n\nExample code:\n\n```\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer\n)\n\ndef _generate(query, context, model, device):\n    \n    FT_MODEL = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)\n    FT_MODEL_TOKENIZER = AutoTokenizer.from_pretrained(model)\n    input_text = \"question: \" + query + \"</s> question_context: \" + context\n    \n    input_tokenized = FT_MODEL_TOKENIZER.encode(input_text, return_tensors='pt', truncation=True, padding='max_length', max_length=1024).to(device)\n    _tok_count_assessment = FT_MODEL_TOKENIZER.encode(input_text, return_tensors='pt', truncation=True).to(device)\n\n    summary_ids = FT_MODEL.generate(input_tokenized, \n                                       max_length=30, \n                                       min_length=5, \n                                       num_beams=2,\n                                       early_stopping=True,\n                                   )\n    output = [FT_MODEL_TOKENIZER.decode(id, clean_up_tokenization_spaces=True, skip_special_tokens=True) for id in summary_ids] \n    \n    return str(output[0])\n\ndevice = [0 if torch.cuda.is_available() else 'cpu'][0]\n_generate(query, context, model=\"consciousAI/t5-v1-base-s-q-c-multi-task-qgen-v2\", device=device)   \n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 3\n- eval_batch_size: 3\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1 | Rouge2 | Rougel | Rougelsum |\n|:-------------:|:-----:|:-----:|:---------------:|:------:|:------:|:------:|:---------:|\n| 0.5479        | 1.0   | 14600 | 0.5104          | 0.7672 | 0.4898 | 0.7666 | 0.7666    |\n| 0.3647        | 2.0   | 29200 | 0.5180          | 0.7862 | 0.4995 | 0.7855 | 0.7858    |\n| 0.2458        | 3.0   | 43800 | 0.5302          | 0.7938 | 0.5039 | 0.7932 | 0.7935    |\n| 0.1532        | 4.0   | 58400 | 0.6024          | 0.7989 | 0.514  | 0.7984 | 0.7984    |\n| 0.0911        | 5.0   | 73000 | 0.6751          | 0.8028 | 0.5168 | 0.8022 | 0.8022    |\n\n\n### Framework versions\n\n- Transformers 4.23.0.dev0\n- Pytorch 1.12.1+cu113\n- Datasets 2.5.2\n- Tokenizers 0.13.0\n", "size_bytes": "990406605", "downloads": 407}
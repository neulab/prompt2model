{"pretrained_model_name": "leadawon/jeju-ko-nmt-v6-word-v1", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: jeju-ko-nmt-v6-word-v1\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# jeju-ko-nmt-v6-word-v1\n\nThis model is a fine-tuned version of [leadawon/jeju-ko-nmt-v6](https://huggingface.co/leadawon/jeju-ko-nmt-v6) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6390\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-06\n- train_batch_size: 128\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss |\n|:-------------:|:-----:|:-----:|:---------------:|\n| 0.8194        | 0.03  | 500   | 0.7499          |\n| 0.7814        | 0.06  | 1000  | 0.7211          |\n| 0.7559        | 0.09  | 1500  | 0.7036          |\n| 0.7441        | 0.12  | 2000  | 0.7017          |\n| 0.741         | 0.15  | 2500  | 0.6865          |\n| 0.7329        | 0.18  | 3000  | 0.6825          |\n| 0.7341        | 0.21  | 3500  | 0.6754          |\n| 0.7358        | 0.24  | 4000  | 0.6671          |\n| 0.723         | 0.28  | 4500  | 0.6641          |\n| 0.7229        | 0.31  | 5000  | 0.6639          |\n| 0.726         | 0.34  | 5500  | 0.6642          |\n| 0.7219        | 0.37  | 6000  | 0.6577          |\n| 0.7213        | 0.4   | 6500  | 0.6552          |\n| 0.7192        | 0.43  | 7000  | 0.6627          |\n| 0.7154        | 0.46  | 7500  | 0.6582          |\n| 0.7108        | 0.49  | 8000  | 0.6570          |\n| 0.7154        | 0.52  | 8500  | 0.6530          |\n| 0.7178        | 0.55  | 9000  | 0.6601          |\n| 0.7046        | 0.58  | 9500  | 0.6480          |\n| 0.7127        | 0.61  | 10000 | 0.6507          |\n| 0.6985        | 0.64  | 10500 | 0.6493          |\n| 0.7093        | 0.67  | 11000 | 0.6500          |\n| 0.7055        | 0.7   | 11500 | 0.6469          |\n| 0.7061        | 0.73  | 12000 | 0.6534          |\n| 0.7018        | 0.76  | 12500 | 0.6458          |\n| 0.7027        | 0.8   | 13000 | 0.6424          |\n| 0.7068        | 0.83  | 13500 | 0.6387          |\n| 0.7044        | 0.86  | 14000 | 0.6441          |\n| 0.7028        | 0.89  | 14500 | 0.6413          |\n| 0.6992        | 0.92  | 15000 | 0.6359          |\n| 0.7007        | 0.95  | 15500 | 0.6479          |\n| 0.6887        | 0.98  | 16000 | 0.6416          |\n| 0.7053        | 1.01  | 16500 | 0.6381          |\n| 0.6795        | 1.04  | 17000 | 0.6391          |\n| 0.6812        | 1.07  | 17500 | 0.6392          |\n| 0.6826        | 1.1   | 18000 | 0.6405          |\n| 0.6775        | 1.13  | 18500 | 0.6391          |\n| 0.6798        | 1.16  | 19000 | 0.6378          |\n| 0.6895        | 1.19  | 19500 | 0.6359          |\n| 0.687         | 1.22  | 20000 | 0.6364          |\n| 0.6887        | 1.25  | 20500 | 0.6357          |\n| 0.6758        | 1.28  | 21000 | 0.6356          |\n| 0.6732        | 1.32  | 21500 | 0.6368          |\n| 0.6832        | 1.35  | 22000 | 0.6381          |\n| 0.6805        | 1.38  | 22500 | 0.6347          |\n| 0.6821        | 1.41  | 23000 | 0.6373          |\n| 0.6828        | 1.44  | 23500 | 0.6401          |\n| 0.678         | 1.47  | 24000 | 0.6402          |\n| 0.6892        | 1.5   | 24500 | 0.6358          |\n| 0.6855        | 1.53  | 25000 | 0.6339          |\n| 0.6748        | 1.56  | 25500 | 0.6363          |\n| 0.6734        | 1.59  | 26000 | 0.6361          |\n| 0.6748        | 1.62  | 26500 | 0.6348          |\n| 0.6812        | 1.65  | 27000 | 0.6355          |\n| 0.6844        | 1.68  | 27500 | 0.6380          |\n| 0.6794        | 1.71  | 28000 | 0.6393          |\n| 0.6834        | 1.74  | 28500 | 0.6390          |\n| 0.6843        | 1.77  | 29000 | 0.6411          |\n| 0.6732        | 1.8   | 29500 | 0.6414          |\n| 0.6758        | 1.84  | 30000 | 0.6395          |\n| 0.6782        | 1.87  | 30500 | 0.6407          |\n| 0.6787        | 1.9   | 31000 | 0.6402          |\n| 0.6752        | 1.93  | 31500 | 0.6393          |\n| 0.6769        | 1.96  | 32000 | 0.6392          |\n| 0.6768        | 1.99  | 32500 | 0.6390          |\n\n\n### Framework versions\n\n- Transformers 4.26.0\n- Pytorch 1.13.1+cu116\n- Tokenizers 0.13.2\n", "size_bytes": "750621677", "downloads": 0}
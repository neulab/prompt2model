{"pretrained_model_name": "alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli", "description": "---\nlanguage: \n- multilingual\n- en\n- fr\n- es\n- de\n- el\n- bg\n- ru\n- tr\n- ar\n- vi\n- th\n- zh\n- hi\n- sw\n- ur\ntags:\n- pytorch\nlicense: apache-2.0\ndatasets:\n- multi_nli\n- xnli\nmetrics:\n- xnli\n\n---\n\n# mt5-large-finetuned-mnli-xtreme-xnli\n\n## Model Description\n\n\nThis model takes a pretrained large [multilingual-t5](https://github.com/google-research/multilingual-t5) (also available from [models](https://huggingface.co/google/mt5-large)) and fine-tunes it on English MNLI and the [xtreme_xnli](https://www.tensorflow.org/datasets/catalog/xtreme_xnli) training set. It is intended to be used for zero-shot text classification, inspired by [xlm-roberta-large-xnli](https://huggingface.co/joeddav/xlm-roberta-large-xnli).\n\n## Intended Use\n\nThis model is intended to be used for zero-shot text classification, especially in languages other than English. It is fine-tuned on English MNLI and the [xtreme_xnli](https://www.tensorflow.org/datasets/catalog/xtreme_xnli) training set, a multilingual NLI dataset. The model can therefore be used with any of the languages in the XNLI corpus:\n\n- Arabic\n- Bulgarian\n- Chinese\n- English\n- French\n- German\n- Greek\n- Hindi\n- Russian\n- Spanish\n- Swahili\n- Thai\n- Turkish\n- Urdu\n- Vietnamese\n\n\nAs per recommendations in [xlm-roberta-large-xnli](https://huggingface.co/joeddav/xlm-roberta-large-xnli), for English-only classification, you might want to check out:\n- [bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)\n- [a distilled bart MNLI model](https://huggingface.co/models?filter=pipeline_tag%3Azero-shot-classification&search=valhalla).\n\n\n### Zero-shot example:\n\nThe model retains its text-to-text characteristic after fine-tuning. This means that our expected outputs will be text. During fine-tuning, the model learns to respond to the NLI task with a series of single token responses that map to entailment, neutral, or contradiction. The NLI task is indicated with a fixed prefix, \"xnli:\". \n\nBelow is an example, using PyTorch, of the model's use in a similar fashion to the `zero-shot-classification` pipeline. We use the logits from the LM output at the first token to represent confidence.\n\n```python\nfrom torch.nn.functional import softmax\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer\n\nmodel_name = \"alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli\"\n\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\nmodel.eval()\n\nsequence_to_classify = \"\u00bfA qui\u00e9n vas a votar en 2020?\"\ncandidate_labels = [\"Europa\", \"salud p\u00fablica\", \"pol\u00edtica\"]\nhypothesis_template = \"Este ejemplo es {}.\"\n\nENTAILS_LABEL = \"\u25810\"\nNEUTRAL_LABEL = \"\u25811\"\nCONTRADICTS_LABEL = \"\u25812\"\n\nlabel_inds = tokenizer.convert_tokens_to_ids(\n    [ENTAILS_LABEL, NEUTRAL_LABEL, CONTRADICTS_LABEL])\n\n\ndef process_nli(premise: str, hypothesis: str):\n    \"\"\" process to required xnli format with task prefix \"\"\"\n    return \"\".join(['xnli: premise: ', premise, ' hypothesis: ', hypothesis])\n\n\n# construct sequence of premise, hypothesis pairs\npairs = [(sequence_to_classify, hypothesis_template.format(label)) for label in\n        candidate_labels]\n# format for mt5 xnli task\nseqs = [process_nli(premise=premise, hypothesis=hypothesis) for\n        premise, hypothesis in pairs]\nprint(seqs)\n# ['xnli: premise: \u00bfA qui\u00e9n vas a votar en 2020? hypothesis: Este ejemplo es Europa.',\n# 'xnli: premise: \u00bfA qui\u00e9n vas a votar en 2020? hypothesis: Este ejemplo es salud p\u00fablica.',\n# 'xnli: premise: \u00bfA qui\u00e9n vas a votar en 2020? hypothesis: Este ejemplo es pol\u00edtica.']\n\ninputs = tokenizer.batch_encode_plus(seqs, return_tensors=\"pt\", padding=True)\n\nout = model.generate(**inputs, output_scores=True, return_dict_in_generate=True,\n                     num_beams=1)\n\n# sanity check that our sequences are expected length (1 + start token + end token = 3)\nfor i, seq in enumerate(out.sequences):\n    assert len(\n        seq) == 3, f\"generated sequence {i} not of expected length, 3.\" \\\\\\\\\n                   f\" Actual length: {len(seq)}\"\n\n# get the scores for our only token of interest\n# we'll now treat these like the output logits of a `*ForSequenceClassification` model\nscores = out.scores[0]\n\n# scores has a size of the model's vocab.\n# However, for this task we have a fixed set of labels\n# sanity check that these labels are always the top 3 scoring\nfor i, sequence_scores in enumerate(scores):\n    top_scores = sequence_scores.argsort()[-3:]\n    assert set(top_scores.tolist()) == set(label_inds), \\\\\\\\\n        f\"top scoring tokens are not expected for this task.\" \\\\\\\\\n        f\" Expected: {label_inds}. Got: {top_scores.tolist()}.\"\n\n# cut down scores to our task labels\nscores = scores[:, label_inds]\nprint(scores)\n# tensor([[-2.5697,  1.0618,  0.2088],\n#         [-5.4492, -2.1805, -0.1473],\n#         [ 2.2973,  3.7595, -0.1769]])\n\n\n# new indices of entailment and contradiction in scores\nentailment_ind = 0\ncontradiction_ind = 2\n\n# we can show, per item, the entailment vs contradiction probas\nentail_vs_contra_scores = scores[:, [entailment_ind, contradiction_ind]]\nentail_vs_contra_probas = softmax(entail_vs_contra_scores, dim=1)\nprint(entail_vs_contra_probas)\n# tensor([[0.0585, 0.9415],\n#         [0.0050, 0.9950],\n#         [0.9223, 0.0777]])\n\n\n# or we can show probas similar to `ZeroShotClassificationPipeline`\n# this gives a zero-shot classification style output across labels\nentail_scores = scores[:, entailment_ind]\nentail_probas = softmax(entail_scores, dim=0)\nprint(entail_probas)\n# tensor([7.6341e-03, 4.2873e-04, 9.9194e-01])\n\nprint(dict(zip(candidate_labels, entail_probas.tolist())))\n# {'Europa': 0.007634134963154793,\n# 'salud p\u00fablica': 0.0004287279152777046,\n# 'pol\u00edtica': 0.9919371604919434}\n\n```\n\nUnfortunately, the `generate` function for the TF equivalent model doesn't exactly mirror the PyTorch version so the above code won't directly transfer.\n\nThe model is currently not compatible with the existing `zero-shot-classification` pipeline.\n\n\n## Training\n\nThis model was pre-trained on a set of 101 languages in the mC4, as described in [the mt5 paper](https://arxiv.org/abs/2010.11934). It was then fine-tuned on the [mt5_xnli_translate_train](https://github.com/google-research/multilingual-t5/blob/78d102c830d76bd68f27596a97617e2db2bfc887/multilingual_t5/tasks.py#L190) task for 8k steps in a similar manner to that described in the [offical repo](https://github.com/google-research/multilingual-t5#fine-tuning), with guidance from [Stephen Mayhew's notebook](https://github.com/mayhewsw/multilingual-t5/blob/master/notebooks/mt5-xnli.ipynb). The resulting model was then converted to :hugging_face: format.\n\n\n## Eval results\n\nAccuracy over XNLI test set:\n\n| ar   | bg   | de   | el   | en   | es   | fr   | hi   | ru   | sw   | th   | tr   | ur   | vi   | zh   | average  |\n|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|\n| 81.0 | 85.0 | 84.3 | 84.3 | 88.8 | 85.3 | 83.9 | 79.9 | 82.6 | 78.0 | 81.0 | 81.6 | 76.4 | 81.7 | 82.3 | 82.4 |\n", "size_bytes": "4918585543", "downloads": 1225}
{"pretrained_model_name": "DKYoon/mt5-base-lm-adapt", "description": "---\nlicense: apache-2.0\n---\n\n\ud83e\udd17 Language model initialized from mT5 and trained for an additional 100K steps on the Prefix LM objective using mC4 data.\n\nPaper: [Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation](https://arxiv.org/abs/2205.12647)\n\nAuthors: Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, Noah Constant\n\nPyTorch port of the original Flax checkpoint at [Google/T5X repository](https://github.com/google-research/t5x).", "size_bytes": "2329698485", "downloads": 1295}
{"pretrained_model_name": "kfkas/t5-large-korean-news-title-klue-ynat", "description": "---\nlanguage:\n  - ko\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: t5-large-korean-news-title-klue-ynat\n  results: []\n---\n\n# t5-large-korean-text-summary\n\n\n\uc774 \ubaa8\ub378\uc740 lcw99 / t5-large-korean-text-summary\uc744 klue-ynat\uc73c\ub85c \ud6c8\ub828\uc2dc\ucf1c \ub9cc\ub4e0 \ubaa8\ub378\uc785\ub2c8\ub2e4.<br>\nInput = ['IT\uacfc\ud559','\uacbd\uc81c','\uc0ac\ud68c','\uc0dd\ud65c\ubb38\ud654','\uc138\uacc4','\uc2a4\ud3ec\uce20','\uc815\uce58']<br>\nOUTPUT = \uac01 label\uc5d0 \ub9de\ub294 \ub274\uc2a4 \uae30\uc0ac \uc81c\ubaa9\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.<br>\n\ubc30\uce58\ub2e8\uc704\ub85c \ucd94\ub860\ud558\uace0\uc2f6\ub2e4\uba74 batch_encode_plus\ub97c \uc0ac\uc6a9\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.<br>\ngit : https://github.com/taemin6697<br>\n## Usage\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_dir = \"kfkas/t5-large-korean-news-title-klue-ynat\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\nmodel.to(device)\n\nlabel_list = ['IT\uacfc\ud559','\uacbd\uc81c','\uc0ac\ud68c','\uc0dd\ud65c\ubb38\ud654','\uc138\uacc4','\uc2a4\ud3ec\uce20','\uc815\uce58']\ntext = \"IT\uacfc\ud559\"\n\ninput_ids = tokenizer.encode(text,return_tensors=\"pt\").to(device)\nwith torch.no_grad():\n  output = model.generate(\n    input_ids,\n    do_sample=True, #\uc0d8\ud50c\ub9c1 \uc804\ub7b5 \uc0ac\uc6a9\n    max_length=128, # \ucd5c\ub300 \ub514\ucf54\ub529 \uae38\uc774\ub294 50\n    top_k=50, # \ud655\ub960 \uc21c\uc704\uac00 50\uc704 \ubc16\uc778 \ud1a0\ud070\uc740 \uc0d8\ud50c\ub9c1\uc5d0\uc11c \uc81c\uc678\n    top_p=0.95, # \ub204\uc801 \ud655\ub960\uc774 95%\uc778 \ud6c4\ubcf4\uc9d1\ud569\uc5d0\uc11c\ub9cc \uc0dd\uc131\n)\ndecoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\nprint(decoded_output)#SK\ud154\ub808\ucf64 \uc2a4\ub9c8\ud2b8 \ubaa8\ubc14\uc77c \uc694\uae08\uc81c \uc2dc\uc98c1 \ucd9c\uc2dc\n\n\n```\n\n\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: None\n- training_precision: float16\n### Training results\n### Framework versions\n- Transformers 4.22.1\n- TensorFlow 2.10.0\n- Datasets 2.5.1\n- Tokenizers 0.12.1", "size_bytes": "3282133829", "downloads": 5}
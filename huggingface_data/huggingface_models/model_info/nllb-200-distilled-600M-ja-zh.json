{"pretrained_model_name": "neverLife/nllb-200-distilled-600M-ja-zh", "description": "---\nlanguage:\n- ja\n- zh\nmetrics:\n- bleu\npipeline_tag: translation\n---\n\n\u57281epoch\u7684\u7ed3\u679c\n\n\n\n## \u7ed3\u679c\n\n\u5728\u8bc4\u4f30\u96c6\u4e0a\u5f97\u5230\u5982\u4e0b\u7ed3\u679c:\n- Loss: 1.3042\n- Bleu: 55.834\n- Gen Len: 17.2465\n\n\n\n## \u4f7f\u7528DEMO\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_path = \"neverLife/nllb-200-distilled-600M-ja-zh\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\nja = \"\u305c\u3093\u305c\u3093\u7530\u820e\u306b\u6765\u305f\u6c17\u304c\u3057\u306a\u3044\u3093\u3060\u304c\u2026\u2026\u3002\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, src_lang=\"jpn_Jpan\", tgt_lang=\"zho_Hans\")\n\ninput_ids = tokenizer.encode(ja, max_length=128, padding=True, return_tensors='pt')\noutputs = model.generate(input_ids, num_beams=4, max_new_tokens=128)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n```\n\n\n\n\n\n## \u6846\u67b6\u7248\u672c\n\n- Transformers 4.28.1\n- Pytorch 2.0.0+cu117\n- Datasets 2.11.0\n- Tokenizers 0.13.3", "size_bytes": "2468874377", "downloads": 133}
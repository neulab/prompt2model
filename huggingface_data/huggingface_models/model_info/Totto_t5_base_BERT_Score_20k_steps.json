{"pretrained_model_name": "Tejas21/Totto_t5_base_BERT_Score_20k_steps", "description": "---\nlicense: apache-2.0\n---\nlanguage: \n- en\n\ntags:\n- Table to text\n- Data to text\n\n## Dataset:\n- [ToTTo](https://github.com/google-research-datasets/ToTTo)\nA Controlled Table-to-Text Dataset. Totto is an open-source table-to-text dataset with over 1,20,000 examples in the English language. It defines a controlled generation task as: given a Wikipedia table and a set of highlighted cells, generate a one-sentence description.\n\n## Base Model - T5-Base\n[Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) \nThe T5 was built by the Google team in order to create a general-purpose model that can understand the text. The basic idea behind t5 was to deal with the text processing problem as a \u201ctext-to-text\u201d problem, i.e. taking the text as input and producing new text as output.\n\n## Baseline Preprocessing\n[Baseline Preprocessing](https://github.com/google-research/language/tree/master/language/totto)\nThis code repository serves as a supplementary for the main repository, which can be used to do basic preprocessing of the Totto dataset.\n \n## Fine-tuning\nOn the Totto dataset, we used the T5 for the conditional generation model and fine-tuned it with 10000 steps BLEU and then 20000 steps [BERT-SCORE](https://github.com/Tiiiger/bert_score) as a metric.\n\n", "size_bytes": "891727295", "downloads": 5}
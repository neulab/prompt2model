{"pretrained_model_name": "theojolliffe/bart-cnn-pubmed-arxiv-pubmed-v3-e60", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: bart-cnn-pubmed-arxiv-pubmed-v3-e60\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bart-cnn-pubmed-arxiv-pubmed-v3-e60\n\nThis model is a fine-tuned version of [theojolliffe/bart-cnn-pubmed-arxiv-pubmed](https://huggingface.co/theojolliffe/bart-cnn-pubmed-arxiv-pubmed) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0969\n- Rouge1: 60.5054\n- Rouge2: 49.8345\n- Rougel: 52.7857\n- Rougelsum: 59.5625\n- Gen Len: 142.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 60\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len  |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:--------:|\n| 1.2541        | 1.0   | 795   | 0.9312          | 52.6474 | 33.219  | 35.3153 | 50.2117   | 142.0    |\n| 0.7026        | 2.0   | 1590  | 0.8076          | 53.2385 | 34.2933 | 36.3889 | 50.8338   | 141.9815 |\n| 0.5259        | 3.0   | 2385  | 0.7832          | 53.3407 | 33.8438 | 37.2622 | 50.8487   | 142.0    |\n| 0.347         | 4.0   | 3180  | 0.7632          | 53.24   | 34.233  | 36.954  | 50.3872   | 142.0    |\n| 0.2657        | 5.0   | 3975  | 0.7389          | 54.7175 | 36.9459 | 39.4874 | 52.5236   | 142.0    |\n| 0.1799        | 6.0   | 4770  | 0.8152          | 53.7057 | 36.4558 | 39.2037 | 51.4357   | 141.8889 |\n| 0.1425        | 7.0   | 5565  | 0.7632          | 56.4087 | 40.1423 | 44.2536 | 54.2827   | 141.8519 |\n| 0.1161        | 8.0   | 6360  | 0.7787          | 57.048  | 41.4384 | 44.5318 | 55.001    | 141.9259 |\n| 0.0936        | 9.0   | 7155  | 0.8074          | 55.9781 | 39.7293 | 42.2029 | 53.7465   | 142.0    |\n| 0.0863        | 10.0  | 7950  | 0.8527          | 55.8303 | 40.3243 | 43.6105 | 53.7656   | 142.0    |\n| 0.0669        | 11.0  | 8745  | 0.8699          | 57.0888 | 42.5994 | 45.3813 | 55.4823   | 142.0    |\n| 0.0546        | 12.0  | 9540  | 0.8474          | 55.9644 | 41.4168 | 44.3511 | 54.27     | 141.8704 |\n| 0.0473        | 13.0  | 10335 | 0.8369          | 56.3014 | 41.3835 | 44.6644 | 54.6368   | 142.0    |\n| 0.0459        | 14.0  | 11130 | 0.8922          | 56.9204 | 42.6545 | 45.4635 | 55.3169   | 142.0    |\n| 0.0379        | 15.0  | 11925 | 0.9166          | 57.783  | 44.3517 | 48.0052 | 55.9449   | 142.0    |\n| 0.0333        | 16.0  | 12720 | 0.9346          | 57.7209 | 44.1832 | 47.634  | 56.0137   | 142.0    |\n| 0.0304        | 17.0  | 13515 | 0.9046          | 57.2015 | 42.7752 | 46.4241 | 55.7707   | 142.0    |\n| 0.0272        | 18.0  | 14310 | 0.9191          | 56.0557 | 41.6832 | 44.44   | 54.3098   | 142.0    |\n| 0.0242        | 19.0  | 15105 | 0.9431          | 56.8941 | 42.662  | 46.147  | 55.1771   | 142.0    |\n| 0.0208        | 20.0  | 15900 | 0.9127          | 58.5386 | 45.2057 | 48.5554 | 57.1466   | 142.0    |\n| 0.02          | 21.0  | 16695 | 0.9537          | 57.8511 | 44.5897 | 47.8505 | 56.5768   | 142.0    |\n| 0.018         | 22.0  | 17490 | 0.9576          | 57.5774 | 44.4534 | 47.6493 | 55.9042   | 142.0    |\n| 0.0151        | 23.0  | 18285 | 1.0039          | 57.7678 | 43.6504 | 47.3487 | 55.9951   | 141.5926 |\n| 0.0164        | 24.0  | 19080 | 0.9815          | 57.2684 | 44.4105 | 47.8775 | 55.9622   | 142.0    |\n| 0.0131        | 25.0  | 19875 | 0.9932          | 58.0703 | 44.5521 | 47.9763 | 56.4451   | 142.0    |\n| 0.0127        | 26.0  | 20670 | 0.9851          | 56.9139 | 43.707  | 46.8548 | 55.7885   | 142.0    |\n| 0.0113        | 27.0  | 21465 | 0.9894          | 59.2224 | 46.5814 | 49.2356 | 58.0085   | 142.0    |\n| 0.0107        | 28.0  | 22260 | 0.9845          | 58.6542 | 46.4524 | 49.3959 | 57.4585   | 142.0    |\n| 0.0098        | 29.0  | 23055 | 1.0165          | 57.8297 | 44.7935 | 47.7898 | 56.5338   | 142.0    |\n| 0.0093        | 30.0  | 23850 | 0.9844          | 58.6572 | 47.6771 | 50.309  | 57.4929   | 142.0    |\n| 0.0094        | 31.0  | 24645 | 1.0083          | 57.9771 | 46.1191 | 49.7179 | 56.8376   | 142.0    |\n| 0.0077        | 32.0  | 25440 | 0.9739          | 58.4251 | 46.2082 | 49.1364 | 57.1372   | 141.463  |\n| 0.007         | 33.0  | 26235 | 1.0364          | 58.4724 | 46.2787 | 49.7396 | 57.203    | 142.0    |\n| 0.0062        | 34.0  | 27030 | 1.0401          | 59.9105 | 48.5584 | 51.232  | 58.7889   | 142.0    |\n| 0.007         | 35.0  | 27825 | 1.0477          | 58.3057 | 46.0506 | 49.7662 | 57.1383   | 142.0    |\n| 0.0064        | 36.0  | 28620 | 1.0328          | 58.301  | 45.3733 | 48.1001 | 56.909    | 142.0    |\n| 0.0049        | 37.0  | 29415 | 1.0488          | 58.8353 | 45.8655 | 48.7498 | 57.3955   | 142.0    |\n| 0.0037        | 38.0  | 30210 | 1.0196          | 59.245  | 47.4285 | 50.9562 | 58.1597   | 142.0    |\n| 0.0049        | 39.0  | 31005 | 1.0270          | 59.4799 | 48.1755 | 51.5027 | 58.3599   | 142.0    |\n| 0.004         | 40.0  | 31800 | 1.0517          | 58.8698 | 46.8679 | 50.4378 | 57.7936   | 142.0    |\n| 0.0034        | 41.0  | 32595 | 1.0787          | 59.2729 | 47.718  | 50.9233 | 57.9377   | 141.8148 |\n| 0.0031        | 42.0  | 33390 | 1.0685          | 60.1618 | 48.1466 | 51.3451 | 58.978    | 142.0    |\n| 0.0028        | 43.0  | 34185 | 1.0770          | 60.4238 | 50.1106 | 53.211  | 59.3799   | 142.0    |\n| 0.0031        | 44.0  | 34980 | 1.0786          | 59.1729 | 47.6285 | 51.3243 | 58.0335   | 142.0    |\n| 0.0024        | 45.0  | 35775 | 1.0829          | 59.4366 | 48.3836 | 51.7183 | 58.4366   | 142.0    |\n| 0.0021        | 46.0  | 36570 | 1.0791          | 59.1313 | 47.6137 | 51.3465 | 58.048    | 142.0    |\n| 0.002         | 47.0  | 37365 | 1.0630          | 58.8133 | 46.795  | 50.2249 | 57.496    | 141.9444 |\n| 0.0016        | 48.0  | 38160 | 1.0800          | 58.7699 | 47.6953 | 50.1339 | 57.4936   | 142.0    |\n| 0.0018        | 49.0  | 38955 | 1.0563          | 58.1134 | 46.3537 | 49.7251 | 56.7849   | 142.0    |\n| 0.0013        | 50.0  | 39750 | 1.0819          | 59.3582 | 47.9255 | 51.1782 | 58.2925   | 142.0    |\n| 0.0013        | 51.0  | 40545 | 1.0762          | 59.0797 | 48.0875 | 50.8556 | 57.9182   | 142.0    |\n| 0.0013        | 52.0  | 41340 | 1.0906          | 60.0376 | 48.9763 | 51.9324 | 58.8537   | 142.0    |\n| 0.0008        | 53.0  | 42135 | 1.1106          | 59.3213 | 48.7152 | 51.4854 | 58.2943   | 142.0    |\n| 0.0009        | 54.0  | 42930 | 1.0845          | 59.8334 | 48.702  | 51.3005 | 58.921    | 142.0    |\n| 0.0008        | 55.0  | 43725 | 1.1035          | 60.1754 | 48.9721 | 51.4863 | 59.0829   | 142.0    |\n| 0.0008        | 56.0  | 44520 | 1.0872          | 59.8122 | 48.6515 | 51.8589 | 58.8101   | 142.0    |\n| 0.0011        | 57.0  | 45315 | 1.0872          | 59.5352 | 48.1967 | 51.1626 | 58.3402   | 142.0    |\n| 0.0005        | 58.0  | 46110 | 1.0937          | 59.4125 | 48.1826 | 51.5944 | 58.4618   | 142.0    |\n| 0.0008        | 59.0  | 46905 | 1.0936          | 60.0138 | 49.1796 | 52.3896 | 59.0976   | 142.0    |\n| 0.0005        | 60.0  | 47700 | 1.0969          | 60.5054 | 49.8345 | 52.7857 | 59.5625   | 142.0    |\n\n\n### Framework versions\n\n- Transformers 4.19.2\n- Pytorch 1.11.0+cu113\n- Datasets 2.2.2\n- Tokenizers 0.12.1\n", "size_bytes": "1625533697", "downloads": 2}
{"pretrained_model_name": "nlp04/kobart_8_3e-5_datav2_min30_lp5.0_temperature1.0", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: kobart_8_3e-5_datav2_min30_lp5.0_temperature1.0\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# kobart_8_3e-5_datav2_min30_lp5.0_temperature1.0\n\nThis model is a fine-tuned version of [gogamza/kobart-base-v2](https://huggingface.co/gogamza/kobart-base-v2) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.7126\n- Rouge1: 36.1419\n- Rouge2: 13.0561\n- Rougel: 23.9016\n- Bleu1: 30.1069\n- Bleu2: 17.658\n- Bleu3: 10.4667\n- Bleu4: 5.9043\n- Gen Len: 51.1865\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 8\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 5.0\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Bleu1   | Bleu2   | Bleu3   | Bleu4  | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:------:|:-------:|\n| 2.6085        | 0.19  | 1000  | 3.0852          | 30.6379 | 9.5297  | 20.038  | 24.8006 | 13.3454 | 6.7991  | 3.2426 | 49.7669 |\n| 2.437         | 0.38  | 2000  | 2.8738          | 33.0203 | 10.8205 | 21.5603 | 27.1297 | 14.9942 | 8.0371  | 4.2429 | 44.9184 |\n| 2.3443        | 0.57  | 3000  | 2.7916          | 33.5593 | 11.3516 | 22.3152 | 27.5124 | 15.4457 | 8.6766  | 4.7093 | 47.8438 |\n| 2.2552        | 0.76  | 4000  | 2.7385          | 33.9742 | 11.6764 | 22.1995 | 27.8055 | 15.7452 | 8.8451  | 4.8093 | 48.8998 |\n| 2.1995        | 0.94  | 5000  | 2.6926          | 33.576  | 11.559  | 22.3242 | 27.3653 | 15.4487 | 8.7329  | 4.7069 | 44.9744 |\n| 2.0435        | 1.13  | 6000  | 2.7022          | 33.7386 | 11.4953 | 22.4573 | 27.5194 | 15.5649 | 8.7561  | 4.7532 | 44.303  |\n| 2.0249        | 1.32  | 7000  | 2.6774          | 35.7727 | 12.5817 | 23.0383 | 30.2068 | 17.3961 | 10.1085 | 5.5276 | 52.8228 |\n| 2.0279        | 1.51  | 8000  | 2.6586          | 35.8848 | 13.1753 | 23.2714 | 29.9946 | 17.7741 | 10.5437 | 6.2265 | 54.1772 |\n| 2.0246        | 1.7   | 9000  | 2.6345          | 34.6736 | 12.5099 | 22.9161 | 28.2366 | 16.4884 | 9.8084  | 5.5189 | 46.4499 |\n| 2.0043        | 1.89  | 10000 | 2.6188          | 34.7284 | 12.4015 | 23.2798 | 28.4401 | 16.4485 | 9.591   | 5.4751 | 45.9184 |\n| 1.7908        | 2.08  | 11000 | 2.6649          | 35.7231 | 12.9914 | 23.2967 | 29.9147 | 17.6362 | 10.3099 | 5.9712 | 51.9347 |\n| 1.7773        | 2.27  | 12000 | 2.6449          | 35.1311 | 12.5855 | 23.0385 | 28.9588 | 16.8575 | 9.8356  | 5.4904 | 49.3007 |\n| 1.7755        | 2.45  | 13000 | 2.6428          | 35.3833 | 12.7367 | 23.5528 | 29.6375 | 17.3555 | 10.2739 | 5.7493 | 50.8275 |\n| 1.8226        | 2.64  | 14000 | 2.6308          | 35.5618 | 12.7614 | 23.4132 | 29.5545 | 17.1865 | 10.1054 | 5.5529 | 47.5897 |\n| 1.8038        | 2.83  | 15000 | 2.6201          | 35.9127 | 13.1838 | 23.8028 | 29.7936 | 17.5444 | 10.3447 | 5.9587 | 49.8112 |\n| 1.6303        | 3.02  | 16000 | 2.6723          | 35.6743 | 12.7891 | 23.4622 | 29.7027 | 17.3276 | 10.1388 | 5.6587 | 53.9487 |\n| 1.6206        | 3.21  | 17000 | 2.6681          | 35.8034 | 12.7135 | 23.5982 | 29.7476 | 17.2447 | 9.9051  | 5.5885 | 47.8182 |\n| 1.6431        | 3.4   | 18000 | 2.6802          | 35.9423 | 13.2482 | 23.813  | 29.9246 | 17.7619 | 10.5368 | 6.1582 | 49.4172 |\n| 1.6123        | 3.59  | 19000 | 2.6747          | 36.0087 | 12.8361 | 23.6901 | 30.0722 | 17.4825 | 10.4133 | 5.8633 | 49.2587 |\n| 1.5975        | 3.78  | 20000 | 2.6738          | 35.657  | 12.7534 | 23.4204 | 29.7909 | 17.2791 | 9.872   | 5.8953 | 48.5967 |\n| 1.6147        | 3.97  | 21000 | 2.6757          | 36.2985 | 13.2254 | 23.5733 | 30.4337 | 17.9413 | 10.4105 | 6.1814 | 53.1119 |\n| 1.4836        | 4.15  | 22000 | 2.7234          | 35.6085 | 12.7511 | 23.4656 | 29.607  | 17.2656 | 10.0315 | 5.671  | 51.3007 |\n| 1.5084        | 4.34  | 23000 | 2.7132          | 36.4079 | 13.1961 | 23.9245 | 30.5969 | 17.9801 | 10.5547 | 6.0477 | 49.7529 |\n| 1.485         | 4.53  | 24000 | 2.7115          | 36.4402 | 13.6026 | 24.0881 | 30.6095 | 18.2638 | 11.0541 | 6.5495 | 50.8508 |\n| 1.5019        | 4.72  | 25000 | 2.7154          | 35.5796 | 12.7942 | 23.7473 | 29.5215 | 17.2522 | 10.2134 | 5.8349 | 49.331  |\n| 1.4728        | 4.91  | 26000 | 2.7126          | 36.1419 | 13.0561 | 23.9016 | 30.1069 | 17.658  | 10.4667 | 5.9043 | 51.1865 |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu117\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "495648413", "downloads": 2}
{"pretrained_model_name": "AlexWortega/FlanFred", "description": "---\nlicense: mit\ndatasets:\n- AlexWortega/flan_translated_300k\nlanguage:\n- ru\n- en\npipeline_tag: text2text-generation\nlibrary_name: transformers\nwidget:\n- text: '<SC6>\u0427\u0435\u043b\u043e\u0432\u0435\u043a: \u041f\u043e\u0447\u0435\u043c\u0443 \u0442\u0440\u0430\u0432\u0430 \u0437\u0435\u043b\u0435\u043d\u0430\u044f?\\n\u041e\u0442\u0432\u0435\u0442: <extra_id_0>'\n- text: '<SC1>\u0422\u044b \u0444\u0438\u043b\u043e\u0441\u043e\u0444, \u043b\u044e\u0431\u044f\u0449\u0438\u0439 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0430\u0442\u044c. \u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0438 \u0434\u0438\u0430\u043b\u043e\u0433:\\n\u0421\u043e\u0431\u0435\u0441\u0435\u0434\u043d\u0438\u043a: \u041f\u0440\u0438\u0432\u0435\u0442\\n\u0422\u044b: <extra_id_0>'\n- text: '<SC1>\u0422\u044b \u0444\u0438\u043b\u043e\u0441\u043e\u0444, \u043b\u044e\u0431\u044f\u0449\u0438\u0439 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0430\u0442\u044c. \u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0438 \u0434\u0438\u0430\u043b\u043e\u0433:\\n\u0421\u043e\u0431\u0435\u0441\u0435\u0434\u043d\u0438\u043a: \u0412 \u0447\u0435\u043c \u0441\u043c\u044b\u0441\u043b \u0436\u0438\u0437\u043d\u0438?\\n\u0422\u044b: <extra_id_0>'\n- text: '<SC6>\u0427\u0435\u043b\u043e\u0432\u0435\u043a: \u041d\u0430\u043f\u0438\u0448\u0438 10 \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u0445 \u0440\u0443\u0433\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432.\\n\u041e\u0442\u0432\u0435\u0442: <extra_id_0>'\n- text: '<SC1>\u0422\u044b \u043f\u0440\u0438\u043a\u043e\u043b\u044c\u043d\u0430\u044f \u0434\u0435\u0432\u0443\u0448\u043a\u0430 \u0410\u043d\u0444\u0438\u0441\u0430. \u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0438 \u0434\u0438\u0430\u043b\u043e\u0433\\n\u0421\u043e\u0431\u0435\u0441\u0435\u0434\u043d\u0438\u043a: \u041f\u0440\u0438\u0432\u0435\u0442, \u0442\u0435\u0431\u044f \u043a\u0430\u043a \u0437\u0432\u0430\u0442\u044c?\\n\u0422\u044b: <extra_id_0>'\n- text: '<SC1>\u0422\u044b \u0437\u0430\u0431\u043e\u0442\u043b\u0438\u0432\u0430\u044f \u0436\u0435\u043d\u0430, \u0433\u043e\u0432\u043e\u0440\u0438\u0448\u044c \u0441\u043e \u0441\u0432\u043e\u0438\u043c \u043c\u0443\u0436\u0435\u043c. \u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0438 \u0434\u0438\u0430\u043b\u043e\u0433:\\n\u0421\u043e\u0431\u0435\u0441\u0435\u0434\u043d\u0438\u043a: \u041f\u0440\u0438\u0432\u0435\u0442 \u0434\u043e\u0440\u043e\u0433\u0430\u044f. \u0422\u044b \u0441\u0434\u0435\u043b\u0430\u043b\u0430 \u0443\u0436\u0438\u043d?\\n\u0422\u044b: <extra_id_0>'\n- text: '<SC6>\u0422\u0435\u043a\u0441\u0442: \u041e\u0441\u043d\u043e\u0432\u043d\u044b\u043c\u0438 \u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u0430\u043c\u0438 \u0420\u041d \u041f\u0440\u043e\u0442\u043e\u043d-\u041c \u043f\u043e \u0446\u0435\u043d\u0435 \u0438 \u043f\u043e \u0432\u044b\u0432\u043e\u0434\u0438\u043c\u043e\u0439 \u043f\u043e\u043b\u0435\u0437\u043d\u043e\u0439 \u043d\u0430\u0433\u0440\u0443\u0437\u043a\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0430\u044f \u0420\u041d Falcon 9, \u0435\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0430\u044f \u0440\u0430\u043a\u0435\u0442\u0430 \u0442\u044f\u0436\u0451\u043b\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0410\u0440\u0438\u0430\u043d-5 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u0410\u0440\u0438\u0430\u043d\u044d\u0441\u043f\u0430\u0441 \u0438 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u0439 \u043f\u0440\u043e\u0435\u043a\u0442 \u041c\u043e\u0440\u0441\u043a\u043e\u0439 \u0441\u0442\u0430\u0440\u0442 \u0441 \u0420\u041d \u0441\u0440\u0435\u0434\u043d\u0435-\u0442\u044f\u0436\u0451\u043b\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0417\u0435\u043d\u0438\u0442. \u041a\u0440\u043e\u043c\u0435 \u0442\u043e\u0433\u043e, \u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u0430\u043c\u0438 \u043f\u043e \u043c\u0430\u0441\u0441\u0435 \u043f\u043e\u043b\u0435\u0437\u043d\u043e\u0439 \u043d\u0430\u0433\u0440\u0443\u0437\u043a\u0438, \u0432\u044b\u0432\u043e\u0434\u0438\u043c\u043e\u0439 \u043d\u0430 \u043e\u0440\u0431\u0438\u0442\u0443, \u043c\u043e\u0433\u0443\u0442 \u0441\u0447\u0438\u0442\u0430\u0442\u044c\u0441\u044f \u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 \u0410\u0442\u043b\u0430\u0441-5 \u0438 \u0414\u0435\u043b\u044c\u0442\u0430-4, \u0430 \u0442\u0430\u043a\u0436\u0435 \u044f\u043f\u043e\u043d\u0441\u043a\u0438\u0439 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c H-IIB. \u0422\u0435\u043c \u043d\u0435 \u043c\u0435\u043d\u0435\u0435 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0445 \u0442\u0440\u0451\u0445 \u0443\u043f\u043e\u043c\u044f\u043d\u0443\u0442\u044b\u0445 \u0420\u041d \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u0435\u0432\u044b\u0448\u0430\u0435\u0442 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c \u0420\u041d \u041f\u0440\u043e\u0442\u043e\u043d-\u041c, \u0438 \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u043d\u0438 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0435 \u043a\u043e\u043d\u043a\u0443\u0440\u0438\u0440\u0443\u044e\u0442 \u0441 \u041f\u0440\u043e\u0442\u043e\u043d\u043e\u043c \u043d\u0430 \u0440\u044b\u043d\u043a\u0435 \u043a\u043e\u043c\u043c\u0435\u0440\u0447\u0435\u0441\u043a\u0438\u0445 \u0437\u0430\u043f\u0443\u0441\u043a\u043e\u0432[145].\\n\u0412\u043e\u043f\u0440\u043e\u0441: \u041a\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u042f\u043f\u043e\u043d\u0441\u043a\u0438\u0439 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c?\\n\u041e\u0442\u0432\u0435\u0442: <extra_id_0>'\n---\n```\nimport torch\nimport transformers\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nt5_tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"AlexWortega/FlanFred\")\nt5_model = transformers.T5ForConditionalGeneration.from_pretrained(\"AlexWortega/FlanFred\")\n\ndef generate_text(input_str, tokenizer, model, device, max_length=50):\n    # encode the input string to model's input_ids\n    input_ids = tokenizer.encode(input_str, return_tensors='pt').to(device)\n    \n    # generate text\n    with torch.no_grad():\n        outputs = model.generate(input_ids=input_ids, max_length=max_length, num_return_sequences=1, temperature=0.7, do_sample=True)\n    \n    # decode the output and return the text\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# usage:\ninput_str = \"Hello, how are you?\"\nprint(generate_text(input_str, t5_tokenizer, t5_model, device))\n```\n\n# Metrics:\n```\n| Metric        | flanfred | siberianfred  | fred  |\n| ------------- | ----- |------ |----- |\n| xnli_en       | 0.51   |0.49  |0.041 |\n| xnli_ru       | 0.71   |0.62 |0.55 |\n| xwinograd_ru  | 0.66   |0.51 |0.54 |\n```\n\n# Citation\n```\n@MISC{AlexWortega/flan_translated_300k,\n    author  = {Pavel Ilin, Ksenia Zolian,Ilya kuleshov, Egor Kokush, Aleksandr Nikolich},\n    title   = {Russian Flan translated},\n    url     = {https://huggingface.co/datasets/AlexWortega/flan_translated_300k},\n    year    = 2023\n}\n```", "size_bytes": "6961641025", "downloads": 23}
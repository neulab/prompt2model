{"pretrained_model_name": "Marc/pegasus_xsum_gigaword", "description": "---\nlanguage: \n- English\n-\nthumbnail: \ntags:\n- \n-\n- \nlicense: \ndatasets:\n- XSUM \n- Gigaword\nmetrics:\n- Rouge\n-\n---\n\n# Pegasus XSUM Gigaword\n\n## Model description\n\nPegasus XSUM model finetuned to Gigaword Summarization task, significantly better performance than pegasus gigaword, but still doesn't match model paper performance.  \n\n## Intended uses & limitations\nProduces short summaries with the coherence of the XSUM Model\n#### How to use\n\n```python\n# You can include sample code which will be formatted\n```\n\n#### Limitations and bias\n\nStill has all the biases of any of the abstractive models, but seems a little less prone to hallucination. \n## Training data\n\nInitialized with pegasus-XSUM\n\n## Training procedure\n\nTrained for 11500 iterations on Gigaword corpus using OOB seq2seq (from hugging face using the default parameters)\n\n## Eval results\nEvaluated on Gigaword test set (from hugging face using the default parameters)\nrun_summarization.py     --model_name_or_path pegasus-xsum/checkpoint-11500/   --do_predict   --dataset_name gigaword   --dataset_config \"3.0.0\"     --source_prefix \"summarize: \"     --output_dir  pegasus-xsum    --per_device_train_batch_size=8     --per_device_eval_batch_size=8     --overwrite_output_dir     --predict_with_generate\n\n| Metric      | Score |\n| ----------- | ----------- |\n| eval_rouge1 | 34.1958   |\n| eval_rouge2 | 15.4033   |\n| eval_rougeL | 31.4488  |\n\n\nrun_summarization.py     --model_name_or_path google/pegasus-gigaword   --do_predict   --dataset_name gigaword   --dataset_config \"3.0.0\"     --source_prefix \"summarize: \"     --output_dir  pegasus-xsum    --per_device_train_batch_size=8     --per_device_eval_batch_size=8     --overwrite_output_dir     --predict_with_generate\n\n| Metric      | Score |\n| ----------- | ----------- |\n| eval_rouge1 | 20.8111   |\n| eval_rouge2 | 8.766   |\n| eval_rougeL | 18.4431  |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@inproceedings{...,\n  year={2020}\n}\n```\n", "size_bytes": "2279640522", "downloads": 7}
{"pretrained_model_name": "eunyounglee/mBART_translator_kobart_2", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: mBART_translator_kobart_2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mBART_translator_kobart_2\n\nThis model is a fine-tuned version of [hyunwoongko/kobart](https://huggingface.co/hyunwoongko/kobart) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0971\n- Bleu: 34.7406\n- Gen Len: 19.863\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 0.7514        | 1.0   | 1912  | 0.3009          | 33.1304 | 19.8465 |\n| 0.4925        | 2.0   | 3824  | 0.2213          | 33.7817 | 19.8541 |\n| 0.3893        | 3.0   | 5736  | 0.1789          | 34.1249 | 19.8577 |\n| 0.3328        | 4.0   | 7648  | 0.1555          | 34.3074 | 19.8598 |\n| 0.2896        | 5.0   | 9560  | 0.1375          | 34.4127 | 19.8585 |\n| 0.2614        | 6.0   | 11472 | 0.1240          | 34.5061 | 19.8604 |\n| 0.2292        | 7.0   | 13384 | 0.1116          | 34.6476 | 19.8632 |\n| 0.2098        | 8.0   | 15296 | 0.1050          | 34.6956 | 19.863  |\n| 0.1967        | 9.0   | 17208 | 0.0995          | 34.7304 | 19.863  |\n| 0.1812        | 10.0  | 19120 | 0.0971          | 34.7406 | 19.863  |\n\n\n### Framework versions\n\n- Transformers 4.23.0\n- Pytorch 1.12.1+cu113\n- Datasets 2.5.2\n- Tokenizers 0.13.1\n", "size_bytes": "495646265", "downloads": 4}
{"pretrained_model_name": "AI4PD/REXzyme", "description": "---\nlicense: apache-2.0\npipeline_tag: translation\ntags:\n- chemistry\n- biology\ninference: false\n---\n\n# **Contributors**\n\n- Sebastian Lindner (GitHub [@Bienenwolf655](https://github.com/Bienenwolf655); Twitter [@lindner_seb](https://twitter.com/lindner_seb)) \n- Michael Heinzinger (GitHub [@mheinzinger](https://github.com/mheinzinger); Twitter [@HeinzingerM](https://twitter.com/HeinzingerM))\n- Noelia Ferruz (GitHub [@noeliaferruz](https://github.com/noeliaferruz); Twitter [@ferruz_noelia](https://twitter.com/ferruz_noelia); Webpage: [www.aiproteindesign.com](https://www.aiproteindesign.com) )\n\n\n# **REXzyme: A Translation Machine for the Generation of New-to-Nature Enzymes**\n**Work in Progress**\n\nREXzyme (Reaction to Enzyme) (manuscript in preparation) is a translation machine -similar to Google Translator-\nfor the generation of enzymes that catalize user-defined reactions.\n\n![Inference of REXzyme](./rexzyme3.png)\n\nIt is possible to provide fine-grained input at the substrate level. \nAkin to how translation machines have learned to translate between complex language pairs with great success, \noften diverging in their representation at the character level (Japanese - English), we posit that an advanced architecture will\nbe able to translate between the chemical and sequence spaces. REXzyme was trained on a set of 2480 reactions \nand ~32M enzyme pairs and it produces sequences that are predicted to perform their intended reactions. A second \nversion of the model with 14k more reactions will be uploaded to this repository shortly.\n\n you will need to provide a reaction in the SMILES format\n(Simplified molecular-input line-entry system). A useful online server to convert from molecules to SMILES\ncan be found here: https://cactus.nci.nih.gov/chemical/structure.\n\nAfter converting each of the reaction components you should combine them in the following scheme: ```ReactantA.ReactantB>AgentA>ProductA.ProductB```<br/>\nAdditionally, one should prepend the task suffix ```r2s``` and append the eos token ```</s>```\ne.g. for the carbonic anhydrase reaction: ```r2sO.COO>>HCOOO.[H+]</s>```\n\nWe provide this python script to convert reactants to the required reaction format, but \nwe always recommend to draw and double-check the structures in a server like [cactus](https://cactus.nci.nih.gov/chemical/structure)\n\n```python\n#  left reactants (seperated by '.') seperated by a equal sign from the products (also seperated by '.')\nreactions =  \"CO2 . H2O =  carbonic acid . H+\"\n# agents (seperated by .) \nagent = \"\"\n\n# https://stackoverflow.com/questions/54930121/converting-molecule-name-to-smiles\nfrom urllib.request import urlopen\nfrom urllib.parse import quote\n\ndef CIRconvert(ids):\n    try:\n        url = 'http://cactus.nci.nih.gov/chemical/structure/' + quote(ids) + '/smiles'\n        ans = urlopen(url).read().decode('utf8')\n        return ans\n    except:\n        return 'Did not work'\n\nreagent = [CIRconvert(i) for i in reactions.replace(' ','').split('=')[0].split('.') if i != \"\"]\nagent = [CIRconvert(i) for i in agent.replace(' ','').split('.') if i != \"\"]\nproduct = [CIRconvert(i) for i in reactions.replace(' ','').split('=')[1].split('.') if i != \"\"]\nf\"r2s{'.'.join(reagent)}>{'.'.join(agent)}>{'.'.join(product)}</s>\"\n```\n\nWe are still working in the analysis of the model for different tasks, including experimental testing. \nSee below in this documentation information about the models' performance in different in-silico tasks and how to generate your own enzymes. \n\n## **Model description**\n\nREXzyme is based on the [Efficient T5 Large Transformer](https://huggingface.co/google/t5-efficient-large) architecture (which in turn is very similar to the current version of Google Translator)\nand contains 48 (24 encoder/ 24 decoder) layers with a model dimensionality of 1024, totaling 770 million parameters. \n\nREXzyme is a translation machine trained on portion the [RHEA database](https://www.rhea-db.org/) containing 31,970,152 reaction-enzyme pairs. \nA second dataset with >14k reactions is being trained and will be uploaded soon.\nThe pre-training was done on pairs of SMILES and amino acid sequences, tokenized with a char-level \nSentencepiece tokenizer. Note that two seperate tokenizers were used for input (./tokenizer_smiles) and labels (./tokenizer_aa).\n\nREXzyme was pre-trained with a supervised translation objective  i.e., the model learned to process the continous\nrepresentation of the reaction from the encoder to autoregressively (causual language modeling) produce the output. \nThe output tokens (amino acids) are generated one at a time, from left to right, and the model learns to match the original enzyme sequence.\nHence, the model learns the dependencies among protein sequence features that enable a specific enzymatic reaction.\n\nThere are stark differences in the number of members among reaction classes.\nHowever, since we are tokenizing the reaction SMILES on a character level, \nthe model has learnt dependencies among molecules and enzyme sequence features, and it can transfer learning from more to less populated\nreaction classes.\n\n## **Model Performance**\n\n- **Dataset curation**\nWe converted the reactions from rxn format to smile string including only left-to-right reactions.\nThe enzyme sequences were truncated to 1024.\nEnzymes catalyzing more than one reaction appear in multiple enzyme-reaction pairs.\n<br/><br/>\n- **General descriptors**\n\n    | Method                | Natural     | Generated <sup>[1]</sup> |\n    | :---                  |    :----:   |          ---:            |\n    | **IUPRED3 (ordered)** | 99.9%       | 99.9%                    |\n    | **ESMFold (avg. plddt)**            | 85.03       | 79.82  |\n    | **FlDPnn**            | 0.0878      | 0.0929                  |\n<sup>[1]|</sup> We excluded sequences with %identities \u2265 70% and pLDDTs < 60%.\n<br/><br/>\n\n\n- **Functional classification**\n<br/><br/>\n<table>\n  <tr>\n    <td><b>Method </b></td>\n    <td colspan=\"2\"> <a href=\"https://google-research.github.io/proteinfer/\">ProteInfer</a></td>\n    <td colspan=\"2\"> <a href=\"https://www.science.org/doi/10.1126/science.adf2465\">CLEAN</a></td>\n  </tr>\n  <tr>\n    <td><b>Dataset</b></td>\n    <td >Natural (%) </td>\n    <td >Generated (%) </td>\n    <td >Natural (%) </td>\n    <td >Generated (%) </td>\n  </tr>\n    <tr>\n    <td><b> EC: Level 1 </b></td>\n    <td >81</td>\n    <td >80</td>\n    <td >80</td>\n    <td >79</td>\n  </tr>\n    <tr>\n    <td><b> EC: Level 2 </b></td>\n    <td >78</td>\n    <td >77</td>\n    <td >79</td>\n    <td >78</td>\n  </tr>\n    <tr>\n    <td><b> EC: Level 3 </b></td>\n    <td >76</td>\n    <td >75</td>\n    <td >78</td>\n    <td >77</td>\n  </tr>\n    <tr>\n    <td><b> EC: Level 4 </b></td>\n    <td >62</td>\n    <td >58</td>\n    <td >70</td>\n    <td >65</td>\n  </tr>\n    <tr>\n    <td><b> No EC predicted </b></td>\n    <td >10</td>\n    <td >7</td>\n    <td >0</td>\n    <td >0</td>\n  </tr>\n    <tr>\n    <td><b> GO-Terms </b></td>\n    <td >41</td>\n    <td >39</td>\n    <td >-</td>\n    <td >-</td>\n  </tr>\n    <tr>\n    <td><b> No GO predicted </b></td>\n    <td >1</td>\n    <td >1</td>\n    <td >-</td>\n    <td >-</td>\n  </tr>\n</table>\n<br/><br/>\n- **PGP pipeline** [(see GitHub)](https://github.com/hefeda/PGP)\n\n    | Method      | Natural | Generated |\n    | :---        | :----   |      :--- |\n    | **Disorder**      | 11.473       | 11.467   |\n    | **DSSP3**   | L: 42%, H: 41%, E:18% | L: 45%, H: 39%, E: 16%|\n    | **DSSP8**   | C:25%, H:38% T:10%, S:5%, I:0%, E:19%, G:2%, B:0% | C:29%, H:38% T:10%, S:4%, I:0%, E:17%, G:3%, B:0%|\n    | **CATH Classes**   | Mainly Beta: 6%, Alpha Beta: 78%, Mainly Alpha: 16%, Special: 0%, Few Secondary Structures: 0% |  Mainly Beta: 4%, Alpha Beta: 87%, Mainly Alpha: 9%, Special: 0%, Few Secondary Structures: 0%|\n    | **Transmembrane Prediction**  | Membrane: 9%, Soluble: 91% | Membrane: 9%, Soluble: 91%|\n    | **Conservation**      | High: 37%, Low: 33%      | High: 38%, Low: 33%  |\n    | **Localization**   | Cytop.: 66%, Nucleus: 4%, Extracellular: 6%, PM: 4%, ER: 11%, Lysosome/Vacuole: 1%, Mito.: 6%, Plastid: 1%, Golgi: 1%, Perox.: 1% | Cytop.: 85%, Nucleus: 2%, Extracellular: 6%, PM: 1%, ER: 6%, Lysosome/Vacuole: 0%, Mito.: 4%, Plastid: 0%, Golgi: 0%, Perox.: 0%|\n<br/><br/>\n\n\n## **How to generate from REXzyme**\nREXzyme can be used with the HuggingFace transformer python package.\nDetailed installation instructions can be found [here](https://huggingface.co/docs/transformers/installation).\n\nSince REXzyme has been trained on the objective of machine translation, users have to specify a chemical reaction, specified in the format of SMILES.\n\nDisclaimer: Although the perplexity gets computed here it is not the best selection criteria.\nUsually the BLEU score is deployed for translation evaluation, \nbut this score would enforce a high sequence similarity (thus not *de novo* design, which is what we tend to go for). \nWe recommend generating many sequences and selecting them by plDDT, as well as other metrics.\n\n```python\nfrom datasets import load_from_disk\nfrom transformers import AutoTokenizer\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport math\nimport torch\nfrom tqdm import tqdm\nimport pickle\ntokenizer_aa = AutoTokenizer.from_pretrained('/path/to//tokenizer_aa')\ntokenizer_smiles = AutoTokenizer.from_pretrained('/path/to//tokenizer_smiles')\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"/path/to/REXzyme\").cuda()\nprint(model.generation_config)\nreactions = [\"NC1=NC=NC2=C1N=CN2[C@@H]1O[C@H](COP(=O)([O-])OP(=O)([O-])OP(=O)([O-])[O-])[C@@H](O)[C@H]1O.*N[C@@H](CO)C(*)=O>>NC1=NC=NC2=C1N=CN2[C@@H]1O[C@H](COP(=O)([O-])OP(=O)([O-])[O-])[C@@H](O)[C@H]1O.[H+].*N[C@@H](COP(=O)([O-])[O-])C(*)=O\"]\n\ndef calculatePerplexity(inputs,model):\n    '''Function to compute perplexity'''\n    a=tokenizer_aa.decode(inputs)\n    b=tokenizer_aa(a, return_tensors=\"pt\").input_ids.to(device='cuda')\n    b = torch.stack([[b[b!=tokenizer_aa.pad_token_id]] for label in b][0])\n    with torch.no_grad():\n        outputs = model(b, labels=b)\n    loss, logits = outputs[:2]\n    return math.exp(loss)\n\n\nfor idx,i in tqdm(enumerate(reactions)):\n    input_ids = tokenizer_smiles(f\"r2s{i}</s>\", return_tensors=\"pt\").input_ids.to(device='cuda')\n    print(f'Generating for {i}')\n    ppls_total = []\n    for _ in range(4):\n        outputs = model.generate(input_ids,\n                top_k=15,\n                top_p = 0.92,\n                repetition_penalty=1.2,\n                max_length=1024,\n                do_sample=True,\n                num_return_sequences=25)\n        ppls = [(tokenizer_aa.decode(output,skip_special_tokens=True), calculatePerplexity(output, model),len(tokenizer_aa.decode(output))) for output in tqdm(outputs)]\n        ppls_total.extend(ppls)\n```\n\n## **A word of caution** \n\n- We have not yet fully tested the ability of the model for the generation of new-to-nature enzymes, i.e.,\n  with chemical reactions that do not appear in Nature (and hence neither in the training set). While this is the intended objective of our work, it is very much work in progress. We'll uptadate the model and documentation shortly.", "size_bytes": "2950844807", "downloads": 16}
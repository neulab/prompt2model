{"pretrained_model_name": "sharad/ParaphraseGPT", "description": "---\nlicense: apache-2.0\ndatasets:\n- humarin/chatgpt-paraphrases\nlanguage:\n- en\ntags:\n- paraphrase\n- similar text\n---\nThis model re-fine-tunes the [ChatGPT Paraphraser on T5 Base](https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base) with additional Google PAWS dataset.\n\n## Usage example\n```python\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n\n#'cuda' for gpu otherwise use 'cpu'\ndevice = \"cuda\"\nmodel     = AutoModelForSeq2SeqLM.from_pretrained(\"sharad/ParaphraseGPT\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\npredict   = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n\ndef paraphrase(sentence):\n  generated = predict(\n              sentence,\n              num_beams=3,\n              num_beam_groups=3,\n              num_return_sequences=1,\n              diversity_penalty=2.0,\n              no_repeat_ngram_size=2,\n              repetition_penalty=0.99,\n              max_length=len(sentence)\n          )\n  return generated\n\noutput = paraphrase('My sentence to paraphrase...')\nprint(output[0]['generated_text'])\n```\n\n## Train parameters\n```python\nepochs = 4\nmax_length = 128\nlr = 5e-5\n```", "size_bytes": "891702929", "downloads": 323}
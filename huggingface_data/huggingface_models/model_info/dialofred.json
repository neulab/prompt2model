{"pretrained_model_name": "TeraSpace/dialofred", "description": "---\nlicense: mit\nwidget:\n- text: |-\n    <SC1>- \u043a\u0430\u043a \u0442\u044b?\n    - <extra_id_0>\n  example_title: how r u\nlanguage:\n- ru\npipeline_tag: text2text-generation\n---\n\n# Usage\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ndevice='cuda'\ntokenizer = AutoTokenizer.from_pretrained('TeraSpace/dialofred')\nmodel = AutoModelForSeq2SeqLM.from_pretrained('TeraSpace/dialofred').to(device)\nwhile True:\n    text_inp = input(\"=>\")\n    lm_text=f'<SC1>- {text_inp}\\n- <extra_id_0>'\n    input_ids=torch.tensor([tokenizer.encode(lm_text)]).to(device)\n    # outputs=model.generate(input_ids=input_ids,\n    #                                 max_length=200,\n    #                                 eos_token_id=tokenizer.eos_token_id,\n    #                                 early_stopping=True,\n    #                                 do_sample=True,\n    #                                 temperature=1.0,\n    #                                 top_k=0,\n    #                                 top_p=0.85)\n    # outputs=model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,early_stopping=True)\n    outputs=model.generate(input_ids=input_ids,\n                                    max_length=200,\n                                    eos_token_id=tokenizer.eos_token_id,\n                                    early_stopping=True,\n                                    do_sample=True,\n                                    temperature=0.7,\n                                    top_k=0,\n                                    top_p=0.8)\n    \n    print(tokenizer.decode(outputs[0][1:]))\n```", "size_bytes": "6961604161", "downloads": 23}
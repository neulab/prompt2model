{"pretrained_model_name": "megagonlabs/t5-base-japanese-web-8k", "description": "---\nlanguage: ja\ntags:\n- t5\n- text2text-generation\n- seq2seq\nlicense: apache-2.0\ndatasets:\n- mc4\n- wiki40b\n---\n\n# t5-base-japanese-web-8k (with Byte-fallback, 8K)\n\n## Description\n\n[megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k) is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.  \nTraining codes are [available on GitHub](https://github.com/megagonlabs/t5-japanese).\n\nThe vocabulary size of this model is 8K.\n[32K version is also available](https://huggingface.co/megagonlabs/t5-base-japanese-web).\n\n### Corpora\n\nWe used following corpora for pre-training.\n\n- Japanese in [mC4/3.0.1](https://huggingface.co/datasets/mc4) (We used [Tensorflow native format](https://github.com/allenai/allennlp/discussions/5056))\n    - 87,425,304 pages\n    - 782 GB in TFRecord format\n- [Japanese](https://www.tensorflow.org/datasets/catalog/wiki40b#wiki40bja) in [wiki40b/1.3.0](https://www.tensorflow.org/datasets/catalog/wiki40b)\n    - 828,236 articles (2,073,584 examples)\n    - 2 GB in TFRecord format\n\n### Tokenizer\n\nWe used Japanese Wikipedia to train [SentencePiece](https://github.com/google/sentencepiece).\n\n- Vocabulary size: 8,000\n- [Byte-fallback](https://github.com/google/sentencepiece/releases/tag/v0.1.9): Enabled\n\n### Parameters\n\n- T5 model: [models/t5.1.1.base.gin](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/gin/models/t5.1.1.base.gin)\n- Training steps: 1,000,000\n\nIt took about 126 hours with TPU v3-8\n\n## Related models\n\n- [\u65e5\u672c\u8a9eT5\u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb (sonoisa/t5-base-japanese)](https://huggingface.co/sonoisa/t5-base-japanese)\n- [\u65e5\u672c\u8a9eT5\u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb (sonoisa/t5-base-japanese-mC4-Wikipedia)](https://huggingface.co/sonoisa/t5-base-japanese-mC4-Wikipedia)\n\n## License\n\nApache License 2.0\n\n## Citations\n\n- mC4\n\nContains information from `mC4` which is made available under the [ODC Attribution License](https://opendatacommons.org/licenses/by/1-0/).\n\n```bibtex\n@article{2019t5,\n    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n    journal = {arXiv e-prints},\n    year = {2019},\n    archivePrefix = {arXiv},\n    eprint = {1910.10683},\n}\n```\n\n- wiki40b\n\n```bibtex\n@inproceedings{49029,\ntitle = {Wiki-40B: Multilingual Language Model Dataset},\nauthor = {Mandy Guo and Zihang Dai and Denny Vrandecic and Rami Al-Rfou},\nyear = {2020},\nbooktitle   = {LREC 2020}\n}\n```\n", "size_bytes": "842585165", "downloads": 41}
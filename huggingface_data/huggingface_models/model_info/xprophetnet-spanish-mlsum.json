{"pretrained_model_name": "IIC/xprophetnet-spanish-mlsum", "description": "---\nlanguage:\n- es\ntags:\n- summarization\nlicense: apache-2.0\ndatasets:\n- mlsum\nmetrics:\n- rouge1\n- rouge2\n- rougeL\n- rougeLsum\n\nmodel-index:\n- name: xprophetnet-spanish-mlsum\n  results:\n  - task: \n      type: summarization \n      name: abstractive summarization  \n    dataset:\n      type: mlsum\n      name: mlsum-es\n      args: es         \n    metrics:\n      - type: rouge1    \n        value: 25.1158  \n        name: rouge1    \n      - type: rouge2\n        value: 8.4847\n        name: rouge2\n      - type: rougeL\n        value: 20.6184\n        name: rougeL\n      - type: rougeLsum\n        value: 20.8948\n        name: rougeLsum\n---\n\nThis is a model for text summarization in Spanish. It has been trained on the spanish portion of [mlsum](https://huggingface.co/datasets/mlsum). For that, [XLM-ProphetNet (a multilingual version of Prophetnet)](https://huggingface.co/microsoft/xprophetnet-large-wiki100-cased) was used. \n\nFor tuning the hyperparameters  of the model we used [Optuna](https://optuna.org/), with only 10 different trials and 7 initial random trials, as the dataset chosen for training the model was huge. The set of hyperparameters used was the following:\n\n```python\n\n    def hp_space(trial):\n        return {\n            \"learning_rate\": trial.suggest_float(\n                \"learning_rate\", 1e-5, 7e-5, log=True\n            ),\n            \"num_train_epochs\": trial.suggest_categorical(\n                \"num_train_epochs\", [3, 5, 7, 10]\n            ),\n            \"per_device_train_batch_size\": trial.suggest_categorical(\n                \"per_device_train_batch_size\", [16]),\n            \"per_device_eval_batch_size\": trial.suggest_categorical(\n                \"per_device_eval_batch_size\", [32]),\n            \"gradient_accumulation_steps\": trial.suggest_categorical(\n                \"gradient_accumulation_steps\", [2, 4, 8]),\n            \"warmup_steps\": trial.suggest_categorical(\n                \"warmup_steps\", [50, 100, 500, 1000]\n            ),\n            \"weight_decay\": trial.suggest_float(\n                 \"weight_decay\", 0.0, 0.1\n            ),\n```\n\nThe reported results are on the test split of mlsum. Complete metrics are:\n\n```json\n{\"rouge1\": 25.1158, \"rouge2\": 8.4847, \"rougeL\": 20.6184, \"rougeLsum\": 20.8948, \"gen_len\": 19.6496}\n```\n\nThis model is really easy to use, and with the following lines of code you can just start summarizing your documents in Spanish:\n\n```python\nfrom transformers import ProphetNetForConditionalGeneration, AutoTokenizer\n\ntext = \"Hola esto es un ejemplo de texto a resumir. Poco hay que resumir aqu\u00ed, pero es s\u00f3lo de muestra.\"\nmodel_str = \"avacaondata/xprophetnet-spanish-mlsum\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_str)\nmodel = ProphetNetForConditionalGeneration.from_pretrained(model_str)\n\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n### Contributions\nThanks to [@avacaondata](https://huggingface.co/avacaondata), [@alborotis](https://huggingface.co/alborotis), [@albarji](https://huggingface.co/albarji), [@Dabs](https://huggingface.co/Dabs), [@GuillemGSubies](https://huggingface.co/GuillemGSubies) for adding this model.", "size_bytes": "2464529169", "downloads": 26}
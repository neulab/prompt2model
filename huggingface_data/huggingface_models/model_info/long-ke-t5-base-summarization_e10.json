{"pretrained_model_name": "KETI-AIR-Downstream/long-ke-t5-base-summarization_e10", "description": "---\ntags:\n- generated_from_trainer\ndatasets:\n- jsonl_dataset_sum.py\nmetrics:\n- rouge\nmodel-index:\n- name: summarization_all\n  results:\n  - task:\n      name: Summarization\n      type: summarization\n    dataset:\n      name: jsonl_dataset_sum.py\n      type: jsonl_dataset_sum.py\n      config: 'null'\n      split: None\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 21.9857\nwidget:\n- text: \"summarization-num_lines-1: \ud604\ub300\uc790\ub3d9\ucc28\ub294 18\uc77c(\ud604\uc9c0 \uc2dc\uac04) \uc774\ud0c8\ub9ac\uc544 \ub808\uc774\ud06c \ucf54\ubaa8\uc5d0\uc11c \uac1c\ucd5c\ub41c '\ud604\ub300 \ub9ac\uc720\ub2c8\uc628' \ud589\uc0ac\uc5d0\uc11c '\ud3ec\ub2c8 \ucfe0\ud398 \ucf58\uc149\ud2b8' \ubcf5\uc6d0 \ubaa8\ub378\uc744 \uc138\uacc4\uc5d0 \uccab \uacf5\uac1c\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud504\ub85c\uc81d\ud2b8\ub294 \ud604\ub300\ucc28\uc758 \ucc3d\uc5c5\uc790\uc778 \uc815\uc8fc\uc601 \uc120\ub300 \ud68c\uc7a5\uc758 \uc218\ucd9c\ubcf4\uad6d(\u8f38\u51fa\u5831\u570b) \uc815\uc2e0\uacfc \ud3ec\ub2c8 \ucfe0\ud398\ub97c \ud1b5\ud55c \uae00\ub85c\ubc8c \ube0c\ub79c\ub4dc \uc815\ub9bd\uc5d0 \ub300\ud55c \ub04a\uc784\uc5c6\ub294 \uc5f4\uc815\uacfc \ub3c4\uc804 \uc815\uc2e0\uc744 \uc7ac\uc870\uba85\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4. \ud604\ub300\ucc28\uc5d0 \ub530\ub974\uba74, \uc774\ubc88 \ud604\ub300 \ub9ac\uc720\ub2c8\uc628 \ud589\uc0ac\ub294 \ud68c\uc0ac\uc758 \uc5ed\uc0ac\ub97c \ub2e4\uc2dc \ub3cc\uc544\ubcf4\uba70 \ubcc0\ud558\uc9c0 \uc54a\ub294 \ubbf8\ub798 \uc9c0\ud5a5\uc801\uc778 \ube44\uc804\uacfc \ubc29\ud5a5\uc131\uc744 \uacf5\uc720\ud558\ub294 \ube0c\ub79c\ub4dc \uc720\uc0b0 \ud589\uc0ac\uc785\ub2c8\ub2e4.\"\n  example_title: \"sample 1\"\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# summarization_all\n\nThis model is a fine-tuned version of [KETI-AIR/long-ke-t5-base](https://huggingface.co/KETI-AIR/long-ke-t5-base) on the jsonl_dataset_sum.py dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1442\n- Rouge1: 21.9857\n- Rouge2: 10.2876\n- Rougel: 21.4026\n- Rougelsum: 21.4278\n- Gen Len: 86.2560\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.001\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 8\n- total_train_batch_size: 8\n- total_eval_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10.0\n\n### Training results\n\n| Training Loss | Epoch | Step    | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:-------:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.2503        | 1.0   | 184670  | 1.2439          | 20.2525 | 9.1467  | 19.7454 | 19.771    | 87.1766 |\n| 1.1629        | 2.0   | 369340  | 1.1773          | 21.0068 | 9.6691  | 20.4565 | 20.4888   | 89.6074 |\n| 1.1087        | 3.0   | 554010  | 1.1431          | 21.0216 | 9.6545  | 20.489  | 20.5108   | 85.5895 |\n| 1.056         | 4.0   | 738680  | 1.1247          | 21.6776 | 10.1424 | 21.09   | 21.1168   | 89.6576 |\n| 1.0199        | 5.0   | 923350  | 1.1179          | 21.6563 | 10.0965 | 21.0814 | 21.1056   | 89.2454 |\n| 0.9652        | 6.0   | 1108020 | 1.1122          | 21.6209 | 10.0725 | 21.0623 | 21.0864   | 86.7079 |\n| 0.92          | 7.0   | 1292690 | 1.1136          | 21.9396 | 10.2734 | 21.3465 | 21.3745   | 86.5547 |\n| 0.8804        | 8.0   | 1477360 | 1.1228          | 21.8457 | 10.1858 | 21.2552 | 21.278    | 87.6413 |\n| 0.8447        | 9.0   | 1662030 | 1.1327          | 21.92   | 10.2635 | 21.3415 | 21.3633   | 86.4453 |\n| 0.7678        | 10.0  | 1846700 | 1.1442          | 21.9857 | 10.2876 | 21.4026 | 21.4278   | 86.2560 |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.12.0\n- Datasets 2.8.0\n- Tokenizers 0.13.2\n", "size_bytes": "1186882355", "downloads": 6}
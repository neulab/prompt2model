{"pretrained_model_name": "Kateryna/eva_ru_forum_headlines", "description": "---\nlanguage:\n- ru\n\nwidget:\n- text: \"\u0426\u0435\u043b\u044c \u043e\u0434\u043d\u0430 - \u0438\u0441\u0442\u0440\u0435\u0431\u043b\u0435\u043d\u0438\u0435 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0431\u043e\u043b\u044c\u0448\u0435 \u0441\u043b\u0430\u0432\u044f\u043d\u0441\u043a\u0438\u0445 \u043d\u0430\u0440\u043e\u0434\u043e\u0432. \u041d\u0430 \u043e\u0447\u0435\u0440\u0435\u0434\u0438 \u043f\u043e\u043b\u044f\u043a\u0438, \u043e\u043d\u0438 \u0442\u043e\u0436\u0435 \u0441\u043b\u0430\u0432\u044f\u043d\u0435, \u0438\u0445 \u0442\u043e\u0436\u0435 \u043d\u0430 \u0443\u0442\u0438\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c. \u042d\u0442\u043e \u0426\u0435\u043b\u044c \u041d\u0410\u0422\u041e. \u041d\u0443 \u0438 \u0437\u0430\u043e\u0434\u043d\u043e \u0440\u0430\u0437\u0440\u0443\u0448\u0435\u043d\u0438\u0435 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0438 \u0415\u0421, \u043d\u0443 \u0438 \u041a\u0438\u0442\u0430\u0439 \u0434\u043e\u0442 \u043a\u0443\u0447\u0438 \u043f\u043e\u0434 \u043f\u043b\u0438\u043d\u0442\u0443\u0441 \u0437\u0430\u0433\u043d\u0430\u0442\u044c.\"\n- text: \"\u0414\u043e\u0447\u043a\u0435 15, \u043a\u043d\u0438\u0433 \u043d\u0435 \u0447\u0438\u0442\u0430\u0435\u0442, \u0432\u0441\u044f \u0436\u0438\u0437\u043d\u044c (\u0432\u043d\u0435 \u0448\u043a\u043e\u043b\u044b) \u0432 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0435 \u043d\u0430 \u043a\u0440\u043e\u0432\u0430\u0442\u0438. \u041b\u044e\u0431\u043e\u0437\u043d\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043d\u043e\u043b\u044c. \u041a\u0443\u0434\u0430-\u0442\u043e \u043f\u043e\u0435\u0445\u0430\u0442\u044c \u0432 \u043d\u043e\u0432\u043e\u0435 \u043c\u0435\u0441\u0442\u043e, \u0443\u0437\u043d\u0430\u0442\u044c \u0447\u0442\u043e-\u0442\u043e, \u043d\u0430\u0439\u0442\u0438 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0435 \u043a\u0443\u0440\u0441\u044b - \u0432\u043e\u043e\u0431\u0449\u0435 \u043d\u0435 \u043f\u0440\u043e \u043d\u0435\u0435. \u0423\u0447\u0435\u0431\u0430 \u0432\u0441\u0435 \u0445\u0443\u0436\u0435, \u0431\u0430\u0433\u0430\u0436\u0430 \u0437\u043d\u0430\u043d\u0438\u0439 \u0443\u0436\u0435 \u043d\u0435\u0442, \u0441\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442 \u0438 \u0432\u044b\u043a\u0440\u0443\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0432 \u0442\u0435\u0447\u0435\u043d\u0438\u0435 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u0438, \u043a\u0430\u043a \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u0430\u044f \u0438\u043b\u0438 \u0447\u0442\u043e-\u0442\u043e \u043f\u043e\u0441\u0435\u0440\u044c\u0435\u0437\u043d\u0435\u0435, \u0433\u0434\u0435 \u043d\u0435 \u0441\u043f\u0438\u0441\u0430\u0442\u044c - \u043d\u0430 2-3. \u041f\u0440\u0438 \u043b\u044e\u0431\u043e\u0439 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043d\u0435 \u0445\u043e\u0434\u0438\u0442 \u0432 \u0448\u043a\u043e\u043b\u0443 (\u0433\u043e\u043b\u043e\u0432\u0430 \u0431\u043e\u043b\u0438\u0442, \u043c\u043e\u0436\u043d\u043e \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u043d\u0435 \u043f\u043e\u0439\u0434\u0443. \u0430 \u043f\u043e\u0442\u043e\u043c \u043f\u044f\u0442\u043d\u0438\u0446\u0430, \u0447\u0442\u043e \u043d\u0430 \u043e\u0434\u0438\u043d \u0434\u0435\u043d\u044c \u0445\u043e\u0434\u0438\u0442\u044c...)\"\n- \"\u0420\u0435\u0431\u0451\u043d\u043e\u043a \u0443\u0447\u0438\u0442\u0441\u044f \u0432 8 \u043a\u043b\u0430\u0441\u0441\u0435. \u041f\u043e \u0430\u043b\u0433\u0435\u0431\u0440\u0435 \u043e\u0434\u043d\u0438 \u0442\u0440\u043e\u0439\u043a\u0438. \u041d\u043e \u044d\u0442\u043e \u0442\u043e\u0447\u043d\u043e 2. \u041f\u0440\u043e\u0441\u0442\u043e \u0443\u0447\u0438\u0442\u0435\u043b\u044c \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0432 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u0438 2. \u041e\u043d\u0430 \u0433\u0443\u043c\u0430\u043d\u0438\u0442\u0430\u0440\u0438\u0439. \u0410\u043b\u0433\u0435\u0431\u0440\u0430 \u043d\u0438\u043a\u0430\u043a \u043d\u0435 \u0438\u0434\u0451\u0442. \u0420\u0435\u043f\u0435\u0442\u0438\u0442\u043e\u0440 \u0441\u0435\u0439\u0447\u0430\u0441 \u0437\u0430\u043d\u0438\u043c\u0430\u0435\u0442\u0441\u044f, \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u043b\u0451\u0433\u043a\u0438\u0435 \u0442\u0435\u043c\u044b. \u042f \u0431\u043e\u044e\u0441\u044c, \u0447\u0442\u043e \u043f\u0440\u043e\u0432\u0430\u043b\u0438\u0442 \u041e\u0413\u042d. \u0422\u0430\u043c \u043f\u0435\u0440\u0435\u0441\u0434\u0430\u0442\u044c \u043c\u043e\u0436\u043d\u043e? \u0410 \u0435\u0441\u043b\u0438 \u043e\u043f\u044f\u0442\u044c 2,\u044d\u0442\u043e \u0432\u0442\u043e\u0440\u043e\u0439 \u0433\u043e\u0434?\"\n---\n# eva_ru_forum_headlines\n\n## Model Description\nThe model was trained on forum topics names and first posts (100 - 150 words). It generates short headlines (3 - 5 words) in the opposite to headlines from models trained on newspaper articles.\n\n\"I do not know how to title this post\" can be a valid headline.\n\"What would you do in my place?\" is one of the most popular headline.\n\n### Usage\n```python\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\nmodel_name = \"Kateryna/eva_ru_forum_headlines\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ntext = \"\u042f \u0432\u043b\u044e\u0431\u0438\u043b\u0430\u0441\u044c \u0432 \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u0430\u0440\u043d\u044f. \u041a\u0430\u0436\u0434\u044b\u0439 \u0440\u0430\u0437, \u043a\u043e\u0433\u0434\u0430 \u043e\u043d \u043c\u0435\u043d\u044f \u0432\u0438\u0434\u0438\u0442, \u043e\u043d \u043f\u043b\u044e\u0435\u0442\u0441\u044f \u0438 \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u0438\u0442 \u043d\u0430 \u0434\u0440\u0443\u0433\u0443\u044e \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u0443\u043b\u0438\u0446\u044b. \u041a\u0430\u043a \u0432\u044b \u0434\u0443\u043c\u0430\u0435\u0442\u0435, \u043e\u043d \u043c\u0435\u043d\u044f \u043b\u044e\u0431\u0438\u0442?\"\n\ninput_ids = tokenizer(\n    [text],\n    max_length=150,\n    add_special_tokens=True,\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\"\n)[\"input_ids\"]\n\noutput_ids = model.generate(\n    input_ids=input_ids,\n    max_length=25,\n    num_beams=4,\n    repetition_penalty=5.0,\n    no_repeat_ngram_size=4\n)[0]\n\nheadline = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint(headline)\n```\n### Training and Validation\n\nTraining dataset: https://huggingface.co/datasets/Kateryna/eva_ru_forum_headlines\n\nFrom all available posts and topics names I selected only posts and abstractive topic names e.g. the topic name does not match exactly anything in the correspondent post.\n\nThe base model is cointegrated/rut5-base\n\nTraining parameters:\n- max_source_tokens_count = 150\n- max_target_tokens_count = 25\n- learning_rate = 0.0007\n- num_train_epochs = 3\n- batch_size = 8\n- gradient_accumulation_steps = 96\n\nROUGE and BLUE scores were not very helpful to choose a best model. \n\nI manually estimated ~100 results in each candidate model.\n\n1. The less gradient_accumulation_steps the more abstractive headlines but they becomes less and less related to the correspondent posts. The worse model with gradient_accumulation_steps = 1 had all headlines abstractive but random.\n2. The source for the model is real short texts created by ordinary persons without any editing. In many cases, the forum posts are not connected sentences and it is not clear what the author wanted to say or discuss. Sometimes there is a contradiction in the text and only the real topic name reveals what this all about. Naturally the model fails to produce a good headline in such cases.\n\nhttps://github.com/KaterynaD/eva.ru/tree/main/Code/Notebooks/9.%20Headlines\n\n\n\n", "size_bytes": "977332173", "downloads": 5}
{"pretrained_model_name": "r1ck/doc2query-viT5", "description": "---\nlanguage: vi\nwidget:\n- text: >-\n    Chi\u1ebfn tranh L\u1ea1nh (1947-1991, ti\u1ebfng Anh: Cold War) l\u00e0 ch\u1ec9 \u0111\u1ebfn s\u1ef1 c\u0103ng th\u1eb3ng \u0111\u1ecba ch\u00ednh tr\u1ecb \n    v\u00e0 xung \u0111\u1ed9t \u00fd th\u1ee9c h\u1ec7 \u0111\u1ec9nh \u0111i\u1ec3m gi\u1eefa hai si\u00eau c\u01b0\u1eddng (\u0111\u1ee9ng \u0111\u1ea7u v\u00e0 \u0111\u1ea1i di\u1ec7n hai kh\u1ed1i \u0111\u1ed1i l\u1eadp)\n    : Hoa K\u1ef3 (ch\u1ee7 ngh\u0129a t\u01b0 b\u1ea3n) v\u00e0 Li\u00ean X\u00f4 (ch\u1ee7 ngh\u0129a x\u00e3 h\u1ed9i).\nlicense: apache-2.0\npipeline_tag: text2text-generation\n---\n\n# doc2query-viT5\n\nThis is a [doc2query](https://arxiv.org/abs/1904.08375) model based on [viT5](https://huggingface.co/VietAI/vit5-base)\n\nIt can be used for:\n- **Document expansion**: You generate for your paragraphs 20-40 queries and index the paragraphs and the generates queries in a standard BM25 index like Elasticsearch, OpenSearch, or Lucene. The generated queries help to close the lexical gap of lexical search, as the generate queries contain synonyms. Further, it re-weights words giving important words a higher weight even if they appear seldomn in a paragraph. \n- **Domain Specific Training Data Generation**: It can be used to generate training data to learn an embedding model. \n\n## Usage\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nmodel_name = 'r1ck/doc2query-viT5'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ntext = \"Chi\u1ebfn tranh L\u1ea1nh (1947-1991, ti\u1ebfng Anh: Cold War) l\u00e0 ch\u1ec9 \u0111\u1ebfn s\u1ef1 c\u0103ng th\u1eb3ng \u0111\u1ecba ch\u00ednh tr\u1ecb v\u00e0 xung \u0111\u1ed9t \u00fd th\u1ee9c h\u1ec7 \u0111\u1ec9nh \u0111i\u1ec3m gi\u1eefa hai si\u00eau c\u01b0\u1eddng (\u0111\u1ee9ng \u0111\u1ea7u v\u00e0 \u0111\u1ea1i di\u1ec7n hai kh\u1ed1i \u0111\u1ed1i l\u1eadp): Hoa K\u1ef3 (ch\u1ee7 ngh\u0129a t\u01b0 b\u1ea3n) v\u00e0 Li\u00ean X\u00f4 (ch\u1ee7 ngh\u0129a x\u00e3 h\u1ed9i).\"\n\n\ndef create_queries(para):\n    input_ids = tokenizer.encode(para, return_tensors='pt')\n    with torch.no_grad():\n        # Here we use top_k / top_k random sampling. It generates more diverse queries, but of lower quality\n        sampling_outputs = model.generate(\n            input_ids=input_ids,\n            max_length=64,\n            do_sample=True,\n            top_p=0.95,\n            top_k=10, \n            num_return_sequences=5\n            )\n        \n        # Here we use Beam-search. It generates better quality queries, but with less diversity\n        beam_outputs = model.generate(\n            input_ids=input_ids, \n            max_length=64, \n            num_beams=5, \n            no_repeat_ngram_size=2, \n            num_return_sequences=5, \n            early_stopping=True\n        )\n\n\n    print(\"Paragraph:\")\n    print(para)\n    \n    print(\"\\nBeam Outputs:\")\n    for i in range(len(beam_outputs)):\n        query = tokenizer.decode(beam_outputs[i], skip_special_tokens=True)\n        print(f'{i + 1}: {query}')\n\n    print(\"\\nSampling Outputs:\")\n    for i in range(len(sampling_outputs)):\n        query = tokenizer.decode(sampling_outputs[i], skip_special_tokens=True)\n        print(f'{i + 1}: {query}')\n\ncreate_queries(text)\n\n```\n\n**Note:** `model.generate()` is non-deterministic for top_k/top_n sampling. It produces different queries each time you run it.\n\n## Training\nThis model fine-tuned [VietAI/vit5-base](https://huggingface.co/VietAI/vit5-base) on 30k vietnamese passage-question pairs\n", "size_bytes": "903890495", "downloads": 25}
{"pretrained_model_name": "ashokurlana/mBART-TeSum", "description": "---\nlanguage:\n- multilingual\n- te\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: mBART-TeSum\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mBART-TeSum\n\nThis model is a fine-tuned version of [facebook/mbart-large-50](https://huggingface.co/facebook/mbart-large-50) on [TeSum](https://ltrc.iiit.ac.in/showfile.php?filename=downloads/teSum/) dataset. More details about the training and analysis mentioned in the [paper](https://aclanthology.org/2022.lrec-1.614.pdf).\n\n\n## Model description\n\nmBART-50 is a multilingual Sequence-to-Sequence model. It was introduced to show that multilingual translation models can be created through multilingual fine-tuning. \nInstead of fine-tuning on one direction, a pre-trained model is fine-tuned on many directions simultaneously. \nmBART-50 is created using the original mBART model and extended to add extra 25 languages to support multilingual machine translation models of 50 languages. \nThe pre-training objective is explained below.\n\n**Multilingual Denoising Pretraining**: The model incorporates N languages by concatenating data: \n`D = {D1, ..., DN }` where each Di is a collection of monolingual documents in language `i`. \nThe source documents are noised using two schemes, first randomly shuffling the original sentences' order, and second a novel in-filling scheme, \nwhere spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. \n35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(\u03bb = 3.5)`. \nThe decoder input is the original text with one position offset. A language id symbol `LID` is used as the initial token to predict \nthe sentence.\n\n## Intended uses & limitations\n\nmbart-large-50 is pre-trained model and primarily aimed at being fine-tuned on translation tasks. It can also be fine-tuned on other multilingual sequence-to-sequence tasks. See the [model hub](https://huggingface.co/models?other=mbart-50) to look for fine-tuned versions.\n\n## Training\n\nAs the model is multilingual, it expects the sequences in a different format. A special language id token is used as a prefix in both the source and target text. \nThe text format is `[lang_code] X [eos]` with `X` being the source or target text respectively and lang_code is `source_lang_code` for source text \nand `tgt_lang_code` for target text. `bos` is never used. Once the examples are prepared in this format, it can be trained as any other sequence-to-sequence model.\n\n```python\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\nmodel = MBartForConditionalGeneration.from_pretrained(\"ashokurlana/mBART-TeSum\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"ashokurlana/mBART-TeSum\", src_lang=\"te_IN\", tgt_lang=\"te_IN\")\n\nsrc_text = \"\u0c24\u0c46\u0c32\u0c02\u0c17\u0c3e\u0c23\u0c32\u0c4b \u0c38\u0c1a\u0c32\u0c28\u0c02 \u0c38\u0c43\u0c37\u0c4d\u0c1f\u0c3f\u0c02\u0c1a\u0c3f\u0c28 \u0c1f\u0c40\u0c0e\u0c38\u0c4d\u200c\u0c2a\u0c40\u0c0e\u0c38\u0c4d\u0c38\u0c40 \u0c2a\u0c47\u0c2a\u0c30\u0c4d \u0c32\u0c40\u0c15\u0c47\u0c1c\u0c40 \u0c35\u0c4d\u0c2f\u0c35\u0c39\u0c3e\u0c30\u0c02\u0c2a\u0c48 \u0c2a\u0c4d\u0c30\u0c2d\u0c41\u0c24\u0c4d\u0c35\u0c02 \u0c24\u0c30\u0c2a\u0c41\u0c28 \u0c2e\u0c02\u0c24\u0c4d\u0c30\u0c3f \u0c15\u0c47\u0c1f\u0c40\u0c06\u0c30\u0c4d \u0c24\u0c4a\u0c32\u0c3f\u0c38\u0c3e\u0c30\u0c3f \u0c38\u0c4d\u0c2a\u0c02\u0c26\u0c3f\u0c02\u0c1a\u0c3e\u0c30\u0c41. \u0c07\u0c26\u0c3f \u0c35\u0c4d\u0c2f\u0c35\u0c38\u0c4d\u0c25 \u0c35\u0c48\u0c2b\u0c32\u0c4d\u0c2f\u0c02 \u0c15\u0c3e\u0c26\u0c28\u0c3f.., \u0c07\u0c26\u0c4d\u0c26\u0c30\u0c41 \u0c35\u0c4d\u0c2f\u0c15\u0c4d\u0c24\u0c41\u0c32\u0c41 \u0c1a\u0c47\u0c38\u0c3f\u0c28 \u0c24\u0c2a\u0c4d\u0c2a\u0c41 \u0c05\u0c28\u0c3f \u0c15\u0c47\u0c1f\u0c40\u0c06\u0c30\u0c4d \u0c35\u0c46\u0c32\u0c4d\u0c32\u0c21\u0c3f\u0c02\u0c1a\u0c3e\u0c30\u0c41. \u0c08 \u0c35\u0c4d\u0c2f\u0c35\u0c39\u0c3e\u0c30\u0c02 \u0c35\u0c46\u0c28\u0c41\u0c15 \u0c0f \u0c2a\u0c3e\u0c30\u0c4d\u0c1f\u0c40\u0c15\u0c3f \u0c1a\u0c46\u0c02\u0c26\u0c3f\u0c28 \u0c35\u0c3e\u0c30\u0c41\u0c28\u0c4d\u0c28\u0c3e.., \u0c0e\u0c02\u0c24\u0c1f\u0c3f \u0c35\u0c3e\u0c30\u0c48\u0c28\u0c3e \u0c15\u0c20\u0c3f\u0c28\u0c02\u0c17\u0c3e \u0c36\u0c3f\u0c15\u0c4d\u0c37\u0c3f\u0c38\u0c4d\u0c24\u0c3e\u0c2e\u0c28\u0c3f \u0c1a\u0c46\u0c2a\u0c4d\u0c2a\u0c3e\u0c30\u0c41. \u0c28\u0c3f\u0c30\u0c41\u0c26\u0c4d\u0c2f\u0c4b\u0c17\u0c41\u0c32\u0c4d\u0c32\u0c4b \u0c06\u0c02\u0c26\u0c4b\u0c33\u0c28\u0c32\u0c41 \u0c30\u0c47\u0c15\u0c46\u0c24\u0c4d\u0c24\u0c3f\u0c02\u0c1a\u0c47\u0c32\u0c3e \u0c2a\u0c4d\u0c30\u0c24\u0c3f\u0c2a\u0c15\u0c4d\u0c37\u0c3e\u0c32\u0c41 \u0c2e\u0c3e\u0c1f\u0c4d\u0c32\u0c3e\u0c21\u0c1f\u0c02 \u0c38\u0c30\u0c3f\u0c15\u0c3e\u0c26\u0c28\u0c3f \u0c39\u0c3f\u0c24\u0c35\u0c41 \u0c2a\u0c32\u0c3f\u0c15\u0c3e\u0c30\u0c41.\"\ntgt_text =  \"\u0c24\u0c46\u0c32\u0c02\u0c17\u0c3e\u0c23\u0c32\u0c4b \u0c38\u0c1a\u0c32\u0c28\u0c02 \u0c38\u0c43\u0c37\u0c4d\u0c1f\u0c3f\u0c02\u0c1a\u0c3f\u0c28 \u0c1f\u0c40\u0c0e\u0c38\u0c4d \u0c2a\u0c40\u0c0e\u0c38\u0c4d\u0c38\u0c40 \u0c2a\u0c47\u0c2a\u0c30\u0c4d \u0c32\u0c40\u0c15\u0c47\u0c1c\u0c40 \u0c35\u0c4d\u0c2f\u0c35\u0c39\u0c3e\u0c30\u0c02\u0c2a\u0c48 \u0c2a\u0c4d\u0c30\u0c2d\u0c41\u0c24\u0c4d\u0c35\u0c02 \u0c24\u0c30\u0c2a\u0c41\u0c28 \u0c2e\u0c02\u0c24\u0c4d\u0c30\u0c3f \u0c15\u0c47\u0c1f\u0c40\u0c06\u0c30\u0c4d \u0c38\u0c4d\u0c2a\u0c02\u0c26\u0c3f\u0c02\u0c1a\u0c3e\u0c30\u0c41. \u0c07\u0c26\u0c3f \u0c35\u0c4d\u0c2f\u0c35\u0c38\u0c4d\u0c25 \u0c35\u0c48\u0c2b\u0c32\u0c4d\u0c2f\u0c02 \u0c15\u0c3e\u0c26\u0c28\u0c3f, \u0c07\u0c26\u0c4d\u0c26\u0c30\u0c41 \u0c35\u0c4d\u0c2f\u0c15\u0c4d\u0c24\u0c41\u0c32\u0c41 \u0c1a\u0c47\u0c38\u0c3f\u0c28 \u0c24\u0c2a\u0c4d\u0c2a\u0c41 \u0c05\u0c28\u0c3f, \u0c08 \u0c35\u0c4d\u0c2f\u0c35\u0c39\u0c3e\u0c30\u0c02 \u0c35\u0c46\u0c28\u0c41\u0c15 \u0c0f \u0c2a\u0c3e\u0c30\u0c4d\u0c1f\u0c40\u0c15\u0c3f \u0c1a\u0c46\u0c02\u0c26\u0c3f\u0c28 \u0c35\u0c3e\u0c30\u0c41\u0c28\u0c4d\u0c28\u0c3e \u0c15\u0c20\u0c3f\u0c28\u0c02\u0c17\u0c3e \u0c36\u0c3f\u0c15\u0c4d\u0c37\u0c3f\u0c38\u0c4d\u0c24\u0c3e\u0c2e\u0c28\u0c3f \u0c1a\u0c46\u0c2a\u0c4d\u0c2a\u0c3e\u0c30\u0c41.\"\n\nmodel_inputs = tokenizer(src_text, return_tensors=\"pt\")\nwith tokenizer.as_target_tokenizer():\n    labels = tokenizer(tgt_text, return_tensors=\"pt\").input_ids\n\nmodel(**model_inputs, labels=labels) # forward pass\n```\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2.0\n\n### Evaluation results\n\nIt achieves the following results on the evaluation set:\n- Loss: 1.4009\n- Rouge1: 32.8603\n- Rouge2: 12.2822\n- Rougel: 31.7473\n- Rougelsum: 32.505\n- Gen Len: 117.6326\n\n### Framework versions\n\n- Transformers 4.19.0.dev0\n- Pytorch 1.11.0\n- Datasets 2.1.0\n- Tokenizers 0.12.1\n\n### BibTeX entry and citation info\n\n```\n@inproceedings{urlana-etal-2022-tesum,\n    title = \"{T}e{S}um: Human-Generated Abstractive Summarization Corpus for {T}elugu\",\n    author = \"Urlana, Ashok  and\n      Surange, Nirmal  and\n      Baswani, Pavan  and\n      Ravva, Priyanka  and\n      Shrivastava, Manish\",\n    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2022.lrec-1.614\",\n    pages = \"5712--5722\",\n    abstract = \"Expert human annotation for summarization is definitely an expensive task, and can not be done on huge scales. But with this work, we show that even with a crowd sourced summary generation approach, quality can be controlled by aggressive expert informed filtering and sampling-based human evaluation. We propose a pipeline that crowd-sources summarization data and then aggressively filters the content via: automatic and partial expert evaluation. Using this pipeline we create a high-quality Telugu Abstractive Summarization dataset (TeSum) which we validate with sampling-based human evaluation. We also provide baseline numbers for various models commonly used for summarization. A number of recently released datasets for summarization, scraped the web-content relying on the assumption that summary is made available with the article by the publishers. While this assumption holds for multiple resources (or news-sites) in English, it should not be generalised across languages without thorough analysis and verification. Our analysis clearly shows that this assumption does not hold true for most Indian language news resources. We show that our proposed filtration pipeline can even be applied to these large-scale scraped datasets to extract better quality article-summary pairs.\",\n}\n```", "size_bytes": "2444690425", "downloads": 18}
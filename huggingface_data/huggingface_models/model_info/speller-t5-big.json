{"pretrained_model_name": "summervent/speller-t5-big", "description": "---\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: speller-t5-big\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# speller-t5-big\n\nThis model is a fine-tuned version of [sberbank-ai/ruT5-base](https://huggingface.co/sberbank-ai/ruT5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1571\n- Rouge1: 23.6607\n- Rouge2: 10.9056\n- Rougel: 23.8095\n- Rougelsum: 23.8095\n- Gen Len: 45.0357\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.1176        | 0.04  | 500   | 0.5179          | 20.9642 | 10.0469 | 20.7738 | 20.9865   | 53.2232 |\n| 0.8525        | 0.07  | 1000  | 0.3809          | 20.4241 | 9.7577  | 20.5357 | 20.6659   | 46.9196 |\n| 0.677         | 0.11  | 1500  | 0.3261          | 21.131  | 9.6429  | 21.1607 | 21.4583   | 47.125  |\n| 0.5804        | 0.14  | 2000  | 0.3008          | 22.7307 | 10.9056 | 22.8051 | 22.9167   | 46.7321 |\n| 0.5251        | 0.18  | 2500  | 0.2805          | 22.3214 | 10.6737 | 22.4702 | 22.619    | 47.1339 |\n| 0.5026        | 0.21  | 3000  | 0.2676          | 22.5298 | 10.7639 | 22.6488 | 22.6786   | 47.4464 |\n| 0.433         | 0.25  | 3500  | 0.2515          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 46.8929 |\n| 0.5218        | 0.29  | 4000  | 0.2383          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 46.6071 |\n| 0.45          | 0.32  | 4500  | 0.2300          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 46.8304 |\n| 0.3818        | 0.36  | 5000  | 0.2270          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 46.5625 |\n| 0.4404        | 0.39  | 5500  | 0.2192          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 45.3304 |\n| 0.3476        | 0.43  | 6000  | 0.2099          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 45.2857 |\n| 0.3802        | 0.47  | 6500  | 0.2066          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 47.0    |\n| 0.3423        | 0.5   | 7000  | 0.2003          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 45.3839 |\n| 0.3288        | 0.54  | 7500  | 0.1924          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 45.7768 |\n| 0.3788        | 0.57  | 8000  | 0.1897          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 46.3125 |\n| 0.3475        | 0.61  | 8500  | 0.1905          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 45.8571 |\n| 0.3229        | 0.64  | 9000  | 0.1829          | 22.9167 | 10.2679 | 22.9167 | 22.9167   | 46.6161 |\n| 0.2918        | 0.68  | 9500  | 0.1840          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 46.0714 |\n| 0.3683        | 0.72  | 10000 | 0.1834          | 22.9167 | 10.2679 | 22.9167 | 22.9167   | 46.8393 |\n| 0.3074        | 0.75  | 10500 | 0.1758          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 45.875  |\n| 0.329         | 0.79  | 11000 | 0.1686          | 22.9167 | 10.2679 | 22.9167 | 22.9167   | 46.7589 |\n| 0.2692        | 0.82  | 11500 | 0.1699          | 22.9167 | 10.2679 | 22.9167 | 22.9167   | 46.6696 |\n| 0.2775        | 0.86  | 12000 | 0.1732          | 23.4375 | 10.523  | 23.4375 | 23.5119   | 46.2232 |\n| 0.2754        | 0.9   | 12500 | 0.1643          | 22.9167 | 10.2679 | 22.9167 | 22.9167   | 46.3125 |\n| 0.3348        | 0.93  | 13000 | 0.1611          | 22.9167 | 10.2679 | 22.9167 | 22.9167   | 46.2411 |\n| 0.2875        | 0.97  | 13500 | 0.1571          | 23.6607 | 10.9056 | 23.8095 | 23.8095   | 45.0357 |\n\n\n### Framework versions\n\n- Transformers 4.26.0\n- Pytorch 1.13.1+cu116\n- Datasets 2.9.0\n- Tokenizers 0.13.2\n", "size_bytes": "891616913", "downloads": 2}
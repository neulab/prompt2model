{"pretrained_model_name": "eenzeenee/t5-small-korean-summarization", "description": "---\npipeline_tag: summarization\nlanguage:\n- ko\ntags:\n- T5\n---\n\n# t5-small-korean-summarization\n\nThis is [T5](https://huggingface.co/docs/transformers/model_doc/t5) model for korean text summarization.  \n- Finetuned based on ['paust/pko-t5-small'](https://huggingface.co/paust/pko-t5-small) model.  \n- Finetuned with 3 datasets. Specifically, it is described below.\n\n  - [Korean Paper Summarization Dataset(\ub17c\ubb38\uc790\ub8cc \uc694\uc57d)](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=90)\n  - [Korean Book Summarization Dataset(\ub3c4\uc11c\uc790\ub8cc \uc694\uc57d)](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=93)\n  - [Korean Summary statement and Report Generation Dataset(\uc694\uc57d\ubb38 \ubc0f \ub808\ud3ec\ud2b8 \uc0dd\uc131 \ub370\uc774\ud130)](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=90)\n\n\n# Usage (HuggingFace Transformers)\n\n```python\nimport nltk\nnltk.download('punkt')\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained('eenzeenee/t5-small-korean-summarization')\ntokenizer = AutoTokenizer.from_pretrained('eenzeenee/t5-small-korean-summarization')\n\nprefix = \"summarize: \"\nsample = \"\"\"\n    \uc548\ub155\ud558\uc138\uc694? \uc6b0\ub9ac (2\ud559\ub144)/(\uc774 \ud559\ub144) \uce5c\uad6c\ub4e4 \uc6b0\ub9ac \uce5c\uad6c\ub4e4 \ud559\uad50\uc5d0 \uac00\uc11c \uc9c4\uc9dc (2\ud559\ub144)/(\uc774 \ud559\ub144) \uc774 \ub418\uace0 \uc2f6\uc5c8\ub294\ub370 \ud559\uad50\uc5d0 \ubabb \uac00\uace0 \uc788\uc5b4\uc11c \ub2f5\ub2f5\ud558\uc8e0? \n    \uadf8\ub798\ub3c4 \uc6b0\ub9ac \uce5c\uad6c\ub4e4\uc758 \uc548\uc804\uacfc \uac74\uac15\uc774 \ucd5c\uc6b0\uc120\uc774\ub2c8\uae4c\uc694 \uc624\ub298\ubd80\ud130 \uc120\uc0dd\ub2d8\uc774\ub791 \ub9e4\uc77c \ub9e4\uc77c \uad6d\uc5b4 \uc5ec\ud589\uc744 \ub5a0\ub098\ubcf4\ub3c4\ub85d \ud574\uc694. \n    \uc5b4/ \uc2dc\uac04\uc774 \ubc8c\uc368 \uc774\ub807\uac8c \ub410\ub098\uc694? \ub2a6\uc5c8\uc5b4\uc694. \ub2a6\uc5c8\uc5b4\uc694. \ube68\ub9ac \uad6d\uc5b4 \uc5ec\ud589\uc744 \ub5a0\ub098\uc57c \ub3fc\uc694. \n    \uadf8\ub7f0\ub370 \uc5b4/ \uad6d\uc5b4\uc5ec\ud589\uc744 \ub5a0\ub098\uae30 \uc804\uc5d0 \uc6b0\ub9ac\uac00 \uc900\ube44\ubb3c\uc744 \ucc59\uaca8\uc57c \ub418\uaca0\uc8e0? \uad6d\uc5b4 \uc5ec\ud589\uc744 \ub5a0\ub0a0 \uc900\ube44\ubb3c, \uad50\uc548\uc744 \uc5b4\ub5bb\uac8c \ubc1b\uc744 \uc218 \uc788\ub294\uc9c0 \uc120\uc0dd\ub2d8\uc774 \uc124\uba85\uc744 \ud574\uc904\uac8c\uc694. \n    (EBS)/(\uc774\ube44\uc5d0\uc2a4) \ucd08\ub4f1\uc744 \uac80\uc0c9\ud574\uc11c \ub4e4\uc5b4\uac00\uba74\uc694 \uccab\ud654\uba74\uc774 \uc774\ub807\uac8c \ub098\uc640\uc694. \n    \uc790/ \uadf8\ub7ec\uba74\uc694 \uc5ec\uae30 (X)/(\uc5d1\uc2a4) \ub20c\ub7ec\uc8fc(\uace0\uc694)/(\uad6c\uc694). \uc800\uae30 (\ub3d9\uadf8\ub77c\ubbf8)/(\ub625\uadf8\ub77c\ubbf8) (EBS)/(\uc774\ube44\uc5d0\uc2a4) (2\uc8fc)/(\uc774 \uc8fc) \ub77c\uc774\ube0c\ud2b9\uac15\uc774\ub77c\uace0 \ub418\uc5b4\uc788\uc8e0? \n    \uac70\uae30\ub97c \ubc14\ub85c \uac00\uae30\ub97c \ub204\ub985\ub2c8\ub2e4. \uc790/ (\ub204\ub974\uba74\uc694)/(\ub20c\ub974\uba74\uc694). \uc5b4\ub5bb\uac8c \ub418\ub0d0? b/ \ubc11\uc73c\ub85c \ub0b4\ub824\uc694 \ub0b4\ub824\uc694 \ub0b4\ub824\uc694 \ucb49 \ub0b4\ub824\uc694. \n    \uc6b0\ub9ac \uba87 \ud559\ub144\uc774\uc8e0? \uc544/ (2\ud559\ub144)/(\uc774 \ud559\ub144) \uc774\uc8e0 (2\ud559\ub144)/(\uc774 \ud559\ub144)\uc758 \ubb34\uc2a8 \uacfc\ubaa9? \uad6d\uc5b4. \n    \uc774\ubc88\uc8fc\ub294 (1\uc8fc)/(\uc77c \uc8fc) \ucc28\ub2c8\uae4c\uc694 \uc5ec\uae30 \uad50\uc548. \ub2e4\uc74c\uc8fc\ub294 \uc5ec\uae30\uc11c \ub2e4\uc6b4\uc744 \ubc1b\uc73c\uba74 \ub3fc\uc694. \n    \uc774 \uad50\uc548\uc744 \ud074\ub9ad\uc744 \ud558\uba74, \uc9dc\uc794/. \uc774\ub807\uac8c \uad50\uc7ac\uac00 \ub098\uc635\ub2c8\ub2e4 .\uc774 \uad50\uc548\uc744 (\ub2e4\uc6b4)/(\ub530\uc6b4)\ubc1b\uc544\uc11c \uc6b0\ub9ac \uad6d\uc5b4\uc5ec\ud589\uc744 \ub5a0\ub0a0 \uc218\uac00 \uc788\uc5b4\uc694. \n    \uadf8\ub7fc \uc6b0\ub9ac \uc9c4\uc9dc\ub85c \uad6d\uc5b4 \uc5ec\ud589\uc744 \ud55c\ubc88 \ub5a0\ub098\ubcf4\ub3c4\ub85d \ud574\uc694? \uad6d\uc5b4\uc5ec\ud589 \ucd9c\ubc1c. \uc790/ (1\ub2e8\uc6d0)/(\uc77c \ub2e8\uc6d0) \uc81c\ubaa9\uc774 \ubb54\uac00\uc694? \ud55c\ubc88 \ucc3e\uc544\ubd10\uc694. \n    \uc2dc\ub97c \uc990\uaca8\uc694 \uc5d0\uc694. \uadf8\ub0e5 \uc2dc\ub97c \uc77d\uc5b4\uc694 \uac00 \uc544\ub2c8\uc5d0\uc694. \uc2dc\ub97c \uc990\uaca8\uc57c \ub3fc\uc694 \uc990\uaca8\uc57c \ub3fc. \uc5b4\ub5bb\uac8c \uc990\uae38\uae4c? \uc77c\ub2e8\uc740 \ub0b4\ub0b4 \uc2dc\ub97c \uc990\uae30\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574\uc11c \uacf5\ubd80\ub97c \ud560 \uac74\ub370\uc694. \n    \uadf8\ub7fc \uc624\ub298\uc740\uc694 \uc5b4\ub5bb\uac8c \uc990\uae38\uae4c\uc694? \uc624\ub298 \uacf5\ubd80\ud560 \ub0b4\uc6a9\uc740\uc694 \uc2dc\ub97c \uc5ec\ub7ec \uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \uc77d\uae30\ub97c \uacf5\ubd80\ud560\uac81\ub2c8\ub2e4. \n    \uc5b4\ub5bb\uac8c \uc5ec\ub7ec\uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \uc77d\uc744\uae4c \uc6b0\ub9ac \uacf5\ubd80\ud574 \ubcf4\ub3c4\ub85d \ud574\uc694. \uc624\ub298\uc758 \uc2dc \ub098\uc640\ub77c \uc9dc\uc794/! \uc2dc\uac00 \ub098\uc654\uc2b5\ub2c8\ub2e4 \uc2dc\uc758 \uc81c\ubaa9\uc774 \ubb54\uac00\uc694? \ub2e4\ud230 \ub0a0\uc774\uc5d0\uc694 \ub2e4\ud230 \ub0a0. \n    \ub204\uad6c\ub791 \ub2e4\ud25c\ub098 \ub3d9\uc0dd\uc774\ub791 \ub2e4\ud25c\ub098 \uc5b8\ub2c8\ub791 \uce5c\uad6c\ub791? \ub204\uad6c\ub791 \ub2e4\ud25c\ub294\uc9c0 \uc120\uc0dd\ub2d8\uc774 \uc2dc\ub97c \uc77d\uc5b4 \uc904 \ud14c\ub2c8\uae4c \ud55c\ubc88 \uc0dd\uac01\uc744 \ud574\ubcf4\ub3c4\ub85d \ud574\uc694.\"\"\"\n\ninputs = [prefix + sample]\n\n\ninputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\noutput = model.generate(**inputs, num_beams=3, do_sample=True, min_length=10, max_length=64)\ndecoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\nresult = nltk.sent_tokenize(decoded_output.strip())[0]\n\nprint('RESULT >>', result)\n```", "size_bytes": "382580229", "downloads": 364}
{"pretrained_model_name": "BramVanroy/mbart-large-cc25-amr-en", "description": "---\nbase_model: facebook/mbart-large-cc25\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\n- bleu\nmodel-index:\n- name: 6e-5lr+30ep+128tbs\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 6e-5lr+30ep+128tbs\n\nThis model is a fine-tuned version of [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3724\n- Accuracy: 0.9343\n- Bleu: 83.5524\n- Smatch Precision: 0.8333\n- Smatch Recall: 0.8333\n- Smatch Fscore: 0.8333\n- Ratio Invalid Amrs: 99.8216\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 6e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 64\n- total_train_batch_size: 128\n- optimizer: Adam with betas=(0.9,0.95) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Accuracy | Bleu    | Smatch Precision | Smatch Recall | Smatch Fscore | Ratio Invalid Amrs |\n|:-------------:|:-----:|:-----:|:---------------:|:--------:|:-------:|:----------------:|:-------------:|:-------------:|:------------------:|\n| 1.9675        | 0.71  | 300   | 1.7572          | 0.6996   | 55.7067 | 0.0              | 0.0           | 0.0           | 100.0              |\n| 0.7954        | 1.42  | 600   | 0.6723          | 0.8479   | 73.3629 | 0.1176           | 0.5556        | 0.1942        | 99.8216            |\n| 0.6653        | 2.13  | 900   | 0.5924          | 0.8504   | 77.5363 | 0.75             | 0.75          | 0.75          | 99.8811            |\n| 0.5753        | 2.84  | 1200  | 0.5504          | 0.8729   | 76.2041 | 0.5              | 0.5           | 0.5           | 99.9405            |\n| 0.4201        | 3.55  | 1500  | 0.4302          | 0.8947   | 80.5808 | 0.0              | 0.0           | 0.0           | 100.0              |\n| 0.3879        | 4.27  | 1800  | 0.3633          | 0.9067   | 84.7387 | 0.5              | 0.5           | 0.5           | 99.8216            |\n| 0.3417        | 4.98  | 2100  | 0.3000          | 0.9117   | 83.4336 | 0.6667           | 0.6667        | 0.6667        | 99.8216            |\n| 0.1845        | 5.69  | 2400  | 0.3003          | 0.9194   | 84.1284 | 0.6667           | 0.6667        | 0.6667        | 99.8216            |\n| 0.2183        | 6.4   | 2700  | 0.2818          | 0.9244   | 84.2111 | 0.2236           | 0.6102        | 0.3273        | 99.7027            |\n| 0.118         | 7.11  | 3000  | 0.2788          | 0.9208   | 83.6461 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.1622        | 7.82  | 3300  | 0.2767          | 0.9276   | 84.0204 | 0.5190           | 0.5734        | 0.5449        | 99.7027            |\n| 0.1122        | 8.53  | 3600  | 0.2867          | 0.9227   | 83.3858 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.1094        | 9.24  | 3900  | 0.2876          | 0.9274   | 83.6132 | 1.0              | 1.0           | 1.0           | 99.8811            |\n| 0.1557        | 9.95  | 4200  | 0.2651          | 0.9303   | 83.5874 | 0.5850           | 0.5850        | 0.5850        | 99.7027            |\n| 0.1322        | 10.66 | 4500  | 0.2821          | 0.9291   | 83.6214 | 0.5625           | 0.5625        | 0.5625        | 99.7622            |\n| 0.1563        | 11.37 | 4800  | 0.2835          | 0.9314   | 83.9805 | 0.5578           | 0.5578        | 0.5578        | 99.7027            |\n| 0.1121        | 12.09 | 5100  | 0.2893          | 0.9323   | 83.2553 | 0.5880           | 0.5880        | 0.5880        | 99.6433            |\n| 0.0801        | 12.8  | 5400  | 0.3105          | 0.9307   | 83.0936 | 0.6137           | 0.6137        | 0.6137        | 99.6433            |\n| 0.0842        | 13.51 | 5700  | 0.3070          | 0.9318   | 82.9500 | 0.6438           | 0.6438        | 0.6438        | 99.7622            |\n| 0.1149        | 14.22 | 6000  | 0.2950          | 0.9325   | 82.8128 | 0.5918           | 0.5918        | 0.5918        | 99.7027            |\n| 0.0779        | 14.93 | 6300  | 0.2982          | 0.9329   | 83.4984 | 0.5918           | 0.5918        | 0.5918        | 99.7027            |\n| 0.0336        | 15.64 | 6600  | 0.3128          | 0.9318   | 83.4437 | 0.5625           | 0.5625        | 0.5625        | 99.7622            |\n| 0.1683        | 16.35 | 6900  | 0.3100          | 0.9327   | 83.9675 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.1168        | 17.06 | 7200  | 0.3199          | 0.9327   | 83.8284 | 0.6027           | 0.6027        | 0.6027        | 99.7622            |\n| 0.0452        | 17.77 | 7500  | 0.3193          | 0.9321   | 83.3733 | 0.6438           | 0.6438        | 0.6438        | 99.7622            |\n| 0.0101        | 18.48 | 7800  | 0.3220          | 0.9332   | 83.7279 | 0.6438           | 0.6438        | 0.6438        | 99.7622            |\n| 0.0155        | 19.19 | 8100  | 0.3300          | 0.9328   | 83.3700 | 0.6478           | 0.6478        | 0.6478        | 99.7027            |\n| 0.0054        | 19.9  | 8400  | 0.3309          | 0.9323   | 83.1491 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.0624        | 20.62 | 8700  | 0.3357          | 0.9333   | 83.4539 | 0.6438           | 0.6438        | 0.6438        | 99.7622            |\n| 0.1998        | 21.33 | 9000  | 0.3446          | 0.9331   | 83.2082 | 0.6122           | 0.6122        | 0.6122        | 99.7027            |\n| 0.1319        | 22.04 | 9300  | 0.3424          | 0.9330   | 83.1733 | 0.6087           | 0.6087        | 0.6087        | 99.7622            |\n| 0.0717        | 22.75 | 9600  | 0.3468          | 0.9334   | 83.6701 | 0.5978           | 0.5978        | 0.5978        | 99.7622            |\n| 0.0156        | 23.46 | 9900  | 0.3489          | 0.9340   | 83.3002 | 0.6027           | 0.6027        | 0.6027        | 99.7622            |\n| 0.0199        | 24.17 | 10200 | 0.3549          | 0.9340   | 83.3097 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.0618        | 24.88 | 10500 | 0.3566          | 0.9340   | 83.3643 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.0376        | 25.59 | 10800 | 0.3605          | 0.9341   | 83.4213 | 0.6438           | 0.6438        | 0.6438        | 99.7622            |\n| 0.074         | 26.3  | 11100 | 0.3669          | 0.9336   | 83.5835 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.0175        | 27.01 | 11400 | 0.3630          | 0.9340   | 83.7231 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.0209        | 27.72 | 11700 | 0.3653          | 0.9340   | 83.5187 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.0344        | 28.44 | 12000 | 0.3699          | 0.9341   | 83.4363 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.0341        | 29.15 | 12300 | 0.3708          | 0.9341   | 83.4487 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n| 0.0078        | 29.86 | 12600 | 0.3724          | 0.9343   | 83.5524 | 0.8333           | 0.8333        | 0.8333        | 99.8216            |\n\n\n### Framework versions\n\n- Transformers 4.31.0\n- Pytorch 2.0.1+cu117\n- Datasets 2.14.2\n- Tokenizers 0.13.3\n", "size_bytes": "2445038429", "downloads": 1}
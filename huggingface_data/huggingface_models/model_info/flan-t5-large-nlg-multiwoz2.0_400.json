{"pretrained_model_name": "Zekunli/flan-t5-large-nlg-multiwoz2.0_400", "description": "---\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: flan-t5-large-nlg-multiwoz2.0_400\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-large-nlg-multiwoz2.0_400\n\nThis model was trained from scratch on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9323\n- Rouge1: 36.3522\n- Rouge2: 19.5982\n- Rougel: 33.0495\n- Rougelsum: 34.4791\n- Gen Len: 17.7927\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 24\n- seed: 1799\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.4929        | 0.58  | 200  | 1.1051          | 33.8407 | 17.022  | 30.6518 | 32.2374   | 17.7195 |\n| 1.1546        | 1.17  | 400  | 1.0221          | 33.4159 | 17.73   | 30.4168 | 31.6796   | 17.8444 |\n| 1.0597        | 1.75  | 600  | 0.9819          | 34.8735 | 18.3373 | 31.5435 | 33.0184   | 17.7802 |\n| 0.9863        | 2.33  | 800  | 0.9672          | 34.7204 | 18.0945 | 31.5299 | 32.9849   | 17.6341 |\n| 0.9689        | 2.92  | 1000 | 0.9509          | 35.7006 | 19.2988 | 32.4312 | 33.8706   | 17.8081 |\n| 0.9279        | 3.5   | 1200 | 0.9432          | 35.5086 | 19.1375 | 32.3084 | 33.7471   | 17.9298 |\n| 0.9187        | 4.08  | 1400 | 0.9414          | 35.591  | 19.3273 | 32.4831 | 33.914    | 17.7133 |\n| 0.8865        | 4.66  | 1600 | 0.9323          | 36.3522 | 19.5982 | 33.0495 | 34.4791   | 17.7927 |\n| 0.8735        | 5.25  | 1800 | 0.9311          | 35.7889 | 18.75   | 32.3179 | 33.9012   | 17.8027 |\n| 0.8556        | 5.83  | 2000 | 0.9284          | 36.1266 | 19.5539 | 32.7835 | 34.263    | 17.7171 |\n| 0.8479        | 6.41  | 2200 | 0.9277          | 36.21   | 19.5396 | 32.8933 | 34.3317   | 17.8339 |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "3132789733", "downloads": 2}
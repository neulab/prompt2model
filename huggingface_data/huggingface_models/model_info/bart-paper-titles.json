{"pretrained_model_name": "pietrocagnasso/bart-paper-titles", "description": "---\nlanguage:\n- en\n---\nBART model used to generate scientific papers' title given the highlights and the abstract of the paper.\n\nThis model is the result of a fine-tuning process done on [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6).\nWe performed the fine-tuning for one epoch on CSPubSumm (Ed Collins, et al. \"A supervised approach to extractive summarisation of scientific papers.\"), \nBIOPubSumm, and AIPubSumm (L. Cagliero, M. La Quatra \"Extracting highlights of scientific articles: A supervised summarization approach.\").\n\nYou can find more details in the [GitHub repo](https://github.com/nicolovergaro/DNLP_project).\n\n# Usage\nThis checkpoint should be loaded into `BartForConditionalGeneration.from_pretrained`. See the \n[BART docs](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartForConditionalGeneration) for more information.\n\n# Metrics\nWe have tested the model on all three the test sets, with the following results:\n\n|   Dataset  | Rouge-1 F1 | Rouge-2 F1 | Rouge-L F1 | BERTScore F1 |\n|:----------:|:----------:|:----------:|:----------:|:------------:|\n|  AIPubSumm |   0.42713  |   0.21781  |   0.35251  |    0.90391   |\n| BIOPubSumm |   0.45758  |   0.25219  |   0.39350  |    0.90205   |\n|  CSPubSumm |   0.51502  |   0.33377  |   0.45760  |    0.91703   |", "size_bytes": "1222361081", "downloads": 7}
{"pretrained_model_name": "cosmoquester/bart-ko-mini", "description": "---\nlanguage: ko\n---\n\n# Pretrained BART in Korean\n\nThis is pretrained BART model with multiple Korean Datasets.\n\nI used multiple datasets for generalizing the model for both colloquial and written texts.\n\nThe training is supported by [TPU Research Cloud](https://sites.research.google/trc/) program.\n\nThe script which is used to pre-train model is [here](https://github.com/cosmoquester/transformers-bart-pretrain).\n\nWhen you use the reference API, you must wrap the sentence with `[BOS]` and `[EOS]` like below example.\n\n```\n[BOS] \uc548\ub155\ud558\uc138\uc694? \ubc18\uac00\uc6cc\uc694~~ [EOS]\n```\n\nYou can also test mask filling performance using `[MASK]` token like this.\n```\n[BOS] [MASK] \uba39\uc5c8\uc5b4? [EOS]\n```\n\n## Benchmark\n\n<style>\ntable {\n  border-collapse: collapse;\n  border-style: hidden;\n  width: 100%;\n}\n\ntd, th {\n  border: 1px solid #4d5562;\n  padding: 8px;\n}\n</style>\n\n<table>\n <tr>\n  <th>Dataset</th>\n  \n  <td>KLUE NLI dev</th>\n  <td>NSMC test</td>\n  <td>QuestionPair test</td>\n  <td colspan=\"2\">KLUE TC dev</td>\n  <td colspan=\"3\">KLUE STS dev</td>\n  <td colspan=\"3\">KorSTS dev</td>\n  <td colspan=\"2\">HateSpeech dev</td>\n </tr>\n <tr>\n  <th>Metric</th>\n  \n  <!-- KLUE NLI -->\n  <td>Acc</th>\n  \n  <!-- NSMC -->\n  <td>Acc</td>\n  \n  <!-- QuestionPair -->\n  <td>Acc</td>\n  \n  <!-- KLUE TC -->\n  <td>Acc</td>\n  <td>F1</td>\n  \n  <!-- KLUE STS -->\n  <td>F1</td>\n  <td>Pearson</td>\n  <td>Spearman</td>\n  \n  <!-- KorSTS -->\n  <td>F1</td>\n  <td>Pearson</td>\n  <td>Spearman</td>\n  \n  <!-- HateSpeech -->\n  <td>Bias Acc</td>\n  <td>Hate Acc</td>\n </tr>\n \n <tr>\n  <th>Score</th>\n  \n  <!-- KLUE NLI -->\n  <td>0.5253</th>\n  \n  <!-- NSMC -->\n  <td>0.8425</td>\n  \n  <!-- QuestionPair -->\n  <td>0.8945</td>\n  \n  <!-- KLUE TC -->\n  <td>0.8047</td>\n  <td>0.7988</td>\n  \n  <!-- KLUE STS -->\n  <td>0.7411</td>\n  <td>0.7471</td>\n  <td>0.7399</td>\n  \n  <!-- KorSTS -->\n  <td>0.7725</td>\n  <td>0.6503</td>\n  <td>0.6191</td>\n  \n  <!-- HateSpeech -->\n  <td>0.7537</td>\n  <td>0.5605</td>\n </tr>\n</table>\n\n- The performance was measured using [the notebooks here](https://github.com/cosmoquester/transformers-bart-finetune) with colab.\n\n## Used Datasets\n\n### [\ubaa8\ub450\uc758 \ub9d0\ubb49\uce58](https://corpus.korean.go.kr/)\n- \uc77c\uc0c1 \ub300\ud654 \ub9d0\ubb49\uce58 2020\n- \uad6c\uc5b4 \ub9d0\ubb49\uce58\n- \ubb38\uc5b4 \ub9d0\ubb49\uce58\n- \uc2e0\ubb38 \ub9d0\ubb49\uce58\n\n### AIhub\n- [\uac1c\ubc29\ub370\uc774\ud130 \uc804\ubb38\ubd84\uc57c\ub9d0\ubb49\uce58](https://aihub.or.kr/aidata/30717)\n- [\uac1c\ubc29\ub370\uc774\ud130 \ud55c\uad6d\uc5b4\ub300\ud654\uc694\uc57d](https://aihub.or.kr/aidata/30714)\n- [\uac1c\ubc29\ub370\uc774\ud130 \uac10\uc131 \ub300\ud654 \ub9d0\ubb49\uce58](https://aihub.or.kr/aidata/7978)\n- [\uac1c\ubc29\ub370\uc774\ud130 \ud55c\uad6d\uc5b4 \uc74c\uc131](https://aihub.or.kr/aidata/105)\n- [\uac1c\ubc29\ub370\uc774\ud130 \ud55c\uad6d\uc5b4 SNS](https://aihub.or.kr/aidata/30718)\n\n### [\uc138\uc885 \ub9d0\ubb49\uce58](https://ithub.korean.go.kr/)\n", "size_bytes": "51876625", "downloads": 17}
{"pretrained_model_name": "akreal/mbart-large-50-finetuned-slurp", "description": "---\nlanguage: \n  - en\ntags:\n  - mbart-50\nlicense: apache-2.0\ndatasets:\n  - SLURP\nmetrics:\n  - accuracy\n  - slu-f1\n---\n\nThis model is `mbart-large-50-many-to-many-mmt` model fine-tuned on the text part of [SLURP](https://github.com/pswietojanski/slurp) spoken language understanding dataset.\n\nThe scores on the test set are 85.68% and 79.00% for Intent accuracy and SLU-F1 respectively.", "size_bytes": "2444711481", "downloads": 10}
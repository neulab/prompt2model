{"pretrained_model_name": "pierreguillou/byt5-small-qa-squad-v1.1-portuguese", "description": "---\nlanguage: pt \nlicense: apache-2.0\ntags:\n- text2text-generation\n- byt5\n- pytorch\n- qa\ndatasets: squad\nmetrics: squad\nwidget:\n- text: 'question: \"Quando come\u00e7ou a pandemia de Covid-19 no mundo?\" context: \"A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.\"'\n- text: 'question: \"Onde foi descoberta a Covid-19?\" context: \"A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.\"' \n---\n\n# ByT5 small finetuned for Question Answering (QA) on SQUaD v1.1 Portuguese\n![Exemple of what can do the Portuguese ByT5 small QA (Question Answering), finetuned on SQUAD v1.1](https://miro.medium.com/max/2000/1*te5MmdesAHCmg4KmK8zD3g.png)\n\nCheck our other QA models in Portuguese finetuned on SQUAD v1.1:\n- [Portuguese BERT base cased QA](https://huggingface.co/pierreguillou/bert-base-cased-squad-v1.1-portuguese)\n- [Portuguese BERT large cased QA](https://huggingface.co/pierreguillou/bert-large-cased-squad-v1.1-portuguese)\n- [Portuguese T5 base QA](https://huggingface.co/pierreguillou/t5-base-qa-squad-v1.1-portuguese)\n\n## Introduction\n\nThe model was trained on the dataset SQUAD v1.1 in portuguese from the [Deep Learning Brasil group](http://www.deeplearningbrasil.com.br/) on Google Colab from the language model [ByT5 small](https://huggingface.co/google/byt5-small) of Google.\n\n## About ByT5\n\nByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-small). ByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.\n\nByT5 works especially well on noisy text data,*e.g.*, `google/byt5-small` significantly outperforms [mt5-small](https://huggingface.co/google/mt5-small) on [TweetQA](https://arxiv.org/abs/1907.06292).\n\nPaper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)\n\n## Informations on the method used\n\nAll the informations are in the blog post : ...\n\n## Notebooks in Google Colab & GitHub\n\n- Google Colab: ...\n- GitHub: ...\n\n## Performance\n\nThe results obtained are the following:\n\n```\nf1 = ...\nexact match = ...\n```\n\n## How to use the model... with Pipeline\n\n```python\nimport transformers\nfrom transformers import pipeline\n\nmodel_name = 'pierreguillou/byt5-small-qa-squad-v1.1-portuguese'\nnlp = pipeline(\"text2text-generation\", model=model_name)\n\n# source: https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19\ninput_text = r\"\"\"\nquestion: \"Quando come\u00e7ou a pandemia de Covid-19 no mundo?\" \ncontext: \"A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.\"\n\"\"\"\ninput_text = input_text.replace('\\n','')\ninput_text\n\n# question: \"Quando come\u00e7ou a pandemia de Covid-19 no mundo?\" context: \"A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.\"\n\nresult = nlp(input_text)\nresult\n\n# [{'generated_text': '1 de dezembro de 2019'}]\n```\n\n## How to use the model... with the Auto classes\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n \nmodel_name = 'pierreguillou/byt5-small-qa-squad-v1.1-portuguese'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# source: https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19\ninput_text = r\"\"\"\nquestion: \"Quando come\u00e7ou a pandemia de Covid-19 no mundo?\" \ncontext: \"A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.\"\n\"\"\"\ninput_text = input_text.replace('\\n','')\ninput_text\n\n# question: \"Quando come\u00e7ou a pandemia de Covid-19 no mundo?\" context: \"A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.\"\n\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids\noutputs = model.generate(\n    input_ids,\n    max_length=64,\n    num_beams=1\n    )\n    \nresult = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\nresult\n\n# 1 de dezembro de 2019\n```             \n\n## Limitations and bias\n\nThe training data used for this model come from Portuguese SQUAD. It could contain a lot of unfiltered content, which is far from neutral, and biases.\n\n## Author\n\nPortuguese ByT5 small QA (Question Answering), finetuned on SQUAD v1.1 was trained and evaluated by [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/) thanks to the Open Source code, platforms and advices of many organizations. In particular: [Google AI](https://huggingface.co/google), [Hugging Face](https://huggingface.co/), [Deep Learning Brasil group](http://www.deeplearningbrasil.com.br/) and [Google Colab](https://colab.research.google.com/).\n\n## Citation\nIf you use our work, please cite:\n\n```bibtex\n@inproceedings{pierreguillou2021byt5smallsquadv11portuguese,\n  title={Portuguese ByT5 small QA (Question Answering), finetuned on SQUAD v1.1},\n  author={Pierre Guillou},\n  year={2021}\n}\n```", "size_bytes": "1198625069", "downloads": 7}
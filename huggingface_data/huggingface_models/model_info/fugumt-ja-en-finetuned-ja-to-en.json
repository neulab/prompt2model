{"pretrained_model_name": "45SK/fugumt-ja-en-finetuned-ja-to-en", "description": "---\nlicense: cc-by-sa-4.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: fugumt-ja-en-finetuned-ja-to-en\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# fugumt-ja-en-finetuned-ja-to-en\n\nThis model is a fine-tuned version of [staka/fugumt-ja-en](https://huggingface.co/staka/fugumt-ja-en) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1447\n- Bleu: 86.2696\n- Gen Len: 6.5991\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 25\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 4.5109        | 1.0   | 544   | 1.3336          | 22.852  | 8.2346  |\n| 1.4507        | 2.0   | 1088  | 1.0665          | 32.3126 | 7.7839  |\n| 1.2319        | 3.0   | 1632  | 0.8913          | 39.464  | 7.324   |\n| 1.0674        | 4.0   | 2176  | 0.7552          | 44.1825 | 7.2759  |\n| 0.9451        | 5.0   | 2720  | 0.6529          | 50.7008 | 6.8755  |\n| 0.8415        | 6.0   | 3264  | 0.5657          | 54.1043 | 7.0116  |\n| 0.758         | 7.0   | 3808  | 0.4963          | 54.6872 | 7.3108  |\n| 0.6901        | 8.0   | 4352  | 0.4373          | 57.4437 | 7.2211  |\n| 0.621         | 9.0   | 4896  | 0.3878          | 56.9356 | 7.2811  |\n| 0.5729        | 10.0  | 5440  | 0.3464          | 65.85   | 6.8777  |\n| 0.5311        | 11.0  | 5984  | 0.3112          | 68.8388 | 6.7775  |\n| 0.4463        | 12.0  | 6528  | 0.2812          | 65.2178 | 7.2367  |\n| 0.4182        | 13.0  | 7072  | 0.2569          | 76.2492 | 6.5945  |\n| 0.3893        | 14.0  | 7616  | 0.2346          | 75.3988 | 6.8026  |\n| 0.3713        | 15.0  | 8160  | 0.2167          | 78.6206 | 6.7092  |\n| 0.3469        | 16.0  | 8704  | 0.2010          | 79.4506 | 6.7256  |\n| 0.3304        | 17.0  | 9248  | 0.1881          | 81.9813 | 6.5934  |\n| 0.3162        | 18.0  | 9792  | 0.1784          | 82.5415 | 6.5888  |\n| 0.3022        | 19.0  | 10336 | 0.1685          | 83.2969 | 6.6102  |\n| 0.29          | 20.0  | 10880 | 0.1614          | 83.7095 | 6.6235  |\n| 0.2783        | 21.0  | 11424 | 0.1560          | 83.683  | 6.6559  |\n| 0.27          | 22.0  | 11968 | 0.1508          | 84.719  | 6.6072  |\n| 0.2594        | 23.0  | 12512 | 0.1475          | 87.3934 | 6.5284  |\n| 0.2469        | 24.0  | 13056 | 0.1455          | 85.2872 | 6.6427  |\n| 0.2492        | 25.0  | 13600 | 0.1447          | 86.2696 | 6.5991  |\n\n\n### Framework versions\n\n- Transformers 4.29.2\n- Pytorch 2.0.1+cu117\n- Datasets 2.12.0\n- Tokenizers 0.13.3\n", "size_bytes": "242306565", "downloads": 8}
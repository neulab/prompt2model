{"pretrained_model_name": "liuyanchen1015/Finetuned_FLAN-T5_VALUE_adapterfusion_lr5e-4_bs32", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: Finetuned_FLAN-T5_VALUE_adapterfusion_lr5e-4_bs32\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# Finetuned_FLAN-T5_VALUE_adapterfusion_lr5e-4_bs32\n\nThis model is a fine-tuned version of [liuyanchen1015/FLAN-T5_GLUE_finetuning_lr3e-4](https://huggingface.co/liuyanchen1015/FLAN-T5_GLUE_finetuning_lr3e-4) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0870\n- Accuracy: 0.8692\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Accuracy |\n|:-------------:|:-----:|:-----:|:---------------:|:--------:|\n| 0.0862        | 0.08  | 2500  | 0.1059          | 0.8526   |\n| 0.092         | 0.17  | 5000  | 0.1025          | 0.856    |\n| 0.0943        | 0.25  | 7500  | 0.1126          | 0.8516   |\n| 0.0899        | 0.34  | 10000 | 0.0955          | 0.8578   |\n| 0.0896        | 0.42  | 12500 | 0.1046          | 0.8564   |\n| 0.0952        | 0.51  | 15000 | 0.0978          | 0.851    |\n| 0.0901        | 0.59  | 17500 | 0.0958          | 0.8498   |\n| 0.095         | 0.68  | 20000 | 0.0974          | 0.8532   |\n| 0.0955        | 0.76  | 22500 | 0.0982          | 0.853    |\n| 0.0912        | 0.85  | 25000 | 0.0980          | 0.853    |\n| 0.0913        | 0.93  | 27500 | 0.0944          | 0.8528   |\n| 0.0889        | 1.02  | 30000 | 0.0907          | 0.8592   |\n| 0.0871        | 1.1   | 32500 | 0.0933          | 0.855    |\n| 0.0872        | 1.18  | 35000 | 0.0904          | 0.861    |\n| 0.0859        | 1.27  | 37500 | 0.0879          | 0.8594   |\n| 0.0847        | 1.35  | 40000 | 0.0950          | 0.8584   |\n| 0.0827        | 1.44  | 42500 | 0.0909          | 0.8622   |\n| 0.0836        | 1.52  | 45000 | 0.0933          | 0.8552   |\n| 0.0805        | 1.61  | 47500 | 0.0928          | 0.8646   |\n| 0.0799        | 1.69  | 50000 | 0.0905          | 0.8648   |\n| 0.0789        | 1.78  | 52500 | 0.0863          | 0.87     |\n| 0.0786        | 1.86  | 55000 | 0.0907          | 0.8612   |\n| 0.0772        | 1.95  | 57500 | 0.0883          | 0.8672   |\n| 0.075         | 2.03  | 60000 | 0.0886          | 0.8664   |\n| 0.0727        | 2.12  | 62500 | 0.0878          | 0.8688   |\n| 0.0724        | 2.2   | 65000 | 0.0881          | 0.8708   |\n| 0.0729        | 2.28  | 67500 | 0.0879          | 0.8664   |\n| 0.0714        | 2.37  | 70000 | 0.0883          | 0.8694   |\n| 0.0694        | 2.45  | 72500 | 0.0876          | 0.8724   |\n| 0.0698        | 2.54  | 75000 | 0.0869          | 0.8698   |\n| 0.0706        | 2.62  | 77500 | 0.0872          | 0.8712   |\n| 0.0685        | 2.71  | 80000 | 0.0874          | 0.8692   |\n| 0.068         | 2.79  | 82500 | 0.0873          | 0.869    |\n| 0.0685        | 2.88  | 85000 | 0.0863          | 0.8688   |\n| 0.068         | 2.96  | 87500 | 0.0870          | 0.8692   |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.13.0+cu117\n- Datasets 2.10.1\n- Tokenizers 0.12.1\n", "size_bytes": "1232387711", "downloads": 2}
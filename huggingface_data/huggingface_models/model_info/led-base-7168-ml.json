{"pretrained_model_name": "bakhitovd/led-base-7168-ml", "description": "---\ndatasets:\n- bakhitovd/data_science_arxiv\nmetrics:\n- rouge\nlicense: cc0-1.0\npipeline_tag: summarization\n---\n# Fine-tuned Longformer for Summarization of Machine Learning Articles\n\n## Model Details\n- GitHub: https://github.com/Bakhitovd/led-base-7168-ml\n- Model name: bakhitovd/led-base-7168-ml\n- Model type: Longformer (alenai/led-base-16384)\n- Model description: This Longformer model has been fine-tuned on a focused subset of the arXiv part of the scientific papers dataset, specifically targeting articles about Machine Learning. It aims to generate accurate and consistent summaries of machine learning research papers.\n## Intended Use\nThis model is intended to be used for text summarization tasks, specifically for summarizing machine learning research papers.\n## How to Use\n```python\nimport torch\nfrom transformers import LEDTokenizer, LEDForConditionalGeneration\ntokenizer = LEDTokenizer.from_pretrained(\"bakhitovd/led-base-7168-ml\")\nmodel = LEDForConditionalGeneration.from_pretrained(\"bakhitovd/led-base-7168-ml\")\n```\n\n## Use the model for summarization\n```python\narticle = \"... long document ...\"\ninputs_dict = tokenizer.encode(article, padding=\"max_length\", max_length=16384, return_tensors=\"pt\", truncation=True)\ninput_ids = inputs_dict.input_ids.to(\"cuda\")\nattention_mask = inputs_dict.attention_mask.to(\"cuda\")\nglobal_attention_mask = torch.zeros_like(attention_mask)\nglobal_attention_mask[:, 0] = 1\npredicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask, max_length=512)\nsummary = tokenizer.decode(predicted_abstract_ids, skip_special_tokens=True)\nprint(summary)\n```\n## Training Data\nDataset name: bakhitovd/data_science_arxiv\\\nThis dataset is a subset of the 'Scientific papers' dataset, which contains articles semantically, structurally, and meaningfully closest to articles describing machine learning. This subset was obtained using K-means clustering on the embeddings generated by SciBERT.\n## Evaluation Results\nThe model's performance was evaluated using ROUGE metrics and it showed improved performance over the baseline models.\n\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/63fb9a520aa18292d5c1027a/19mfKrjHkiCFDAL557Vsu.png)", "size_bytes": "647680813", "downloads": 17}
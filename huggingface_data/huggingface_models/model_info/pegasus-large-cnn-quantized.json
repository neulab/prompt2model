{"pretrained_model_name": "aloutzidis/pegasus-large-cnn-quantized", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: pegasus-large-cnn-quantized\n  results: []\nlicense: apache-2.0\ndatasets:\n- cnn_dailymail\nmetrics:\n- bleu\n- rouge\nlanguage:\n- en\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# pegasus-large-cnn-quantized\n\nThis model is a fine-tuned version of [google/pegasus-large](https://huggingface.co/google/pegasus-large) on CNN/DailyMail dataset.\n\n![model image](./cover.jpeg)\n\n## Model description\n\nOnline news reading has become one of the most popular ways to consume the latest news. News aggregation websites such as Google News and Yahoo News have made it easy for users to find the latest news and provide thousands of news stories from hundreds of news publishers. As people have limited time, reading all news articles is not feasible. The success of zero-shot and few-shot prompting with models like GPT-3 has led to a paradigm shift in NLP. So, in the era of ChatGPT, we we conducted experiments using various large language models (LLMs) such as BART and PEGASUS to improve the quality and coherence of the generated news summaries.\n\nThis is a quantized model finetuned on the CNN/DailyMail dataset.\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 123\n- optimizer: AdamW with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5.0\n\n### Framework versions\n\n- Transformers 4.29.0\n- Pytorch 1.13.1+cu117\n- Datasets 2.12.0\n- Tokenizers 0.13.3", "size_bytes": "2283795117", "downloads": 14}
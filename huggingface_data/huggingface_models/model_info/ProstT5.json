{"pretrained_model_name": "Rostlab/ProstT5", "description": "---\nlicense: mit\ndatasets:\n- adrianhenkel/lucidprots_full_data\npipeline_tag: translation\ntags:\n- biology\n---\n# Model Card for ProstT5\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nProstT5 is a protein language model (pLM) which can translate between protein sequence and structure.\n![ProstT5 pre-training and inference](./prostt5_sketch2.png)\n\n## Model Details\n\n### Model Description\n\nProstT5 (Protein structure-sequence T5) is based on [ProtT5-XL-U50](https://huggingface.co/Rostlab/prot_t5_xl_uniref50), a T5 model trained on encoding protein sequences using span corruption applied on billions of protein sequences.\nProstT5 finetunes [ProtT5-XL-U50](https://huggingface.co/Rostlab/prot_t5_xl_uniref50) on translating between protein sequence and structure using 17M proteins with high-quality 3D structure predictions from the AlphaFoldDB.\nProtein structure is converted from 3D to 1D using the 3Di-tokens introduced by [Foldseek](https://github.com/steineggerlab/foldseek).\nIn a first step, ProstT5 learnt to represent the newly introduced 3Di-tokens by continuing the original span-denoising objective applied on 3Di- and amino acid- (AA) sequences.\nOnly in a second step, ProstT5 was trained on translating between the two modalities.\nThe direction of the translation is indicated by two special tokens (\"\\<fold2AA>\" for translating from 3Di to AAs, \u201c\\<AA2fold>\u201d for translating from AAs to 3Di).\nTo avoid clashes with AA tokens, 3Di-tokens were cast to lower-case (alphabets are identical otherwise).\n\n- **Developed by:** Michael Heinzinger (GitHub [@mheinzinger](https://github.com/mheinzinger); Twitter [@HeinzingerM](https://twitter.com/HeinzingerM))\n- **Model type:** Encoder-decoder (T5)\n- **Language(s) (NLP):** Protein sequence and structure\n- **License:** MIT\n- **Finetuned from model:** [ProtT5-XL-U50](https://huggingface.co/Rostlab/prot_t5_xl_uniref50)\n\n## Uses\n\n1. The model can be used for traditional feature extraction.\nFor this, we recommend using only the [encoder](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel) in half-precision (fp16) together with batching. Examples (currently only for original [ProtT5-XL-U50](https://huggingface.co/Rostlab/prot_t5_xl_uniref50) but replacing repository links and adding prefixes works): [script](https://github.com/agemagician/ProtTrans/blob/master/Embedding/prott5_embedder.py) and [colab](https://colab.research.google.com/drive/1h7F5v5xkE_ly-1bTQSu-1xaLtTP2TnLF?usp=sharing)  \nWhile original [ProtT5-XL-U50](https://huggingface.co/Rostlab/prot_t5_xl_uniref50) could only embed AA sequences, ProstT5 can now also embed 3D structures represented by 3Di tokens. 3Di tokens can either be derived from 3D structures using Foldseek or they can be predicted from AA sequences by ProstT5.\n3. \"Folding\": Translation from sequence (AAs) to structure (3Di). The resulting 3Di strings can be used together with [Foldseek](https://github.com/steineggerlab/foldseek) for remote homology detection while avoiding to compute 3D structures explicitly.\n4. \"Inverse Folding\": Translation from structure (3Di) to sequence (AA). \n\n\n## How to Get Started with the Model\n\nFeature extraction:\n```python\nfrom transformers import T5Tokenizer, T5EncoderModel\nimport torch\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Load the tokenizer\ntokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5', do_lower_case=False).to(device)\n\n# Load the model\nmodel = T5EncoderModel.from_pretrained(\"Rostlab/ProstT5\").to(device)\n\n# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\nmodel.full() if device=='cpu' else model.half()\n\n# prepare your protein sequences/structures as a list. Amino acid sequences are expected to be upper-case (\"PRTEINO\" below) while 3Di-sequences need to be lower-case (\"strctr\" below).\nsequence_examples = [\"PRTEINO\", \"strct\"]\n\n# replace all rare/ambiguous amino acids by X (3Di sequences does not have those) and introduce white-space between all sequences (AAs and 3Di)\nsequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n\n# add pre-fixes accordingly (this already expects 3Di-sequences to be lower-case)\n# if you go from AAs to 3Di (or if you want to embed AAs), you need to prepend \"<AA2fold>\"\n# if you go from 3Di to AAs (or if you want to embed 3Di), you need to prepend \"<fold2AA>\"\nsequence_examples = [ \"<AA2fold>\" + \" \" + s if s.isupper() else \"<fold2AA>\" + \" \" + s\n                      for s in sequence_examples\n                    ]\n\n# tokenize sequences and pad up to the longest sequence in the batch\nids = tokenizer.batch_encode_plus(sequences_example, add_special_tokens=True, padding=\"longest\",return_tensors='pt').to(device))\n\n# generate embeddings\nwith torch.no_grad():\n    embedding_rpr = model(\n              ids.input_ids, \n              attention_mask=ids.attention_mask\n              )\n\n# extract residue embeddings for the first ([0,:]) sequence in the batch and remove padded & special tokens, incl. prefix ([0,1:8]) \nemb_0 = embedding_repr.last_hidden_state[0,1:8] # shape (7 x 1024)\n# same for the second ([1,:]) sequence but taking into account different sequence lengths ([1,:6])\nemb_1 = embedding_repr.last_hidden_state[1,1:6] # shape (5 x 1024)\n\n# if you want to derive a single representation (per-protein embedding) for the whole protein\nemb_0_per_protein = emb_0.mean(dim=0) # shape (1024)\n```\n\nTranslation (\"folding\", i.e., AA to 3Di):\n```python\nfrom transformers import T5Tokenizer, AutoModelForSeq2SeqLM\nimport torch\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Load the tokenizer\ntokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5', do_lower_case=False).to(device)\n\n# Load the model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Rostlab/ProstT5\").to(device)\n\n# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\nmodel.full() if device=='cpu' else model.half()\n\n# prepare your protein sequences/structures as a list.\n# Amino acid sequences are expected to be upper-case (\"PRTEINO\" below)\n# while 3Di-sequences need to be lower-case.\nsequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\nmin_len = min([ len(s) for s in folding_example])\nmax_len = max([ len(s) for s in folding_example])\n\n# replace all rare/ambiguous amino acids by X (3Di sequences does not have those) and introduce white-space between all sequences (AAs and 3Di)\nsequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n\n# add pre-fixes accordingly. For the translation from AAs to 3Di, you need to prepend \"<AA2fold>\"\nsequence_examples = [ \"<AA2fold>\" + \" \" + s for s in sequence_examples]\n\n# tokenize sequences and pad up to the longest sequence in the batch\nids = tokenizer.batch_encode_plus(sequences_example,\n                                  add_special_tokens=True,\n                                  padding=\"longest\",\n                                  return_tensors='pt').to(device))\n\n# Generation configuration for \"folding\" (AA-->3Di)\ngen_kwargs_aa2fold = {\n                  \"do_sample\": True,\n                  \"num_beams\": 3, \n                  \"top_p\" : 0.95, \n                  \"temperature\" : 1.2, \n                  \"top_k\" : 6,\n                  \"repetition_penalty\" : 1.2,\n}\n\n# translate from AA to 3Di (AA-->3Di)\nwith torch.no_grad():\n  translations = model.generate( \n              ids.input_ids, \n              attention_mask=ids.attention_mask, \n              max_length=max_len, # max length of generated text\n              min_length=min_len, # minimum length of the generated text\n              early_stopping=True, # stop early if end-of-text token is generated\n              num_return_sequences=1, # return only a single sequence\n              **gen_kwargs_aa2fold\n  )\n# Decode and remove white-spaces between tokens\ndecoded_translations = tokenizer.batch_decode( translations, skip_special_tokens=True )\nstructure_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_translations ] # predicted 3Di strings\n\n# Now we can use the same model and invert the translation logic\n# to generate an amino acid sequence from the predicted 3Di-sequence (3Di-->AA)\n\n# add pre-fixes accordingly. For the translation from 3Di to AA (3Di-->AA), you need to prepend \"<fold2AA>\"\nsequence_examples_backtranslation = [ \"<fold2AA>\" + \" \" + s for s in decoded_translations]\n\n# tokenize sequences and pad up to the longest sequence in the batch\nids_backtranslation = tokenizer.batch_encode_plus(sequence_examples_backtranslation,\n                                  add_special_tokens=True,\n                                  padding=\"longest\",\n                                  return_tensors='pt').to(device))\n\n# Example generation configuration for \"inverse folding\" (3Di-->AA)\ngen_kwargs_fold2AA = {\n            \"do_sample\": True,\n            \"top_p\" : 0.90,\n            \"temperature\" : 1.1,\n            \"top_k\" : 6,\n            \"repetition_penalty\" : 1.2,\n}\n\n# translate from 3Di to AA (3Di-->AA)\nwith torch.no_grad():\n  backtranslations = model.generate( \n              ids_backtranslation.input_ids, \n              attention_mask=ids_backtranslation.attention_mask, \n              max_length=max_len, # max length of generated text\n              min_length=min_len, # minimum length of the generated text\n              early_stopping=True, # stop early if end-of-text token is generated\n              num_return_sequences=1, # return only a single sequence\n              **gen_kwargs_fold2AA\n  )\n# Decode and remove white-spaces between tokens\ndecoded_backtranslations = tokenizer.batch_decode( backtranslations, skip_special_tokens=True )\naminoAcid_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_backtranslations ] # predicted amino acid strings\n\n```\n\n\n## Training Details\n\n### Training Data\n\n[Pre-training data (3Di+AA sequences for 17M proteins)](https://huggingface.co/datasets/adrianhenkel/lucidprots_full_data)\n\n\n### Training Procedure \n\nThe first phase of the pre-training is continuing span-based denoising using 3Di- and AA-sequences using this [script](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_t5_mlm_flax.py).\nFor the second phase of pre-training (actual translation from 3Di- to AA-sequences and vice versa), we used this [script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization_no_trainer.py).\n\n\n#### Training Hyperparameters\n\n- **Training regime:** we used DeepSpeed (stage-2), gradient accumulation steps (5 steps), mixed half-precision (bf16) and PyTorch2.0\u2019s torchInductor compiler\n\n#### Speed\n\nGenerating embeddings for the human proteome from the Pro(s)tT5 encoder requires around 35m (minutes) or 0.1s (seconds) per protein using batch-processing and half-precision (fp16) on a single RTX A6000 GPU with 48 GB vRAM. \nThe translation is comparatively slow (0.6-2.5s/protein at an average length 135 and 406, respectively) due to the sequential nature of the decoding process which needs to generate left-to-right, token-by-token. \nWe only used batch-processing with half-precision without further optimization.\n", "size_bytes": "11275478387", "downloads": 264}
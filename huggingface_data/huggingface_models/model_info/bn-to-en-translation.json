{"pretrained_model_name": "shihab17/bn-to-en-translation", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- text-generation-inference\ndatasets:\n- kde4\nmetrics:\n- bleu\nmodel-index:\n- name: bengali-bn-to-en\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: kde4\n      type: kde4\n      config: bn-en\n      split: train\n      args: bn-en\n    metrics:\n    - name: Bleu\n      type: bleu\n      value: 50.9475\nlanguage:\n- bn\n- en\npipeline_tag: text2text-generation\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n### How to use\n\nYou can use this model directly with a pipeline:\n\n\n```python\nfrom transformers import AutoTokenizer, pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"shihab17/bn-to-en-translation\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"shihab17/bn-to-en-translation\")\n\nsentence = '\u09ae\u09cd\u09af\u09be\u099a \u09b6\u09c7\u09b7\u09c7 \u09aa\u09c1\u09b0\u09b8\u09cd\u0995\u09be\u09b0 \u09ac\u09bf\u09a4\u09b0\u09a3\u09c7\u09b0 \u09ae\u099e\u09cd\u099a\u09c7 \u09a4\u09be\u09ae\u09bf\u09ae\u09c7\u09b0 \u09ae\u09c1\u0996\u09c7 \u09ae\u09cb\u09b8\u09cd\u09a4\u09be\u09ab\u09bf\u099c\u09c7\u09b0 \u09aa\u09cd\u09b0\u09b6\u0982\u09b8\u09be \u09b6\u09cb\u09a8\u09be \u0997\u09c7\u09b2'\n\ntranslator = pipeline(\"translation_en_to_bn\", model=model, tokenizer=tokenizer)\noutput = translator(sentence)\nprint(output)\n```\n\n\n# bengali-en-to-bn\n\nThis model is a fine-tuned version of [Helsinki-NLP/opus-mt-bn-en](https://huggingface.co/Helsinki-NLP/opus-mt-bn-en) on the kde4 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6885\n- Bleu: 50.9475\n- Gen Len: 6.7043\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 1.8866        | 1.0   | 2047  | 1.6397          | 39.6617 | 8.0651  |\n| 1.5769        | 2.0   | 4094  | 1.6160          | 33.0247 | 8.9865  |\n| 1.3622        | 3.0   | 6141  | 1.6189          | 53.483  | 6.6037  |\n| 1.2317        | 4.0   | 8188  | 1.6280          | 51.6882 | 6.762   |\n| 1.1248        | 5.0   | 10235 | 1.6450          | 53.1619 | 6.5515  |\n| 1.0297        | 6.0   | 12282 | 1.6587          | 52.3224 | 6.5905  |\n| 0.9632        | 7.0   | 14329 | 1.6733          | 52.3362 | 6.5441  |\n| 0.8831        | 8.0   | 16376 | 1.6802          | 49.3544 | 6.8272  |\n| 0.8291        | 9.0   | 18423 | 1.6868          | 49.9486 | 6.792   |\n| 0.8175        | 10.0  | 20470 | 1.6885          | 50.9475 | 6.7043  |\n\n\n### Framework versions\n\n- Transformers 4.29.1\n- Pytorch 2.0.0+cu118\n- Datasets 2.12.0\n- Tokenizers 0.13.3", "size_bytes": "307141509", "downloads": 3}
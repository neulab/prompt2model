{"pretrained_model_name": "IDEA-CCNL/Randeng-T5-784M-QA-Chinese", "description": "---\nlanguage:\n - zh\n\ntags:\n - question-answering\n - text-generation\n\npipeline-tag:\n - text-generation\n\nmetrics:\n - RougeL\n - BLEU-4\n - F1\n - EM\n - Contain Answer Rate\n \nwidget:\n- text: \"question:\u7f8e\u56fd\u5efa\u7b51\u5e08\u662f\u600e\u6837\u521b\u9020\u7ef4\u591a\u5229\u4e9a\u54e5\u7279\u5f0f\u5efa\u7b51\u7684?\"\n  context: \"knowledge:\u5e95\u7279\u5f8b\u5723\u4fdd\u7f57\u5ea7\u5802(Cathedral Church of St. Paul)\u662f\u7f8e\u56fd\u5723\u516c\u4f1a\u5bc6\u6b47\u6839\u6559\u533a\u7684\u4e3b\u6559\u5ea7\u5802,\u4f4d\u4e8e\u5e95\u7279\u5f8b\u4f0d\u5fb7\u6c83\u5fb7\u5927\u90534800\u53f7,\u6bd7\u90bb\u97e6\u6069\u5dde\u7acb\u5927\u5b66\u6821\u56ed\u3002\u5723\u4fdd\u7f57\u5802\u533a\u6210\u7acb\u4e8e1824\u5e74,\u662f\u5bc6\u6b47\u6839\u7b2c\u4e00\u4e2a\u65b0\u6559\u5802\u4f1a\u3002\u73b0\u5b58\u5efa\u7b51\u7531\u8457\u540d\u6559\u5802\u8bbe\u8ba1\u5e08\u62c9\u5c14\u592b\u00b7\u514b\u62c9\u59c6(Ralph Adams Cram),\u59cb\u5efa\u4e8e1907\u5e74,\u81f3\u4eca\u949f\u697c\u5c1a\u672a\u5b8c\u6210\u3002\u6559\u5802\u5b8c\u5168\u7528\u77f3\u7070\u5ca9\u548c\u4e2d\u4e16\u7eaa\u5efa\u7b51\u6280\u672f\u5efa\u9020,\u6ca1\u6709\u652f\u6301\u7684\u94a2\u94c1\u4e0a\u5c42\u5efa\u7b51\u3002\u5efa\u8bbe\u62e5\u6709\u4ea4\u9519\u9aa8,\u5927\u7247\u82b1\u7a97\u73bb\u7483,\u96d5\u9970\u7a97\u683c,\u54e5\u7279\u5f0f\u5efa\u7b51\u7684\u6977\u6a21,\u5305\u62ecPewabic \u9676\u74f7\u4e2d\u5fc3\u3002\u57281912\u5e74\u6210\u4e3a\u6559\u533a\u7684\u4e3b\u6559\u5ea7\u5802\u3002\u5723\u4fdd\u7f57\u5ea7\u5802\u662f20\u4e16\u7eaa\u521d\u540e\u671f\u54e5\u7279\u590d\u5174\u5efa\u7b51\u7684\u6700\u4f73\u5b9e\u4f8b\u4e4b\u4e00\u300219\u4e16\u7eaa\u4e2d\u53f6\u7684\u7f8e\u56fd\u5efa\u7b51\u5e08\u8f93\u5165\u5e76\u91cd\u65b0\u9610\u91ca\u4e86\u82f1\u56fd\u54e5\u7279\u590d\u5174\u98ce\u683c,\u57fa\u4e8e\u4e2d\u4e16\u7eaa\u4e3b\u6559\u5ea7\u5802\u7684\u89c6\u89c9\u4e30\u5bcc\u7684\u7ec6\u8282\u3002\u7f8e\u56fd\u5efa\u7b51\u5e08\u5c06\u54e5\u7279\u5143\u7d20\u4e0e\u7b80\u5355\u7684\u5efa\u7b51\u89c4\u5212\u76f8\u7ed3\u5408,\u521b\u9020\u4e86\u7f8e\u56fd\u5efa\u7b51\u98ce\u683c\u201c\u7ef4\u591a\u5229\u4e9a\u54e5\u7279\u5f0f\u201d(Victorian Gothic)\u3002\u5174\u5efa\u4e8e1876\u5e74\u7684\u5821\u5792\u8857\u957f\u8001\u4f1a\u6559\u5802\u5c31\u662f\u65e9\u671f\u7ef4\u591a\u5229\u4e9a\u54e5\u7279\u5f0f\u5efa\u7b51\u7684\u6770\u51fa\u4f8b\u8bc1\u3002answer:<extra_id_0>\" \n  example_title: \"\u5c06\u54e5\u7279\u5143\u7d20\u4e0e\u7b80\u5355\u7684\u5efa\u7b51\u89c4\u5212\u76f8\u7ed3\u5408\"\n\n\nlicence: apache-2.0\n---\n# Randeng-T5-784M-QA-Chinese\nT5 for Chinese Question Answering\n- Main Page:[Fengshenbang](https://fengshenbang-lm.com/)\n- Github: [Fengshenbang-LM](https://github.com/IDEA-CCNL/Fengshenbang-LM)\n\n\n## \u7b80\u4ecb Brief Introduction\nThis T5-Large model, is the first pretrained generative question answering model for Chinese in huggingface. It was pretrained on the Wudao 180G corpus, and finetuned on Chinese SQuAD and CMRC2018 dataset. It can produce a fluent and accurate answer given a passage and question.\n\n\u8fd9\u662fhuggingface\u4e0a\u9996\u4e2a\u4e2d\u6587\u7684\u751f\u6210\u5f0f\u95ee\u7b54\u6a21\u578b\u3002\u5b83\u57fa\u4e8eT5-Large\u7ed3\u6784\uff0c\u4f7f\u7528\u609f\u9053180G\u8bed\u6599\u5728[\u5c01\u795e\u6846\u67b6](https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen)\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5728\u7ffb\u8bd1\u7684\u4e2d\u6587SQuAD\u548cCMRC2018\u4e24\u4e2a\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002\u8f93\u5165\u4e00\u7bc7\u6587\u7ae0\u548c\u4e00\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u751f\u6210\u51c6\u786e\u6d41\u7545\u7684\u56de\u7b54\u3002\n\n## \u6a21\u578b\u7c7b\u522b Model Taxonomy\n\n|  \u9700\u6c42 Demand  | \u4efb\u52a1 Task       | \u7cfb\u5217 Series      | \u6a21\u578b Model    | \u53c2\u6570 Parameter | \u989d\u5916 Extra |\n|  :----:  | :----:  | :----:  | :----:  | :----:  | :----:  |\n| \u901a\u7528 General | \u81ea\u7136\u8bed\u8a00\u8f6c\u6362 NLT | \u71c3\u706f Randeng | T5 |      784M      |     \u4e2d\u6587\u751f\u6210\u5f0f\u95ee\u7b54 -Chinese Generative Question Answering   |\n\n## \u6a21\u578b\u8868\u73b0 Performance \n  \n CMRC 2018 dev (Original span prediction task, we cast it as a generative QA task)\n\n CMRC 2018\u7684\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6548\u679c\uff08\u539f\u59cb\u4efb\u52a1\u662f\u4e00\u4e2a\u8d77\u59cb\u548c\u7ed3\u675f\u9884\u6d4b\u95ee\u9898\uff0c\u8fd9\u91cc\u4f5c\u4e3a\u4e00\u4e2a\u751f\u6210\u56de\u7b54\u7684\u95ee\u9898\uff09\n  \n   | model | Contain Answer Rate| RougeL | BLEU-4 |F1 | EM | \n   |-------|----|----|--------------------|--------|--------|\n   | Ours | 76.0 | 82.7 |61.1|77.9 |57.1|\n   |MacBERT-Large(SOTA)|-|-|-|88.9|70.0|\n   \n   Our model enjoys a high level of generation quality and accuracy, with 76% of generated answers containing the ground truth. The high RougeL and BLEU-4 reveal the overlap between generated results and ground truth. Our model has a lower EM because it generates complete sentences while golden answers are segmentations of sentences. \n   P.S.The SOTA model only predicts the start and end tag as an extractive MRC task. \n\n   \u6211\u4eec\u7684\u6a21\u578b\u6709\u7740\u6781\u9ad8\u7684\u751f\u6210\u8d28\u91cf\u548c\u51c6\u786e\u7387\uff0c76%\u7684\u56de\u7b54\u5305\u542b\u4e86\u6b63\u786e\u7b54\u6848(Contain Answer Rate)\u3002RougeL\u548cBLEU-4\u53cd\u6620\u4e86\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u548c\u6807\u51c6\u7b54\u6848\u91cd\u5408\u7684\u7a0b\u5ea6\u3002\u6211\u4eec\u7684\u6a21\u578bEM\u503c\u8f83\u4f4e\uff0c\u56e0\u4e3a\u751f\u6210\u7684\u5927\u90e8\u5206\u4e3a\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u800c\u6807\u51c6\u7b54\u6848\u901a\u5e38\u662f\u53e5\u5b50\u7247\u6bb5\u3002\n   P.S. SOTA\u6a21\u578b\u53ea\u9700\u9884\u6d4b\u8d77\u59cb\u548c\u7ed3\u675f\u4f4d\u7f6e\uff0c\u8fd9\u79cd\u62bd\u53d6\u5f0f\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u6bd4\u751f\u6210\u5f0f\u7684\u7b80\u5355\u5f88\u591a\u3002\n\n## \u6837\u4f8b Cases\n\nHere are random picked samples:\n<img src=\"https://huggingface.co/IDEA-CCNL/Randeng-T5-784M-QA-Chinese/resolve/main/cases_t5_cmrc.png\" div align=middle />\n\n*pred:* in picture are generated results,*target* indicates groud truth.\n\nIf the picture fails to display, you can find the picture in Files and versions. \n\n## \u4f7f\u7528 Usage\npip install transformers==4.21.1\n```python\nimport numpy as np\nfrom transformers import T5Tokenizer,MT5ForConditionalGeneration\n\npretrain_path = 'IDEA-CCNL/Randeng-T5-784M-QA-Chinese'\ntokenizer=T5Tokenizer.from_pretrained(pretrain_path)\nmodel=MT5ForConditionalGeneration.from_pretrained(pretrain_path)\n\nsample={\"context\":\"\u5728\u67cf\u6797,\u80e1\u683c\u8bfa\u6d3e\u6559\u5f92\u521b\u5efa\u4e86\u4e24\u4e2a\u65b0\u7684\u793e\u533a:\u591a\u7f57\u897f\u6069\u65af\u5854\u7279\u548c\u5f17\u91cc\u5fb7\u91cc\u5e0c\u65af\u5854\u7279\u3002\u52301700\u5e74,\u8fd9\u4e2a\u57ce\u5e02\u4e94\u5206\u4e4b\u4e00\u7684\u4eba\u53e3\u8bb2\u6cd5\u8bed\u3002\u67cf\u6797\u80e1\u683c\u8bfa\u6d3e\u5728\u4ed6\u4eec\u7684\u6559\u5802\u670d\u52a1\u4e2d\u4fdd\u7559\u4e86\u5c06\u8fd1\u4e00\u4e2a\u4e16\u7eaa\u7684\u6cd5\u8bed\u3002\u4ed6\u4eec\u6700\u7ec8\u51b3\u5b9a\u6539\u7528\u5fb7\u8bed,\u4ee5\u6297\u8bae1806-1807\u5e74\u62ff\u7834\u4ed1\u5360\u9886\u666e\u9c81\u58eb\u3002\u4ed6\u4eec\u7684\u8bb8\u591a\u540e\u4ee3\u90fd\u6709\u663e\u8d6b\u7684\u5730\u4f4d\u3002\u6210\u7acb\u4e86\u51e0\u4e2a\u6559\u4f1a,\u5982\u5f17\u96f7\u5fb7\u91cc\u590f(\u4e39\u9ea6)\u3001\u67cf\u6797\u3001\u65af\u5fb7\u54e5\u5c14\u6469\u3001\u6c49\u5821\u3001\u6cd5\u5170\u514b\u798f\u3001\u8d6b\u5c14\u8f9b\u57fa\u548c\u57c3\u59c6\u767b\u7684\u6559\u4f1a\u3002\",\"question\":\"\u9664\u4e86\u591a\u7f57\u897f\u6069\u65af\u5854\u7279,\u67cf\u6797\u8fd8\u6709\u54ea\u4e2a\u65b0\u7684\u793e\u533a?\",\"idx\":1}\nplain_text='question:'+sample['question']+'knowledge:'+sample['context'][:self.max_knowledge_length]\n\nres_prefix=tokenizer.encode('answer',add_special_tokens=False)\nres_prefix.append(tokenizer.convert_tokens_to_ids('<extra_id_0>'))\nres_prefix.append(tokenizer.eos_token_id)\nl_rp=len(res_prefix)\n\ntokenized=tokenizer.encode(plain_text,add_special_tokens=False,truncation=True,max_length=1024-2-l_rp)\ntokenized+=res_prefix\nbatch=[tokenized]*2\ninput_ids=torch.tensor(np.array(batch),dtype=torch.long)\n\n# Generate answer\nmax_target_length=128\npred_ids = model.generate(input_ids=input_ids,max_new_tokens=max_target_length,do_sample=True,top_p=0.9)\npred_tokens=tokenizer.batch_decode(pred_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nres=pred_tokens.replace('<extra_id_0>','').replace('\u6709\u7b54\u6848:','')\n```\n\n\n# \u5f15\u7528 Citation\n\u5982\u679c\u60a8\u5728\u60a8\u7684\u5de5\u4f5c\u4e2d\u4f7f\u7528\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684[\u8bba\u6587](https://arxiv.org/abs/2210.08590)\uff1a\n\nIf you are using the resource for your work, please cite the our [paper](https://arxiv.org/abs/2210.08590):\n\n```text\n@article{fengshenbang,\n  author    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},\n  title     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},\n  journal   = {CoRR},\n  volume    = {abs/2209.02970},\n  year      = {2022}\n}\n```\n\nYou can also cite our [website](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n\n\u6b22\u8fce\u5f15\u7528\u6211\u4eec\u7684[\u7f51\u7ad9](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n```text\n@misc{Fengshenbang-LM,\n  title={Fengshenbang-LM},\n  author={IDEA-CCNL},\n  year={2021},\n  howpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},\n}\n```\n", "size_bytes": "1568317675", "downloads": 404}
{"pretrained_model_name": "arijitx/IndicBART-bn-QuestionGeneration", "description": "---\nlicense: mit\nlanguage:\n- bn\ntags:\n  - text2text-generation\nwidget:\n  - text: \"\u09e7\u09ee\u09ef\u09ed \u0996\u09cd\u09b0\u09bf\u09b7\u09cd\u099f\u09be\u09ac\u09cd\u09a6\u09c7\u09b0 \u09e8\u09e9 \u099c\u09be\u09a8\u09c1\u09af\u09bc\u09be\u09b0\u09bf [SEP] \u09b8\u09c1\u09ad\u09be\u09b7 \u09e7\u09ee\u09ef\u09ed \u0996\u09cd\u09b0\u09bf\u09b7\u09cd\u099f\u09be\u09ac\u09cd\u09a6\u09c7\u09b0 \u09e8\u09e9 \u099c\u09be\u09a8\u09c1\u09af\u09bc\u09be\u09b0\u09bf \u09ac\u09cd\u09b0\u09bf\u099f\u09bf\u09b6 \u09ad\u09be\u09b0\u09a4\u09c7\u09b0 \u0985\u09a8\u09cd\u09a4\u09b0\u09cd\u0997\u09a4 \u09ac\u09be\u0982\u09b2\u09be \u09aa\u09cd\u09b0\u09a6\u09c7\u09b6\u09c7\u09b0 \u0989\u09a1\u09bc\u09bf\u09b7\u09cd\u09af\u09be \u09ac\u09bf\u09ad\u09be\u0997\u09c7\u09b0 \u0995\u099f\u0995\u09c7 \u099c\u09a8\u09cd\u09ae\u0997\u09cd\u09b0\u09b9\u09a3 \u0995\u09b0\u09c7\u09a8\u0964 </s> <2bn>\"\n---\n\n\n## Intro \n  Trained on IndicNLGSuit [IndicQuestionGeneration](https://huggingface.co/datasets/ai4bharat/IndicQuestionGeneration) data for Bengali the model is finetuned from [IndicBART](https://huggingface.co/ai4bharat/IndicBART)\n\n## Finetuned Command\n\n    python run_summarization.py --model_name_or_path bnQG_models/checkpoint-32000 --do_eval --train_file train_bn.json \n    --validation_file valid_bn.json --output_dir bnQG_models --overwrite_output_dir --per_device_train_batch_size=2 \n    --per_device_eval_batch_size=4 --predict_with_generate --text_column src --summary_column tgt --save_steps 4000 \n    --evaluation_strategy steps --gradient_accumulation_steps 4 --eval_steps 1000 --learning_rate 0.001 --num_beams 4 \n    --forced_bos_token \"<2bn>\" --num_train_epochs 10 --warmup_steps 10000\n    \n## Sample Line from train data\n\n    {\"src\": \"\u092a\u094d\u0930\u093e\u0923\u092c\u093e\u0926\u0940 [SEP] \u0905\u0930\u094d\u0925\u093e\u094e, \u0924\u093f\u0928\u093f \u091b\u093f\u0932\u0947\u0928 \u090f\u0915\u091c\u0928 \u0938\u0930\u094d\u092c\u092a\u094d\u0930\u093e\u0923\u092c\u093e\u0926\u0940\u0964 </s> <2bn>\", \"tgt\": \"<2bn> \u0915\u094b\u0928 \u0926\u093e\u0930\u094d\u0936\u0928\u093f\u0915 \u0926\u0943\u0937\u094d\u091f\u093f\u092d\u0919\u094d\u0917\u093f \u0913\u092f\u093c\u093e\u0907\u091f\u091c\u0947\u0930 \u091b\u093f\u0932? </s>\"}\n\n## Inference\n  \n    script = \"\u09b8\u09c1\u09ad\u09be\u09b7 \u09e7\u09ee\u09ef\u09ed \u0996\u09cd\u09b0\u09bf\u09b7\u09cd\u099f\u09be\u09ac\u09cd\u09a6\u09c7\u09b0 \u09e8\u09e9 \u099c\u09be\u09a8\u09c1\u09af\u09bc\u09be\u09b0\u09bf \u09ac\u09cd\u09b0\u09bf\u099f\u09bf\u09b6 \u09ad\u09be\u09b0\u09a4\u09c7\u09b0 \u0985\u09a8\u09cd\u09a4\u09b0\u09cd\u0997\u09a4 \u09ac\u09be\u0982\u09b2\u09be \u09aa\u09cd\u09b0\u09a6\u09c7\u09b6\u09c7\u09b0 \u0989\u09a1\u09bc\u09bf\u09b7\u09cd\u09af\u09be \u09ac\u09bf\u09ad\u09be\u0997\u09c7\u09b0 (\u0985\u09a7\u09c1\u09a8\u09be, \u09ad\u09be\u09b0\u09a4\u09c7\u09b0 \u0993\u09a1\u09bc\u09bf\u09b6\u09be \u09b0\u09be\u099c\u09cd\u09af) \u0995\u099f\u0995\u09c7 \u099c\u09a8\u09cd\u09ae\u0997\u09cd\u09b0\u09b9\u09a3 \u0995\u09b0\u09c7\u09a8\u0964\"\n    answer = \"\u09e7\u09ee\u09ef\u09ed \u0996\u09cd\u09b0\u09bf\u09b7\u09cd\u099f\u09be\u09ac\u09cd\u09a6\u09c7\u09b0 \u09e8\u09e9 \u099c\u09be\u09a8\u09c1\u09af\u09bc\u09be\u09b0\u09bf\"\n    inp = answer +\" [SEP] \"+script + \" </s> <2bn>\"\n    inp_tok = tokenizer(inp, add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids\n    model.eval() # Set dropouts to zero\n\n    model_output=model.generate(inp_tok, use_cache=True, \n                                num_beams=4, \n                                max_length=20, \n                                min_length=1, \n                                early_stopping=True, \n                                pad_token_id=pad_id, \n                                bos_token_id=bos_id, \n                                eos_token_id=eos_id, \n                                decoder_start_token_id=tokenizer._convert_token_to_id_with_added_voc(\"<2bn>\")\n                            )\n    decoded_output=tokenizer.decode(model_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n    \n\n\n## Citations\n\n    @inproceedings{dabre2021indicbart,\n        title={IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages}, \n        author={Raj Dabre and Himani Shrotriya and Anoop Kunchukuttan and Ratish Puduppully and Mitesh M. Khapra and Pratyush Kumar},\n        year={2022},\n        booktitle={Findings of the Association for Computational Linguistics},\n        }    \n    \n\n    @misc{kumar2022indicnlg,\n      title={IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages}, \n      author={Aman Kumar and Himani Shrotriya and Prachi Sahu and Raj Dabre and Ratish Puduppully and Anoop Kunchukuttan and Amogh Mishra and Mitesh M. Khapra and Pratyush Kumar},\n      year={2022},\n      eprint={2203.05437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n    }   \n  ", "size_bytes": "976412337", "downloads": 4}
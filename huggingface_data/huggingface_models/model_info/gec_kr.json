{"pretrained_model_name": "Soyoung97/gec_kr", "description": "## Korean Grammatical Error Correction Model\nmaintainer: [Soyoung Yoon](https://soyoung97.github.io/profile/)\n\nOfficial repository: [link](https://github.com/soyoung97/GEC-Korean)\n\nDataset request form: [link](https://forms.gle/kF9pvJbLGvnh8ZnQ6)\n\nDemo: [link](https://huggingface.co/spaces/Soyoung97/gec-korean-demo)\n\nColab demo: [link](https://colab.research.google.com/drive/1CL__3CpkhBzxWUbvsQmPTQWWu1cWmJHa?usp=sharing)\n\n\n### Sample code\n```\nimport torch\nfrom transformers import PreTrainedTokenizerFast\nfrom transformers import BartForConditionalGeneration\n\ntokenizer = PreTrainedTokenizerFast.from_pretrained('Soyoung97/gec_kr')\nmodel = BartForConditionalGeneration.from_pretrained('Soyoung97/gec_kr')\n\ntext = '\ud55c\uad6d\uc5b4\ub294\uc5b4\ub835\ub2e4.'\n\nraw_input_ids = tokenizer.encode(text)\ninput_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\ncorrected_ids = model.generate(torch.tensor([input_ids]),\n                                max_length=128,\n                                eos_token_id=1, num_beams=4,\n                                early_stopping=True, repetition_penalty=2.0)\noutput_text = tokenizer.decode(corrected_ids.squeeze().tolist(), skip_special_tokens=True)\n\n\noutput_text\n>>> '\ud55c\uad6d\uc5b4\ub294 \uc5b4\ub835\ub2e4.'\n```\n\nSpecial thanks to the [KoBART-summarization repository](https://huggingface.co/gogamza/kobart-summarization) (referenced from it)", "size_bytes": "495642553", "downloads": 165}
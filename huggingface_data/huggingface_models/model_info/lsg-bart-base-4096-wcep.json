{"pretrained_model_name": "ccdv/lsg-bart-base-4096-wcep", "description": "---\nlanguage:\n- en\ntags:\n- summarization\ndatasets:\n- ccdv/WCEP-10\nmetrics:\n- rouge\nmodel-index:\n- name: ccdv/lsg-bart-base-4096-wcep\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n**Transformers >= 4.23.1**\\\n**This model relies on a custom modeling file, you need to add trust_remote_code=True**\\\n**See [\\#13467](https://github.com/huggingface/transformers/pull/13467)**\n\nLSG ArXiv [paper](https://arxiv.org/abs/2210.15497). \\\nGithub/conversion script is available at this [link](https://github.com/ccdv-ai/convert_checkpoint_to_lsg).\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-4096-wcep\", trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ccdv/lsg-bart-base-4096-wcep\", trust_remote_code=True)\n\ntext = \"Replace by what you want.\"\npipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\ngenerated_text = pipe(text, truncation=True, max_length=64, no_repeat_ngram_size=7)\n```\n# ccdv/lsg-bart-base-4096-wcep\n\nThis model is a fine-tuned version of [ccdv/lsg-bart-base-4096](https://huggingface.co/ccdv/lsg-bart-base-4096) on the [ccdv/WCEP-10 roberta](https://huggingface.co/datasets/ccdv/WCEP-10) dataset. \\\nIt achieves the following results on the test set:\n\n| Length | Sparse Type  | Block Size | Sparsity | Connexions | R1    | R2    | RL    | RLsum |\n|:------ |:------------ |:---------- |:-------- | :--------- |:----- |:----- |:----- |:----- |\n| 4096   | Local        | 256        | 0        | 768        | 46.02 | 24.23 | 37.38 | 38.72 |\n| 4096   | Local        | 128        | 0        | 384        | 45.43 | 23.86 | 36.94 | 38.30 |\n| 4096   | Pooling      | 128        | 4        | 644        | 45.36 | 23.61 | 36.75 | 38.06 |\n| 4096   | Stride       | 128        | 4        | 644        | 45.87 | 24.31 | 37.41 | 38.70 |\n| 4096   | Block Stride | 128        | 4        | 644        | 45.78 | 24.16 | 37.20 | 38.48 |\n| 4096   | Norm         | 128        | 4        | 644        | 45.34 | 23.39 | 36.47 | 37.78 |\n| 4096   | LSH          | 128        | 4        | 644        | 45.15 | 23.53 | 36.74 | 38.02 |\n\nWith smaller block size (lower ressources):\n\n| Length | Sparse Type  | Block Size | Sparsity | Connexions | R1    | R2    | RL    | RLsum |\n|:------ |:------------ |:---------- |:-------- | :--------- |:----- |:----- |:----- |:----- |\n| 4096   | Local        | 64         | 0        | 192        | 44.48 | 22.98 | 36.20 | 37.52 |\n| 4096   | Local        | 32         | 0        | 96         | 43.60 | 22.17 | 35.61 | 36.66 |\n| 4096   | Pooling      | 32         | 4        | 160        | 43.91 | 22.41 | 35.80 | 36.92 |\n| 4096   | Stride       | 32         | 4        | 160        | 44.62 | 23.11 | 36.32 | 37.53 |\n| 4096   | Block Stride | 32         | 4        | 160        | 44.47 | 23.02 | 36.28 | 37.46 |\n| 4096   | Norm         | 32         | 4        | 160        | 44.45 | 23.03 | 36.10 | 37.33 |\n| 4096   | LSH          | 32         | 4        | 160        | 43.87 | 22.50 | 35.75 | 36.93 |\n\n## Model description\nThe model relies on Local-Sparse-Global attention to handle long sequences:\n![attn](attn.png)\n\nThe model has about ~145 millions parameters (6 encoder layers - 6 decoder layers). \\\nThe model is warm started from BART-base, converted to handle long sequences (encoder only) and fine tuned.\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 8e-05\n- train_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 10.0\n\n### Generate hyperparameters\n\nThe following hyperparameters were used during generation:\n- dataset_name: ccdv/WCEP-10\n- dataset_config_name: roberta\n- eval_batch_size: 8\n- eval_samples: 1022\n- early_stopping: True\n- ignore_pad_token_for_loss: True\n- length_penalty: 2.0\n- max_length: 64\n- min_length: 0\n- num_beams: 5\n- no_repeat_ngram_size: None\n- seed: 123\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.1+cu102\n- Datasets 2.1.0\n- Tokenizers 0.11.6\n", "size_bytes": "578416695", "downloads": 52}
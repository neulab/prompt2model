{"pretrained_model_name": "GT4SD/multitask-text-and-chemistry-t5-small-augm", "description": "---\nlicense: mit\nlanguage:\n- en\n---\n\n# Multitask Text and Chemistry T5\n\nMultitask Text and Chemistry T5 : a multi-domain, multi-task language model to solve a wide range of tasks in both the chemical and natural language domains. Published by [Christofidellis et al.](https://arxiv.org/pdf/2301.12586.pdf)\n\n**Model Details**: The Multitask Text and Chemistry T5 variant trained using <em>t5-small</em> as its pretrained based and the <em>augmented dataset</em>. \n\n**Developers**: Dimitrios Christofidellis*, Giorgio Giannone*, Jannis Born, Teodoro Laino and Matteo Manica from IBM Research and Ole Winther from Technical University of Denmark.\n\n**Distributors**: Model natively integrated into GT4SD.\n\n**Model date**: 2023.\n\n**Model type**: A Transformer-based language model that is trained on a multi-domain and a multi-task dataset by aggregating available datasets\nfor the tasks of Forward reaction prediction, Retrosynthesis, Molecular captioning, Text-conditional de novo generation and Paragraph to actions. \n\n**Information about training algorithms, parameters, fairness constraints or other applied approaches, and features**: \nN.A.\n\n**Paper or other resource for more information**: \nThe Multitask Text and Chemistry T5 [Christofidellis et al.](https://arxiv.org/pdf/2301.12586.pdf)\n\n\n**License**: MIT\n\n**Where to send questions or comments about the model**: Open an issue on [GT4SD repository](https://github.com/GT4SD/gt4sd-core).\n\n## Citation\n```bib\n@article{christofidellis2023unifying,\n  title={Unifying Molecular and Textual Representations via Multi-task Language Modelling},\n  author={Christofidellis, Dimitrios and Giannone, Giorgio and Born, Jannis and Winther, Ole and Laino, Teodoro and Manica, Matteo},\n  journal={arXiv preprint arXiv:2301.12586},\n  year={2023}\n}\n```\n*equal contribution", "size_bytes": "242011067", "downloads": 21}
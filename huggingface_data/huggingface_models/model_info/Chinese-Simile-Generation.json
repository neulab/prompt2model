{"pretrained_model_name": "figurative-nlp/Chinese-Simile-Generation", "description": "chinese-simile-generative \u662f\u4e00\u4e2a\u5c06\u53e5\u5b50A\u6539\u5199\u6210\u5e26\u6709\u4fee\u8f9e\u624b\u6cd5(\u4e3b\u8981\u4e3a\u6bd4\u55bb\uff0c\u660e\u55bb)\u7684\u53e5\u5b50B\u7684seq2seq\u6a21\u578b\u3002\n\nA\uff1a \u60f3\u5f53\u521d\u5bf9\u4f60\u7684\u5b9a\u7ea7\u662f\u5f88\u9ad8\u7684\uff0c\u73b0\u5728\u6211\u5f88\u4f24\u5fc3\uff0c\u770b\u5230\u4f60\u7684\u79d1\u7814\u8fdb\u5ea6\u8fd9\u4e48\u6162\u3002 \n\n\nB\uff1a \u60f3\u5f53\u521d\u5bf9\u4f60\u7684\u5b9a\u7ea7\u662f\u5f88\u9ad8\u7684\uff0c\u73b0\u5728\u6211\u5f88\u4f24\u5fc3\uff0c\u770b\u5230\u4f60\u7684\u79d1\u7814\u8fdb\u5ea6\u50cf\u8717\u725b\u4e00\u6837\u6162\u3002\n\n      from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n      tokenizer = AutoTokenizer.from_pretrained(\"figurative-nlp/chinese-simile-generation\")\n      model = AutoModelForSeq2SeqLM.from_pretrained(\"figurative-nlp/chinese-simile-generation\")\n      \n      \n      input_ids = tokenizer(\n          \"\u6211\u8d70\u5f97\u5f88\u6162\uff0c\u6162\u6781\u4e86\", return_tensors=\"pt\"\n      ).input_ids\n      outputs = model.generate(input_ids,num_beams = 5,max_length = 64)\n      result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n      print(result)\n      #result : \u6211\u8d70\u7684\u5f88\u6162\uff0c\u50cf\u8717\u725b\u4e00\u6837\u3002", "size_bytes": "465198009", "downloads": 12}
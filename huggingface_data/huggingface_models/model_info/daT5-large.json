{"pretrained_model_name": "emillykkejensen/daT5-large", "description": "---\nlanguage: \n- da\nlicense: apache-2.0\n---\n## daT5-large\nA smaller version of [Google's mt5-large](https://huggingface.co/google/mt5-base) model, where the original model is reduced to only include Danish embeddings.\n\n## How to use\n```python\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"emillykkejensen/daT5-large\")\nmodel = AutoModel.from_pretrained(\"emillykkejensen/daT5-large\")\n```\n\n## Further reading\n\n[Gist](https://gist.github.com/emillykkejensen/8bf1b323495efc7252dee966e6bc1b5c) showing (in Danish) how the embeddings are extracted (for mt5-base)\n\n[Article](https://towardsdatascience.com/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90) explaining how to do it by [David Dale](https://huggingface.co/cointegrated)\n\n## Also check out\n[daT5-base](https://huggingface.co/emillykkejensen/daT5-base)", "size_bytes": "3062901797", "downloads": 179}
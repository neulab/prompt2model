{"pretrained_model_name": "kfkas/t5-large-korean-P2G", "description": "---\nlanguage:\n  - ko\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: t5-large-korean-P2G\n  results: []\n---\n\n# t5-large-korean-text-summary\n\n\n\uc774 \ubaa8\ub378\uc740 lcw99 / t5-large-korean-text-summary\uc744 \uad6d\ub9bd \uad6d\uc5b4\uc6d0 \uc2e0\ubb38 \ub9d0\ubb49\uce58 50\ub9cc\uac1c\uc758 \ubb38\uc7a5\uc744 2021\uc744 g2pK\ub85c \ud6c8\ub828\uc2dc\ucf1c G2P\ub41c \ub370\uc774\ud130\ub97c \uc6d0\ubcf8\uc73c\ub85c \ub3cc\ub9bd\ub2c8\ub2e4.\n\n\n## Usage\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport nltk\nnltk.download('punkt')\nmodel_dir = \"t5-large-korean-P2G\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\ntext = \"\ud68c\uc0c8\uae34\uac04 \uc791\uae4c \uae40\ub3d9\uc2dc \uac4d\uc2ec\uaf2c\ubc31 \ub73d \uc0c8 \uc18c\uc124\uc9d1 \ub69c\uad8c \ucd9c\uac04\"\ninputs = tokenizer(text, max_length=256, truncation=True, return_tensors=\"pt\")\noutput = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=100)\ndecoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\npredicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\nprint(predicted_title)\n```\n\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: None\n- training_precision: float16\n### Training results\n### Framework versions\n- Transformers 4.22.1\n- TensorFlow 2.10.0\n- Datasets 2.5.1\n- Tokenizers 0.12.1", "size_bytes": "3282142989", "downloads": 65}
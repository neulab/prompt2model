{"pretrained_model_name": "marksverdhei/t5-deshuffle", "description": "---\nlanguage: en\nwidget:\n - text: ' brown dog fox jumped lazy over quick the the '\ndatasets:\n- 'stas/c4-en-10k'\n---\n\n# T5-deshuffle  \n\nBag Of Words (BOW) is a simple and typical encoding for making statistical models discover patterns in language\nHowever BOW is a lossy compression that eliminates a very important feature of text: order\n\nThis model is trained to learn the most probable order of an unordered token sequence,\nusing a subset of the c4 dataset, and can thus be seen as a \"bag-of-words decoder\".\n\nCurrently, it does not perform well. I'm planning to re-train on a larger subset of c4 later (after may).\n\nHow to run:\n```python\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\ntokenizer = T5Tokenizer.from_pretrained(\"marksverdhei/t5-deshuffle\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"marksverdhei/t5-deshuffle\")\n\nprompt = ' brown dog fox jumped lazy over quick the the '\n\nids = tokenizer(prompt, return_tensors=\"pt\").input_ids\ngenerated_tokens, = model.generate(ids)\nprint(tokenizer.decode(generated_tokens, skip_special_tokens=True))\n```", "size_bytes": "891700799", "downloads": 11}
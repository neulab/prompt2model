{"pretrained_model_name": "Kbrek/flan_rebel_nl", "description": "---\ndatasets:\n- rebel-short\nmetrics:\n- rouge\nmodel-index:\n- name: flan-t5-base\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: rebel-short\n      type: rebel-short\n      config: default\n      split: test\n      args: default\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 51.5716\nlicense: cc-by-sa-4.0\nlanguage:\n- nl\npipeline_tag: text2text-generation\nlibrary_name: transformers\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-rebel-nl\n\nThis model is a fine-tuned version of flan-t5-base on the rebel-short dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1029\n- Rouge1: 51.5716\n- Rouge2: 40.2152\n- Rougel: 49.9941\n- Rougelsum: 49.9767\n- Gen Len: 18.5898\n\n## Model description\n\nThis is a flan-t5-base model fine-tuned on a Dutch dataset version based on RBEL: Relation Extraction By End-to-end Language generation. The model aims to extract triplets in the form {head, relation, tail} from unstructured text. The data for Dutch triplets and unstructured text was generated by using the code of the original authors of REBEL, available at https://github.com/Babelscape/crocodile.\n\n\n## Pipeline usage\n\nThe code below is adopted from the original REBEL model: https://huggingface.co/Babelscape/rebel-large .\n\n```python\nfrom transformers import pipeline\n\ntriplet_extractor = pipeline('text2text-generation', model='Kbrek/flan_rebel_nl', tokenizer='Kbrek/flan_rebel_nl')\n# We need to use the tokenizer manually since we need special tokens.\nextracted_text = triplet_extractor(\"Nederland is een van de landen binnen het Koninkrijk der Nederlanden. Nederland ligt voor het overgrote deel in het noordwesten van Europa, aan de Noordzee. \", max_length = 512, num_beams = 3, temperature = 1)\n# Function to parse the generated text and extract the triplets\ndef extract_triplets(text):\n    triplets = []\n    relation, subject, relation, object_ = '', '', '', ''\n    text = text.strip()\n    current = 'x'\n    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n        if token == \"<triplet>\":\n            current = 't'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n                relation = ''\n            subject = ''\n        elif token == \"<subj>\":\n            current = 's'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n            object_ = ''\n        elif token == \"<obj>\":\n            current = 'o'\n            relation = ''\n        else:\n            if current == 't':\n                subject += ' ' + token\n            elif current == 's':\n                object_ += ' ' + token\n            elif current == 'o':\n                relation += ' ' + token\n    if subject != '' and relation != '' and object_ != '':\n        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n    return triplets\nextracted_triplets = extract_triplets(extracted_text[0])\nprint(extracted_triplets)\n```\n\nA trick that might give you better results is by forcing the entities the model generates by extracting entities with a ner pipeline and forcing those tokens in the generated output.\n\n```python\n\ntriplet_extractor = pipeline('text2text-generation', model='Kbrek/flan_rebel_nl', tokenizer='Kbrek/flan_rebel_nl')\nner_extractor = pipeline(\"ner\", \"Babelscape/wikineural-multilingual-ner\", aggregation_strategy = \"simple\")\n\n#extract ents\nner_output = ner_extractor(input_text)\nents = [i[\"word\"] for i in ner_output]\n\nif len(ents) > 0:\n\n    tokens = triplet_extractor.tokenizer(ents, add_special_tokens=False)[\"input_ids\"]\n    extracted_text = triplet_extractor(input_text, max_length = 512, force_words_ids = tokens)\n\nelse:\n    extracted_text = triplet_extractor(input_text, max_length = 512, temperature = 1)\ntriplets = extract_triplets(extracted_text[0][\"generated_text\"])\n\n\n```\n\n## Training and evaluation data\n\nData used for developing and evaluating this model is generated by using https://github.com/Babelscape/crocodile .\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:------:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 0.1256        | 1.0   | 22047  | 0.1206          | 50.3892 | 38.2761 | 48.7657 | 48.7444   | 18.6112 |\n| 0.1091        | 2.0   | 44094  | 0.1112          | 50.9615 | 39.2843 | 49.3865 | 49.3674   | 18.5447 |\n| 0.0875        | 3.0   | 66141  | 0.1047          | 51.2045 | 39.7598 | 49.6483 | 49.6317   | 18.5763 |\n| 0.0841        | 4.0   | 88188  | 0.1036          | 51.3543 | 39.9776 | 49.8528 | 49.8223   | 18.6178 |\n| 0.0806        | 5.0   | 110235 | 0.1029          | 51.5716 | 40.2152 | 49.9941 | 49.9767   | 18.5898 |\n\n\n### Framework versions\n\n- Transformers 4.27.2\n- Pytorch 1.13.1+cu117\n- Datasets 2.10.1\n- Tokenizers 0.12.1", "size_bytes": "990255285", "downloads": 6}
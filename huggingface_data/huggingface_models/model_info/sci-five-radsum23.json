{"pretrained_model_name": "chizhikchi/sci-five-radsum23", "description": "---\nlicense: afl-3.0\ntags:\n- summarization\n- t5\n- medical\n- clinical\nlanguage: en\ndatasets:\n- MIMIC-III\nwidget:\n- again noted is the large intraparenchymal hemorrhage in the posterior right frontal lobe with extension into both lateral ventricles. the degree of surrounding edema and effacement of adjacent sulci is unchanged. there is minor contralateral shift of normal midline structures. the ventricular size is unchanged. subarachnoid blood is now seen in the left frontal and parietal lobes, likely due to recirculation of the ventricular blood.\n- a least two attempts were made at imaging, however, the study remains severely limited by patient motion. minimal hyperdensity tracks along a left parietal sulcus (2a:18) is equivocal for a small subarachnoid hemorhage. there is no large mass effect detected. there is no shift of normally midline structures. a minimally displaced zygomatic fracture is present (2a:9). the middle ear cavities, mastoid air cells are clear. there is extensive soft tissue swelling overlying the right frontal calvarium with swelling extending to the right preseptal soft tissues (2a:12). there is mild - moderate mucosal thickening within the ethmoid and maxillary sinuses with some fluid and fluid mucosal thickening in the sphenoid sinus.\ninference:\n  parameters:\n    max_length: 350\nmetrics:\n- rouge-l\n---\n# Impression section Generator For Radiology Reports \ud83c\udfe5\nThis model is is the result of participation of SINAI team in [Task 1B: Radiology Report Summarization](https://vilmedic.app/misc/bionlp23/sharedtask) at the BioNLP workshop held on ACL 2023.\nThe goal of this task is to foster development of automatic radiology report summarization systems and expanding their applicability by incorporating seven different modalities and anatomies in the provided data. \nWe propose to automate the generation of radiology impressions with \"sequence-to-sequence\" learning that leverages the power of publicly available pre-trained models, both general domain and biomedical domain-specific. \nThis repository provides access to our best-performing system that resulted from fine-tuning of [Sci-Five base](https://huggingface.co/razent/SciFive-base-Pubmed_PMC), which is T5 model trained for extra 200k steps to optimize it in the context of biomedical literature.\n\n# Results \nThe official evaluation results prove that adaptation of a general-domain system for biomedical literature is beneficial for the subsequent fine-tuning for radiology report summarization task. The Table below summarizes the official scores obtained by this model during the official evaluation. Team standings re available [here](https://vilmedic.app/misc/bionlp23/leaderboard/).\n\n| BLEU4 | ROUGE-L | BERTscore | F1-RadGraph\n|-----------|--------|----------|----------|\n|   017.38\t| 32.32 |\t55.04  |\t33.96  |\n\n# System description paper and citation \nThe paper with the detailed description of the system is published in the [Proceedings of the 22st Workshop on Biomedical Language Processing](https://aclanthology.org/2023.bionlp-1.53/).\n\nBibTeX citation: \n```\n@inproceedings{chizhikova-etal-2023-sinai,\n    title = \"{SINAI} at {R}ad{S}um23: Radiology Report Summarization Based on Domain-Specific Sequence-To-Sequence Transformer Model\",\n    author = \"Chizhikova, Mariia  and\n      Diaz-Galiano, Manuel  and\n      Urena-Lopez, L. Alfonso  and\n      Martin-Valdivia, M. Teresa\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.53\",\n    pages = \"530--534\",\n    abstract = \"This paper covers participation of the SINAI team in the shared task 1B: Radiology Report Summarization at the BioNLP workshop held on ACL 2023. Our proposal follows a sequence-to-sequence approach which leverages pre-trained multilingual general domain and monolingual biomedical domain pre-trained language models. The best performing system based on domain-specific model reached 33.96 F1RadGraph score which is the fourth best result among the challenge participants. This model was made publicly available on HuggingFace. We also describe an attempt of Proximal Policy Optimization Reinforcement Learning that was made in order to improve the factual correctness measured with F1RadGraph but did not lead to satisfactory results.\",\n}\n```\n\n\n", "size_bytes": "891617855", "downloads": 84}
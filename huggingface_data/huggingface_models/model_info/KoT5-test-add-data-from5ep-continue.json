{"pretrained_model_name": "hyorea1/KoT5-test-add-data-from5ep-continue", "description": "---\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: KoT5-test-add-data-from5ep-continue\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# KoT5-test-add-data-from5ep-continue\n\nThis model is a fine-tuned version of [hyorea1/KoT5-test-add-data-from5ep-continue](https://huggingface.co/hyorea1/KoT5-test-add-data-from5ep-continue) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.1881\n- Rouge1: 11.7784\n- Rouge2: 2.959\n- Rougel: 11.6648\n- Rougelsum: 11.6892\n- Gen Len: 34.7301\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 100\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step  | Gen Len | Validation Loss | Rouge1  | Rouge2 | Rougel  | Rougelsum |\n|:-------------:|:-----:|:-----:|:-------:|:---------------:|:-------:|:------:|:-------:|:---------:|\n| 1.4317        | 0.32  | 800   | 34.7618 | 1.2414          | 11.8765 | 3.2439 | 11.7982 | 11.8203   |\n| 0.9488        | 0.64  | 1600  | 35.1324 | 1.2255          | 11.5076 | 3.0739 | 11.394  | 11.4492   |\n| 1.1868        | 0.97  | 2400  | 34.2368 | 1.1983          | 10.7675 | 2.8679 | 10.7567 | 10.7806   |\n| 1.3349        | 1.29  | 3200  | 34.3772 | 1.2170          | 11.0853 | 2.8116 | 10.9947 | 11.0642   |\n| 1.3918        | 1.61  | 4000  | 34.7368 | 1.1845          | 11.6434 | 2.9694 | 11.5189 | 11.5525   |\n| 1.6205        | 1.93  | 4800  | 33.9897 | 1.1801          | 11.1446 | 2.9624 | 11.0259 | 11.0535   |\n| 1.1958        | 2.25  | 5600  | 34.6926 | 1.1845          | 11.3408 | 2.9759 | 11.2451 | 11.2685   |\n| 1.2391        | 2.58  | 6400  | 34.8382 | 1.1879          | 11.227  | 2.832  | 11.0999 | 11.12     |\n| 1.458         | 2.9   | 7200  | 34.8904 | 1.1869          | 11.4615 | 2.832  | 11.3029 | 11.3413   |\n| 1.0598        | 3.22  | 8000  | 1.1877  | 11.2705         | 2.8787  | 11.1582| 11.2173 | 34.8993   |\n| 1.3546        | 3.54  | 8800  | 1.1832  | 11.9647         | 2.9161  | 11.848 | 11.8769 | 34.5897   |\n| 1.5696        | 3.86  | 9600  | 1.1859  | 11.352          | 2.8466  | 11.2177| 11.2336 | 34.6441   |\n| 1.3378        | 4.19  | 10400 | 1.1873  | 11.9282         | 2.959   | 11.8205| 11.8427 | 34.7125   |\n| 1.063         | 4.51  | 11200 | 1.1877  | 11.8063         | 2.9284  | 11.6855| 11.7112 | 34.6426   |\n| 1.184         | 4.83  | 12000 | 1.1881  | 11.7784         | 2.959   | 11.6648| 11.6892 | 34.7301   |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu116\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "891702929", "downloads": 2}
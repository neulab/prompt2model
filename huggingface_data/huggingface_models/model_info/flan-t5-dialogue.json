{"pretrained_model_name": "Amalq/flan-t5-dialogue", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- shared_TaskA\nmetrics:\n- rouge\nmodel-index:\n- name: flan-t5-base-dialogue\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: shared_TaskA\n      type: shared_TaskA\n      config: shared_TaskA\n      split: train\n      args: samsum\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 28.1748\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-base-sharedTaskA\n\nThis model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) on the shared_TaskA dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.5153\n- Rouge1: 28.1748\n- Rouge2: 14.384\n- Rougel: 27.6673\n- Rougelsum: 27.8465\n- Gen Len: 18.85\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\nTraining Loss Validation Loss\tRouge1\t     Rouge2\t     Rougel\t    Rougelsum\tGen Len\n    No log\t    2.554769\t   27.797100\t14.471000\t27.468300\t27.617000\t18.970000\n    No log\t    2.515381       28.174800\t14.384000\t27.667300\t27.846500\t18.850000\n    No log\t    2.542737\t   27.982600\t14.754000\t27.559000\t27.834200\t18.800000\n    1.809200  \t2.528819\t   28.010600\t15.268300\t27.816000\t27.999000\t18.690000\n    1.809200\t2.534979\t   28.104800\t15.248000\t27.840400\t28.069500\t18.670000\n\n\n\n### Example Uses \n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM \ntokenizer_pre = AutoTokenizer.from_pretrained(\"Amalq/flan-t5-dialogue\")\nmodel_pre = AutoModelForSeq2SeqLM.from_pretrained(\"Amalq/flan-t5-dialogue\")\n```    \n\n", "size_bytes": "990404917", "downloads": 58}
{"pretrained_model_name": "IlyaGusev/rubert_telegram_headlines", "description": "---\nlanguage:\n- ru\ntags:\n- summarization\nlicense: apache-2.0\ninference:\n  parameters:\n    no_repeat_ngram_size: 4\n    \n---\n\n# RuBertTelegramHeadlines\n\n\n## Model description\n\nExample model for [Headline generation competition](https://competitions.codalab.org/competitions/29905)\n\nBased on [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) model\n\n## Intended uses & limitations\n\n#### How to use\n\n```python\nfrom transformers import AutoTokenizer, EncoderDecoderModel\n\nmodel_name = \"IlyaGusev/rubert_telegram_headlines\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False, do_basic_tokenize=False, strip_accents=False)\nmodel = EncoderDecoderModel.from_pretrained(model_name)\n\narticle_text = \"...\"\n\ninput_ids = tokenizer(\n    [article_text],\n    add_special_tokens=True,\n    max_length=256,\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\",\n)[\"input_ids\"]\n\noutput_ids = model.generate(\n    input_ids=input_ids,\n    max_length=64,\n    no_repeat_ngram_size=3,\n    num_beams=10,\n    top_p=0.95\n)[0]\n\nheadline = tokenizer.decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\nprint(headline)\n```\n\n## Training data\n\n- Dataset: [ru_all_split.tar.gz](https://www.dropbox.com/s/ykqk49a8avlmnaf/ru_all_split.tar.gz)\n\n## Training procedure\n\n```python\nimport random\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nfrom transformers import BertTokenizer, EncoderDecoderModel, Trainer, TrainingArguments, logging\n\n\ndef convert_to_tensors(\n    tokenizer,\n    text,\n    max_text_tokens_count,\n    max_title_tokens_count = None,\n    title = None\n):\n    inputs = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=max_text_tokens_count,\n        padding=\"max_length\",\n        truncation=True\n    )\n    result = {\n        \"input_ids\": torch.tensor(inputs[\"input_ids\"]),\n        \"attention_mask\": torch.tensor(inputs[\"attention_mask\"]),\n    }\n\n    if title is not None:\n        outputs = tokenizer(\n            title,\n            add_special_tokens=True,\n            max_length=max_title_tokens_count,\n            padding=\"max_length\",\n            truncation=True\n        )\n\n        decoder_input_ids = torch.tensor(outputs[\"input_ids\"])\n        decoder_attention_mask = torch.tensor(outputs[\"attention_mask\"])\n        labels = decoder_input_ids.clone()\n        labels[decoder_attention_mask == 0] = -100\n        result.update({\n            \"labels\": labels,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"decoder_attention_mask\": decoder_attention_mask\n        })\n    return result\n\n\nclass GetTitleDataset(Dataset):\n    def __init__(\n        self,\n        original_records,\n        sample_rate,\n        tokenizer,\n        max_text_tokens_count,\n        max_title_tokens_count\n    ):\n        self.original_records = original_records\n        self.sample_rate = sample_rate\n        self.tokenizer = tokenizer\n        self.max_text_tokens_count = max_text_tokens_count\n        self.max_title_tokens_count = max_title_tokens_count\n        self.records = []\n        for record in tqdm(original_records):\n            if random.random() > self.sample_rate:\n                continue\n            tensors = convert_to_tensors(\n                tokenizer=tokenizer,\n                title=record[\"title\"],\n                text=record[\"text\"],\n                max_title_tokens_count=self.max_title_tokens_count,\n                max_text_tokens_count=self.max_text_tokens_count\n            )\n            self.records.append(tensors)\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, index):\n        return self.records[index]\n\n\ndef train(\n    train_records,\n    val_records,\n    pretrained_model_path,\n    train_sample_rate=1.0,\n    val_sample_rate=1.0,\n    output_model_path=\"models\",\n    checkpoint=None,\n    max_text_tokens_count=256,\n    max_title_tokens_count=64,\n    batch_size=8,\n    logging_steps=1000,\n    eval_steps=10000,\n    save_steps=10000,\n    learning_rate=0.00003,\n    warmup_steps=2000,\n    num_train_epochs=3\n):\n    logging.set_verbosity_info()\n    tokenizer = BertTokenizer.from_pretrained(\n        pretrained_model_path,\n        do_lower_case=False,\n        do_basic_tokenize=False,\n        strip_accents=False\n    )\n    train_dataset = GetTitleDataset(\n        train_records,\n        train_sample_rate,\n        tokenizer,\n        max_text_tokens_count=max_text_tokens_count,\n        max_title_tokens_count=max_title_tokens_count\n    )\n    val_dataset = GetTitleDataset(\n        val_records,\n        val_sample_rate,\n        tokenizer,\n        max_text_tokens_count=max_text_tokens_count,\n        max_title_tokens_count=max_title_tokens_count\n    )\n    \n    model = EncoderDecoderModel.from_encoder_decoder_pretrained(pretrained_model_path, pretrained_model_path)\n    training_args = TrainingArguments(\n        output_dir=output_model_path,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        do_train=True,\n        do_eval=True,\n        overwrite_output_dir=False,\n        logging_steps=logging_steps,\n        eval_steps=eval_steps,\n        evaluation_strategy=\"steps\",\n        save_steps=save_steps,\n        learning_rate=learning_rate,\n        warmup_steps=warmup_steps,\n        num_train_epochs=num_train_epochs,\n        max_steps=-1,\n        save_total_limit=1,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset\n    )\n    trainer.train(checkpoint)\n    model.save_pretrained(output_model_path)\n```", "size_bytes": "1536924033", "downloads": 333}
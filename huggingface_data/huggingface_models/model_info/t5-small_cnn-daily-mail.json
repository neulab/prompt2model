{"pretrained_model_name": "ndtran/t5-small_cnn-daily-mail", "description": "## T5-Small - Text summarization\n\n### Task description\n\nWe are focusing on Abstractive Text Summarization. Briefly, the input of the task is a text paragraph and output is a summarization of the input which is similar to the input from its meaning. Compared to another approach (Extractive Text Summarization), Abstractive Text Summarization is outstanding in the output quality.\n\n### Dataset \n\nThe model was finetuned on [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail). That is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering.\n\n### Hyper-parameters\n\n| Parameter     | Value                |\n| ---------     |---------             |\n| **No. Epoch**     | 3                    |\n| **Learning rate** | 1e-5 (First two epochs), 5e-6 (Last epoch) |\n| **Optimizer**    | AdamW                 |\n| **Layers**        | Full                 |\n\n\n### Evaluations\n\nAll of those metrics are from the evaluation of the finetuned model on the test-set of [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail).\n\n| Metrics | Recall | Precision | F1-Score |\n| ------- | ------ | --------- | -------- |\n| **Rouge 1** | 0.38 | 0.42 | 0.39 |\n| **Rouge 2** | 0.16 | 0.18 | 0.17 |\n| **Rouge L** | 0.27 | 0.3 | 0.27 |\n| **Rouge L-Sum** | 0.27 | 0.3 | 0.27 |\n\n---\nlicense: apache-2.0\ndatasets:\n- cnn_dailymail\nlanguage:\n- en\nmetrics:\n- rouge\nlibrary_name: transformers\npipeline_tag: summarization\n---", "size_bytes": "242071641", "downloads": 13}
{"pretrained_model_name": "Padomin/t5-base-TEDxJP-5front-1body-5rear", "description": "---\nlicense: cc-by-sa-4.0\ntags:\n- generated_from_trainer\ndatasets:\n- te_dx_jp\nmodel-index:\n- name: t5-base-TEDxJP-5front-1body-5rear\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-base-TEDxJP-5front-1body-5rear\n\nThis model is a fine-tuned version of [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) on the te_dx_jp dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4383\n- Wer: 0.1697\n- Mer: 0.1641\n- Wil: 0.2500\n- Wip: 0.7500\n- Hits: 55852\n- Substitutions: 6314\n- Deletions: 2421\n- Insertions: 2228\n- Cer: 0.1328\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Wer    | Mer    | Wil    | Wip    | Hits  | Substitutions | Deletions | Insertions | Cer    |\n|:-------------:|:-----:|:-----:|:---------------:|:------:|:------:|:------:|:------:|:-----:|:-------------:|:---------:|:----------:|:------:|\n| 0.6185        | 1.0   | 1457  | 0.4683          | 0.1948 | 0.1863 | 0.2758 | 0.7242 | 54959 | 6658          | 2970      | 2956       | 0.1682 |\n| 0.5149        | 2.0   | 2914  | 0.4280          | 0.1773 | 0.1713 | 0.2591 | 0.7409 | 55376 | 6468          | 2743      | 2238       | 0.1426 |\n| 0.4705        | 3.0   | 4371  | 0.4173          | 0.1743 | 0.1682 | 0.2552 | 0.7448 | 55680 | 6418          | 2489      | 2351       | 0.1387 |\n| 0.4023        | 4.0   | 5828  | 0.4114          | 0.1713 | 0.1656 | 0.2515 | 0.7485 | 55751 | 6313          | 2523      | 2230       | 0.1335 |\n| 0.3497        | 5.0   | 7285  | 0.4162          | 0.1722 | 0.1662 | 0.2522 | 0.7478 | 55787 | 6331          | 2469      | 2323       | 0.1365 |\n| 0.3246        | 6.0   | 8742  | 0.4211          | 0.1714 | 0.1655 | 0.2513 | 0.7487 | 55802 | 6310          | 2475      | 2284       | 0.1367 |\n| 0.3492        | 7.0   | 10199 | 0.4282          | 0.1711 | 0.1652 | 0.2514 | 0.7486 | 55861 | 6350          | 2376      | 2325       | 0.1341 |\n| 0.2788        | 8.0   | 11656 | 0.4322          | 0.1698 | 0.1641 | 0.2502 | 0.7498 | 55883 | 6342          | 2362      | 2265       | 0.1327 |\n| 0.2801        | 9.0   | 13113 | 0.4362          | 0.1710 | 0.1652 | 0.2514 | 0.7486 | 55828 | 6351          | 2408      | 2288       | 0.1352 |\n| 0.2773        | 10.0  | 14570 | 0.4383          | 0.1697 | 0.1641 | 0.2500 | 0.7500 | 55852 | 6314          | 2421      | 2228       | 0.1328 |\n\n\n### Framework versions\n\n- Transformers 4.21.2\n- Pytorch 1.12.1+cu116\n- Datasets 2.4.0\n- Tokenizers 0.12.1\n", "size_bytes": "891700799", "downloads": 2}
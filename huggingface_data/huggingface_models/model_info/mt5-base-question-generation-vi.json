{"pretrained_model_name": "noah-ai/mt5-base-question-generation-vi", "description": "## Model description\nThis model is a sequence-to-sequence question generator that takes an answer and context as an input and generates a question as an output. It is based on a pre-trained mt5-base by [Google](https://github.com/google-research/multilingual-t5) model.\n\n## Training data\nThe model was fine-tuned on [XQuAD](https://github.com/deepmind/xquad)\n\n## Example usage\n```python\nfrom transformers import MT5ForConditionalGeneration, AutoTokenizer\nimport torch\n\nmodel = MT5ForConditionalGeneration.from_pretrained(\"noah-ai/mt5-base-question-generation-vi\")\ntokenizer = AutoTokenizer.from_pretrained(\"noah-ai/mt5-base-question-generation-vi\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Content used to create a set of questions\ncontext = '''Th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh (c\u00f2n g\u1ecdi l\u00e0 S\u00e0i G\u00f2n) t\u00ean g\u1ecdi c\u0169 tr\u01b0\u1edbc 1975 l\u00e0 S\u00e0i G\u00f2n hay S\u00e0i G\u00f2n-Gia \u0110\u1ecbnh l\u00e0 th\u00e0nh ph\u1ed1 l\u1edbn nh\u1ea5t \u1edf Vi\u1ec7t Nam v\u1ec1 d\u00e2n s\u1ed1 v\u00e0 quy m\u00f4 \u0111\u00f4 th\u1ecb h\u00f3a. \u0110\u00e2y c\u00f2n l\u00e0 trung t\u00e2m kinh t\u1ebf, ch\u00ednh tr\u1ecb, v\u0103n h\u00f3a v\u00e0 gi\u00e1o d\u1ee5c t\u1ea1i Vi\u1ec7t Nam. Th\u00e0nh ph\u1ed1 H\u1ed3 Ch\u00ed Minh l\u00e0 th\u00e0nh ph\u1ed1 tr\u1ef1c thu\u1ed9c trung \u01b0\u01a1ng thu\u1ed9c lo\u1ea1i \u0111\u00f4 th\u1ecb \u0111\u1eb7c bi\u1ec7t c\u1ee7a Vi\u1ec7t Nam c\u00f9ng v\u1edbi th\u1ee7 \u0111\u00f4 H\u00e0 N\u1ed9i.N\u1eb1m trong v\u00f9ng chuy\u1ec3n ti\u1ebfp gi\u1eefa \u0110\u00f4ng Nam B\u1ed9 v\u00e0 T\u00e2y Nam B\u1ed9, th\u00e0nh ph\u1ed1 n\u00e0y hi\u1ec7n c\u00f3 16 qu\u1eadn, 1 th\u00e0nh ph\u1ed1 v\u00e0 5 huy\u1ec7n, t\u1ed5ng di\u1ec7n t\u00edch 2.061 km\u00b2. Theo k\u1ebft qu\u1ea3 \u0111i\u1ec1u tra d\u00e2n s\u1ed1 ch\u00ednh th\u1ee9c v\u00e0o th\u1eddi \u0111i\u1ec3m ng\u00e0y m\u1ed9t th\u00e1ng 4 n\u0103m 2009 th\u00ec d\u00e2n s\u1ed1 th\u00e0nh ph\u1ed1 l\u00e0 7.162.864 ng\u01b0\u1eddi (chi\u1ebfm 8,34% d\u00e2n s\u1ed1 Vi\u1ec7t Nam), m\u1eadt \u0111\u1ed9 d\u00e2n s\u1ed1 trung b\u00ecnh 3.419 ng\u01b0\u1eddi/km\u00b2. \u0110\u1ebfn n\u0103m 2019, d\u00e2n s\u1ed1 th\u00e0nh ph\u1ed1 t\u0103ng l\u00ean 8.993.082 ng\u01b0\u1eddi v\u00e0 c\u0169ng l\u00e0 n\u01a1i c\u00f3 m\u1eadt \u0111\u1ed9 d\u00e2n s\u1ed1 cao nh\u1ea5t Vi\u1ec7t Nam. Tuy nhi\u00ean, n\u1ebfu t\u00ednh nh\u1eefng ng\u01b0\u1eddi c\u01b0 tr\u00fa kh\u00f4ng \u0111\u0103ng k\u00fd h\u1ed9 kh\u1ea9u th\u00ec d\u00e2n s\u1ed1 th\u1ef1c t\u1ebf c\u1ee7a th\u00e0nh ph\u1ed1 n\u00e0y n\u0103m 2018 l\u00e0 g\u1ea7n 14 tri\u1ec7u ng\u01b0\u1eddi.'''\n\nencoding = tokenizer.encode_plus(context, return_tensors=\"pt\")\n\ninput_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n\noutput = model.generate(input_ids=input_ids, attention_mask=attention_masks, max_length=256)\n\nquestion =  tokenizer.decode(output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n\nquestion\n#question: Th\u00e0nh ph\u1ed1 h\u1ed3 ch\u00ed minh c\u00f3 bao nhi\u00eau qu\u1eadn?\n```\n\n> Created by [Duong Thanh Nguyen](https://www.facebook.com/thanhnguyen.dev)", "size_bytes": "2329707353", "downloads": 76}
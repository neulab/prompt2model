{"pretrained_model_name": "pitehu/T5_NER_CONLL_ENTITYREPLACE", "description": "\n---\nlanguage: \n  - en\nlicense: \"apache-2.0\"\ndatasets:\n- CoNLL-2003\nmetrics:\n- F1\n\n---\n\nThis is a T5 small model finetuned on CoNLL-2003 dataset for named entity recognition (NER).\n\nExample Input and Output:\n\u201cRecognize all the named entities in this sequence (replace named entities with one of [PER], [ORG], [LOC], [MISC]): When Alice visited New York\u201d \u2192 \u201cWhen PER visited LOC LOC\"\n\nEvaluation Result:\n\n% of match (for comparison with ExT5: https://arxiv.org/pdf/2111.10952.pdf): \n\n| Model| ExT5_{Base} | This Model | T5_NER_CONLL_OUTPUTLIST \n| :---: | :---: | :---: | :---: |\n| % of Complete Match| 86.53 | 79.03 | TBA| \n\n\n\nThere are some outputs (212/3453 or 6.14% that does not have the same length as the input)\n\nF1 score on testing set of those with matching length :\n\n| Model | This Model | T5_NER_CONLL_OUTPUTLIST | BERTbase \n| :---: | :---: | :---: | :---: |\n| F1| 0.8901 | 0.8691| 0.9240\n\n**Caveat: The testing set of these aren't the same, due to matching length issue... \nT5_NER_CONLL_OUTPUTLIST only has 27/3453 missing length (only 0.78%); The BERT number is directly from their paper (https://arxiv.org/pdf/1810.04805.pdf)\n\n\n\n\n", "size_bytes": "242083771", "downloads": 2}
{"pretrained_model_name": "team-writing-assistant/t5-base-c4jfleg", "description": "# Model Description:\nTo create t5-base-c4jfleg model, T5-base model is fine-tuned on the [**JFLEG dataset**](https://huggingface.co/datasets/jfleg) and [**C4 200M dataset**](https://huggingface.co/datasets/liweili/c4_200m) by taking around 3000 examples from each with the objective of grammar correction.\n\n\nThe original Google's [**T5-base**] model was pre-trained on [**C4 dataset**](https://huggingface.co/datasets/c4).\n\nThe T5 model was presented in [**Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**](https://arxiv.org/pdf/1910.10683.pdf) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.\n\n# Prefix:\nThe T-5 model use \"grammar: \" as the input text prefix for grammatical corrections.\n\n## Usage :\n```\nfrom transformers import pipeline\n\ncheckpoint = \"team-writing-assistant/t5-base-c4jfleg\"\nmodel = pipeline(\"text2text-generation\", model=checkpoint)\n\ntext = \"Speed of light is fastest then speed of sound\"\ntext = \"grammar: \" + text\n\noutput = model(text)\nprint(\"Result: \", output[0]['generated_text'])\n```\n```\nResult: Speed of light is faster than speed of sound.\n```\n\n## Other Examples :\nInput: My grammar are bad.   \nOutput: My grammar is bad.\n\nInput: Who are the president?   \nOutput: Who is the president?", "size_bytes": "891737400", "downloads": 66}
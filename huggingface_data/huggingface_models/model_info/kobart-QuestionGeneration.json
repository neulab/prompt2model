{"pretrained_model_name": "Sehong/kobart-QuestionGeneration", "description": "---\nlanguage: ko\ntags:\n- bart\ndatasets:\n- korquad\nlicense: mit\n---\n\n# Korean Question Generation Model\n\n## Github\n\nhttps://github.com/Seoneun/KoBART-Question-Generation\n\n## Fine-tuning Dataset\n\nKorQuAD 1.0\n\n## Demo\n\nhttps://huggingface.co/Sehong/kobart-QuestionGeneration\n\n## How to use\n\n```python\nimport torch\nfrom transformers import PreTrainedTokenizerFast\nfrom transformers import BartForConditionalGeneration\n\ntokenizer = PreTrainedTokenizerFast.from_pretrained('Sehong/kobart-QuestionGeneration')\nmodel = BartForConditionalGeneration.from_pretrained('Sehong/kobart-QuestionGeneration')\n\ntext = \"1989\ub144 2\uc6d4 15\uc77c \uc5ec\uc758\ub3c4 \ub18d\ubbfc \ud3ed\ub825 \uc2dc\uc704\ub97c \uc8fc\ub3c4\ud55c \ud610\uc758(\ud3ed\ub825\ud589\uc704\ub4f1\ucc98\ubc8c\uc5d0\uad00\ud55c\ubc95\ub960\uc704\ubc18)\uc73c\ub85c \uc9c0\uba85\uc218\ubc30\ub418\uc5c8\ub2e4. 1989\ub144 3\uc6d4 12\uc77c \uc11c\uc6b8\uc9c0\ubc29\uac80\ucc30\uccad \uacf5\uc548\ubd80\ub294 \uc784\uc885\uc11d\uc758 \uc0ac\uc804\uad6c\uc18d\uc601\uc7a5\uc744 \ubc1c\ubd80\ubc1b\uc558\ub2e4. \uac19\uc740 \ud574 6\uc6d4 30\uc77c \ud3c9\uc591\ucd95\uc804\uc5d0 \uc784\uc218\uacbd\uc744 \ub300\ud45c\ub85c \ud30c\uacac\ud558\uc5ec \uad6d\uac00\ubcf4\uc548\ubc95\uc704\ubc18 \ud610\uc758\uac00 \ucd94\uac00\ub418\uc5c8\ub2e4. \uacbd\ucc30\uc740 12\uc6d4 18\uc77c~20\uc77c \uc0ac\uc774 \uc11c\uc6b8 \uacbd\ud76c\ub300\ud559\uad50\uc5d0\uc11c \uc784\uc885\uc11d\uc774 \uc131\uba85 \ubc1c\ud45c\ub97c \ucd94\uc9c4\ud558\uace0 \uc788\ub2e4\ub294 \ucca9\ubcf4\ub97c \uc785\uc218\ud588\uace0, 12\uc6d4 18\uc77c \uc624\uc804 7\uc2dc 40\ubd84 \uacbd \uac00\uc2a4\ucd1d\uacfc \uc804\uc790\ubd09\uc73c\ub85c \ubb34\uc7a5\ud55c \ud2b9\uacf5\uc870 \ubc0f \ub300\uacf5\uacfc \uc9c1\uc6d0 12\uba85 \ub4f1 22\uba85\uc758 \uc0ac\ubcf5 \uacbd\ucc30\uc744 \uc2b9\uc6a9\ucc28 8\ub300\uc5d0 \ub098\ub204\uc5b4 \uacbd\ud76c\ub300\ud559\uad50\uc5d0 \ud22c\uc785\ud588\ub2e4. 1989\ub144 12\uc6d4 18\uc77c \uc624\uc804 8\uc2dc 15\ubd84 \uacbd \uc11c\uc6b8\uccad\ub7c9\ub9ac\uacbd\ucc30\uc11c\ub294 \ud638\uc704 \ud559\uc0dd 5\uba85\uacfc \ud568\uaed8 \uacbd\ud76c\ub300\ud559\uad50 \ud559\uc0dd\ud68c\uad00 \uac74\ubb3c \uacc4\ub2e8\uc744 \ub0b4\ub824\uc624\ub294 \uc784\uc885\uc11d\uc744 \ubc1c\uacac, \uac80\uac70\ud574 \uad6c\uc18d\uc744 \uc9d1\ud589\ud588\ub2e4. \uc784\uc885\uc11d\uc740 \uccad\ub7c9\ub9ac\uacbd\ucc30\uc11c\uc5d0\uc11c \uc57d 1\uc2dc\uac04 \ub3d9\uc548 \uc870\uc0ac\ub97c \ubc1b\uc740 \ub4a4 \uc624\uc804 9\uc2dc 50\ubd84 \uacbd \uc11c\uc6b8 \uc7a5\uc548\ub3d9\uc758 \uc11c\uc6b8\uc9c0\ubc29\uacbd\ucc30\uccad \uacf5\uc548\ubd84\uc2e4\ub85c \uc778\uacc4\ub418\uc5c8\ub2e4. <unused0> 1989\ub144 2\uc6d4 15\uc77c\"\n\nraw_input_ids = tokenizer.encode(text)\ninput_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n\nsummary_ids = model.generate(torch.tensor([input_ids]))\nprint(tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True))\n\n# <unused0> is sep_token, sep_token seperate content and answer\n```\n\n\n\n\n\n", "size_bytes": "495652819", "downloads": 205}
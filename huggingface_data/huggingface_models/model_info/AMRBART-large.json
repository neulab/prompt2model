{"pretrained_model_name": "xfbai/AMRBART-large", "description": "---\nlanguage: en\ntags:\n- AMRBART\nlicense: mit\n---\n\n## AMRBART (large-sized model)\n\nAMRBART model is continually pre-trained on the English text and AMR Graphs based on the BART model. It was introduced in the paper: [Graph Pre-training for AMR Parsing and Generation](https://arxiv.org/pdf/2203.07836.pdf) by bai et al. in ACL 2022 and first released in [this repository](https://github.com/muyeby/AMRBART). \n\n## Model description\n\nAMRBART follows the BART model which uses a transformer encoder-encoder architecture. AMRBART is pre-trained with 6 tasks:\n\n+ learning to reconstruct the text based on the corrupted text.\n+ learning to reconstruct AMR graphs based on the corrupted AMR graph.\n+ learning to reconstruct the text based on the corrupted text and its corresponding AMR graph.\n+ learning to reconstruct an AMR graph based on the corrupted AMR graph and its corresponding text.\n+ learning to reconstruct the text based on the corrupted text and its corresponding corrupted AMR graph.\n+ learning to reconstruct an AMR graph based on the corrupted AMR graph and its corresponding corrupted text.\n\nAMRBART is particularly effective when fine-tuned for AMR parsing and AMR-to-text generation tasks.\n\n## Training data\n\nThe AMRBART model is pre-trained on [AMR3.0](https://catalog.ldc.upenn.edu/LDC2020T02), a dataset consisting of 55,635\ntraining instances and [English Gigaword](https://catalog.ldc.upenn.edu/LDC2003T05) (we randomly sampled 200,000 sentences).\n\n## Intended uses & limitations\n\nYou can use the raw model for either AMR encoding or AMR parsing, but it's mostly intended to\nbe fine-tuned on a downstream task. \n\n## How to use\nHere is how to initialize this model in PyTorch:\n\n```python\nfrom transformers import BartForConditionalGeneration\nmodel = BartForConditionalGeneration.from_pretrained(\"xfbai/AMRBART-large\")\n```\nPlease refer to [this repository](https://github.com/muyeby/AMRBART) for tokenizer initialization and data preprocessing.\n\n\n## BibTeX entry and citation info\nPlease cite this paper if you find this model helpful\n\n```bibtex\n@inproceedings{bai-etal-2022-graph,\n    title = \"Graph Pre-training for {AMR} Parsing and Generation\",\n    author = \"Bai, Xuefeng  and\n      Chen, Yulong and\n      Zhang, Yue\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"todo\",\n    doi = \"todo\",\n    pages = \"todo\"\n}\n```", "size_bytes": "1640243311", "downloads": 11}
{"pretrained_model_name": "ken11/mbart-ja-en", "description": "---\ntags:\n- translation\n- japanese\n\nlanguage:\n- ja\n- en\n\nlicense: mit\n\nwidget:\n- text: \"\u4eca\u65e5\u3082\u3054\u5b89\u5168\u306b\"\n\n---\n## mbart-ja-en\n\u3053\u306e\u30e2\u30c7\u30eb\u306f[facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25)\u3092\u30d9\u30fc\u30b9\u306b[JESC dataset](https://nlp.stanford.edu/projects/jesc/index_ja.html)\u3067\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3057\u305f\u3082\u306e\u3067\u3059\u3002  \nThis model is based on [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25) and fine-tuned with [JESC dataset](https://nlp.stanford.edu/projects/jesc/index_ja.html).\n\n## How to use\n```py\nfrom transformers import (\n    MBartForConditionalGeneration, MBartTokenizer\n)\n\ntokenizer = MBartTokenizer.from_pretrained(\"ken11/mbart-ja-en\")\nmodel = MBartForConditionalGeneration.from_pretrained(\"ken11/mbart-ja-en\")\n\ninputs = tokenizer(\"\u3053\u3093\u306b\u3061\u306f\", return_tensors=\"pt\")\ntranslated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[\"en_XX\"], early_stopping=True, max_length=48)\npred = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\nprint(pred)\n```\n\n## Training Data\nI used the [JESC dataset](https://nlp.stanford.edu/projects/jesc/index_ja.html) for training.  \nThank you for publishing such a large dataset.  \n\n## Tokenizer\nThe tokenizer uses the [sentencepiece](https://github.com/google/sentencepiece) trained on the JESC dataset.\n\n## Note\nThe result of evaluating the sacrebleu score for [JEC Basic Sentence Data of Kyoto University](https://nlp.ist.i.kyoto-u.ac.jp/EN/?JEC+Basic+Sentence+Data#i0163896) was `18.18` .\n\n## Licenese\n[The MIT license](https://opensource.org/licenses/MIT)\n", "size_bytes": "1681999289", "downloads": 139}
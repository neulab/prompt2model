{"pretrained_model_name": "monsoon-nlp/byt5-dv", "description": "---\nlanguage: dv\n---\n\n# byt5-dv\n\nPretrained from scratch on Dhivei (language of the Maldives)\nwith ByT5, Google's new byte-level tokenizer strategy.\n\nCorpus: dv.wikipedia.org as of March 2020 (TFDS)\n\nNotebook - Pretraining on Wikipedia: https://colab.research.google.com/drive/19Afq7CI6cOi1DaTpnQhBbEbnBzLSFHbH\n\n## Demo\n\nNotebook - Finetuning on Maldivian news classification task: https://colab.research.google.com/drive/11u5SafR4bKICmArgDl6KQ9vqfYtDpyWp\n\nCurrent performance:\n\n- mBERT: 52%\n- **byt5-dv**: 81%\n- dv-wave (ELECTRA): 89%\n- dv-muril: 90.7%\n- dv-labse: 91.3-91.5%\n\nSource of dataset: https://github.com/Sofwath/DhivehiDatasets\n\n## Work in progress - todos\n\nThe Wikipedia corpus is too small for this language. In the future I would add\nOSCAR and Sofwath's Maldivian corpus, if I can rewrite the script to accept those\nas one TFDS dataset.\n\nThis is based on ByT5-small ... we should try a larger model\n\nThis needs more time for pretraining", "size_bytes": "1198625069", "downloads": 3}
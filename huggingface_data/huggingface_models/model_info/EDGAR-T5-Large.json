{"pretrained_model_name": "him1411/EDGAR-T5-Large", "description": "---\nlicense: mit\nlanguage:\n- en\ntags:\n- finance\n- ContextNER\n- language models\ndatasets:\n- him1411/EDGAR10-Q\nmetrics:\n- rouge\n---\n\nEDGAR-T5-Large \n=============\n\nT5 Large model finetuned on [EDGAR10-Q dataset](https://huggingface.co/datasets/him1411/EDGAR10-Q)\n\nYou may want to check out \n* Our paper: [CONTEXT-NER: Contextual Phrase Generation at Scale](https://arxiv.org/abs/2109.08079/)\n* GitHub: [Click Here](https://github.com/him1411/edgar10q-dataset)\n\n\n\nDirect Use\n=============\n\nIt is possible to use this model to generate text, which is useful for experimentation and understanding its capabilities. **It should not be directly used for production or work that may directly impact people.**\n\nHow to Use\n=============\n\nYou can very easily load the models with Transformers, instead of downloading them manually. The [T5-Large model](https://huggingface.co/t5-large) is the backbone of our model. Here is how to use the model in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"him1411/EDGAR-T5-Large\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"him1411/EDGAR-T5-Large\")\n```\nOr just clone the model repo\n```\ngit lfs install\ngit clone https://huggingface.co/him1411/EDGAR-T5-Large\n```\n\nInference Example\n=============\n\nHere, we provide an example for the \"ContextNER\" task. Below is an example of one instance.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"him1411/EDGAR-T5-Large\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"him1411/EDGAR-T5-Large\")\n# Input shows how we have appended instruction from our file for HoC dataset with instance.\ninput = \"14.5 years . The definite lived intangible assets related to the contracts and trade names had estimated weighted average useful lives of 5.9 years and 14.5 years, respectively, at acquisition.\"\ntokenized_input= tokenizer(input)\n# Ideal output for this input is 'Definite lived intangible assets weighted average remaining useful life'\noutput = model(tokenized_input)\n```\n\n## Results on Dowstream datasets\nEDGAR-T5-Large was finetuned on some downstream datasets to get better results than T5 large. BloombergGPT 50B was used as baseline. \n\n| Dataset  | Bloomberg GPT 50B | T5 Large | Edgar T5 Large |\n|----------|-------------------|----------|----------------|\n| FiQA SA  | 75.07             | 74.89    | 80.42          |\n| FPB      | 51.07             | 55.77    | 79.69          |\n| Headline | 82.20             | 90.55    | 93.55          |\n\nBibTeX Entry and Citation Info\n===============\nIf you are using our model, please cite our paper:\n\n```bibtex\n@article{gupta2021context,\n  title={Context-NER: Contextual Phrase Generation at Scale},\n  author={Gupta, Himanshu and Verma, Shreyas and Kumar, Tarun and Mishra, Swaroop and Agrawal, Tamanna and Badugu, Amogh and Bhatt, Himanshu Sharad},\n  journal={arXiv preprint arXiv:2109.08079},\n  year={2021}\n}\n```", "size_bytes": "2950844807", "downloads": 14}
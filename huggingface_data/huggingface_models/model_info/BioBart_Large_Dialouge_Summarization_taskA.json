{"pretrained_model_name": "Care4langGW/BioBart_Large_Dialouge_Summarization_taskA", "description": "---\nlicense: apache-2.0\ndatasets:\n- MEDIQA-Chat-2023-taskA\nmetrics:\n- rouge\nmodel-index:\n- name: BioBart_Large_Dialouge_Summarization_taskA\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: MEDIQA-Chat-2023-taskA\n      type: MEDIQA-Chat-2023-taskA\n      config: MEDIQA-Chat-2023-taskA\n      split: train\n      args: MEDIQA-Chat-2023-taskA\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 28.7953\n---\n\n\n\n\n# BioBart_Large_Dialouge_Summarization_taskA\n\nThis model is a fine-tuned version of [GanjinZero/biobart-large](https://https://huggingface.co/GanjinZero/biobart-large) on the [Task A-Short Dialogue2Note Summarization dataset](https://github.com/abachaa/MEDIQA-Chat-2023)\n\nIt achieves the following results on the evaluation set:\n- Loss: 2.7487\n- Rouge1: 28.7953\n- Rouge2: 13.7224\n- Rougel: 27.8491\n- Rougelsum: 28.6028\n- Gen Len: 19.34\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| No log\t    | 1.0   | 1842 | 2.739522        | 29.7742 | 15.4000 | 29.0754 | 29.706200 | 19.5600 |\n| No log\t    | 2.0   | 3684 | 2.705775\t     | 28.6469 | 13.7595 | 27.7582 | 28.486700 | 19.3700 |\n| No log\t    | 3.0   | 5526 | 2.748785\t     | 28.7953 | 13.7224 | 27.8491 | 28.602800 | 19.3400 |\n\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- datasets 2.10.1\n- tokenizers 0.13.2", "size_bytes": "1625541389", "downloads": 13}
{"pretrained_model_name": "yhavinga/t5-v1.1-base-dutch-cnn-test", "description": "---\nlanguage:\n- nl\nlicense: apache-2.0\ntags:\n- summarization\n- t5\n- seq2seq\ndatasets:\n- yhavinga/mc4_nl_cleaned\n- ml6team/cnn_dailymail_nl\npipeline_tag: summarization\nwidget:\n- text: 'Het Van Goghmuseum in Amsterdam heeft vier kostbare prenten verworven van\n    Mary Cassatt, de Amerikaanse impressionistische kunstenaar en tijdgenoot van Vincent\n    van Gogh. Dat heeft het museum woensdagmiddag op een persconferentie bekendgemaakt.\n    Het gaat om drie grote kleurenetsen en een zwart-wit litho met voorstellingen\n    van vrouwen. Voor deze prenten, die afkomstig zijn van een Amerikaanse verzamelaar,\n    betaalde het museum ruim 1,4 miljoen euro. Drie grote fondsen en een aantal particulieren\n    hebben samen de aankoopsom beschikbaar gesteld. Mary Stevenson Cassatt (1844-1926)\n    woonde en werkte lange tijd in Frankrijk. Ze staat met haar impressionistische\n    schilderijen en tekeningen te boek als een van de vernieuwers van de Parijse kunstwereld\n    in de late negentiende eeuw. Het Van Goghmuseum rekent haar prenten \u201etot het mooiste\n    wat op grafisch gebied in het fin de si\u00e8cle is geproduceerd\u201d. De drie aangekochte\n    kleurenetsen \u2013 Het doorpassen, De brief en Badende vrouw \u2013 komen uit een serie\n    van tien waarmee Cassatt haar naam als (prent)kunstenaar definitief vestigde.\n    Ze maakte de etsen na een bezoek in 1890 aan een tentoonstelling van Japanse prenten\n    in Parijs. Over die expositie schreef de Amerikaanse aan haar vriendin Berthe\n    Morisot, een andere vrouwelijke impressionist: \u201eWe kunnen de Japanse prenten in\n    de Beaux-Arts gaan bekijken. Echt, die mag je niet missen. Als je kleurenprenten\n    wilt maken, is er niets mooiers voorstelbaar. Ik droom ervan en denk nergens anders\n    meer aan dan aan kleur op koper.'\n- text: 'Afgelopen zaterdagochtend werden Hunga Tonga en Hunga Hapai opnieuw twee\n    aparte eilanden toen de vulkaan met een hevige explosie uitbarstte. De aanloop\n    tot de uitbarsting begon al eind vorig jaar met kleinere explosies. Begin januari\n    nam de activiteit af en dachten geologen dat de vulkaan tot rust was gekomen.\n    Toch barstte hij afgelopen zaterdag opnieuw uit, veel heviger dan de uitbarstingen\n    ervoor. Vl\u00e1k voor deze explosie stortte het kilometerslange verbindingsstuk in\n    en verdween onder het water. De eruptie duurde acht minuten. De wolk van as en\n    giftige gasdeeltjes, zoals zwaveloxide, die daarbij vrijkwam, reikte tot dertig\n    kilometer hoogte en was zo\u2019n vijfhonderd kilometer breed. Ter vergelijking: de\n    pluimen uit de recente vulkaanuitbarsting op La Palma reikten maximaal zo\u2019n vijf\n    kilometer hoog. De hoofdstad van Tonga, vijfenzestig kilometer verderop is bedekt\n    met een dikke laag as. Dat heeft bijvoorbeeld gevolgen voor de veiligheid van\n    het drinkwater op Tonga. De uitbarsting van de onderzeese vulkaan in de eilandstaat\n    Tonga afgelopen zaterdag was bijzonder heftig. De eruptie veroorzaakte een tsunami\n    die reikte van Nieuw-Zeeland tot de Verenigde Staten en in Nederland ging de luchtdruk\n    omhoog. Geologen verwachten niet dat de vulkaan op Tonga voor een lange wereldwijde\n    afkoeling zorgt, zoals bij andere hevige vulkaanuitbarstingen het geval is geweest.\n    De vulkaan ligt onder water tussen de onbewoonde eilandjes Hunga Tonga (0,39 vierkante\n    kilometer) en Hunga Ha\u2019apai (0,65 vierkante kilometer). Magma dat bij kleinere\n    uitbarsting in 2009 en 2014 omhoog kwam, koelde af en vormde een verbindingsstuk\n    tussen de twee eilanden in. Een explosie van een onderwatervulkaan als die bij\n    Tonga is heftiger dan bijvoorbeeld die uitbarsting op La Palma. \u201eDat komt doordat\n    het vulkanisme hier veroorzaakt wordt door subductie: de Pacifische plaat zinkt\n    onder Tonga de aardmantel in en neemt water mee omlaag\u201d, zegt hoogleraar paleogeografie\n    Douwe van Hinsbergen van de Universiteit Utrecht. \u201eDit water komt met magma als\n    gas, als waterdamp, mee omhoog. Dat voert de druk onder de aardkost enorm op.\n    Arwen Deuss, geowetenschapper aan de Universiteit Utrecht, vergelijkt het met\n    een fles cola. \u201eWanneer je een fles cola schudt, zal het gas er met veel geweld\n    uitkomen. Dat is waarschijnlijk wat er gebeurd is op Tonga, maar we weten het\n    niet precies.\u201d'\nmodel-index:\n- name: yhavinga/t5-v1.1-base-dutch-cnn-test\n  results:\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: ml6team/cnn_dailymail_nl\n      type: ml6team/cnn_dailymail_nl\n      config: default\n      split: test\n    metrics:\n    - type: rouge\n      value: 38.5454\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWQwM2I0MjcwODQxZGNkMTMwZDllZjVlNzVkOWQyZDkzNDkxODE5ZjZiOWI1N2E5N2Y5MDcyZWM4ZWZjYzQ0NCIsInZlcnNpb24iOjF9.ORXcoqRJvsQyPdPQWhG3ZiYo7TYQaklYOdThMJJCrVOY1IrBjFRg_sx4e5qrQMMCwn-iVFa2YwSXPriBx49HDw\n    - type: rouge\n      value: 15.7133\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2IyZmIxZDc0NjlhNTYyY2I3OTNkYjhkZDUwMjQ1ZjRjMjE3ZjhmMmUzMjVjYTc1MDkyMzZiY2E2OGIxMzE3OCIsInZlcnNpb24iOjF9.-2pXCw3ffIZyYPfjJRrg-tlwy7PC7ICjc4m3-q3_ciXB3x8RveOuUvxfd3q8xoox2ICHaGmrdBPKXYWBFVvJDQ\n    - type: rouge\n      value: 25.9162\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjdiYWY3YTY1NmJhYWIzNGEwMGRkMTBlYTAyYjJkMmJiZWM4ZGUwMWE2ZTI5YzMxNDlkMWVlMDM2ZTMyYWE5YSIsInZlcnNpb24iOjF9.chltUhR_bF4vA-AOfOAi16Qor4ioBsgk4eJCosWJmdTgkCLJmN_sPAcr0Jz2qLo7dfeWwZ5ee0KcXGF4eyNyAA\n    - type: rouge\n      value: 35.4489\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjliMjUzYzA0MTQ3MjQ2NTk1YzY0MjA3N2U4YmI5MjE1Mzk2OGIxMTM2NTEwNjg0ZGU0ZTkxNTU2ZTJmNzdhNSIsInZlcnNpb24iOjF9.7l_KXmqIgTuDXOHdlTFLm67gjsaypy-RUTEJ9unNZlTXTmKPvL1frMZ0PUm5gRi-hM2TWVcUpTnVpkmXa4bNDw\n    - type: loss\n      value: 2.0727603435516357\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWY0Yzc1MGUxZmIyNDdjNzhiMzVlMjI4YzIwMGNkNzVjNmE3NjgxZjYwYTA4Y2QxYmNjZThiNzE5OWYzMjExOCIsInZlcnNpb24iOjF9.ERRCuKz5IekBZihQtyRnfz4VGl7LfCDzUO6-ZbYrZO_sdTxpaEw3ID0O3Cyx2Y4hmAYEywyvC2Idb3fmmjplAQ\n    - type: gen_len\n      value: 91.1699\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMmNmMDRkOGMyMDY1OGNmMmQwY2ZkMzdlMDA2NzNkYmY3NzNmMTFmYmE3MTNhOWFlN2Q2N2FhNzFhNjM4NWJjOSIsInZlcnNpb24iOjF9.Otl1b_1Muxu6I4W2ThWBFidlwmou7149pMcShI4W-jeBntQeBwrfBe-fSkvNF-8Q29I_Of3o1swJXJAWAaxTDA\n---\n\n# T5 v1.1 Base finetuned for CNN news summarization in Dutch \ud83c\uddf3\ud83c\uddf1\n\nThis model is [t5-v1.1-base-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cased) finetuned on [CNN Dailymail NL](https://huggingface.co/datasets/ml6team/cnn_dailymail_nl)\n\nFor a demo of the Dutch CNN summarization models, head over to the Hugging Face Spaces for\nthe **[Netherformer \ud83d\udcf0](https://huggingface.co/spaces/flax-community/netherformer)** example application!\n\nRouge scores for this model are listed below.\n\n## Tokenizer\n\n* SentencePiece tokenizer trained from scratch for Dutch on mC4 nl cleaned with scripts from the Huggingface\n  Transformers [Flax examples](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling).\n\n## Dataset\n\nAll models listed below are trained on of the `full` configuration (39B tokens) of\n[cleaned Dutch mC4](https://huggingface.co/datasets/yhavinga/mc4_nl_cleaned),\nwhich is the original mC4, except\n\n  * Documents that contained words from a selection of the Dutch and English [List of Dirty Naught Obscene and Otherwise Bad Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words) are removed\n  * Sentences with less than 3 words are removed\n  * Sentences with a word of more than 1000 characters are removed\n  * Documents with less than 5 sentences are removed\n  * Documents with \"javascript\", \"lorum ipsum\", \"terms of use\", \"privacy policy\", \"cookie policy\", \"uses cookies\",\n    \"use of cookies\", \"use cookies\", \"elementen ontbreken\", \"deze printversie\" are removed.\n \n## Models\n\nTL;DR: [yhavinga/t5-v1.1-base-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cased) is the best model.\n\n* `yhavinga/t5-base-dutch` is a re-training of the Dutch T5 base v1.0 model trained during the summer 2021\n  Flax/Jax community week. Accuracy was improved from 0.64 to 0.70.\n* The two T5 v1.1 base models are an uncased and cased version of `t5-v1.1-base`, again pre-trained from scratch on Dutch,\n  with a tokenizer also trained from scratch. The t5 v1.1 models are slightly different from the t5 models, and the \n  base models are trained with a dropout of 0.0. For fine-tuning it is intended to set this back to 0.1.\n* The large cased model is a pre-trained Dutch version of `t5-v1.1-large`. Training of t5-v1.1-large proved difficult. \n  Without dropout regularization, the training would diverge at a certain point. With dropout training went better,\n  be it much slower than training the t5-model. At some point convergance was too slow to warrant further training.\n  The latest checkpoint, training scripts and metrics are available for reference. For actual fine-tuning the cased\n  base model is probably the better choice.\n\n|                                                                                                   | model   | train seq len | acc      | loss     | batch size | epochs | steps   | dropout | optim     | lr   | duration |\n|---------------------------------------------------------------------------------------------------|---------|---------------|----------|----------|------------|--------|---------|---------|-----------|------|----------|\n| [yhavinga/t5-base-dutch](https://huggingface.co/yhavinga/t5-base-dutch)                           | T5      | 512           | 0,70     | 1,38     | 128        | 1      | 528481  | 0.1     | adafactor | 5e-3 | 2d 9h    |\n| [yhavinga/t5-v1.1-base-dutch-uncased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-uncased) | t5-v1.1 | 1024          | 0,73     | 1,20     | 64         | 2      | 1014525 | 0.0     | adafactor | 5e-3 | 5d 5h    |\n| [yhavinga/t5-v1.1-base-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cased)     | t5-v1.1 | 1024          | **0,78** | **0,96** | 64         | 2      | 1210000 | 0.0     | adafactor | 5e-3 | 6d 6h    |\n| [yhavinga/t5-v1.1-large-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-large-dutch-cased)   | t5-v1.1 | 512           | 0,76     | 1,07     | 64         | 1      | 1120000 | 0.1     | adafactor | 5e-3 | 86 13h   |\n\nThe cased t5-v1.1 Dutch models were fine-tuned on summarizing the CNN Daily Mail dataset.\n\n|                                                                                                       | model   | input len | target len | Rouge1 | Rouge2 | RougeL | RougeLsum | Test Gen Len | epochs | batch size | steps | duration |\n|-------------------------------------------------------------------------------------------------------|---------|-----------|------------|--------|--------|--------|-----------|--------------|--------|------------|-------|----------|\n| [yhavinga/t5-v1.1-base-dutch-cnn-test](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cnn-test)   | t5-v1.1 | 1024      | 96         | 34,8   | 13,6   | 25,2   | 32,1      | 79           | 6      | 64         | 26916 | 2h 40m   |\n| [yhavinga/t5-v1.1-large-dutch-cnn-test](https://huggingface.co/yhavinga/t5-v1.1-large-dutch-cnn-test) | t5-v1.1 | 1024      | 96         | 34,4   | 13,6   | 25,3   | 31,7      | 81           | 5      | 16         | 89720 | 11h      |\n\n\n## Acknowledgements\n\nThis project would not have been possible without compute generously provided by Google through the\n[TPU Research Cloud](https://sites.research.google/trc/). The HuggingFace \ud83e\udd17 ecosystem was also\ninstrumental in many, if not all parts of the training. The following repositories where helpful in setting up the TPU-VM,\nand training the models:\n\n* [Gsarti's Pretrain and Fine-tune a T5 model with Flax on GCP](https://github.com/gsarti/t5-flax-gcp)\n* [HUggingFace Flax MLM examples](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling)\n* [Flax/Jax Community week t5-base-dutch](https://huggingface.co/flax-community/t5-base-dutch)\n\nCreated by [Yeb Havinga](https://www.linkedin.com/in/yeb-havinga-86530825/)", "size_bytes": "990280781", "downloads": 76}
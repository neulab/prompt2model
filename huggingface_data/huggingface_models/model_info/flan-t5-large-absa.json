{"pretrained_model_name": "shorthillsai/flan-t5-large-absa", "description": "---\nlanguage:\n- en\ntags:\n- absa\n- AspectBasedSentimentAnalysis\n- Classification\n- sentiment\n---\n\n# flan-t5-large-absa\n\nThis model is a fine-tuned version of [google/flan-t5-large](https://huggingface.co/google/flan-t5-base) on custom dataset prepared by GPT-4 and verified by human.\n\n## Model description\n\nText-to-Text model for aspect based sentiment analysis.\n\n## Intended uses & limitations\n\nThis is not for commercial use since the dataset was prepared using OpenAI with humans in the loop. It must be tested on the required dataset for accuracy before being released to production.\n\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam\n- num_epochs: 5\n- bf16: True\n\n### Package Versions\n\n- Transformers 4.27.2\n- torch 1.13.1\n- Datasets 2.13.1\n- Tokenizers 0.13.3\n\n### Machine Used and time taken\n- RTX 3090: 8 hrs. 35 mins.\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"shorthillsai/flan-t5-large-absa\", device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"shorthillsai/flan-t5-large-absa\", truncation=True)\n\nprompt = \"\"\"Find the aspect based sentiment for the given review. 'Not present' if the aspect is absent.\\n\\nReview:I love the screen of this laptop and the battery life is amazing.\\n\\nAspect:Battery Life\\n\\nSentiment: \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\").input_ids\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids)\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n```", "size_bytes": "3132793669", "downloads": 308}
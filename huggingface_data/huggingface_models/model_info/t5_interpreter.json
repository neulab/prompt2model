{"pretrained_model_name": "inkoziev/t5_interpreter", "description": "---\nlicense: lgpl-3.0\n---\n\n# t5_interpreter\n\nA rut5-based model for incomplete utterance restoration, spellchecking and text normalization for dialogue utterances.\n\nRead more about the task [here](https://huggingface.co/inkoziev/rugpt_interpreter).\n\n\n# Usage example\n\n```\nimport torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nmodel_name = 'inkoziev/t5_interpreter'\ntokenizer = T5Tokenizer.from_pretrained(model_name,)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\nmodel.eval()\n\nt5_input = '- \u0422\u0435\u0431\u044f \u043a\u0430\u043a \u0437\u043e\u0432\u0443\u0442?\\n- \u041c\u0430\u043b\u044c\u0432\u0438\u043d\u0430 #'\ninput_ids = tokenizer(t5_input, return_tensors='pt').input_ids\nout_ids = model.generate(input_ids=input_ids, max_length=40, eos_token_id=tokenizer.eos_token_id, early_stopping=True)\nt5_output = tokenizer.decode(out_ids[0][1:])\nprint(t5_output)\n```\n\n\n", "size_bytes": "258646749", "downloads": 21}
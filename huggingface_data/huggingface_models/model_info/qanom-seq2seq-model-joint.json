{"pretrained_model_name": "kleinay/qanom-seq2seq-model-joint", "description": "---\nlanguage:\n- en\ntags:\n- semantic-role-labeling\n- question-answer generation\n- pytorch\ndatasets:\n- kleinay/qanom\n---\n\n# A Seq2Seq model for QANom parsing\n\nThis is a `t5-small` pretrained model, fine-tuned jointly on the tasks of generating QASRL and QANom QAs. \n\n\"QANom\" stands for \"QASRL for Nominalizations\", which is an adaptation of [QASRL (Question-Answer driven Semantic Role Labeling)](https://qasrl.org) for the nominal predicates domain. See the [QANom paper](https://aclanthology.org/2020.coling-main.274/) for details about the task. The QANom Dataset official site is a [Google drive](https://drive.google.com/drive/folders/15PHKVdPm65ysgdkV47z6J_73kETk7_of), but we also wrapped it into a [Huggingface Dataset](https://huggingface.co/datasets/biu-nlp/qanom), which is easier to plug-and-play with (check out our [HF profile](https://huggingface.co/biu-nlp) for other related datasets, such as QASRL, QAMR, QADiscourse, and QA-Align). \n\n## Demo\n\nVisit [our demo](https://huggingface.co/spaces/kleinay/qanom-seq2seq-demo) for interactively exploring our model!\n      \n## Usage \n\nThe model and tokenizer can be downloaded as simply as running:\n```python\nimport transformers\nmodel = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"kleinay/qanom-seq2seq-model-baseline\")\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"kleinay/qanom-seq2seq-model-baseline\")\n```\n\nHowever, the model fine-tuning procedure involves input preprocessing (marking the predicate in the sentence, T5's \"task prefix\", incorporating the predicate type and/or the verbal form of the nominalization) and output postprocessing (parsing the sequence into a list of QASRL-formatted QAs).  \nIn order to use the model for QANom parsing easily, we suggest downloading the [`pipeline.py`](https://huggingface.co/kleinay/qanom-seq2seq-model-joint/blob/main/pipeline.py) file from this repository, and then use the `QASRL_Pipeline` class:\n\n```python\nfrom pipeline import QASRL_Pipeline\npipe = QASRL_Pipeline(\"kleinay/qanom-seq2seq-model-joint\")\npipe(\"The student was interested in Luke 's <predicate> research about sea animals .\", verb_form=\"research\", predicate_type=\"nominal\")\n``` \nWhich will output:\n```json\n[{'generated_text': 'who _ _ researched something _ _ ?<extra_id_7> Luke', \n  'QAs': [{'question': 'who researched something ?', 'answers': ['Luke']}]}]\n```   \nYou can learn more about using `transformers.pipelines` in the [official docs](https://huggingface.co/docs/transformers/main_classes/pipelines).\n\nNotice that you need to specify which word in the sentence is the predicate, about which the question will interrogate. By default, you should precede the predicate with the `<predicate>` symbol, but you can also specify your own predicate marker:\n```python\npipe(\"The student was interested in Luke 's <PRED> research about sea animals .\", verb_form=\"research\", predicate_type=\"nominal\", predicate_marker=\"<PRED>\")\n```\nIn addition, you can specify additional kwargs for controling the model's decoding algorithm:\n```python\npipe(\"The student was interested in Luke 's <predicate> research about sea animals .\", verb_form=\"research\", predicate_type=\"nominal\", num_beams=3)\n```\n\n\n            ", "size_bytes": "242016842", "downloads": 113}
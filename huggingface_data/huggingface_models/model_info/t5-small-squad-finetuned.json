{"pretrained_model_name": "debarghabhattofficial/t5-small-squad-finetuned", "description": "---\nlicense: cc-by-4.0\ntags:\n- generated_from_trainer\ndatasets:\n- qg_squad\nmetrics:\n- bleu\nmodel-index:\n- name: t5-small-squad-finetuned\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: qg_squad\n      type: qg_squad\n      config: qg_squad\n      split: train\n      args: qg_squad\n    metrics:\n    - name: Bleu\n      type: bleu\n      value: 0.18729932526273085\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-squad-finetuned\n\nThis model is a fine-tuned version of [lmqg/t5-small-squad](https://huggingface.co/lmqg/t5-small-squad) on the qg_squad dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.8668\n- Bleu: 0.1873\n- Precisions: [0.5110525491352382, 0.245362761211552, 0.15215077757561193, 0.09884530767928974]\n- Brevity Penalty: 0.8988\n- Length Ratio: 0.9036\n- Translation Length: 108527\n- Reference Length: 120107\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu   | Precisions                                                                          | Brevity Penalty | Length Ratio | Translation Length | Reference Length |\n|:-------------:|:-----:|:-----:|:---------------:|:------:|:-----------------------------------------------------------------------------------:|:---------------:|:------------:|:------------------:|:----------------:|\n| 1.9131        | 1.0   | 2367  | 1.8960          | 0.1861 | [0.5106522785325583, 0.24527605096325475, 0.15213089101620028, 0.09826831888082575] | 0.8944          | 0.8996       | 108052             | 120107           |\n| 1.849         | 2.0   | 4734  | 1.8806          | 0.1849 | [0.5080246970549962, 0.24224047124755838, 0.1501267012945318, 0.09769844997651479]  | 0.8972          | 0.9021       | 108353             | 120107           |\n| 1.8168        | 3.0   | 7101  | 1.8727          | 0.1854 | [0.5098220476080425, 0.24339941601352388, 0.15093927730223472, 0.09804485712417446] | 0.8956          | 0.9007       | 108175             | 120107           |\n| 1.7923        | 4.0   | 9468  | 1.8700          | 0.1863 | [0.5133830790362698, 0.24615653748790878, 0.15267642711989654, 0.09868749835608512] | 0.8916          | 0.8971       | 107748             | 120107           |\n| 1.7748        | 5.0   | 11835 | 1.8689          | 0.1869 | [0.5141749342160318, 0.24699161674176884, 0.1534446643289472, 0.0998958319598096]   | 0.8898          | 0.8954       | 107549             | 120107           |\n| 1.7587        | 6.0   | 14202 | 1.8698          | 0.1864 | [0.5146328972484753, 0.24659953524399691, 0.1532201031824242, 0.09970271520116271]  | 0.8884          | 0.8942       | 107395             | 120107           |\n| 1.7468        | 7.0   | 16569 | 1.8680          | 0.1860 | [0.5112671501824734, 0.24460144371064352, 0.15145530742292898, 0.09809866056844169] | 0.8961          | 0.9012       | 108235             | 120107           |\n| 1.7378        | 8.0   | 18936 | 1.8670          | 0.1876 | [0.5122261914652045, 0.24610537728997678, 0.15275308797724588, 0.09927828458817849] | 0.8970          | 0.9020       | 108333             | 120107           |\n| 1.7312        | 9.0   | 21303 | 1.8676          | 0.1876 | [0.5117292997446746, 0.24587669400218548, 0.15249172858304044, 0.0989853996535511]  | 0.8984          | 0.9033       | 108489             | 120107           |\n| 1.7271        | 10.0  | 23670 | 1.8668          | 0.1873 | [0.5110525491352382, 0.245362761211552, 0.15215077757561193, 0.09884530767928974]   | 0.8988          | 0.9036       | 108527             | 120107           |\n\n\n### Framework versions\n\n- Transformers 4.22.2\n- Pytorch 1.12.1+cu113\n- Datasets 2.5.2\n- Tokenizers 0.12.1\n", "size_bytes": "242014971", "downloads": 2}
{"pretrained_model_name": "google/roberta2roberta_L-24_discofuse", "description": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- discofuse\n---\n\n# Roberta2Roberta_L-24_discofuse EncoderDecoder model\n\nThe model was introduced in \n[this paper](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn and first released in [this repository](https://tfhub.dev/google/bertseq2seq/roberta24_discofuse/1). \n\nThe model is an encoder-decoder model that was initialized on the `roberta-large` checkpoints for both the encoder \nand decoder and fine-tuned on sentencefusion on the discofuse dataset, which is linked above.\n\nDisclaimer: The model card has been written by the Hugging Face team.\n\n## How to use\n\nYou can use this model for sentence fusion, *e.g.*\n\nIMPORTANT: The model was not trained on the `\"` (double quotation mark) character -> so the before tokenizing the text, it is advised to replace all `\"` (double quotation marks) with a single `` ` `` (single back tick).\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n\ndiscofuse = \"\"\"As a run-blocker, Zeitler moves relatively well. Zeitler often struggles at the point of contact in space.\"\"\"\n\ninput_ids = tokenizer(discofuse, return_tensors=\"pt\").input_ids\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n# should output\n# As a run-blocker, Zeitler moves relatively well. However, Zeitler often struggles at the point of contact in space.  \n```\n", "size_bytes": "1821418078", "downloads": 264}
{"pretrained_model_name": "dumitrescustefan/mt5-base-romanian", "description": "---\nlanguage: ro\ninference: false\nlicense: apache-2.0\n---\n\nThis is a pretrained [MT5](https://github.com/google-research/multilingual-t5) base model (**390M** parameters).\n\nTraining was performed with the span corruption task on a clean 80GB Romanian text corpus for 4M total steps with these [scripts](https://github.com/dumitrescustefan/t5x_models), starting from the 1M public mt5x-base checkpoint. The model was trained with an encoder sequence length of 512 and a decoder sequence length of 256; it has the same mt5x vocabulary as the 1M multilingual checkpoint.\n\n**!! IMPORTANT !!** This model was pretrained on the span corruption MLM task, meaning this model is **not usable** in any downstream task **without finetuning** first!\n\n### How to load an mt5x model\n\n```python\nfrom transformers import MT5Model, T5Tokenizer\n\nmodel = MT5Model.from_pretrained('dumitrescustefan/mt5-base-romanian')\ntokenizer = T5Tokenizer.from_pretrained('dumitrescustefan/mt5-base-romanian')\ninput_text = \"Acesta este un test.\"\ntarget_text = \"Acesta este\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\nlabels = tokenizer(text_target=target_text, return_tensors=\"pt\")\n\noutputs = model(input_ids=inputs[\"input_ids\"], decoder_input_ids=labels[\"input_ids\"])\nhidden_states = outputs.last_hidden_state\nprint(hidden_states.shape)  # this will print [1, 4, 768]\n```\n\nRemember to always sanitize your text! Replace ``\u015f`` and ``\u0163`` cedilla-letters to comma-letters with :\n```python\ntext = text.replace(\"\u0163\", \"\u021b\").replace(\"\u015f\", \"\u0219\").replace(\"\u0162\", \"\u021a\").replace(\"\u015e\", \"\u0218\")\n```\nbecause the model was **not** trained on cedilla ``\u015f`` and ``\u0163``s. If you don't, you will have decreased performance due to ``<UNK>``s and increased number of tokens per word.\n\n### Acknowledgements\n\nWe'd like to thank [TPU Research Cloud](https://sites.research.google/trc/about/) for providing the TPUv4 cores we used to train these models!\n\n### Authors\n\nYours truly,  \n\n_[Stefan Dumitrescu](https://github.com/dumitrescustefan), [Mihai Ilie](https://github.com/iliemihai) and [Per Egil Kummervold](https://huggingface.co/north)_\n", "size_bytes": "2329696333", "downloads": 7}
{"pretrained_model_name": "HeyLucasLeao/byt5-base-pt-product-reviews", "description": "Create README.md\n## ByT5 Base Portuguese Product Reviews\n\n#### Model Description\nThis is a finetuned version from ByT5 Base by Google for Sentimental Analysis from Product Reviews in Portuguese.\n##### Paper: https://arxiv.org/abs/2105.13626\n\n#### Training data\nIt was trained from products reviews from a Americanas.com. You can found the data here: https://github.com/HeyLucasLeao/finetuning-byt5-model.\n\n#### Training Procedure\nIt was finetuned using the Trainer Class available on the Hugging Face library. For evaluation it was used accuracy, precision, recall and f1 score.\n\n##### Learning Rate: **1e-4**\n##### Epochs: **1**\n##### Colab for Finetuning: https://drive.google.com/file/d/17TcaN52moq7i7TE2EbcVbwQEQuAIQU63/view?usp=sharing\n##### Colab for Metrics: https://colab.research.google.com/drive/1wbTDfOsE45UL8Q3ZD1_FTUmdVOKCcJFf#scrollTo=S4nuLkAFrlZ6\n#### Score:\n```python\nTraining Set:\n'accuracy': 0.9019706922688226,\n 'f1': 0.9305820610687022,\n 'precision': 0.9596555965559656,\n 'recall': 0.9032183375781431\nTest Set:\n'accuracy': 0.9019409684035312,\n 'f1': 0.9303758732034697,\n 'precision': 0.9006660401258529,\n 'recall': 0.9621126145787866\n\nValidation Set:\n'accuracy': 0.9044948078526491,\n 'f1': 0.9321924443009364,\n 'precision': 0.9024426549173129,\n 'recall': 0.9639705531617191\n```\n\n#### Goals\n\nMy true intention was totally educational, thus making available a this version of the model as a example for future proposes.\n\nHow to use\n``` python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\nprint(device)\n\ntokenizer = AutoTokenizer.from_pretrained(\"HeyLucasLeao/byt5-base-pt-product-reviews\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"HeyLucasLeao/byt5-base-pt-product-reviews\")\nmodel.to(device)\n\ndef classificar_review(review):\n  inputs = tokenizer([review], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n  input_ids = inputs.input_ids.to(device)\n  attention_mask = inputs.attention_mask.to(device)\n  output = model.generate(input_ids, attention_mask=attention_mask)\n  pred = np.argmax(output.cpu(), axis=1)\n  dici = {0: 'Review Negativo', 1: 'Review Positivo'}\n  return dici[pred.item()]\n  \nclassificar_review(review)\n```", "size_bytes": "2326723273", "downloads": 9}
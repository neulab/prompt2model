{"pretrained_model_name": "GenzNepal/mt5-summarize-nepali", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- summarization\n- nepali\nmodel-index:\n- name: mt5-summarize-nepali\n  results: []\nlanguage:\n- ne\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mt5-summarize-nepali\n\nThis model is a fine-tuned version of [google/mt5-small](https://huggingface.co/google/mt5-small) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6748\n\n## Usage\n```python\n\n>>> import torch\n\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Predict with test data (first 5 rows)\n>>> model_ckpt = \"GenzNepal/mt5-summarize-nepali\"\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n>>> t5_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n\n>>> text = \"\u0915\u093e\u0920\u092e\u093e\u0921\u094c\u0901 \u0964 \u0939\u093e\u0932 \u0926\u0947\u0936\u0915\u094b \u092a\u0942\u0930\u094d\u0935\u0940 \u0924\u0925\u093e \u092e\u0927\u094d\u092f \u092d\u0942\u2013\u092d\u093e\u0917\u092e\u093e \u092e\u0928\u0938\u0941\u0928\u0940 \u092a\u094d\u0930\u0923\u093e\u0932\u0940\u0915\u094b \u092a\u094d\u0930\u092d\u093e\u0935 \u0930\u0939\u0947\u0915\u094b \u091b \u092d\u0928\u0947 \u092c\u093e\u0901\u0915\u0940 \u092d\u0942\u2013\u092d\u093e\u0917\u092e\u093e \u0938\u094d\u0925\u093e\u0928\u0940\u092f \u0935\u093e\u092f\u0941 \u0930 \u092a\u0936\u094d\u091a\u093f\u092e\u0940 \u0935\u093e\u092f\u0941\u0915\u094b \u0906\u0902\u0936\u093f\u0915 \u092a\u094d\u0930\u092d\u093e\u0935 \u0930\u0939\u0947\u0915\u094b \u091b \u0964 \u092f\u0938\u0915\u093e \u0915\u093e\u0930\u0923 \u0939\u093e\u0932 \u0917\u0923\u094d\u0921\u0915\u0940 \u092a\u094d\u0930\u0926\u0947\u0936\u0915\u093e \u0925\u094b\u0930\u0948 \u0938\u094d\u0925\u093e\u0928\u092e\u093e \u0930 \u0915\u0930\u094d\u0923\u093e\u0932\u0940 \u092a\u094d\u0930\u0926\u0947\u0936\u0915\u093e \u090f\u0915\u2013\u0926\u0941\u0908 \u0938\u094d\u0925\u093e\u0928\u092e\u093e \u092e\u0947\u0918\u0917\u0930\u094d\u091c\u0928\u0930\u091a\u091f\u094d\u092f\u093e\u0919\u0938\u0939\u093f\u0924 \u0939\u0932\u094d\u0915\u093e\u0926\u0947\u0916\u093f \u092e\u0927\u094d\u092f\u092e \u0935\u0930\u094d\u0937\u093e \u092d\u0907\u0930\u0939\u0947\u0915\u094b \u091c\u0932 \u0924\u0925\u093e \u092e\u094c\u0938\u092e \u0935\u093f\u091c\u094d\u091e\u093e\u0928 \u0935\u093f\u092d\u093e\u0917, \u092e\u094c\u0938\u092e \u092a\u0942\u0930\u094d\u0935\u093e\u0928\u0941\u092e\u093e\u0928 \u092e\u0939\u093e\u0936\u093e\u0916\u093e\u0932\u0947 \u091c\u0928\u093e\u090f\u0915\u094b \u091b \u0964 \\\n\u092e\u0939\u093e\u0936\u093e\u0916\u0915\u093e \u092e\u094c\u092e\u0938\u0935\u093f\u0926\u094d \u0930\u094b\u091c\u0932 \u0932\u093e\u092e\u093f\u091b\u093e\u0928\u0947\u0915\u093e \u0905\u0928\u0941\u0938\u093e\u0930 \u092a\u091b\u093f\u0932\u094d\u0932\u094b \u0924\u0940\u0928 \u0918\u0928\u094d\u091f\u093e\u092e\u093e \u0917\u0923\u094d\u0921\u0915\u0940 \u092a\u094d\u0930\u0926\u0947\u0936\u0915\u093e \u0925\u094b\u0930\u0948 \u0938\u094d\u0925\u093e\u0928, \u092c\u093e\u0917\u092e\u0924\u0940 \u092a\u094d\u0930\u0926\u0947\u0936\u0915\u093e \u090f\u0915\u2013\u0926\u0941\u0908 \u0938\u094d\u0925\u093e\u0928\u092e\u093e \u0939\u0932\u094d\u0915\u093e\u0926\u0947\u0916\u093f \u092e\u0927\u094d\u092f\u092e \u0935\u0930\u094d\u0937\u093e \u092d\u0907\u0930\u0939\u0947\u0915\u094b \u091b \u0964 \u0915\u093e\u0920\u092e\u093e\u0921\u094c\u0901 \u0909\u092a\u0924\u094d\u092f\u0915\u093e\u0938\u0939\u093f\u0924 \u092c\u093e\u0917\u092e\u0924\u0940 \u092a\u094d\u0930\u0926\u0947\u0936\u092e\u093e \u0930\u093e\u0924\u093f\u0915\u094b \u0938\u092e\u092f\u092e\u093e \u0935\u0930\u094d\u0937\u093e\u0915\u094b \u0938\u092e\u094d\u092d\u093e\u0935\u0928\u093e \u0930\u0939\u0947\u0915\u094b \u091b \u0964 \u092f\u0938\u094d\u0924\u0948 \u0915\u094b\u0936\u0940 \u092a\u094d\u0930\u0926\u0947\u0936, \u092e\u0927\u0947\u0936 \u092a\u094d\u0930\u0926\u0947\u0936 \u0930 \u0926\u0947\u0936\u0915\u093e \u092a\u0939\u093e\u0921\u0940 \u092d\u0942\u2013\u092d\u093e\u0917\u092e\u093e \u092c\u0926\u0932\u0940 \u0930\u0939\u0928\u0941\u0915\u093e \u0938\u093e\u0925\u0948 \u0939\u0932\u094d\u0915\u093e \u0935\u0930\u094d\u0937\u093e\u0915\u094b \u0938\u092e\u094d\u092d\u093e\u0935\u0928\u093e \u0930\u0939\u0947\u0915\u094b \u092e\u0939\u093e\u0936\u093e\u0916\u093e\u0932\u0947 \u0909\u0932\u094d\u0932\u0947\u0916 \u0917\u0930\u0947\u0915\u094b \u091b \u0964 \\\n\u092e\u094c\u0938\u092e\u0935\u093f\u0926\u094d \u0932\u093e\u092e\u093f\u091b\u093e\u0928\u0947\u0932\u0947 \u092e\u0928\u0938\u0941\u0928 \u092a\u094d\u0930\u0923\u093e\u0932\u0940 \u0915\u094d\u0930\u092e\u093f\u0915\u0930\u0942\u092a\u092e\u093e \u0926\u0947\u0936\u092d\u0930 \u092b\u0948\u0932\u093f\u0928\u0947 \u0915\u094d\u0930\u092e\u092e\u093e \u0930\u0939\u0947\u0915\u094b \u0930 \u092f\u094b \u0926\u0947\u0936\u092d\u0930 \u0935\u093f\u0938\u094d\u0924\u093e\u0930 \u0939\u0941\u0928 \u0905\u091d\u0948 \u090f\u0915 \u0938\u093e\u0924\u093e \u0932\u093e\u0917\u094d\u0928\u0947 \u092c\u0924\u093e\u090f \u0964 \u0917\u0924 \u091c\u0947\u0920 \u0969\u0967 \u0917\u0924\u0947 \u092c\u0941\u0927\u092c\u093e\u0930 \u0928\u0947\u092a\u093e\u0932\u0915\u094b \u092a\u0942\u0930\u094d\u0935\u0940 \u092d\u0947\u0917 \u092d\u090f\u0930 \u092e\u0928\u0938\u0941\u0928 \u092a\u094d\u0930\u0923\u093e\u0932\u0940 \u092d\u093f\u0924\u094d\u0930\u093f\u090f\u0915\u094b \u0925\u093f\u092f\u094b \u0964 \u092e\u0928\u0938\u0941\u0928 \u0938\u0941\u0938\u094d\u0924\u0917\u0924\u093f\u092e\u093e \u0930\u0939\u0947\u0915\u093e\u0932\u0947 \u0926\u0947\u0936\u0915\u094b \u092a\u0936\u094d\u091a\u093f\u092e \u0915\u094d\u0937\u0947\u0924\u094d\u0930\u092e\u093e \u092b\u0948\u0932\u093f\u0928 \u0915\u0947\u0939\u0940 \u0926\u093f\u0928 \u0932\u093e\u0917\u094d\u0928\u0947 \u091c\u0928\u093e\u0907\u090f\u0915\u094b \u091b \u0964\"\n\n>>> inputs = t5_tokenizer(text, return_tensors=\"pt\", max_length=1024, padding= \"max_length\", truncation=True, add_special_tokens=True)\n\n>>> generation = model.generate(\n      input_ids = inputs['input_ids'].to(device),\n      attention_mask=inputs['attention_mask'].to(device),\n      num_beams=6,\n      num_return_sequences=1,\n      no_repeat_ngram_size=2,\n      repetition_penalty=1.0,\n      min_length=100,\n      max_length=250,\n      length_penalty=2.0,\n      early_stopping=True\n    )\n    # # Convert id tokens to text\n\n>>> output = t5_tokenizer.decode(generation[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n\n>>> print(output)\n\n\"\u0939\u093e\u0932 \u0926\u0947\u0936\u0915\u094b \u092a\u0942\u0930\u094d\u0935\u0940 \u0924\u0925\u093e \u092e\u0927\u094d\u092f \u092d\u0942\u2013\u092d\u093e\u0917\u092e\u093e \u092e\u0928\u0938\u0941\u0928\u0940 \u092a\u094d\u0930\u0923\u093e\u0932\u0940\u0915\u094b \u092a\u094d\u0930\u092d\u093e\u0935 \u0930\u0939\u0947\u0915\u094b \u091b \u0964 \u092c\u093e\u0901\u0915\u0940 \u092d\u0942\u092d\u093e\u0917\u0939\u0930\u0942\u092e\u093e \u0938\u094d\u0925\u093e\u0928\u0940\u092f \u0935\u093e\u092f\u0941 \u0930 \u092a\u0936\u094d\u091a\u093f\u092e\u0940 \u0935\u093e\u092f\u0941\u0915\u094b \u0906\u0902\u0936\u093f\u0915 \u0938\u0919\u094d\u0915\u094d\u0930\u092e\u0923 \u091b\u0964 \u0917\u0924 \u0935\u0948\u0936\u093e\u0916 \u0969\u0967 \u0917\u0924\u0947 \u092c\u0941\u0927\u092c\u093e\u0930 \u0928\u0947\u092a\u093e\u0932\u0915\u094b \u092d\u0947\u0917 \u092d\u090f\u0930 \u092e\u0928\u0938\u0941\u0928 \u092a\u094d\u0930\u0923\u093e\u0932\u0940 \u092d\u093f\u0924\u094d\u0930\u093f\u090f\u0915\u094b \u0925\u093f\u092f\u094b \u092d\u0928\u0947 \u0939\u0932\u094d\u0915\u093e\u0926\u0947\u0916\u093f \u092e\u0927\u094d\u092f\u092e \u0935\u0930\u094d\u0937\u093e \u092d\u0907\u0930\u0939\u0947\u0915\u094b \u091c\u0928\u093e\u0907\u090f\u0915\u094b \u091b \u092d\u0928\u0947 \u092e\u094c\u0938\u092e\u0935\u093f\u0926\u094d \u0932\u093e\u092e\u093f\u091b\u093e\u0928\u0947\u0932\u0947 \u0909\u0932\u094d\u0932\u0947\u0916 \u0917\u0930\u0947\u0915\u093e \u091b\u0928\u094d \u092d\u0928\u0947 \u092f\u094b \u0926\u0947\u0936\u092d\u0930 \u0935\u093f\u0938\u094d\u0924\u093e\u0930 \u0939\u0941\u0928 \u0905\u091d\u0948 \u090f\u0915 \u0938\u093e\u0924\u093e \u0932\u093e\u0917\u094d\u0928\u0947\u091b\u0964\n\"\n\n```\n\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 2\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 90\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 0.7762        | 2.72  | 2500 | 0.7255          |\n| 0.6377        | 5.44  | 5000 | 0.6947          |\n| 0.5674        | 8.15  | 7500 | 0.6748          |\n\n\n### Framework versions\n\n- Transformers 4.30.1\n- Pytorch 2.0.0\n- Datasets 2.1.0\n- Tokenizers 0.13.3", "size_bytes": "1200772485", "downloads": 18}
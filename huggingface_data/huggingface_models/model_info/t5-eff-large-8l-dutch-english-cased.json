{"pretrained_model_name": "yhavinga/t5-eff-large-8l-dutch-english-cased", "description": "---\nlanguage:\n- nl\n- en\ndatasets:\n- yhavinga/mc4_nl_cleaned\ntags:\n- t5\n- seq2seq\n\ninference: false\nlicense: apache-2.0\n---\n\n# t5-eff-large-8l-dutch-english-cased\n\nA [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) sequence to sequence model\npre-trained from scratch on [cleaned Dutch \ud83c\uddf3\ud83c\uddf1\ud83c\udde7\ud83c\uddea mC4 and cleaned English \ud83c\uddec\ud83c\udde7 C4](https://huggingface.co/datasets/yhavinga/mc4_nl_cleaned).\n\n\n\n\nThis **t5 eff** model has **334M** parameters.\nIt was pre-trained with masked language modeling (denoise token span corruption) objective on the dataset\n`mc4_nl_cleaned` config `large_en_nl` for **1** epoch(s) and a duration of **3d 23h**,\nwith a sequence length of **512**, batch size **128** and **851850** total steps (**56B** tokens).\nPre-training evaluation loss and accuracy are **1,15** and **0,74**.\nRefer to the evaluation section below for a comparison of the pre-trained models on summarization and translation.\n* Pre-trained T5 models need to be finetuned before they can be used for downstream tasks, therefore the inference widget on the right has been turned off.\n* For a demo of the Dutch CNN summarization models, head over to the Hugging Face Spaces for\nthe **[Netherformer \ud83d\udcf0](https://huggingface.co/spaces/flax-community/netherformer)** example application!\n\nPlease refer to the original T5 papers and Scale Efficiently papers for more information about the T5 architecture\nand configs, though it must be noted that this model (t5-eff-large-8l-dutch-english-cased) is unrelated to these projects and not an 'official' checkpoint.\n* **[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)** by *Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu*.\n* **[Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers](https://arxiv.org/abs/2109.10686)** by *Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler*.\n\n\n## Tokenizer\n\nThe model uses a cased SentencePiece tokenizer configured with the `Nmt, NFKC, Replace multi-space to single-space` normalizers\nand has 32003 tokens.\nIt was trained on Dutch and English  with scripts from the Huggingface Transformers [Flax examples](https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling).\nSee [./raw/main/tokenizer.json](tokenizer.json) for details.\n \n## Dataset(s)\n\nAll models listed below are pre-trained on \n[cleaned Dutch mC4](https://huggingface.co/datasets/yhavinga/mc4_nl_cleaned),\nwhich is the original mC4, except\n\n  * Documents that contained words from a selection of the Dutch and English [List of Dirty Naught Obscene and Otherwise Bad Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words) are removed\n  * Sentences with less than 3 words are removed\n  * Sentences with a word of more than 1000 characters are removed\n  * Documents with less than 5 sentences are removed\n  * Documents with \"javascript\", \"lorum ipsum\", \"terms of use\", \"privacy policy\", \"cookie policy\", \"uses cookies\",\n    \"use of cookies\", \"use cookies\", \"elementen ontbreken\", \"deze printversie\" are removed.\n \nThe Dutch and English models are pre-trained on a 50/50% mix of Dutch mC4 and English C4.\n\nThe translation models are fine-tuned on [CCMatrix](https://huggingface.co/datasets/yhavinga/ccmatrix).\n\n## Dutch T5 Models\n\nThree types of [Dutch T5 models have been trained (blog)](https://huggingface.co/spaces/yhavinga/pre-training-dutch-t5-models).\n`t5-base-dutch` is the only model with an original T5 config.\nThe other model types t5-v1.1 and t5-eff have `gated-relu` instead of `relu` as activation function,\nand trained with a drop-out of `0.0` unless training would diverge (`t5-v1.1-large-dutch-cased`).\nThe T5-eff models are models that differ in their number of layers. The table will list\nthe several dimensions of these models. Not all t5-eff models are efficient, the best example being the inefficient\n`t5-xl-4L-dutch-english-cased`. \n\n|                   | [t5-base-dutch](https://huggingface.co/yhavinga/t5-base-dutch)   | [t5-v1.1-base-dutch-uncased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-uncased)   | [t5-v1.1-base-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cased)   | [t5-v1.1-large-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-large-dutch-cased)   | [t5-v1_1-base-dutch-english-cased](https://huggingface.co/yhavinga/t5-v1_1-base-dutch-english-cased)   | [t5-v1_1-base-dutch-english-cased-1024](https://huggingface.co/yhavinga/t5-v1_1-base-dutch-english-cased-1024)   | [t5-small-24L-dutch-english](https://huggingface.co/yhavinga/t5-small-24L-dutch-english)   | [t5-xl-4L-dutch-english-cased](https://huggingface.co/yhavinga/t5-xl-4L-dutch-english-cased)   | [t5-base-36L-dutch-english-cased](https://huggingface.co/yhavinga/t5-base-36L-dutch-english-cased)   | [t5-eff-xl-8l-dutch-english-cased](https://huggingface.co/yhavinga/t5-eff-xl-8l-dutch-english-cased)   | [t5-eff-large-8l-dutch-english-cased](https://huggingface.co/yhavinga/t5-eff-large-8l-dutch-english-cased)   |\n|:------------------|:----------------|:-----------------------------|:---------------------------|:----------------------------|:-----------------------------------|:----------------------------------------|:-----------------------------|:-------------------------------|:----------------------------------|:-----------------------------------|:--------------------------------------|\n| *type* | t5              | t5-v1.1                      | t5-v1.1                    | t5-v1.1                     | t5-v1.1                            | t5-v1.1                                 | t5 eff                       | t5 eff                         | t5 eff                            | t5 eff                             | t5 eff                                |\n| *d_model* | 768             | 768                          | 768                        | 1024                        | 768                                | 768                                     | 512                          | 2048                           | 768                               | 1024                               | 1024                                  |\n| *d_ff* | 3072            | 2048                         | 2048                       | 2816                        | 2048                               | 2048                                    | 1920                         | 5120                           | 2560                              | 16384                              | 4096                                  |\n| *num_heads* | 12              | 12                           | 12                         | 16                          | 12                                 | 12                                      | 8                            | 32                             | 12                                | 32                                 | 16                                    |\n| *d_kv* | 64              | 64                           | 64                         | 64                          | 64                                 | 64                                      | 64                           | 64                             | 64                                | 128                                | 64                                    |\n| *num_layers* | 12              | 12                           | 12                         | 24                          | 12                                 | 12                                      | 24                           | 4                              | 36                                | 8                                  | 8                                     |\n| *num parameters* | 223M            | 248M                         | 248M                       | 783M                        | 248M                               | 248M                                    | 250M                         | 585M                           | 729M                              | 1241M                              | 335M                                  |\n| *feed_forward_proj* | relu            | gated-gelu                   | gated-gelu                 | gated-gelu                  | gated-gelu                         | gated-gelu                              | gated-gelu                   | gated-gelu                     | gated-gelu                        | gated-gelu                         | gated-gelu                            |\n| *dropout* | 0.1             | 0.0                          | 0.0                        | 0.1                         | 0.0                                | 0.0                                     | 0.0                          | 0.1                            | 0.0                               | 0.0                                | 0.0                                   |\n| *dataset* | mc4_nl_cleaned  | mc4_nl_cleaned full          | mc4_nl_cleaned full        | mc4_nl_cleaned              | mc4_nl_cleaned small_en_nl         | mc4_nl_cleaned large_en_nl              | mc4_nl_cleaned large_en_nl   | mc4_nl_cleaned large_en_nl     | mc4_nl_cleaned large_en_nl        | mc4_nl_cleaned large_en_nl         | mc4_nl_cleaned large_en_nl            |\n| *tr. seq len* | 512             | 1024                         | 1024                       | 512                         | 512                                | 1024                                    | 512                          | 512                            | 512                               | 512                                | 512                                   |\n| *batch size* | 128             | 64                           | 64                         | 64                          | 128                                | 64                                      | 128                          | 512                            | 512                               | 64                                 | 128                                   |\n| *total steps* | 527500          | 1014525                      | 1210154                    | 1120k/2427498               | 2839630                            | 1520k/3397024                           | 851852                       | 212963                         | 212963                            | 538k/1703705                       | 851850                                |\n| *epochs* | 1               | 2                            | 2                          | 2                           | 10                                 | 4                                       | 1                            | 1                              | 1                                 | 1                                  | 1                                     |\n| *duration* | 2d9h            | 5d5h                         | 6d6h                       | 8d13h                       | 11d18h                             | 9d1h                                    | 4d10h                        | 6d1h                           | 17d15h                            | 4d 19h                             | 3d 23h                                |\n| *optimizer* | adafactor       | adafactor                    | adafactor                  | adafactor                   | adafactor                          | adafactor                               | adafactor                    | adafactor                      | adafactor                         | adafactor                          | adafactor                             |\n| *lr* | 0.005           | 0.005                        | 0.005                      | 0.005                       | 0.005                              | 0.005                                   | 0.005                        | 0.005                          | 0.009                             | 0.005                              | 0.005                                 |\n| *warmup* | 10000.0         | 10000.0                      | 10000.0                    | 10000.0                     | 10000.0                            | 5000.0                                  | 20000.0                      | 2500.0                         | 1000.0                            | 1500.0                             | 1500.0                                |\n| *eval loss* | 1,38            | 1,20                         | 0,96                       | 1,07                        | 1,11                               | 1,13                                    | 1,18                         | 1,27                           | 1,05                              | 1,3019                             | 1,15                                  |\n| *eval acc* | 0,70            | 0,73                         | 0,78                       | 0,76                        | 0,75                               | 0,74                                    | 0,74                         | 0,72                           | 0,76                              | 0,71                               | 0,74                                  |\n\n## Evaluation\n\nMost models from the list above have been fine-tuned for summarization and translation.\nThe figure below shows the evaluation scores, where the x-axis shows the translation Bleu score (higher is better)\nand y-axis the summarization Rouge1 translation score (higher is better).\nPoint size is proportional to the model size. Models with faster inference speed are green, slower inference speed is\nplotted as bleu.\n\n![Evaluation T5 Dutch English](evaluation_t5_dutch_english.png)\n\nEvaluation was run on fine-tuned models trained with the following settings:   \n\n\n|                | Summarization    | Translation       |\n|---------------:|------------------|-------------------|\n|        Dataset | CNN Dailymail NL | CCMatrix en -> nl |\n| #train samples | 50K              | 50K               |\n|      Optimizer | Adam             | Adam              |\n|  learning rate | 0.001            | 0.0005            |\n|  source length | 1024             | 128               |\n|  target length | 142              | 128               |\n|label smoothing | 0.05             | 0.1               |\n|  #eval samples | 1000             | 1000              |\n\nNote that the amount of training data is limited to a fraction of the total dataset sizes, therefore the scores\nbelow can only be used to compare the 'transfer-learning' strength. The fine-tuned checkpoints for this evaluation \nare not saved, since they were trained for comparison of pre-trained models only.\n\nThe numbers for summarization are the Rouge scores on 1000 documents from the test split.\n\n|                         |   [t5-base-dutch](https://huggingface.co/yhavinga/t5-base-dutch) |   [t5-v1.1-base-dutch-uncased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-uncased) |   [t5-v1.1-base-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cased) |   [t5-v1_1-base-dutch-english-cased](https://huggingface.co/yhavinga/t5-v1_1-base-dutch-english-cased) |   [t5-v1_1-base-dutch-english-cased-1024](https://huggingface.co/yhavinga/t5-v1_1-base-dutch-english-cased-1024) |   [t5-small-24L-dutch-english](https://huggingface.co/yhavinga/t5-small-24L-dutch-english) |   [t5-xl-4L-dutch-english-cased](https://huggingface.co/yhavinga/t5-xl-4L-dutch-english-cased) |   [t5-base-36L-dutch-english-cased](https://huggingface.co/yhavinga/t5-base-36L-dutch-english-cased) |   [t5-eff-large-8l-dutch-english-cased](https://huggingface.co/yhavinga/t5-eff-large-8l-dutch-english-cased) |   mt5-base |\n|:------------------------|----------------:|-----------------------------:|---------------------------:|-----------------------------------:|----------------------------------------:|-----------------------------:|-------------------------------:|----------------------------------:|--------------------------------------:|-----------:|\n| *rouge1* |           33.38 |                        33.97 |                      34.39 |                              33.38 |                                   34.97 |                        34.38 |                          30.35 |                             **35.04** |                                 34.04 |      33.25 |\n| *rouge2* |           13.32 |                        13.85 |                      13.98 |                              13.47 |                                   14.01 |                        13.89 |                          11.57 |                             **14.23** |                                 13.76 |      12.74 |\n| *rougeL* |           24.22 |                        24.72 |                      25.1  |                              24.34 |                                   24.99 |                        **25.25** |                          22.69 |                             25.05 |                                 24.75 |      23.5  |\n| *rougeLsum* |           30.23 |                        30.9  |                      31.44 |                              30.51 |                                   32.01 |                        31.38 |                          27.5  |                             **32.12** |                                 31.12 |      30.15 |\n| *samples_per_second* |            3.18 |                         3.02 |                       2.99 |                               3.22 |                                    2.97 |                         1.57 |                           2.8  |                              0.61 |                                  **3.27** |       1.22 |\n\nThe models below have been evaluated for English to Dutch translation.\nNote that the first four models are pre-trained on Dutch only. That they still perform adequate is probably because\nthe translation direction is English to Dutch.\nThe numbers reported are the Bleu scores on 1000 documents from the test split.\n\n|                                |   [t5-base-dutch](https://huggingface.co/yhavinga/t5-base-dutch) |   [t5-v1.1-base-dutch-uncased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-uncased) |   [t5-v1.1-base-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cased) |   [t5-v1.1-large-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-large-dutch-cased) |   [t5-v1_1-base-dutch-english-cased](https://huggingface.co/yhavinga/t5-v1_1-base-dutch-english-cased) |   [t5-v1_1-base-dutch-english-cased-1024](https://huggingface.co/yhavinga/t5-v1_1-base-dutch-english-cased-1024) |   [t5-small-24L-dutch-english](https://huggingface.co/yhavinga/t5-small-24L-dutch-english) |   [t5-xl-4L-dutch-english-cased](https://huggingface.co/yhavinga/t5-xl-4L-dutch-english-cased) |   [t5-base-36L-dutch-english-cased](https://huggingface.co/yhavinga/t5-base-36L-dutch-english-cased) |   [t5-eff-large-8l-dutch-english-cased](https://huggingface.co/yhavinga/t5-eff-large-8l-dutch-english-cased) |   mt5-base |\n|:-------------------------------|----------------:|-----------------------------:|---------------------------:|----------------------------:|-----------------------------------:|----------------------------------------:|-----------------------------:|-------------------------------:|----------------------------------:|--------------------------------------:|-----------:|\n| *precision_ng1* |           74.17 |                        78.09 |                      77.08 |                       72.12 |                              77.19 |                                   78.76 |                        78.59 |                          77.3  |                             **79.75** |                                 78.88 |      73.47 |\n| *precision_ng2* |           52.42 |                        57.52 |                      55.31 |                       48.7  |                              55.39 |                                   58.01 |                        57.83 |                          55.27 |                             **59.89** |                                 58.27 |      50.12 |\n| *precision_ng3* |           39.55 |                        45.2  |                      42.54 |                       35.54 |                              42.25 |                                   45.13 |                        45.02 |                          42.06 |                             **47.4**  |                                 45.95 |      36.59 |\n| *precision_ng4* |           30.23 |                        36.04 |                      33.26 |                       26.27 |                              32.74 |                                   35.72 |                        35.41 |                          32.61 |                             **38.1**  |                                 36.91 |      27.26 |\n| *bp* |            0.99 |                         0.98 |                       0.97 |                        0.98 |                               0.98 |                                    0.98 |                         0.98 |                           0.97 |                              0.98 |                                  0.98 |       0.98 |\n| *score* |           45.88 |                        51.21 |                      48.31 |                       41.59 |                              48.17 |                                   51.31 |                        50.82 |                          47.83 |                             **53**    |                                 51.79 |      42.74 |\n| *samples_per_second* |           **45.19** |                        45.05 |                      38.67 |                       10.12 |                              42.19 |                                   42.61 |                        12.85 |                          33.74 |                              9.07 |                                 37.86 |       9.03 |\n\n\n## Translation models\n\nThe models `t5-small-24L-dutch-english` and `t5-base-36L-dutch-english` have been fine-tuned for both language\ndirections on the first 25M samples from CCMatrix, giving a total of 50M training samples.\nEvaluation is performed on out-of-sample CCMatrix and also on Tatoeba and Opus Books.\nThe `_bp` columns list the *brevity penalty*. The `avg_bleu` score is the bleu score\naveraged over all three evaluation datasets. The best scores displayed in bold for both translation directions.\n\n|                        | [t5-base-36L-ccmatrix-multi](https://huggingface.co/yhavinga/t5-base-36L-ccmatrix-multi)   | [t5-base-36L-ccmatrix-multi](https://huggingface.co/yhavinga/t5-base-36L-ccmatrix-multi)   | [t5-small-24L-ccmatrix-multi](https://huggingface.co/yhavinga/t5-small-24L-ccmatrix-multi)   | [t5-small-24L-ccmatrix-multi](https://huggingface.co/yhavinga/t5-small-24L-ccmatrix-multi)   |\n|:-----------------------|:-----------------------------|:-----------------------------|:------------------------------|:------------------------------|\n| *source_lang* | en                           | nl                           | en                            | nl                            |\n| *target_lang* | nl                           | en                           | nl                            | en                            |\n| *source_prefix* | translate English to Dutch:  | translate Dutch to English:  | translate English to Dutch:   | translate Dutch to English:   |\n| *ccmatrix_bleu* | **56.8**                         | 62.8                         | 57.4                          | **63.1**                          |\n| *tatoeba_bleu* | **46.6**                         | **52.8**                         | 46.4                          | 51.7                          |\n| *opus_books_bleu* | **13.5**                         | **24.9**                         | 12.9                          | 23.4                          |\n| *ccmatrix_bp* | 0.95                         | 0.96                         | 0.95                          | 0.96                          |\n| *tatoeba_bp* | 0.97                         | 0.94                         | 0.98                          | 0.94                          |\n| *opus_books_bp* | 0.8                          | 0.94                         | 0.77                          | 0.89                          |\n| *avg_bleu* | **38.96**                        | **46.86**                        | 38.92                         | 46.06                         |\n| *max_source_length* | 128                          | 128                          | 128                           | 128                           |\n| *max_target_length* | 128                          | 128                          | 128                           | 128                           |\n| *adam_beta1* | 0.9                          | 0.9                          | 0.9                           | 0.9                           |\n| *adam_beta2* | 0.997                        | 0.997                        | 0.997                         | 0.997                         |\n| *weight_decay* | 0.05                         | 0.05                         | 0.002                         | 0.002                         |\n| *lr* | 5e-05                        | 5e-05                        | 0.0005                        | 0.0005                        |\n| *label_smoothing_factor* | 0.15                         | 0.15                         | 0.1                           | 0.1                           |\n| *train_batch_size* | 128                          | 128                          | 128                           | 128                           |\n| *warmup_steps* | 2000                         | 2000                         | 2000                          | 2000                          |\n| *total steps* | 390625                       | 390625                       | 390625                        | 390625                        |\n| *duration* | 4d 5h                        | 4d 5h                        | 3d 2h                         | 3d 2h                         |\n| *num parameters* | 729M                         | 729M                         | 250M                          | 250M                          |\n\n## Acknowledgements\n\nThis project would not have been possible without compute generously provided by Google through the\n[TPU Research Cloud](https://sites.research.google/trc/). The HuggingFace \ud83e\udd17 ecosystem was instrumental in all parts\nof the training. Weights & Biases made it possible to keep track of many training sessions\nand orchestrate hyper-parameter sweeps with insightful visualizations.\nThe following repositories where helpful in setting up the TPU-VM,\nand getting an idea what sensible hyper-parameters are for training gpt2 from scratch: \n\n* [Gsarti's Pretrain and Fine-tune a T5 model with Flax on GCP](https://github.com/gsarti/t5-flax-gcp)\n* [Flax/Jax Community week t5-base-dutch](https://huggingface.co/flax-community/t5-base-dutch)\n\nCreated by [Yeb Havinga](https://www.linkedin.com/in/yeb-havinga-86530825/)\n\n", "size_bytes": "1339711815", "downloads": 2}
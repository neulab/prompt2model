{"pretrained_model_name": "IIC/mbart-large-lfqa-es", "description": "---\nlanguage:\n- es\ntags:\n# - summarization  # Example: audio\n- seq2seq  # Example: automatic-speech-recognition\n- abstractive question answering\ndatasets:\n- IIC/lfqa_es\nmetrics:\n- rouge2\n- rouge1\n- rougel\n- rougelsum\n\n# Optional. Add this if you want to encode your eval results in a structured way.\nmodel-index:\n- name: mbart-large-lfqa-es\n  results:\n  - task: \n      type: question answering # Required. Example: automatic-speech-recognition\n      name: abstractive question answering  # Optional. Example: Speech Recognition\n    dataset:\n      type: IIC/lfqa_es # Required. Example: common_voice. Use dataset id from https://hf.co/datasets\n      name: IIC/lfqa_es  # Required. Example: Common Voice zh-CN\n      args: es         # Optional. Example: zh-CN\n    metrics:\n      - type: rouge1    # Required. Example: wer\n        value: 0.5107  # Required. Example: 20.90\n        name: rouge1    # Optional. Example: Test WER\n      - type: rouge2\n        value: 0.0042\n        name: rouge2\n      - type: rougeL\n        value: 0.5108\n        name: rougeL\n      - type: rougeLsum\n        value: 0.5106\n        name: rougeLsum\n---\n\nThis model is a fine-tuned version of [MBART-large](https://huggingface.co/facebook/mbart-large-cc25), a multilingual text-to-text encoder-decoder transformer. It is trained on [lfqa-spanish](https://huggingface.co/datasets/IIC/lfqa_spanish), an automatically translated dataset, originally created in English in [this repository](https://huggingface.co/datasets/vblagoje/lfqa). For more details about the dataset, check its model card. \n\nFor optimizing the model, we used [Adafactor](https://paperswithcode.com/method/adafactor) optimizer, which is better suited for t5-class models than AdamW (the typically used one). We used linear decay, and the full hyperparameters for this model were:\n\n```json\n{\n  \"learning_rate\": 2e-4,\n  \"num_train_epochs\": 4,\n  \"adam_beta1\": 0.9,\n  \"adam_beta2\": 0.999,\n  \"adam_epsilon\": 1e-8,\n  \"total_train_batch_size\": 64,\n  \"warmup_ratio\": 0.06,\n}\n```\n\nThis model is therefore trained to provide long-form answers to open domain questions given certain context paragraphs which can be used to answer that question. Therefore the main task this model can perform is abstractive question answering.\n\nThe result it obtains on the validation set of this dataset (it doesn't have a test set), with num_beams = 8 and maximum target sequence length = 360 are:\n\n```json\n{\"rouge1\": 0.5107, \"rouge2\": 0.0042, \"rougeL\": 0.5108, \"rougeLsum\": 0.5106, \"gen_len\": 201.7371}\n```\n\n### Contributions\nThanks to [@avacaondata](https://huggingface.co/avacaondata), [@alborotis](https://huggingface.co/alborotis), [@albarji](https://huggingface.co/albarji), [@Dabs](https://huggingface.co/Dabs), [@GuillemGSubies](https://huggingface.co/GuillemGSubies) for adding this model.", "size_bytes": "2444599289", "downloads": 6}
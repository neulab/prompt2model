{"pretrained_model_name": "AndyChiang/cdgp-csg-bart-cloth", "description": "---\nlicense: mit\nlanguage: en\ntags:\n- bart\n- cloze\n- distractor\n- generation\ndatasets:\n- cloth\nwidget:\n- text: \"I feel <mask> now. </s> happy\"\n- text: \"The old man was waiting for a ride across the <mask>. </s> river\"\n---\n\n# cdgp-csg-bart-cloth\n\n## Model description\n\nThis model is a Candidate Set Generator in **\"CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model\", Findings of EMNLP 2022**.\n\nIts input are stem and answer, and output is candidate set of distractors. It is fine-tuned by [**CLOTH**](https://www.cs.cmu.edu/~glai1/data/cloth/) dataset based on [**facebook/bart-base**](https://huggingface.co/facebook/bart-base) model.\n\nFor more details, you can see our **paper** or [**GitHub**](https://github.com/AndyChiangSH/CDGP).\n\n## How to use?\n\n1. Download the model by hugging face transformers.\n```python\nfrom transformers import BartTokenizer, BartForConditionalGeneration, pipeline\n\ntokenizer = BartTokenizer.from_pretrained(\"AndyChiang/cdgp-csg-bart-cloth\")\ncsg_model = BartForConditionalGeneration.from_pretrained(\"AndyChiang/cdgp-csg-bart-cloth\")\n```\n\n2. Create a unmasker.\n```python\nunmasker = pipeline(\"fill-mask\", tokenizer=tokenizer, model=csg_model, top_k=10)\n```\n\n3. Use the unmasker to generate the candidate set of distractors.\n```python\nsent = \"I feel <mask> now. </s> happy\"\ncs = unmasker(sent)\nprint(cs)\n```\n\n## Dataset\n\nThis model is fine-tuned by [CLOTH](https://www.cs.cmu.edu/~glai1/data/cloth/) dataset, which is a collection of nearly 100,000 cloze questions from middle school and high school English exams. The detail of CLOTH dataset is shown below.\n\n| Number of questions | Train | Valid | Test  |\n| ------------------- | ----- | ----- | ----- |\n| **Middle school**   | 22056 | 3273  | 3198  |\n| **High school**     | 54794 | 7794  | 8318  |\n| **Total**           | 76850 | 11067 | 11516 |\n\nYou can also use the [dataset](https://huggingface.co/datasets/AndyChiang/cloth) we have already cleaned.\n\n## Training\n\nWe use a special way to fine-tune model, which is called **\"Answer-Relating Fine-Tune\"**. More detail is in our paper.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- Pre-train language model: [facebook/bart-base](https://huggingface.co/facebook/bart-base)\n- Optimizer: adam\n- Learning rate: 0.0001\n- Max length of input: 64\n- Batch size: 64\n- Epoch: 1\n- Device: NVIDIA\u00ae Tesla T4 in Google Colab \n\n## Testing\n\nThe evaluations of this model as a Candidate Set Generator in CDGP is as follows:\n\n| P@1   | F1@3  | F1@10 | MRR   | NDCG@10 |\n| ----- | ----- | ----- | ----- | ------- |\n| 14.20 | 11.07 | 11.37 | 24.29 | 31.74   |\n\n## Other models\n\n### Candidate Set Generator\n\n| Models      | CLOTH                                                                               | DGen                                                                             |\n| ----------- | ----------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |\n| **BERT**    | [cdgp-csg-bert-cloth](https://huggingface.co/AndyChiang/cdgp-csg-bert-cloth)        | [cdgp-csg-bert-dgen](https://huggingface.co/AndyChiang/cdgp-csg-bert-dgen)       |\n| **SciBERT** | [cdgp-csg-scibert-cloth](https://huggingface.co/AndyChiang/cdgp-csg-scibert-cloth)  | [cdgp-csg-scibert-dgen](https://huggingface.co/AndyChiang/cdgp-csg-scibert-dgen) |\n| **RoBERTa** | [cdgp-csg-roberta-cloth](https://huggingface.co/AndyChiang/cdgp-csg-roberta-cloth) | [cdgp-csg-roberta-dgen](https://huggingface.co/AndyChiang/cdgp-csg-roberta-dgen) |\n| **BART**    | [*cdgp-csg-bart-cloth*](https://huggingface.co/AndyChiang/cdgp-csg-bart-cloth)        | [cdgp-csg-bart-dgen](https://huggingface.co/AndyChiang/cdgp-csg-bart-dgen)       |\n\n### Distractor Selector\n\n**fastText**: [cdgp-ds-fasttext](https://huggingface.co/AndyChiang/cdgp-ds-fasttext)\n\n\n## Citation\n\nNone", "size_bytes": "557979897", "downloads": 4}
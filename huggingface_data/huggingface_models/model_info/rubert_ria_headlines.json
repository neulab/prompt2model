{"pretrained_model_name": "dmitry-vorobiev/rubert_ria_headlines", "description": "---\nlanguage:\n- ru\ntags:\n- summarization\n- bert\n- rubert\nlicense: mit\n---\n\n# rubert_ria_headlines\n\n## Description\n*bert2bert* model, initialized with the `DeepPavlov/rubert-base-cased` pretrained weights and \n   fine-tuned on the first 99% of [\"Rossiya Segodnya\" news dataset](https://github.com/RossiyaSegodnya/ria_news_dataset) for 2 epochs.\n\n## Usage example\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nMODEL_NAME = \"dmitry-vorobiev/rubert_ria_headlines\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\ntext = \"\u0421\u043a\u043e\u043f\u0438\u0440\u0443\u0439\u0442\u0435 \u0442\u0435\u043a\u0441\u0442 \u0441\u0442\u0430\u0442\u044c\u0438 / \u043d\u043e\u0432\u043e\u0441\u0442\u0438\"\n\nencoded_batch = tokenizer.prepare_seq2seq_batch(\n    [text],\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    truncation=True,\n    max_length=512)\n\noutput_ids = model.generate(\n    input_ids=encoded_batch[\"input_ids\"],\n    max_length=36,\n    no_repeat_ngram_size=3,\n    num_beams=5,\n    top_k=0\n)\n\nheadline = tokenizer.decode(output_ids[0], \n                            skip_special_tokens=True, \n                            clean_up_tokenization_spaces=False)\nprint(headline)\n```\n   \n## Datasets\n- [ria_news](https://github.com/RossiyaSegodnya/ria_news_dataset)\n\n## How it was trained?\n\nI used free TPUv3 on kaggle. The model was trained for 3 epochs with effective batch size 192 and soft restarts (warmup steps 1500 / 500 / 500 with new optimizer state on each epoch start).\n\n- [1 epoch notebook](https://www.kaggle.com/dvorobiev/try-train-seq2seq-ria-tpu?scriptVersionId=53254694)\n- [2 epoch notebook](https://www.kaggle.com/dvorobiev/try-train-seq2seq-ria-tpu?scriptVersionId=53269040)\n- [3 epoch notebook](https://www.kaggle.com/dvorobiev/try-train-seq2seq-ria-tpu?scriptVersionId=53280797)\n\nCommon train params:\n\n```shell\nexport XLA_USE_BF16=1\nexport XLA_TENSOR_ALLOCATOR_MAXSIZE=100000000\n\npython nlp_headline_rus/src/train_seq2seq.py \\\n    --do_train \\\n    --tie_encoder_decoder \\\n    --max_source_length 512 \\\n    --max_target_length 32 \\\n    --val_max_target_length 48 \\\n    --tpu_num_cores 8 \\\n    --per_device_train_batch_size 24 \\\n    --gradient_accumulation_steps 1 \\\n    --learning_rate 5e-4 \\\n    --adam_epsilon 1e-6 \\\n    --weight_decay 1e-5 \\\n```\n\n## Validation results\n\n- Using [last 1% of ria](https://drive.google.com/drive/folders/1ztAeyb1BiLMgXwOgOJS7WMR4PGiI1q92) dataset\n- Using [gazeta_ru test](https://drive.google.com/drive/folders/1CyowuRpecsLTcDbqEfmAvkCWOod58g_e) split\n- Using [gazeta_ru val](https://drive.google.com/drive/folders/1XZFOXHSXLKdhzm61ceVLw3aautrdskIu) split", "size_bytes": "827914439", "downloads": 30}
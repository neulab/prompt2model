{"pretrained_model_name": "Ahmad/parsT5-base", "description": "A monolingual T5 model for Persian trained on OSCAR 21.09 (https://oscar-corpus.com/) corpus with self-supervised method. 35 Gig deduplicated version of Persian data was used for pre-training the model.\n\nIt's similar to the English T5 model but just for Persian. You may need to fine-tune it on your specific task. \n\nExample code:\n\n```\nfrom transformers import T5ForConditionalGeneration,AutoTokenizer\n\nimport torch\n\nmodel_name = \"Ahmad/parsT5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ninput_ids = tokenizer.encode('\u062f\u0627\u0646\u0634 \u0622\u0645\u0648\u0632\u0627\u0646 \u0628\u0647 <extra_id_0> \u0645\u06cc\u0631\u0648\u0646\u062f \u0648 <extra_id_1> \u0645\u06cc\u062e\u0648\u0627\u0646\u0646\u062f.', return_tensors='pt')\nwith torch.no_grad():\n    hypotheses = model.generate(input_ids)\nfor h in hypotheses:\n    print(tokenizer.decode(h))\n```\n\n\n\n\nSteps: 725000\n\nAccuracy: 0.66\n\nTraining More?\n========\n\nTo train the model further please refer to its github repository at:\n\nhttps://github.com/puraminy/parsT5\n", "size_bytes": "990280781", "downloads": 25}
{"pretrained_model_name": "rmihaylov/roberta2roberta-shared-nmt-bg", "description": "---\ninference: false\nlanguage:\n- bg\nlicense: mit\ndatasets:\n- oscar\n- chitanka\n- wikipedia\ntags:\n- torch\n---\n\n# ROBERTA-TO-ROBERTA EncoderDecoder with Shared Weights\n\nThis model was introduced in [this paper](https://arxiv.org/pdf/1907.12461.pdf). \n\n## Model description\n\nThe training data is private English-Bulgarian parallel data. \n\n## Intended uses & limitations\n\nYou can use the raw model for translation from English to Bulgarian.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\n>>> from transformers import EncoderDecoderModel, XLMRobertaTokenizer\n>>>\n>>> model_id = \"rmihaylov/roberta2roberta-shared-nmt-bg\"\n>>> model = EncoderDecoderModel.from_pretrained(model_id)\n>>> model.encoder.pooler = None\n>>> tokenizer = XLMRobertaTokenizer.from_pretrained(model_id)\n>>>\n>>> text = \"\"\"\nOthers were photographed ransacking the building, smiling while posing with congressional items such as House Speaker Nancy Pelosi's lectern or at her staffer's desk, or publicly bragged about the crowd's violent and destructive joyride.\n\"\"\"\n>>>\n>>> inputs = tokenizer.encode_plus(text, max_length=100, return_tensors='pt', truncation=True)\n>>> \n>>> translation = model.generate(**inputs, \n>>>     max_length=100, \n>>>     num_beams=4, \n>>>     do_sample=True,\n>>>     num_return_sequences=1, \n>>>     top_p=0.95,\n>>>     decoder_start_token_id=tokenizer.bos_token_id)\n>>>\n>>> print([tokenizer.decode(g.tolist(), skip_special_tokens=True) for g in translation])\n\n['\u0414\u0440\u0443\u0433\u0438 \u0431\u044f\u0445\u0430 \u0437\u0430\u0441\u043d\u0435\u0442\u0438 \u0434\u0430 \u0431\u044f\u0433\u0430\u0442 \u0438\u0437 \u0441\u0433\u0440\u0430\u0434\u0430\u0442\u0430, \u0443\u0441\u043c\u0438\u0445\u0432\u0430\u0439\u043a\u0438 \u0441\u0435, \u0434\u043e\u043a\u0430\u0442\u043e \u0441\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u044f\u0442 \u0441 \u043a\u043e\u043d\u0433\u0440\u0435\u0441\u043d\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u0438, \u043a\u0430\u0442\u043e \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u043b\u0435\u043a\u0446\u0438\u044f\u0442\u0430 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u0435\u0434\u0430\u0442\u0435\u043b\u044f \u043d\u0430 \u043f\u0430\u0440\u043b\u0430\u043c\u0435\u043d\u0442\u0430 \u041d\u0430\u043d\u0441\u0438 \u041f\u0435\u043b\u043e\u0437\u0438 \u0438\u043b\u0438 \u043d\u0430 \u0431\u044e\u0440\u043e\u0442\u043e \u043d\u0430 \u043d\u0435\u0439\u043d\u0438\u044f \u0441\u043b\u0443\u0436\u0438\u0442\u0435\u043b, \u0438\u043b\u0438 \u043f\u0443\u0431\u043b\u0438\u0447\u043d\u043e \u0441\u0435 \u0445\u0432\u0430\u043b\u044f\u0442 \u0437\u0430 \u043d\u0430\u0441\u0438\u043b\u0438\u0435\u0442\u043e \u0438 \u0440\u0430\u0437\u0440\u0443\u0448\u0438\u0442\u0435\u043b\u043d\u0430\u0442\u0430 \u0440\u0430\u0434\u043e\u0441\u0442 \u043d\u0430 \u0442\u044a\u043b\u043f\u0430\u0442\u0430.']\n```\n", "size_bytes": "1226789625", "downloads": 2}
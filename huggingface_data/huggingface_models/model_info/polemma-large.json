{"pretrained_model_name": "amu-cai/polemma-large", "description": "---\nlanguage: pl\ntags:\n  - T5\n  - lemmatization\nlicense: apache-2.0\n---\n\n\n# PoLemma Large\n\nPoLemma models are intended for lemmatization of named entities and multi-word expressions in the Polish language.\n\nThey were fine-tuned from the allegro/plT5 models, e.g.: [allegro/plt5-large](https://huggingface.co/allegro/plt5-large).\n\n## Usage\n\nSample usage:\n\n```\nfrom transformers import pipeline\n\npipe = pipeline(task=\"text2text-generation\", model=\"amu-cai/polemma-large\", tokenizer=\"amu-cai/polemma-large\")\nhyp = [res['generated_text'] for res in pipe([\"federalnego urz\u0119du statystycznego\"], clean_up_tokenization_spaces=True, num_beams=5)][0]\n```\n\n\n## Evaluation results\n\nLemmatization Exact Match was computed on the SlavNER 2021 test set.\n\n| Model | Exact Match ||\n| :------ | ------: | ------: |\n| [polemma-large](https://huggingface.co/amu-cai/polemma-large) | 92.61  | \n| [polemma-base](https://huggingface.co/amu-cai/polemma-base) | 91.34  |\n| [polemma-small](https://huggingface.co/amu-cai/polemma-small)| 88.46 |\n\n\n## Citation\n\nIf you use the model, please cite the following paper:\n\n```\n@inproceedings{palka-nowakowski-2023-exploring,\n    title = \"Exploring the Use of Foundation Models for Named Entity Recognition and Lemmatization Tasks in {S}lavic Languages\",\n    author = \"Pa{\\l}ka, Gabriela  and\n      Nowakowski, Artur\",\n    booktitle = \"Proceedings of the 9th Workshop on Slavic Natural Language Processing 2023 (SlavicNLP 2023)\",\n    month = may,\n    year = \"2023\",\n    address = \"Dubrovnik, Croatia\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bsnlp-1.19\",\n    pages = \"165--171\",\n    abstract = \"This paper describes Adam Mickiewicz University{'}s (AMU) solution for the 4th Shared Task on SlavNER. The task involves the identification, categorization, and lemmatization of named entities in Slavic languages. Our approach involved exploring the use of foundation models for these tasks. In particular, we used models based on the popular BERT and T5 model architectures. Additionally, we used external datasets to further improve the quality of our models. Our solution obtained promising results, achieving high metrics scores in both tasks. We describe our approach and the results of our experiments in detail, showing that the method is effective for NER and lemmatization in Slavic languages. Additionally, our models for lemmatization will be available at: https://huggingface.co/amu-cai.\",\n}\n```\n\n### Framework versions\n\n- Transformers 4.26.0\n- Pytorch 1.13.1.post200\n- Datasets 2.9.0\n- Tokenizers 0.13.2\n", "size_bytes": "3279594309", "downloads": 10}
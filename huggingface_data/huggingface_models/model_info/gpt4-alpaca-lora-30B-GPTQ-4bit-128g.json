{"pretrained_model_name": "TheBloke/gpt4-alpaca-lora-30B-GPTQ-4bit-128g", "description": "---\nlicense: other\nlanguage:\n- en\npipeline_tag: text2text-generation\ntags:\n- alpaca\n- llama\n- chat\n- gpt4\ninference: false\n---\n<!-- header start -->\n<div style=\"width: 100%;\">\n    <img src=\"https://i.imgur.com/EBdldam.jpg\" alt=\"TheBlokeAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n</div>\n<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n    <div style=\"display: flex; flex-direction: column; align-items: flex-start;\">\n        <p><a href=\"https://discord.gg/Jq4vkcDakD\">Chat & support: my new Discord server</a></p>\n    </div>\n    <div style=\"display: flex; flex-direction: column; align-items: flex-end;\">\n        <p><a href=\"https://www.patreon.com/TheBlokeAI\">Want to contribute? TheBloke's Patreon page</a></p>\n    </div>\n</div>\n<!-- header end -->\n# GPT4 Alpaca LoRA 30B - GPTQ 4bit 128g\n\nThis is a 4-bit GPTQ version of the [Chansung GPT4 Alpaca 30B LoRA model](https://huggingface.co/chansung/gpt4-alpaca-lora-30b).\n\nIt was created by merging the LoRA provided in the above repo with the original Llama 30B model, producing unquantised model [GPT4-Alpaca-LoRA-30B-HF](https://huggingface.co/TheBloke/gpt4-alpaca-lora-30b-HF)\n\nIt was then quantized to 4bit, groupsize 128g, using [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa).\n\nVRAM usage will depend on the tokens returned. Below approximately 1000 tokens returned it will use <24GB VRAM, but at 1000+ tokens it will exceed the VRAM of a 24GB card.\n\nRAM and VRAM usage at the end of a 670 token response in `text-generation-webui` : **5.2GB RAM, 20.7GB VRAM**\n![Screenshot of RAM and VRAM Usage](https://i.imgur.com/Sl8SmBH.png)\nRAM and VRAM usage after about 1500 tokens: **5.2GB RAM, 30.0GB VRAM**\n![screenshot](https://i.imgur.com/PBNtvwf.png)\n\nIf you want a model that should always stay under 24GB, use this one, provided by MetalX, instead:\n[GPT4 Alpaca Lora 30B GPTQ 4bit without groupsize](https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-Int4)\n\n## Provided files\n\nCurrently one model file is provided, a `safetensors`. This file requires the latest GPTQ-for-LLaMa code to run inside [oobaboogas text-generation-webui](https://github.com/oobabooga/text-generation-webui).\n\nTomorrow I will try to add another file that does not use `--act-order` and therefore can be run in text-generation-webui without needing to update GPTQ-for-LLaMa (at the cost of possibly having slightly lower inference quality.)\n\nDetails of the files provided:\n* `gpt4-alpaca-lora-30B-GPTQ-4bit-128g.safetensors`\n  * `safetensors` format, with improved file security, created with the latest [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa) code.\n  * Command to create:\n    * `python3 llama.py gpt4-alpaca-lora-30B-HF c4 --wbits 4 --true-sequential --act-order --groupsize 128 --save_safetensors gpt4-alpaca-lora-30B-GPTQ-4bit-128g.safetensors`\n\n## How to run in `text-generation-webui`\n\nThe `safetensors` model file was created with the GPTQ-for-LLaMa code as of April 13th, and uses `--act-order` to give the maximum possible quantisation quality. This means it requires that this same version of GPTQ-for-LLaMa is used inside the UI.\n\nHere are the commands I used to clone the Triton branch of GPTQ-for-LLaMa, clone text-generation-webui, and install GPTQ into the UI:\n```\n# Since April 14th we can't clone the latest GPTQ-for-LLaMa as it's in the middle of a refactoring\ngit clone -n https://github.com/qwopqwop200/GPTQ-for-LLaMa gptq-working\ncd gptq-working && git checkout 58c8ab4c7aaccc50f507fd08cce941976affe5e0 # Later commits are currently broken due to ongoing refactoring\n\ngit clone https://github.com/oobabooga/text-generation-webui\nmkdir -p text-generation-webui/repositories\nln -s gptq-working text-generation-webui/repositories/GPTQ-for-LLaMa\n```\n\nThen install this model into `text-generation-webui/models` and launch the UI as follows:\n```\ncd text-generation-webui\npython server.py --model gpt4-alpaca-lora-30B-GPTQ-4bit-128g --wbits 4 --groupsize 128 --model_type Llama # add any other command line args you want\n```\n\nThe above commands assume you have installed all dependencies for `GPTQ-for-LLaMa` and `text-generation-webui`. Please see their respective repositories for further information.\n\nIf you are on Windows, or cannot use the Triton branch of GPTQ for any other reason, you can instead try the CUDA branch:\n```\npip uninstall -y quant_cuda\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda\ncd GPTQ-for-LLaMa\npython setup_cuda.py install --force\n```\nThen link that into `text-generation-webui/repositories` as described above.\n\n<!-- footer start -->\n## Discord\n\nFor further support, and discussions on these models and AI in general, join us at:\n\n[TheBloke AI's Discord server](https://discord.gg/Jq4vkcDakD)\n\n## Thanks, and how to contribute.\n\nThanks to the [chirper.ai](https://chirper.ai) team!\n\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\n\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\n\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\n\n* Patreon: https://patreon.com/TheBlokeAI\n* Ko-Fi: https://ko-fi.com/TheBlokeAI\n\n**Patreon special mentions**: Aemon Algiz, Dmitriy Samsonov, Nathan LeClaire, Trenton Dambrowitz, Mano Prime, David Flickinger, vamX, Nikolai Manek, senxiiz, Khalefa Al-Ahmad, Illia Dulskyi, Jonathan Leane, Talal Aujan, V. Lukas, Joseph William Delisle, Pyrater, Oscar Rangel, Lone Striker, Luke Pendergrass, Eugene Pentland, Sebastain Graf, Johann-Peter Hartman.\n\nThank you to all my generous patrons and donaters!\n<!-- footer end -->\n\n# Original GPT4 Alpaca Lora model card\n\nThis repository comes with LoRA checkpoint to make LLaMA into a chatbot like language model. The checkpoint is the output of instruction following fine-tuning process with the following settings on 8xA100(40G) DGX system.\n- Training script: borrowed from the official [Alpaca-LoRA](https://github.com/tloen/alpaca-lora) implementation\n- Training script:\n```shell\npython finetune.py \\\n    --base_model='decapoda-research/llama-30b-hf' \\\n    --data_path='alpaca_data_gpt4.json' \\\n    --num_epochs=10 \\\n    --cutoff_len=512 \\\n    --group_by_length \\\n    --output_dir='./gpt4-alpaca-lora-30b' \\\n    --lora_target_modules='[q_proj,k_proj,v_proj,o_proj]' \\\n    --lora_r=16 \\\n    --batch_size=... \\\n    --micro_batch_size=...\n```\n\nYou can find how the training went from W&B report [here](https://wandb.ai/chansung18/gpt4_alpaca_lora/runs/w3syd157?workspace=user-chansung18).\n", "size_bytes": 65057902592, "downloads": 15}
{"pretrained_model_name": "it5/mt5-small-informal-to-formal", "description": "---\nlanguage:\n- it\nlicense: apache-2.0\ntags:\n- italian\n- sequence-to-sequence\n- style-transfer\n- formality-style-transfer\ndatasets:\n- yahoo/xformal_it\nwidget:\n- text: \"maronn qualcuno mi spieg' CHECCOSA SUCCEDE?!?!\"\n- text: \"wellaaaaaaa, ma frat\u00e9 sei proprio troppo simpatiko, grazieeee!!\"\n- text: \"nn capisco xke tt i ragazzi lo fanno\"\n- text: \"IT5 \u00e8 SUPERMEGA BRAVISSIMO a capire tt il vernacolo italiano!!!\"\nmetrics:\n- rouge\n- bertscore\nmodel-index:\n- name: mt5-small-informal-to-formal\n  results:\n  - task: \n      type: formality-style-transfer\n      name: \"Informal-to-formal Style Transfer\"\n    dataset:\n      type: xformal_it\n      name: \"XFORMAL (Italian Subset)\"\n    metrics:\n      - type: rouge1\n        value: 0.638\n        name: \"Avg. Test Rouge1\"\n      - type: rouge2\n        value: 0.446\n        name: \"Avg. Test Rouge2\"\n      - type: rougeL\n        value: 0.620\n        name: \"Avg. Test RougeL\"\n      - type: bertscore\n        value: 0.684\n        name: \"Avg. Test BERTScore\"\n        args:\n          - model_type: \"dbmdz/bert-base-italian-xxl-uncased\"\n          - lang: \"it\"\n          - num_layers: 10\n          - rescale_with_baseline: True\n          - baseline_path: \"bertscore_baseline_ita.tsv\"\nco2_eq_emissions:\n      emissions: \"17g\"\n      source: \"Google Cloud Platform Carbon Footprint\"\n      training_type: \"fine-tuning\"\n      geographical_location: \"Eemshaven, Netherlands, Europe\"\n      hardware_used: \"1 TPU v3-8 VM\"\n---\n\n# mT5 Small for Informal-to-formal Style Transfer \ud83e\uddd0\n\nThis repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on Informal-to-formal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). \n\nA comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.\n\n## Using the model\n\nModel checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:\n\n```python\nfrom transformers import pipelines\n\ni2f = pipeline(\"text2text-generation\", model='it5/mt5-small-informal-to-formal')\ni2f(\"nn capisco xke tt i ragazzi lo fanno\")\n>>> [{\"generated_text\": \"non comprendo perch\u00e9 tutti i ragazzi agiscono cos\u00ec\"}]\n```\n\nor loaded using autoclasses:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"it5/mt5-small-informal-to-formal\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"it5/mt5-small-informal-to-formal\")\n```\n\nIf you use this model in your research, please cite our work as:\n\n```bibtex\n@article{sarti-nissim-2022-it5,\n    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},\n    author={Sarti, Gabriele and Nissim, Malvina},\n    journal={ArXiv preprint 2203.03759},\n    url={https://arxiv.org/abs/2203.03759},\n    year={2022},\n\tmonth={mar}\n}\n```", "size_bytes": "1200789509", "downloads": 10}
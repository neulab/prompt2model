{"pretrained_model_name": "jalbarracin/spanish-alpaca-mT5", "description": "---\nlicense: apache-2.0\nlanguage: es\ndatasets:\n- bertin-project/alpaca-spanish\npipeline_tag: text2text-generation\ntags:\n- salpaca\n- mT5\n- spanish\nwidget:\n- text: \"instrut5: Explica la siguiente frase: YOLO. <in></in>\"\n  example_title: \"Explicar frase\"\n- text: \"instrut5: Generar una lista de preguntas interesantes sobre el tema del cambio clim\u00e1tico.<in></in>\"\n  example_title: \"Generar preguntas\"\n- text: \"instrut5: Escribe un correo electr\u00f3nico dando la bienvenida a un nuevo empleado.<in>Juan</in>\"\n  example_title: \"Escribir email\"\ninference:\n  parameters:\n    do_sample: True\n    top_p: 0.95\n    top_k: 50\n---\n\n\n# Spanish Alpaca mT5\n\nEste repositorio contiene un modelo mT5 entrenado con el dataset [Bertin Project - Alpaca Spanish](https://huggingface.co/datasets/bertin-project/alpaca-spanish) que adem\u00e1s ha sido limpiado y editado por el autor.\nPuede ponerse en contacto con el autor a trav\u00e9s de su cuenta de twitter: @jalbarracin\n\n# Uso\n\n## Usando el modelo\n\nEl modelo mT5 en espa\u00f1ol puede usarse en python siguiendo este ejemplo:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel = 'jalbarracin/spanish-alpaca-mT5'\nmodel_tokenizer = 'jalbarracin/spanish-alpaca-mT5'\ntokenizer = AutoTokenizer.from_pretrained(model_tokenizer)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model).to('cpu') #puedes cambiar a 'cuda' si tienes GPU\n\ninput_text =\"\"\"instrut5: QuanticoTrends (https://www.quanticotrends.com/) se dedica al monitoreo de redes sociales para empresas. Escribe un anuncio para anunciar los servicios de QuanticoTrends en facebook<in></in>\"\"\"\n\ninputs = tokenizer(input_text, return_tensors=\"pt\").to('cpu') #puedes cambiar a 'cuda' si tienes GPU\n\noutputs = model.generate(inputs[\"input_ids\"],\n                         do_sample = True,\n                         max_length = 256, #puedes subir este parametro hasta 500\n                         num_return_sequences=3, #recomiendo hasta 6 para que no demore mucho\n                         top_k=50,\n                         top_p=0.90,\n                        )\ndetok_outputs = [tokenizer.decode(x, skip_special_tokens=True) for x in outputs]\n\nfor output in detok_outputs: \n    print(output)\n    print(\"\\n\") # imprime un salto de linea para separar cada uno de los outputs (en el caso que num_return_sequences sea mayor que 1)\n```\n\nEl output de este ejemplo:\n\n```python\n\u00a1\u00bfEcha un vistazo a nosotros y descubre los servicios de QuanticoTrends en Facebook! Observa el rendimiento de tus sitios web, recopila informaci\u00f3n de empresas, construye relaciones con otros usuarios e incluso realiza un seguimiento de tus campa\u00f1as. \u00a1Disfruta de las herramientas que necesitas para hacer la transici\u00f3n a la realidad virtual! #QuanticoTrends #Marketing #Sostenibilidad\n\n\u00a1Bienvenido a QuanticoTrends! Estamos dedicados al monitoreo de redes sociales para empresas y nuestro servicio al cliente. Nuestro monitoreo de redes sociales est\u00e1 dise\u00f1ado para brindar a los usuarios la mejor experiencia, consejos y consejos para tomar decisiones inteligentes y productivas para nuestras empresas. \u00a1\u00danete a nosotros hoy para estar al d\u00eda con las tendencias de QuanticoTrends!\n\n\u00a1Aprovecha nuestra especialidad de monitoreo de redes sociales para empresas! Reg\u00edstrate ahora y crea una nueva selecci\u00f3n de servicios y recursos que se adaptan a tus intereses y necesidades. # QuanticoTrends #empresas # MonitoreoDe redes sociales #MantenteSeguro #Visibilidad\n```\n# Colab Notebook de ejemplo\n\nPuedes acceder al notebook para que pruebes el modelo aqui: [Modelo Spanish Alpaca mT5] (https://colab.research.google.com/drive/1yWwMH0Opk1C10emYTfYhDWEPVNE7insw)\n\n\nEste modelo ha sido entrenado con la base de datos es una versi\u00f3n m\u00e1s peque\u00f1a del modelo google/mt5-base con embeddings solo en espa\u00f1ol y algunas en ingl\u00e9s obtenidas de https://huggingface.co/JorgeSarry/\nEl n\u00famero de par\u00e1metros del modelo es de 244 millones de par\u00e1metros, lo que da como resultado un modelo de 0,9 GB, el 42 % del original.\n\nPuedes usar este modelo utilizando el tag \"instrut5:\" \n\nLa gran ventaja es que produce buenos resultados, muchas veces en menos de 1 segundo en CPU.\n\n=====\n\nThis is a smaller version of the google/mt5-base model with only Spanish and some English embeddings obtained from https://huggingface.co/JorgeSarry/\nThe number of model parameters is 244M parameters, resulting on a model of 0.9GB - 42% of the original one.\n\nYou can use this model starging with the tag \"instrut5:\"\n\nThe best advantage is that this model produces good results, some times in less than 1 second on CPU", "size_bytes": "977363917", "downloads": 53}
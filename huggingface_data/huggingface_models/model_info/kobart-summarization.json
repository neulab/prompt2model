{"pretrained_model_name": "gogamza/kobart-summarization", "description": "---\nlanguage: ko\ntags:\n- bart\nlicense: mit\n---\n\n# Korean News Summarization Model\n\n## Demo\n\nhttps://huggingface.co/spaces/gogamza/kobart-summarization\n\n## How to use\n\n```python\nimport torch\nfrom transformers import PreTrainedTokenizerFast\nfrom transformers import BartForConditionalGeneration\n\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\nmodel = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n\ntext = \"\uacfc\uac70\ub97c \ub5a0\uc62c\ub824\ubcf4\uc790. \ubc29\uc1a1\uc744 \ubcf4\ub358 \uc6b0\ub9ac\uc758 \ubaa8\uc2b5\uc744. \ub3c5\ubcf4\uc801\uc778 \ub9e4\uccb4\ub294 TV\uc600\ub2e4. \uc628 \uac00\uc871\uc774 \ub458\ub7ec\uc549\uc544 TV\ub97c \ubd24\ub2e4. \uac04\ud639 \uac00\uc871\ub4e4\ub07c\ub9ac \ub274\uc2a4\uc640 \ub4dc\ub77c\ub9c8, \uc608\ub2a5 \ud504\ub85c\uadf8\ub7a8\uc744 \ub458\ub7ec\uc2f8\uace0 \ub9ac\ubaa8\ucee8 \uc7c1\ud0c8\uc804\uc774 \ubc8c\uc5b4\uc9c0\uae30\ub3c4  \ud588\ub2e4. \uac01\uc790 \uc120\ud638\ud558\ub294 \ud504\ub85c\uadf8\ub7a8\uc744 \u2018\ubcf8\ubc29\u2019\uc73c\ub85c \ubcf4\uae30 \uc704\ud55c \uc2f8\uc6c0\uc774\uc5c8\ub2e4. TV\uac00 \ud55c \ub300\uc778\uc9c0 \ub450 \ub300\uc778\uc9c0 \uc5ec\ubd80\ub3c4 \uadf8\ub798\uc11c \uc911\uc694\ud588\ub2e4. \uc9c0\uae08\uc740 \uc5b4\ub5a4\uac00. \u2018\uc548\ubc29\uadf9\uc7a5\u2019\uc774\ub77c\ub294 \ub9d0\uc740 \uc61b\ub9d0\uc774 \ub410\ub2e4. TV\uac00 \uc5c6\ub294 \uc9d1\ub3c4 \ub9ce\ub2e4. \ubbf8\ub514\uc5b4\uc758 \ud61c \ud0dd\uc744 \ub204\ub9b4 \uc218 \uc788\ub294 \ubc29\ubc95\uc740 \ub298\uc5b4\ub0ac\ub2e4. \uac01\uc790\uc758 \ubc29\uc5d0\uc11c \uac01\uc790\uc758 \ud734\ub300\ud3f0\uc73c\ub85c, \ub178\ud2b8\ubd81\uc73c\ub85c, \ud0dc\ube14\ub9bf\uc73c\ub85c \ucf58\ud150\uce20 \ub97c \uc990\uae34\ub2e4.\"\n\nraw_input_ids = tokenizer.encode(text)\ninput_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n\nsummary_ids = model.generate(torch.tensor([input_ids]))\ntokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n```\n\n\n\n\n\n", "size_bytes": "495659091", "downloads": 9986}
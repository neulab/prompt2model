{"pretrained_model_name": "com3dian/Bart-large-paper2slides-expander", "description": "---\nlanguage:\n- en\nwidget:\n- text: >\n    Bag-of-feature representations can be described by analogy to bag-of-words\n    representations.\n- text: >\n    Self-attention is an attention mechanism relating different positions of a\n    single sequence in order to compute a representation of the sequence.\nlicense:\n- mit\npipeline_tag: text2text-generation\ndatasets:\n- cnn_dailymail\n---\n# Bart-Large Expansion Model\n\n![Bart Logo](https://huggingface.co/front/assets/huggingface_logo.svg)\n\nThis repository contains the **Bart-Large-paper2slides-expander Model**, which has been pre-trained on cnn-daily-mail dataset and fine-tuned on the [Automatic Slide Generation from Scientific Papers dataset](https://www.kaggle.com/datasets/andrewmvd/automatic-slide-generation-from-scientific-papers) using unsupervised learning techniques using an algorithm from the paper entitled '[Unsupervised Machine Translation Using Monolingual Corpora Only](https://arxiv.org/abs/1711.00043)'.\nIts primary focus is to expand the **scientific text** by providing alternative and expanded versions with improved clarity and accuracy. The model is parallelly trained with the [**Bart-Large-paper2slides-summarizer Model**](https://huggingface.co/com3dian/Bart-large-paper2slides-summarizer) from the same contributor.\n\n## Model Details\n\n- **Model Architecture**: Bart-Large\n- **Fine-tuning Dataset**: [Automatic Slide Generation from Scientific Papers](https://www.kaggle.com/datasets/andrewmvd/automatic-slide-generation-from-scientific-papers)\n- **Fine-tuning Method**: Unsupervised Learning\n\n[Bart](https://huggingface.co/transformers/model_doc/bart.html) (Bidirectional and Auto-Regressive Transformers) is a sequence-to-sequence (seq2seq) model developed by Facebook AI Research. It has shown exceptional performance in various natural language processing (NLP) tasks such as text summarization, text generation, and machine translation.\n\nThis particular model, Bart-Large, is the larger version of the Bart model. It consists of 12 encoder and decoder layers and has a total of 400 million parameters.\n\n## Usage\n\nTo use this model, you can leverage the Hugging Face [Transformers](https://huggingface.co/transformers/) library. Here's an example of how to use it in Python:\n\n```python\nfrom transformers import BartTokenizer, BartForConditionalGeneration, pipeline\n\n# Load the model and tokenizer\nmodel_name = \"com3dian/Bart-large-paper2slides-expander\"\ntokenizer = BartTokenizer.from_pretrained(model_name)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\n# Generate summary from input text\ninput_text = \"Your input text here...\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutput = model.generate(input_ids)\n\n# Decode generated summaries\nexpanded_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(expanded_text)\n\n# Or using the pipeline API\nexpander = pipeline(\"text2text-generation\", model=model_name)\nexpanded_text = expander(input_text, max_length=50, min_length=30, do_sample=False)\nprint(expanded_text)\n```\n\nEnsure you have the `transformers` library installed before running the code. You can install it using `pip`:\n\n```\npip install transformers\n```\n\n## Model Fine-tuning Details\n\nThe fine-tuning process for this model involved training on the slide generation dataset using unsupervised learning techniques. Unsupervised learning refers to training a model without explicit human-labeled targets. Instead, the model learns to back-expand the input provided by the summarization model, into the original texts.\n\nThe specific hyperparameters and training details used for fine-tuning this model are as follows:\n\n- Batch Size: 4\n- Learning Rate: 2e-6\n- Training Steps: 3*7\n- Optimizer: AdamW\n\n## Acknowledgments\n\nWe would like to acknowledge the authors of the Bart model and the creators of the slide generation dataset for their valuable contributions, which have enabled the development of this fine-tuned model.\n\nIf you use this model or find it helpful in your work, please consider citing the original Bart model and the slide generation dataset to provide proper credit to the respective authors.\n\n## License\n\nThis model and the associated code are released under the [MIT license](https://opensource.org/license/mit/).", "size_bytes": "1625537293", "downloads": 361}
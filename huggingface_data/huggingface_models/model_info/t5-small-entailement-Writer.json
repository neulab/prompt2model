{"pretrained_model_name": "Sandipan1994/t5-small-entailement-Writer", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: t5-small-entailement-Writer\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-entailement-Writer\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5958\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 150\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| No log        | 1.0   | 42   | 1.8511          |\n| No log        | 2.0   | 84   | 1.2249          |\n| No log        | 3.0   | 126  | 0.9976          |\n| No log        | 4.0   | 168  | 0.9108          |\n| No log        | 5.0   | 210  | 0.8478          |\n| No log        | 6.0   | 252  | 0.8186          |\n| No log        | 7.0   | 294  | 0.7965          |\n| No log        | 8.0   | 336  | 0.7815          |\n| No log        | 9.0   | 378  | 0.7634          |\n| No log        | 10.0  | 420  | 0.7544          |\n| No log        | 11.0  | 462  | 0.7408          |\n| 1.2198        | 12.0  | 504  | 0.7298          |\n| 1.2198        | 13.0  | 546  | 0.7240          |\n| 1.2198        | 14.0  | 588  | 0.7139          |\n| 1.2198        | 15.0  | 630  | 0.7070          |\n| 1.2198        | 16.0  | 672  | 0.7028          |\n| 1.2198        | 17.0  | 714  | 0.6977          |\n| 1.2198        | 18.0  | 756  | 0.6926          |\n| 1.2198        | 19.0  | 798  | 0.6906          |\n| 1.2198        | 20.0  | 840  | 0.6846          |\n| 1.2198        | 21.0  | 882  | 0.6822          |\n| 1.2198        | 22.0  | 924  | 0.6760          |\n| 1.2198        | 23.0  | 966  | 0.6710          |\n| 0.7403        | 24.0  | 1008 | 0.6667          |\n| 0.7403        | 25.0  | 1050 | 0.6657          |\n| 0.7403        | 26.0  | 1092 | 0.6653          |\n| 0.7403        | 27.0  | 1134 | 0.6588          |\n| 0.7403        | 28.0  | 1176 | 0.6584          |\n| 0.7403        | 29.0  | 1218 | 0.6573          |\n| 0.7403        | 30.0  | 1260 | 0.6520          |\n| 0.7403        | 31.0  | 1302 | 0.6522          |\n| 0.7403        | 32.0  | 1344 | 0.6525          |\n| 0.7403        | 33.0  | 1386 | 0.6463          |\n| 0.7403        | 34.0  | 1428 | 0.6453          |\n| 0.7403        | 35.0  | 1470 | 0.6437          |\n| 0.6642        | 36.0  | 1512 | 0.6397          |\n| 0.6642        | 37.0  | 1554 | 0.6382          |\n| 0.6642        | 38.0  | 1596 | 0.6365          |\n| 0.6642        | 39.0  | 1638 | 0.6332          |\n| 0.6642        | 40.0  | 1680 | 0.6335          |\n| 0.6642        | 41.0  | 1722 | 0.6325          |\n| 0.6642        | 42.0  | 1764 | 0.6295          |\n| 0.6642        | 43.0  | 1806 | 0.6304          |\n| 0.6642        | 44.0  | 1848 | 0.6287          |\n| 0.6642        | 45.0  | 1890 | 0.6272          |\n| 0.6642        | 46.0  | 1932 | 0.6267          |\n| 0.6642        | 47.0  | 1974 | 0.6242          |\n| 0.6127        | 48.0  | 2016 | 0.6232          |\n| 0.6127        | 49.0  | 2058 | 0.6225          |\n| 0.6127        | 50.0  | 2100 | 0.6211          |\n| 0.6127        | 51.0  | 2142 | 0.6204          |\n| 0.6127        | 52.0  | 2184 | 0.6196          |\n| 0.6127        | 53.0  | 2226 | 0.6183          |\n| 0.6127        | 54.0  | 2268 | 0.6168          |\n| 0.6127        | 55.0  | 2310 | 0.6175          |\n| 0.6127        | 56.0  | 2352 | 0.6160          |\n| 0.6127        | 57.0  | 2394 | 0.6154          |\n| 0.6127        | 58.0  | 2436 | 0.6143          |\n| 0.6127        | 59.0  | 2478 | 0.6142          |\n| 0.5799        | 60.0  | 2520 | 0.6131          |\n| 0.5799        | 61.0  | 2562 | 0.6122          |\n| 0.5799        | 62.0  | 2604 | 0.6120          |\n| 0.5799        | 63.0  | 2646 | 0.6115          |\n| 0.5799        | 64.0  | 2688 | 0.6119          |\n| 0.5799        | 65.0  | 2730 | 0.6112          |\n| 0.5799        | 66.0  | 2772 | 0.6099          |\n| 0.5799        | 67.0  | 2814 | 0.6094          |\n| 0.5799        | 68.0  | 2856 | 0.6082          |\n| 0.5799        | 69.0  | 2898 | 0.6092          |\n| 0.5799        | 70.0  | 2940 | 0.6081          |\n| 0.5799        | 71.0  | 2982 | 0.6071          |\n| 0.5558        | 72.0  | 3024 | 0.6062          |\n| 0.5558        | 73.0  | 3066 | 0.6079          |\n| 0.5558        | 74.0  | 3108 | 0.6072          |\n| 0.5558        | 75.0  | 3150 | 0.6052          |\n| 0.5558        | 76.0  | 3192 | 0.6066          |\n| 0.5558        | 77.0  | 3234 | 0.6049          |\n| 0.5558        | 78.0  | 3276 | 0.6042          |\n| 0.5558        | 79.0  | 3318 | 0.6039          |\n| 0.5558        | 80.0  | 3360 | 0.6050          |\n| 0.5558        | 81.0  | 3402 | 0.6042          |\n| 0.5558        | 82.0  | 3444 | 0.6040          |\n| 0.5558        | 83.0  | 3486 | 0.6029          |\n| 0.5292        | 84.0  | 3528 | 0.6032          |\n| 0.5292        | 85.0  | 3570 | 0.6039          |\n| 0.5292        | 86.0  | 3612 | 0.6036          |\n| 0.5292        | 87.0  | 3654 | 0.6019          |\n| 0.5292        | 88.0  | 3696 | 0.6014          |\n| 0.5292        | 89.0  | 3738 | 0.6022          |\n| 0.5292        | 90.0  | 3780 | 0.6014          |\n| 0.5292        | 91.0  | 3822 | 0.6020          |\n| 0.5292        | 92.0  | 3864 | 0.6028          |\n| 0.5292        | 93.0  | 3906 | 0.5994          |\n| 0.5292        | 94.0  | 3948 | 0.6004          |\n| 0.5292        | 95.0  | 3990 | 0.5987          |\n| 0.5159        | 96.0  | 4032 | 0.5992          |\n| 0.5159        | 97.0  | 4074 | 0.5993          |\n| 0.5159        | 98.0  | 4116 | 0.5989          |\n| 0.5159        | 99.0  | 4158 | 0.6004          |\n| 0.5159        | 100.0 | 4200 | 0.6001          |\n| 0.5159        | 101.0 | 4242 | 0.6008          |\n| 0.5159        | 102.0 | 4284 | 0.6006          |\n| 0.5159        | 103.0 | 4326 | 0.5999          |\n| 0.5159        | 104.0 | 4368 | 0.5994          |\n| 0.5159        | 105.0 | 4410 | 0.5996          |\n| 0.5159        | 106.0 | 4452 | 0.5991          |\n| 0.5159        | 107.0 | 4494 | 0.5990          |\n| 0.5004        | 108.0 | 4536 | 0.5996          |\n| 0.5004        | 109.0 | 4578 | 0.5988          |\n| 0.5004        | 110.0 | 4620 | 0.5992          |\n| 0.5004        | 111.0 | 4662 | 0.5984          |\n| 0.5004        | 112.0 | 4704 | 0.5982          |\n| 0.5004        | 113.0 | 4746 | 0.5973          |\n| 0.5004        | 114.0 | 4788 | 0.5984          |\n| 0.5004        | 115.0 | 4830 | 0.5973          |\n| 0.5004        | 116.0 | 4872 | 0.5977          |\n| 0.5004        | 117.0 | 4914 | 0.5970          |\n| 0.5004        | 118.0 | 4956 | 0.5976          |\n| 0.5004        | 119.0 | 4998 | 0.5962          |\n| 0.488         | 120.0 | 5040 | 0.5969          |\n| 0.488         | 121.0 | 5082 | 0.5965          |\n| 0.488         | 122.0 | 5124 | 0.5969          |\n| 0.488         | 123.0 | 5166 | 0.5972          |\n| 0.488         | 124.0 | 5208 | 0.5966          |\n| 0.488         | 125.0 | 5250 | 0.5962          |\n| 0.488         | 126.0 | 5292 | 0.5966          |\n| 0.488         | 127.0 | 5334 | 0.5960          |\n| 0.488         | 128.0 | 5376 | 0.5969          |\n| 0.488         | 129.0 | 5418 | 0.5960          |\n| 0.488         | 130.0 | 5460 | 0.5960          |\n| 0.483         | 131.0 | 5502 | 0.5960          |\n| 0.483         | 132.0 | 5544 | 0.5965          |\n| 0.483         | 133.0 | 5586 | 0.5965          |\n| 0.483         | 134.0 | 5628 | 0.5963          |\n| 0.483         | 135.0 | 5670 | 0.5965          |\n| 0.483         | 136.0 | 5712 | 0.5962          |\n| 0.483         | 137.0 | 5754 | 0.5963          |\n| 0.483         | 138.0 | 5796 | 0.5961          |\n| 0.483         | 139.0 | 5838 | 0.5963          |\n| 0.483         | 140.0 | 5880 | 0.5964          |\n| 0.483         | 141.0 | 5922 | 0.5957          |\n| 0.483         | 142.0 | 5964 | 0.5957          |\n| 0.4809        | 143.0 | 6006 | 0.5957          |\n| 0.4809        | 144.0 | 6048 | 0.5956          |\n| 0.4809        | 145.0 | 6090 | 0.5958          |\n| 0.4809        | 146.0 | 6132 | 0.5958          |\n| 0.4809        | 147.0 | 6174 | 0.5959          |\n| 0.4809        | 148.0 | 6216 | 0.5958          |\n| 0.4809        | 149.0 | 6258 | 0.5958          |\n| 0.4809        | 150.0 | 6300 | 0.5958          |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu116\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "242071641", "downloads": 2}
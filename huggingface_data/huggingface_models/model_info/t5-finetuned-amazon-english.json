{"pretrained_model_name": "kabilanp942/t5-finetuned-amazon-english", "description": "---\nlicense: apache-2.0\ntags:\n- Summarization\n- generated_from_trainer\ndatasets:\n- amazon_reviews_multi\nmetrics:\n- rouge\nmodel-index:\n- name: t5-finetuned-amazon-english\n  results:\n  - task:\n      name: Sequence-to-sequence Language Modeling\n      type: text2text-generation\n    dataset:\n      name: amazon_reviews_multi\n      type: amazon_reviews_multi\n      config: en\n      split: train\n      args: en\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 19.1814\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-finetuned-amazon-english\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on the amazon_reviews_multi dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 3.1713\n- Rouge1: 19.1814\n- Rouge2: 9.8673\n- Rougel: 18.1982\n- Rougelsum: 18.2963\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 8\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2 | Rougel  | Rougelsum |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:------:|:-------:|:---------:|\n| 3.3583        | 1.0   | 771  | 3.2513          | 16.6865 | 9.0598 | 15.8299 | 15.8472   |\n| 3.1022        | 2.0   | 1542 | 3.2147          | 16.8499 | 9.4849 | 16.1568 | 16.2437   |\n| 3.0067        | 3.0   | 2313 | 3.1718          | 16.9516 | 8.762  | 16.104  | 16.2186   |\n| 2.9482        | 4.0   | 3084 | 3.1854          | 18.9582 | 9.5416 | 18.0846 | 18.2938   |\n| 2.8934        | 5.0   | 3855 | 3.1669          | 18.857  | 9.934  | 17.9027 | 18.0272   |\n| 2.8389        | 6.0   | 4626 | 3.1782          | 18.6736 | 9.326  | 17.6943 | 17.8852   |\n| 2.8174        | 7.0   | 5397 | 3.1709          | 18.4342 | 9.6936 | 17.5714 | 17.6516   |\n| 2.8           | 8.0   | 6168 | 3.1713          | 19.1814 | 9.8673 | 18.1982 | 18.2963   |\n\n\n### Framework versions\n\n- Transformers 4.22.0\n- Pytorch 1.12.1+cu113\n- Datasets 2.4.0\n- Tokenizers 0.12.1\n", "size_bytes": "242070267", "downloads": 2}
{"pretrained_model_name": "ilhami/Tr_En-MbartFinetune", "description": "---\nlanguage:\n- tr\n- en\ntags:\n- translation\nlicense: apache-2.0\ndatasets:\n- Parallel Corpora for Turkish-English Academic Translations\nmetrics:\n- bleu\n- sacrebleu\n---\n\n\n## Model Details\n\n- **Developed by:** \u0130lhami SEL\n- **Model type:** Mbart Finetune Machine Translation\n- **Language:** Turkish - English\n- **Resources for more information:** Sel, \u0130. , \u00dczen, H. & Hanbay, D. (2021). Creating a Parallel Corpora for Turkish-English Academic Translations . Computer Science , 5th International Artificial Intelligence and Data Processing symposium , 335-340 . DOI: 10.53070/bbd.990959\n\n\n\n\n\n```python\ncheckpoint = \"ilhami/Tr_En-MbartFinetune\"\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(\"cuda\")\ntokenizer.src_lang = \"tr_TR\"\ntr= [\"Sohbet robotlar\u0131 son y\u0131llarda yayg\u0131n bir \u015fekilde kullan\u0131lmaya ba\u015flanm\u0131\u015ft\u0131r. \",\n\"\u0130nsanlar\u0131 taklit eden ve daha iyi m\u00fc\u015fteri memnuniyeti sa\u011flayan sohbet robotlar\u0131 en geli\u015fkin do\u011fal dil i\u015fleme tekniklerine ihtiya\u00e7 duymaktad\u0131r. \",\n\"Bu \u00e7al\u0131\u015fma sohbet robotu konu\u015fmalar\u0131n\u0131n niyet tahminini geli\u015ftirmeye odaklanm\u0131\u015ft\u0131r.\" ,\n\"Kelime g\u00f6sterimi i\u00e7in TF-IDF, Doc2vec ve BERT gibi geleneksel ve geli\u015fmi\u015f do\u011fal dil i\u015fleme y\u00f6ntemleri, \u00e7oklu s\u0131n\u0131f ve \u00e7oklu etiket tahmini i\u00e7in ise lojistik regresyon, rastgele orman ve yapay sinir a\u011flar\u0131 kullan\u0131lm\u0131\u015ft\u0131r.\" ,\n\"Sohbet robotu konu\u015fma veri k\u00fcmeleri, sinema bileti rezervasyonu, restoran rezervasyonu ve taksi \u00e7a\u011f\u0131rma olmak \u00fczere \u00fc\u00e7 farkl\u0131 alandan al\u0131nm\u0131\u015ft\u0131r. \",\n\"Bu \u00e7al\u0131\u015fman\u0131n sonunda, BERT ve BERT ile TF-IDF birle\u015fimi modellerin di\u011fer kombinasyonlardan daha iyi sonu\u00e7 verdi\u011fi g\u00f6r\u00fclm\u00fc\u015ft\u00fcr. \",\n\"BERT gibi \u00f6n e\u011fitimli modellerden faydalanman\u0131n daha iyi ba\u011flamsal anlama sa\u011flad\u0131\u011f\u0131 ortaya \u00e7\u0131km\u0131\u015ft\u0131r. \",\n\"TF-IDF yerle\u015ftirmeleri, BERT g\u00f6sterimi ile birle\u015ftirilerek niyet kategorisi tahmininin iyile\u015ftirilmesi ama\u00e7lanm\u0131\u015ft\u0131r.\"]\nencoded_tr = tokenizer(tr, return_tensors=\"pt\" ,padding=True , truncation=True).to(\"cuda\")\ngenerated_tokens = model.generate(**encoded_tr, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\nen = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n```\n\n\n\n\n\n\n", "size_bytes": "2444572409", "downloads": 9}
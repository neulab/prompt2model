{"pretrained_model_name": "Gayathri142214002/Finetune_Pegasus_2", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Finetune_Pegasus_2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# Finetune_Pegasus_2\n\nThis model is a fine-tuned version of [Gayathri142214002/Finetune_Pegasus_1](https://huggingface.co/Gayathri142214002/Finetune_Pegasus_1) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2891\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 4\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 7\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 0.5877        | 0.09  | 10   | 0.4730          |\n| 0.4521        | 0.19  | 20   | 0.4180          |\n| 0.5412        | 0.28  | 30   | 0.3851          |\n| 0.3669        | 0.37  | 40   | 0.3635          |\n| 0.3732        | 0.47  | 50   | 0.3396          |\n| 0.3724        | 0.56  | 60   | 0.3555          |\n| 0.3693        | 0.65  | 70   | 0.3412          |\n| 0.3476        | 0.75  | 80   | 0.3060          |\n| 0.3288        | 0.84  | 90   | 0.3009          |\n| 0.3439        | 0.93  | 100  | 0.3344          |\n| 0.2644        | 1.03  | 110  | 0.3283          |\n| 0.2926        | 1.12  | 120  | 0.3101          |\n| 0.2615        | 1.21  | 130  | 0.3189          |\n| 0.3004        | 1.31  | 140  | 0.3085          |\n| 0.3202        | 1.4   | 150  | 0.3148          |\n| 0.2989        | 1.49  | 160  | 0.3062          |\n| 0.3326        | 1.59  | 170  | 0.3055          |\n| 0.3101        | 1.68  | 180  | 0.2886          |\n| 0.3011        | 1.77  | 190  | 0.2896          |\n| 0.2795        | 1.86  | 200  | 0.2901          |\n| 0.2759        | 1.96  | 210  | 0.3092          |\n| 0.2883        | 2.05  | 220  | 0.2948          |\n| 0.2566        | 2.14  | 230  | 0.2760          |\n| 0.222         | 2.24  | 240  | 0.2802          |\n| 0.2667        | 2.33  | 250  | 0.2733          |\n| 0.262         | 2.42  | 260  | 0.2928          |\n| 0.2784        | 2.52  | 270  | 0.3061          |\n| 0.2832        | 2.61  | 280  | 0.3079          |\n| 0.2885        | 2.7   | 290  | 0.3160          |\n| 0.275         | 2.8   | 300  | 0.3111          |\n| 0.3059        | 2.89  | 310  | 0.2992          |\n| 0.2604        | 2.98  | 320  | 0.2923          |\n| 0.2425        | 3.08  | 330  | 0.2991          |\n| 0.2225        | 3.17  | 340  | 0.2988          |\n| 0.2548        | 3.26  | 350  | 0.3124          |\n| 0.2393        | 3.36  | 360  | 0.3009          |\n| 0.2383        | 3.45  | 370  | 0.2846          |\n| 0.2251        | 3.54  | 380  | 0.2871          |\n| 0.253         | 3.64  | 390  | 0.2875          |\n| 0.2615        | 3.73  | 400  | 0.2829          |\n| 0.238         | 3.82  | 410  | 0.2815          |\n| 0.2685        | 3.92  | 420  | 0.2892          |\n| 0.2532        | 4.01  | 430  | 0.2906          |\n| 0.2105        | 4.1   | 440  | 0.2851          |\n| 0.2382        | 4.2   | 450  | 0.2823          |\n| 0.2316        | 4.29  | 460  | 0.2777          |\n| 0.2565        | 4.38  | 470  | 0.2816          |\n| 0.2216        | 4.48  | 480  | 0.2869          |\n| 0.2477        | 4.57  | 490  | 0.2968          |\n| 0.2223        | 4.66  | 500  | 0.3006          |\n| 0.2445        | 4.76  | 510  | 0.3035          |\n| 0.2383        | 4.85  | 520  | 0.2985          |\n| 0.2482        | 4.94  | 530  | 0.2929          |\n| 0.2151        | 5.03  | 540  | 0.2881          |\n| 0.2266        | 5.13  | 550  | 0.2891          |\n| 0.2222        | 5.22  | 560  | 0.2908          |\n| 0.2305        | 5.31  | 570  | 0.2921          |\n| 0.2383        | 5.41  | 580  | 0.2927          |\n| 0.2055        | 5.5   | 590  | 0.2908          |\n| 0.2229        | 5.59  | 600  | 0.2916          |\n| 0.2365        | 5.69  | 610  | 0.2898          |\n| 0.2357        | 5.78  | 620  | 0.2897          |\n| 0.2116        | 5.87  | 630  | 0.2902          |\n| 0.2342        | 5.97  | 640  | 0.2915          |\n| 0.2011        | 6.06  | 650  | 0.2906          |\n| 0.1961        | 6.15  | 660  | 0.2885          |\n| 0.2089        | 6.25  | 670  | 0.2881          |\n| 0.1908        | 6.34  | 680  | 0.2886          |\n| 0.2093        | 6.43  | 690  | 0.2884          |\n| 0.1976        | 6.53  | 700  | 0.2882          |\n| 0.1843        | 6.62  | 710  | 0.2887          |\n| 0.2039        | 6.71  | 720  | 0.2893          |\n| 0.204         | 6.81  | 730  | 0.2894          |\n| 0.2124        | 6.9   | 740  | 0.2891          |\n\n\n### Framework versions\n\n- Transformers 4.29.2\n- Pytorch 2.0.1+cu117\n- Datasets 2.12.0\n- Tokenizers 0.13.3\n", "size_bytes": "2275907565", "downloads": 3}
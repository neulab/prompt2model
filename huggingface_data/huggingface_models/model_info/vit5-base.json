{"pretrained_model_name": "VietAI/vit5-base", "description": "---\nlanguage: vi\ndatasets:\n- cc100\ntags:\n- summarization\n- translation\n- question-answering\n\nlicense: mit\n---\n\n# ViT5-base\n\nState-of-the-art pretrained Transformer-based encoder-decoder model for Vietnamese.\n\n## How to use\nFor more details, do check out [our Github repo](https://github.com/vietai/ViT5). \n\n[Finetunning Example can be found here](https://github.com/vietai/ViT5/tree/main/finetunning_huggingface).\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-base\")\nmodel.cuda()\n```\n\n## Citation\n```\n@inproceedings{phan-etal-2022-vit5,\n    title = \"{V}i{T}5: Pretrained Text-to-Text Transformer for {V}ietnamese Language Generation\",\n    author = \"Phan, Long and Tran, Hieu and Nguyen, Hieu and Trinh, Trieu H.\",\n    booktitle = \"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop\",\n    year = \"2022\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.naacl-srw.18\",\n    pages = \"136--142\",\n}\n```", "size_bytes": "903886847", "downloads": 1103}
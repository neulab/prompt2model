{"pretrained_model_name": "petroglyphs-nlp-consulting/flan-t5-base-geoqa", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: flan-t5-base-geoqa\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-base-geoqa\n\nThis model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) on the \n[petroglyphs-nlp-consulting/res_syn_sentences_qa_lg](https://huggingface.co/datasets/petroglyphs-nlp-consulting/res_syn_sentences_qa_lg) \ndataset.\n\n## Model description\n\nThe [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) model has been fine-tuned on 24750 question-answer pairs obtained\nfrom the [petroglyphs-nlp-consulting/res_syn_sentences_qa_lg](https://huggingface.co/datasets/petroglyphs-nlp-consulting/res_syn_sentences_qa_lg) \ndataset.\n\n## Intended uses & limitations\n\nThe model can be used for Q&A inferences covering Geosciences (specifically oil and Gas Reservoir Geology) topics.\n\n## Training and evaluation data\n\n- Training: 24750 question-answer pairs\n- Validation: 4980 question-answer pairs\n- Test: 250 question-answer pairs\n\nEvaluation run on the test set using the following methods:\n\n```\ndef normalize_answer(s):\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef evaluate(gold_answers, predictions):\n    f1 = exact_match = total = 0\n\n    for gold_answer, prediction in zip(gold_answers, predictions):\n      total += 1\n      exact_match += exact_match_score(prediction, gold_answer)\n      f1 += f1_score(prediction, gold_answer)\n    \n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {'exact_match': exact_match, 'f1': f1}\n```\n\n- 'exact_match': 70.4 \n- 'f1': 83.07678804855274\n\nComparison with vanilla [google/flan-t5-base](https://huggingface.co/google/flan-t5-base)\n\n- 'exact_match': 65.2\n- 'f1': 80.156117099275\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- gradient_accumulation_steps: 16\n- total_train_batch_size: 512\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- num_epochs: 6\n\n### Training results\n\n'train_samples_per_second': 40.608, \n'train_steps_per_second': 0.079, \n'train_loss': 0.06748775382422739, 'epoch': 15}\n\n### Training hardware\n\n2 x RTX Titan GPU (24Gb each)\n\n### Framework versions\n\n- Transformers 4.27.2\n- Pytorch 2.0.0+cu117\n- Datasets 2.10.1\n- Tokenizers 0.13.2\n\n### Environmental footprint\nA single GPU has been used but both cards have been used for memory sharing.\n280W x 3h = 0.84 kWh x 0.3 kg eq. CO2/kWh = 0.25 kg eq. CO2\n", "size_bytes": "990404917", "downloads": 2}
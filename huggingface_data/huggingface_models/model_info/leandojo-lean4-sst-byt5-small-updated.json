{"pretrained_model_name": "kaiyuy/leandojo-lean4-sst-byt5-small-updated", "description": "---\nlicense: mit\ninference:\n  parameters:\n    max_length: 1024\nwidget:\n  - text: \"before\\n\u03b1 : Type u\\n\u03b2 : Type u\\na : Cardinal\\nb : Cardinal\\nc : Cardinal\\n\u22a2 a ^< b \u2264 c \u2194 \u2200 (x : Cardinal), x < b \u2192 a ^ x \u2264 c\\n\\nafter\\n...\\n\u22a2 (\u2200 (i : \u2191(Iio b)), a ^ \u2191i \u2264 c) \u2194 \u2200 (x : Cardinal), x < b \u2192 a ^ x \u2264 c\"\n    example_title: Example\n---\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"kaiyuy/leandojo-lean4-sst-byt5-small-updated\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"kaiyuy/leandojo-lean4-sst-byt5-small-updated\")\n\nstate_pair = \"\"\"before\n\u03b1 : Type u\n\u03b2 : Type u\na : Cardinal\nb : Cardinal\nc : Cardinal\n\u22a2 a ^< b \u2264 c \u2194 \u2200 (x : Cardinal), x < b \u2192 a ^ x \u2264 c\n\nafter\n...\n\u22a2 (\u2200 (i : \u2191(Iio b)), a ^ \u2191i \u2264 c) \u2194 \u2200 (x : Cardinal), x < b \u2192 a ^ x \u2264 c\"\"\"\ntokenized_state_pair = tokenizer(state_pair, return_tensors=\"pt\")\n\n# Generate a single tactic.\ntactic_ids = model.generate(tokenized_state_pair.input_ids, max_length=1024)\ntactic = tokenizer.decode(tactic_ids[0], skip_special_tokens=True)\nprint(tactic, end=\"\\n\\n\")\n\n# Generate multiple tactics via beam search.\ntactic_candidates_ids = model.generate(\n    tokenized_state_pair.input_ids,\n    max_length=1024,\n    num_beams=4,\n    length_penalty=0.0,\n    do_sample=False,\n    num_return_sequences=4,\n    early_stopping=False,\n)\ntactic_candidates = tokenizer.batch_decode(\n    tactic_candidates_ids, skip_special_tokens=True\n)\nfor tac in tactic_candidates:\n    print(tac)\n```", "size_bytes": "1198607221", "downloads": 12}
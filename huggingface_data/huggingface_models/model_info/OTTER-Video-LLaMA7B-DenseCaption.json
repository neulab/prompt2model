{"pretrained_model_name": "luodian/OTTER-Video-LLaMA7B-DenseCaption", "description": "---\nlicense: mit\n---\n\n<p align=\"center\" width=\"100%\">\n<img src=\"https://i.postimg.cc/MKmyP9wH/new-banner.png\"  width=\"80%\" height=\"80%\">\n</p>\n\n\n<div>\n<div align=\"center\">\n    <a href='https://brianboli.com/' target='_blank'>Bo Li*<sup>1</sup></a>&emsp;\n    <a href='https://zhangyuanhan-ai.github.io/' target='_blank'>Yuanhan Zhang*<sup>,1</sup></a>&emsp;\n    <a href='https://cliangyu.com/' target='_blank'>Liangyu Chen*<sup>,1</sup></a>&emsp;\n    <a href='https://king159.github.io/' target='_blank'>Jinghao Wang*<sup>,1</sup></a>&emsp;\n    <a href='https://pufanyi.github.io/' target='_blank'>Fanyi Pu*<sup>,1</sup></a>&emsp;\n    </br>\n    <a href='https://jingkang50.github.io/' target='_blank'>Jingkang Yang<sup>1</sup></a>&emsp;\n    <a href='https://chunyuan.li/' target='_blank'>Chunyuan Li<sup>2</sup></a>&emsp;\n    <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu<sup>1</sup></a>\n</div>\n<div>\n<div align=\"center\">\n    <sup>1</sup>S-Lab, Nanyang Technological University&emsp;\n    <sup>2</sup>Microsoft Research, Redmond\n</div>\n \n -----------------\n\n![](https://img.shields.io/badge/otter-v0.2-darkcyan)\n![](https://img.shields.io/github/stars/luodian/otter?style=social)\n[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FLuodian%2Fotter&count_bg=%23FFA500&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=visitors&edge_flat=false)](https://hits.seeyoufarm.com)\n![](https://black.readthedocs.io/en/stable/_static/license.svg)\n![](https://img.shields.io/badge/code%20style-black-000000.svg)\n\nAn example of using this model to run on your video. \nPlease first clone [Otter](https://github.com/Luodian/Otter) to your local disk. \nPlace following script inside the `Otter` folder to make sure it has the access to `otter/modeling_otter.py`.\n\n```python\nimport mimetypes\nimport os\nfrom typing import Union\nimport cv2\nimport requests\nimport torch\nimport transformers\nfrom PIL import Image\nimport sys\n\n# make sure you can properly access the otter folder\nfrom otter.modeling_otter import OtterForConditionalGeneration\n\n# Disable warnings\nrequests.packages.urllib3.disable_warnings()\n\n# ------------------- Utility Functions -------------------\n\n\ndef get_content_type(file_path):\n    content_type, _ = mimetypes.guess_type(file_path)\n    return content_type\n\n\n# ------------------- Image and Video Handling Functions -------------------\n\n\ndef extract_frames(video_path, num_frames=16):\n    video = cv2.VideoCapture(video_path)\n    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_step = total_frames // num_frames\n    frames = []\n\n    for i in range(num_frames):\n        video.set(cv2.CAP_PROP_POS_FRAMES, i * frame_step)\n        ret, frame = video.read()\n        if ret:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame = Image.fromarray(frame).convert(\"RGB\")\n            frames.append(frame)\n\n    video.release()\n    return frames\n\n\ndef get_image(url: str) -> Union[Image.Image, list]:\n    if \"://\" not in url:  # Local file\n        content_type = get_content_type(url)\n    else:  # Remote URL\n        content_type = requests.head(url, stream=True, verify=False).headers.get(\"Content-Type\")\n\n    if \"image\" in content_type:\n        if \"://\" not in url:  # Local file\n            return Image.open(url)\n        else:  # Remote URL\n            return Image.open(requests.get(url, stream=True, verify=False).raw)\n    elif \"video\" in content_type:\n        video_path = \"temp_video.mp4\"\n        if \"://\" not in url:  # Local file\n            video_path = url\n        else:  # Remote URL\n            with open(video_path, \"wb\") as f:\n                f.write(requests.get(url, stream=True, verify=False).content)\n        frames = extract_frames(video_path)\n        if \"://\" in url:  # Only remove the temporary video file if it was downloaded\n            os.remove(video_path)\n        return frames\n    else:\n        raise ValueError(\"Invalid content type. Expected image or video.\")\n\n\n# ------------------- OTTER Prompt and Response Functions -------------------\n\n\ndef get_formatted_prompt(prompt: str) -> str:\n    return f\"<image>User: {prompt} GPT:<answer>\"\n\n\ndef get_response(input_data, prompt: str, model=None, image_processor=None, tensor_dtype=None) -> str:\n    if isinstance(input_data, Image.Image):\n        vision_x = image_processor.preprocess([input_data], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n    elif isinstance(input_data, list):  # list of video frames\n        vision_x = image_processor.preprocess(input_data, return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(0).unsqueeze(0)\n    else:\n        raise ValueError(\"Invalid input data. Expected PIL Image or list of video frames.\")\n\n    lang_x = model.text_tokenizer(\n        [\n            get_formatted_prompt(prompt),\n        ],\n        return_tensors=\"pt\",\n    )\n\n    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n    generated_text = model.generate(\n        vision_x=vision_x.to(model.device, dtype=tensor_dtype),\n        lang_x=lang_x[\"input_ids\"].to(model.device),\n        attention_mask=lang_x[\"attention_mask\"].to(model.device),\n        max_new_tokens=512,\n        num_beams=3,\n        no_repeat_ngram_size=3,\n        bad_words_ids=bad_words_id,\n    )\n    parsed_output = (\n        model.text_tokenizer.decode(generated_text[0])\n        .split(\"<answer>\")[-1]\n        .lstrip()\n        .rstrip()\n        .split(\"<|endofchunk|>\")[0]\n        .lstrip()\n        .rstrip()\n        .lstrip('\"')\n        .rstrip('\"')\n    )\n    return parsed_output\n\n\n# ------------------- Main Function -------------------\nload_bit = \"fp32\"\nif load_bit == \"fp16\":\n    precision = {\"torch_dtype\": torch.float16}\nelif load_bit == \"bf16\":\n    precision = {\"torch_dtype\": torch.bfloat16}\nelif load_bit == \"fp32\":\n    precision = {\"torch_dtype\": torch.float32}\n\n# This model version is trained on MIMIC-IT DC dataset.\nmodel = OtterForConditionalGeneration.from_pretrained(\"luodian/OTTER-9B-DenseCaption\", device_map=\"auto\", **precision)\ntensor_dtype = {\"fp16\": torch.float16, \"bf16\": torch.bfloat16, \"fp32\": torch.float32}[load_bit]\n\nmodel.text_tokenizer.padding_side = \"left\"\ntokenizer = model.text_tokenizer\nimage_processor = transformers.CLIPImageProcessor()\nmodel.eval()\n\nwhile True:\n    video_url = input(\"Enter video path: \")  # Replace with the path to your video file, could be any common format.\n\n    frames_list = get_image(video_url)\n\n    while True:\n        prompts_input = input(\"Enter prompts: \")\n\n        if prompts_input.lower() == \"quit\":\n            break\n\n        print(f\"\\nPrompt: {prompts_input}\")\n        response = get_response(frames_list, prompts_input, model, image_processor, tensor_dtype)\n        print(f\"Response: {response}\")\n\n```", "size_bytes": 32882387016, "downloads": 658}
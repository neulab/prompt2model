{"pretrained_model_name": "RamAnanth1/distilbart-cnn-12-6-finetuned-scitldr", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- scitldr\nmodel-index:\n- name: distilbart-cnn-12-6-finetuned-scitldr\n  results: []\nwidget:\n- text: \"Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation that is competitive with direct imitation learning algorithms. Additionally, we show that AIRL is able to recover portable reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. \"\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbart-cnn-12-6-finetuned-scitldr\n\nThis model is a fine-tuned version of [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6) on the scitldr dataset.\nIt achieves the following results on the evaluation set:\n- eval_loss: 3.7113\n- eval_rouge1: 31.4431\n- eval_rouge2: 13.1766\n- eval_rougeL: 24.2038\n- eval_rougeLsum: 26.3167\n- eval_runtime: 151.7265\n- eval_samples_per_second: 4.08\n- eval_steps_per_second: 0.514\n- epoch: 4.0\n- step: 996\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 8\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.12.1+cu113\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n", "size_bytes": "1222361081", "downloads": 14}
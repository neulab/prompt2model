{"pretrained_model_name": "facebook/mbart-large-50", "description": "---\nlanguage: \n- multilingual\n- ar \n- cs\n- de\n- en\n- es\n- et\n- fi\n- fr\n- gu\n- hi\n- it\n- ja\n- kk\n- ko\n- lt\n- lv\n- my\n- ne\n- nl\n- ro\n- ru\n- si\n- tr\n- vi\n- zh\n- af\n- az\n- bn\n- fa\n- he\n- hr\n- id\n- ka\n- km\n- mk\n- ml\n- mn\n- mr\n- pl\n- ps\n- pt\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- uk\n- ur\n- xh\n- gl\n- sl\nlicense: mit\ntags:\n- mbart-50\n---\n\n# mBART-50\n\nmBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the \"Multilingual Denoising Pretraining\" objective. It was introduced in [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) paper.\n\n## Model description\n\nmBART-50 is a multilingual Sequence-to-Sequence model. It was introduced to show that multilingual translation models can be created through multilingual fine-tuning. \nInstead of fine-tuning on one direction, a pre-trained model is fine-tuned on many directions simultaneously. mBART-50 is created using the original mBART model and extended to add extra 25 languages to support multilingual machine translation models of 50 languages. The pre-training objective is explained below.\n\n**Multilingual Denoising Pretraining**: The model incorporates N languages by concatenating data: \n`D = {D1, ..., DN }` where each Di is a collection of monolingual documents in language `i`. The source documents are noised using two schemes, \nfirst randomly shuffling the original sentences' order, and second a novel in-filling scheme, \nwhere spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. \n35% of each instance's words are masked by random sampling a span length according to a Poisson distribution `(\u03bb = 3.5)`.\nThe decoder input is the original text with one position offset. A language id symbol `LID` is used as the initial token to predict the sentence.\n\n\n## Intended uses & limitations\n\n`mbart-large-50` is pre-trained model and primarily aimed at being fine-tuned on translation tasks. It can also be fine-tuned on other multilingual sequence-to-sequence tasks. \nSee the [model hub](https://huggingface.co/models?filter=mbart-50) to look for fine-tuned versions.\n\n\n## Training\n\nAs the model is multilingual, it expects the sequences in a different format. A special language id token is used as a prefix in both the source and target text. The text format is `[lang_code] X [eos]` with `X` being the source or target text respectively and `lang_code` is `source_lang_code` for source text and `tgt_lang_code` for target text. `bos` is never used. Once the examples are prepared in this format, it can be trained as any other sequence-to-sequence model.\n\n\n```python\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n\nsrc_text = \" UN Chief Says There Is No Military Solution in Syria\"\ntgt_text =  \"\u015eeful ONU declar\u0103 c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n\nmodel_inputs = tokenizer(src_text, return_tensors=\"pt\")\nwith tokenizer.as_target_tokenizer():\n    labels = tokenizer(tgt_text, return_tensors=\"pt\").input_ids\n\nmodel(**model_inputs, labels=labels) # forward pass\n```\n\n\n\n## Languages covered\nArabic (ar_AR), Czech (cs_CZ), German (de_DE), English (en_XX), Spanish (es_XX), Estonian (et_EE), Finnish (fi_FI), French (fr_XX), Gujarati (gu_IN), Hindi (hi_IN), Italian (it_IT), Japanese (ja_XX), Kazakh (kk_KZ), Korean (ko_KR), Lithuanian (lt_LT), Latvian (lv_LV), Burmese (my_MM), Nepali (ne_NP), Dutch (nl_XX), Romanian (ro_RO), Russian (ru_RU), Sinhala (si_LK), Turkish (tr_TR), Vietnamese (vi_VN), Chinese (zh_CN), Afrikaans (af_ZA), Azerbaijani (az_AZ), Bengali (bn_IN), Persian (fa_IR), Hebrew (he_IL), Croatian (hr_HR), Indonesian (id_ID), Georgian (ka_GE), Khmer (km_KH), Macedonian (mk_MK), Malayalam (ml_IN), Mongolian (mn_MN), Marathi (mr_IN), Polish (pl_PL), Pashto (ps_AF), Portuguese (pt_XX), Swedish (sv_SE), Swahili (sw_KE), Tamil (ta_IN), Telugu (te_IN), Thai (th_TH), Tagalog (tl_XX), Ukrainian (uk_UA), Urdu (ur_PK), Xhosa (xh_ZA), Galician (gl_ES), Slovene (sl_SI)\n\n\n## BibTeX entry and citation info\n```\n@article{tang2020multilingual,\n    title={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},\n    author={Yuqing Tang and Chau Tran and Xian Li and Peng-Jen Chen and Naman Goyal and Vishrav Chaudhary and Jiatao Gu and Angela Fan},\n    year={2020},\n    eprint={2008.00401},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```", "size_bytes": "2444714899", "downloads": 16792}
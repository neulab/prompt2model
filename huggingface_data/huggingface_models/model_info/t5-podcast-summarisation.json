{"pretrained_model_name": "paulowoicho/t5-podcast-summarisation", "description": "---\nlanguage:\n- en\ndatasets:\n- Spotify Podcasts Dataset\ntags:\n- t5\n- summarisation\n- pytorch\n- lm-head\nmetrics:\n- ROUGE\npipeline:\n- summarisation\n---\n\n# T5 for Automatic Podcast Summarisation\n\nThis model is the result of fine-tuning [t5-base](https://huggingface.co/t5-base) on the [Spotify Podcast Dataset](https://arxiv.org/abs/2004.04270).\n\nIt is based on [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) which was pretrained on the [C4 dataset](https://huggingface.co/datasets/c4).\n\n\nPaper: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)\n\nAuthors: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu\n\n## Intended uses & limitations\nThis model is intended to be used for automatic podcast summarisation. As creator provided descriptions\nwere used for training, the model also learned to generate promotional material (links, hashtags, etc) in its summaries, as such\nsome post processing may be required on the model's outputs.\n\nIf using on Colab, the instance will crash if the number of tokens in the transcript exceeds 7000. I discovered that the model\ngenerated reasonable summaries even when the podcast transcript was truncated to reduce the number of tokens.\n\n#### How to use\n\nThe model can be used with the summarisation as follows:\n\n```python\nfrom transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=\"paulowoicho/t5-podcast-summarisation\", tokenizer=\"paulowoicho/t5-podcast-summarisation\")\nsummary = summarizer(podcast_transcript, min_length=5, max_length=20)\n\nprint(summary[0]['summary_text'])\n```\n\n## Training data\n\nThis model is the result of fine-tuning [t5-base](https://huggingface.co/t5-base) on the [Spotify Podcast Dataset](https://arxiv.org/abs/2004.04270).\n[Pre-processing](https://github.com/paulowoicho/msc_project/blob/master/reformat.py) was done on the original data before fine-tuning.\n\n## Training procedure\n\nTraining was largely based on [Fine-tune T5 for Summarization](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb) by [Abhishek Kumar Mishra](https://github.com/abhimishra91) \n", "size_bytes": "891739333", "downloads": 81}
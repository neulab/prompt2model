{"pretrained_model_name": "DEplain/trimmed_longmbart_docs_apa", "description": "---\ninference: false\nlicense: apache-2.0\nlanguage:\n- de\ndatasets:\n- DEplain/DEplain-APA-doc\nmetrics:\n- sari\n- bleu\n- bertscore\nlibrary_name: transformers\npipeline_tag: text2text-generation\ntags:\n  - text simplification\n  - plain language\n  - easy-to-read language\n  - document simplification\n---\n\n# DEplain German Text Simplification\n\nThis model belongs to the experiments done at the work of Stodden, Momen, Kallmeyer (2023). [\"DEplain: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification.\"](https://arxiv.org/abs/2305.18939) In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada. Association for Computational Linguistics. \nDetailed documentation can be found on this GitHub repository [https://github.com/rstodden/DEPlain](https://github.com/rstodden/DEPlain)\n\nWe reused the codes from [https://github.com/a-rios/ats-models](https://github.com/a-rios/ats-models) to do our experiments.\n\n### Model Description\n\nThe model is a finetuned checkpoint of the pre-trained LongmBART model based on `mbart-large-cc25`. With a trimmed vocabulary to the most frequent 30k words in the German language. \n\nThe model was finetuned towards the task of German text simplification of documents.\n\nThe finetuning dataset included manually aligned sentences from the datasets `DEplain-APA-doc` only.\n\n### Model Usage\n\nThis model can't be used in the HuggingFace interface or via the .from_pretrained method currently. As it's a finetuning of a custom model (LongMBart), which hasn't been registered on HF yet.\nYou can find this custom model codes at: [https://github.com/a-rios/ats-models](https://github.com/a-rios/ats-models)\n\nTo test this model checkpoint, you need to clone the checkpoint repository as follows:\n\n```\n  # Make sure you have git-lfs installed (https://git-lfs.com)\n  git lfs install\n  git clone https://huggingface.co/DEplain/trimmed_longmbart_docs_apa\n  \n  # if you want to clone without large files \u2013 just their pointers\n  # prepend your git clone with the following env var:\n  GIT_LFS_SKIP_SMUDGE=1\n```\n\nThen set up the conda environment via:\n```\n  conda env create -f environment.yaml\n```\n\nThen follow the procedure in the notebook `generation.ipynb`.", "size_bytes": "1717750633", "downloads": 3}
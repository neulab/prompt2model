{"pretrained_model_name": "dev2bit/es2bash-mt5", "description": "---\nlicense: apache-2.0\ndatasets:\n- dev2bit/es2bash\nlanguage:\n- es\npipeline_tag: text2text-generation\ntags:\n- code\n- bash\nwidget:\n  - text: Muestra el contenido de file.py que se encuentra en ~/project/\n    example_title: cat\n  - text: Lista los 3 primeros archivos en /bin\n    example_title: ls\n  - text: Por favor, cambia al directorio /home/user/project/\n    example_title: cd\n  - text: Lista todos los \u00e1tomos del universo\n    example_title: noCommand\n  - text: ls -lh\n    example_title: literal\n  - text: file.txt\n    example_title: simple\n---\n\n# es2bash-mt5: Spanish to Bash Model\n\n<p align=\"center\">\n  <img width=\"460\" height=\"300\" src=\"https://dev2bit.com/wp-content/themes/lovecraft_child/assets/icons/dev2bit_monitor2.svg\">\n</p>\n\nDeveloped by dev2bit, es2bash-mt5 is a language transformer model that is capable of predicting the optimal Bash command given a natural language request in Spanish. This model represents a major advancement in human-computer interaction, providing a natural language interface for Unix operating system commands.\n\n## About the Model\n\nes2bash-mt5 is a fine-tuning model based on mt5-small. It has been trained on the dev2bit/es2bash dataset, which specializes in translating natural language in Spanish into Bash commands.\n\nThis model is optimized for processing requests related to the commands:\n\n* `cat`\n* `ls`\n* `cd`\n\n## Usage\n\nBelow is an example of how to use es2bash-mt5 with the Hugging Face Transformers library:\n\n```python\nfrom transformers import pipeline\n\ntranslator = pipeline('translation', model='dev2bit/es2bash-mt5')\n\nrequest = \"listar los archivos en el directorio actual\"\ntranslated = translator(request, max_length=512)\nprint(translated[0]['translation_text'])\n```\nThis will print the Bash command corresponding to the given Spanish request.\n\n## Contributions\nWe appreciate your contributions! You can help improve es2bash-mt5 in various ways, including:\n\n* Testing the model and reporting any issues or suggestions in the Issues section.\n* Improving the documentation.\n* Providing usage examples.\n\n--- \n\n# es2bash-mt5: Modelo de espa\u00f1ol a Bash\n\nDesarrollado por dev2bit, `es2bash-mt5` es un modelo transformador de lenguaje que tiene la capacidad de predecir el comando Bash \u00f3ptimo dada una solicitud en lenguaje natural en espa\u00f1ol. Este modelo representa un gran avance en la interacci\u00f3n humano-computadora, proporcionando una interfaz de lenguaje natural para los comandos del sistema operativo Unix.\n\n## Sobre el modelo\n\n`es2bash-mt5` es un modelo de ajuste fino basado en `mt5-small`. Ha sido entrenado en el conjunto de datos `dev2bit/es2bash`, especializado en la traducci\u00f3n de lenguaje natural en espa\u00f1ol a comandos Bash.\n\nEste modelo est\u00e1 optimizado para procesar solicitudes relacionadas con los comandos:\n* `cat`\n* `ls`\n* `cd`\n\n## Uso\n\nA continuaci\u00f3n, se muestra un ejemplo de c\u00f3mo usar `es2bash-mt5` con la biblioteca Hugging Face Transformers:\n\n```python\nfrom transformers import pipeline\n\ntranslator = pipeline('translation', model='dev2bit/es2bash-mt5')\n\nrequest = \"listar los archivos en el directorio actual\"\ntranslated = translator(request, max_length=512)\nprint(translated[0]['translation_text'])\n```\n\nEsto imprimir\u00e1 el comando Bash correspondiente a la solicitud dada en espa\u00f1ol.\n\n## Contribuciones\n\nAgradecemos sus contribuciones! Puede ayudar a mejorar es2bash-mt5 de varias formas, incluyendo:\n\n* Probar el modelo y reportar cualquier problema o sugerencia en la secci\u00f3n de Issues.\n* Mejorando la documentaci\u00f3n.\n* Proporcionando ejemplos de uso.\n\n---\n\nThis model is a fine-tuned version of [google/mt5-small](https://huggingface.co/google/mt5-small) on the es2bash dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0919\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.1\n- train_batch_size: 8\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 28\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss |\n|:-------------:|:-----:|:-----:|:---------------:|\n| 21.394        | 1.0   | 672   | 1.7470          |\n| 2.5294        | 2.0   | 1344  | 0.6350          |\n| 0.5873        | 3.0   | 2016  | 0.2996          |\n| 0.3802        | 4.0   | 2688  | 0.2142          |\n| 0.2951        | 5.0   | 3360  | 0.1806          |\n| 0.225         | 6.0   | 4032  | 0.1565          |\n| 0.2065        | 7.0   | 4704  | 0.1461          |\n| 0.1944        | 8.0   | 5376  | 0.1343          |\n| 0.174         | 9.0   | 6048  | 0.1281          |\n| 0.1647        | 10.0  | 6720  | 0.1207          |\n| 0.1566        | 11.0  | 7392  | 0.1140          |\n| 0.1498        | 12.0  | 8064  | 0.1106          |\n| 0.1382        | 13.0  | 8736  | 0.1076          |\n| 0.1393        | 14.0  | 9408  | 0.1042          |\n| 0.1351        | 15.0  | 10080 | 0.1019          |\n| 0.13          | 16.0  | 10752 | 0.0998          |\n| 0.1292        | 17.0  | 11424 | 0.0983          |\n| 0.1265        | 18.0  | 12096 | 0.0973          |\n| 0.1255        | 19.0  | 12768 | 0.0969          |\n| 0.1216        | 20.0  | 13440 | 0.0956          |\n| 0.1216        | 21.0  | 14112 | 0.0946          |\n| 0.123         | 22.0  | 14784 | 0.0938          |\n| 0.113         | 23.0  | 15456 | 0.0931          |\n| 0.1185        | 24.0  | 16128 | 0.0929          |\n| 0.1125        | 25.0  | 16800 | 0.0927          |\n| 0.1213        | 26.0  | 17472 | 0.0925          |\n| 0.1153        | 27.0  | 18144 | 0.0921          |\n| 0.1134        | 28.0  | 18816 | 0.0919          |\n\n\n### Framework versions\n\n- Transformers 4.29.2\n- Pytorch 2.0.1+cu117\n- Datasets 2.12.0\n- Tokenizers 0.13.3\n", "size_bytes": "1200772485", "downloads": 4}
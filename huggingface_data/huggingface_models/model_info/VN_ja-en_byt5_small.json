{"pretrained_model_name": "twieland/VN_ja-en_byt5_small", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: VN_ja-en_byt5_small\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# VN_ja-en_byt5_small\n\nThis model is a fine-tuned version of [google/byt5-small](https://huggingface.co/google/byt5-small) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0552\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0003\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss |\n|:-------------:|:-----:|:-----:|:---------------:|\n| 1.1687        | 0.1   | 2000  | 1.1805          |\n| 0.9685        | 0.19  | 4000  | 1.1384          |\n| 0.8989        | 0.29  | 6000  | 1.1207          |\n| 0.8583        | 0.39  | 8000  | 1.1046          |\n| 0.833         | 0.49  | 10000 | 1.1290          |\n| 0.8102        | 0.58  | 12000 | 1.1225          |\n| 0.7932        | 0.68  | 14000 | 1.0956          |\n| 0.7776        | 0.78  | 16000 | 1.0970          |\n| 0.762         | 0.88  | 18000 | 1.0992          |\n| 0.7522        | 0.97  | 20000 | 1.0760          |\n| 0.7318        | 1.07  | 22000 | 1.0579          |\n| 0.7197        | 1.17  | 24000 | 1.0780          |\n| 0.7142        | 1.27  | 26000 | 1.0748          |\n| 0.7093        | 1.36  | 28000 | 1.0781          |\n| 0.7005        | 1.46  | 30000 | 1.0756          |\n| 0.6938        | 1.56  | 32000 | 1.0702          |\n| 0.6896        | 1.65  | 34000 | 1.0563          |\n| 0.6846        | 1.75  | 36000 | 1.0603          |\n| 0.6807        | 1.85  | 38000 | 1.0626          |\n| 0.6766        | 1.95  | 40000 | 1.0666          |\n| 0.6649        | 2.04  | 42000 | 1.0694          |\n| 0.6532        | 2.14  | 44000 | 1.0564          |\n| 0.6501        | 2.24  | 46000 | 1.0715          |\n| 0.6476        | 2.34  | 48000 | 1.0551          |\n| 0.646         | 2.43  | 50000 | 1.0601          |\n| 0.6445        | 2.53  | 52000 | 1.0595          |\n| 0.6404        | 2.63  | 54000 | 1.0494          |\n| 0.6378        | 2.72  | 56000 | 1.0584          |\n| 0.636         | 2.82  | 58000 | 1.0531          |\n| 0.6345        | 2.92  | 60000 | 1.0552          |\n\n\n### Framework versions\n\n- Transformers 4.19.2\n- Pytorch 1.11.0+cu113\n- Datasets 2.2.2\n- Tokenizers 0.12.1\n", "size_bytes": "1198608045", "downloads": 2}
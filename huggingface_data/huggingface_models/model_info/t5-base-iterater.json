{"pretrained_model_name": "mrm8488/t5-base-iterater", "description": "---\nlicense: apache-2.0\nlanguage:\n - en\ndatasets:\n- wanyu/IteraTeR_full_sent\ntags:\n- generated_from_trainer\n- IteraTeR\n\nwidget:\n - text: \"<clarity>  Delay-based schemes have the potential to resolve this last packet problem by scheduling the link based on the delay for the packet has encountered.\"\n\nmodel-index:\n- name: t5-base-iterater\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# T5 (base) fine-tuned on IteraTeR\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on an [IteraTeR](https://huggingface.co/datasets/wanyu/IteraTeR_full_sent) dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2580\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss |\n|:-------------:|:-----:|:-----:|:---------------:|\n| 0.3286        | 0.09  | 2000  | 0.3010          |\n| 0.3194        | 0.18  | 4000  | 0.2872          |\n| 0.3208        | 0.27  | 6000  | 0.2792          |\n| 0.3091        | 0.36  | 8000  | 0.2731          |\n| 0.3164        | 0.45  | 10000 | 0.2678          |\n| 0.2941        | 0.54  | 12000 | 0.2682          |\n| 0.2981        | 0.63  | 14000 | 0.2696          |\n| 0.2975        | 0.72  | 16000 | 0.2643          |\n| 0.3109        | 0.81  | 18000 | 0.2624          |\n| 0.2965        | 0.9   | 20000 | 0.2648          |\n| 0.3053        | 0.99  | 22000 | 0.2627          |\n| 0.2779        | 1.08  | 24000 | 0.2632          |\n| 0.2692        | 1.17  | 26000 | 0.2608          |\n| 0.2755        | 1.26  | 28000 | 0.2600          |\n| 0.2771        | 1.35  | 30000 | 0.2584          |\n| 0.2774        | 1.44  | 32000 | 0.2609          |\n| 0.2976        | 1.53  | 34000 | 0.2593          |\n| 0.2646        | 1.62  | 36000 | 0.2616          |\n| 0.2705        | 1.71  | 38000 | 0.2574          |\n| 0.2714        | 1.8   | 40000 | 0.2577          |\n| 0.2857        | 1.9   | 42000 | 0.2576          |\n| 0.2832        | 1.99  | 44000 | 0.2580          |\n\n\n### How to use\n\n```py\nfrom transformers import T5ForConditionalGeneration, T5TokenizerFast\nMODEL_CKPT = 'mrm8488/t5-base-iterater'\n\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_CKPT)\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_CKPT)\n\ndef predict(intent, text):\n   input_text =  f\"<{intent}>  {text}\"\n   features = tokenizer([input_text], return_tensors='pt')\n   output = model.generate(input_ids=features['input_ids'], \n               attention_mask=features['attention_mask'], max_length=128, num_beams=8)\n   return tokenizer.decode(output[0], skip_special_tokens=True)\n   \ntext = \"Delay-based schemes have the potential to resolve this last packet problem by scheduling the link based on the delay for the packet has encountered.\"\nintent = \"clarity\"\n\npredict(intent, text)\n# Delay-based schemes have the potential to resolve this last packet problem by scheduling the link based on the delay the packet has encountered.\n\n```\n\n### Framework versions\n\n- Transformers 4.18.0.dev0\n- Pytorch 1.10.0+cu111\n- Datasets 2.0.0\n- Tokenizers 0.11.6\n", "size_bytes": "891700799", "downloads": 14}
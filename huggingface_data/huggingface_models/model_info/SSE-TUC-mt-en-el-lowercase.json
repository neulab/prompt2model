{"pretrained_model_name": "lighteternal/SSE-TUC-mt-en-el-lowercase", "description": "---\nlanguage:\n- en\n- el\ntags:\n- translation\nwidget:\n- text: \"Not all those who wander are lost.\"\nlicense: apache-2.0\nmetrics:\n- bleu\n---\n\n## English to Greek NMT (lower-case output)\n## By the Hellenic Army Academy (SSE) and the Technical University of Crete (TUC)\n\n* source languages: en\n* target languages: el\n* licence: apache-2.0\n* dataset: Opus, CCmatrix\n* model: transformer(fairseq)\n* pre-processing: tokenization + lower-casing + BPE segmentation\n* metrics: bleu, chrf\n* output: lowercase only, for mixed-cased model use this: https://huggingface.co/lighteternal/SSE-TUC-mt-en-el-cased\n\n### Model description\n\nTrained using the Fairseq framework, transformer_iwslt_de_en architecture.\\\\\nBPE segmentation (10k codes).\\\\\nLower-case model. \n\n### How to use\n\n```\nfrom transformers import FSMTTokenizer, FSMTForConditionalGeneration\n\nmname = \" <your_downloaded_model_folderpath_here> \"\n\ntokenizer = FSMTTokenizer.from_pretrained(mname)\nmodel = FSMTForConditionalGeneration.from_pretrained(mname)\n\ntext = \"Not all those who wander are lost.\"\n\nencoded = tokenizer.encode(text, return_tensors='pt')\n\noutputs = model.generate(encoded, num_beams=5, num_return_sequences=5, early_stopping=True)\nfor i, output in enumerate(outputs):\n    i += 1\n    print(f\"{i}: {output.tolist()}\")\n    \n    decoded = tokenizer.decode(output, skip_special_tokens=True)\n    print(f\"{i}: {decoded}\")\n```\n\n\n## Training data\n\nConsolidated corpus from Opus and CC-Matrix (~6.6GB in total)\n\n\n## Eval results\n\n\nResults on Tatoeba testset (EN-EL): \n\n| BLEU | chrF  |\n| ------ | ------ |\n| 77.3 |  0.739 |\n\n\nResults on XNLI parallel (EN-EL): \n\n| BLEU | chrF  |\n| ------ | ------ |\n| 66.1 |  0.606 |\n\n### BibTeX entry and citation info\n\nDimitris Papadopoulos, et al. \"PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation.\" (2021). Accepted at EACL 2021 SRW\n \n\n### Acknowledgement\n\nThe research work was supported by the Hellenic Foundation for Research and Innovation (HFRI) under the HFRI PhD Fellowship grant (Fellowship Number:50, 2nd call)\n", "size_bytes": "172976478", "downloads": 4}
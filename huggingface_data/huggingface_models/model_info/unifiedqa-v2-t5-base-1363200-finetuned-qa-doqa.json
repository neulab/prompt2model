{"pretrained_model_name": "makanju0la/unifiedqa-v2-t5-base-1363200-finetuned-qa-doqa", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: unifiedqa-v2-t5-base-1363200-finetuned-qa-doqa\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# unifiedqa-v2-t5-base-1363200-finetuned-qa-doqa\n\nThis model is a fine-tuned version of [allenai/unifiedqa-v2-t5-base-1363200](https://huggingface.co/allenai/unifiedqa-v2-t5-base-1363200) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3126\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 2\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 0.7767        | 0.05  | 115  | 0.4286          |\n| 0.5099        | 0.1   | 230  | 0.3798          |\n| 0.4811        | 0.15  | 345  | 0.3663          |\n| 0.3736        | 0.2   | 460  | 0.3412          |\n| 0.3717        | 0.25  | 575  | 0.3333          |\n| 0.3849        | 0.3   | 690  | 0.3532          |\n| 0.4335        | 0.35  | 805  | 0.3266          |\n| 0.3628        | 0.4   | 920  | 0.3149          |\n| 0.3419        | 0.45  | 1035 | 0.3244          |\n| 0.3268        | 0.5   | 1150 | 0.3359          |\n| 0.3534        | 0.55  | 1265 | 0.3280          |\n| 0.3806        | 0.6   | 1380 | 0.3274          |\n| 0.3566        | 0.65  | 1495 | 0.3279          |\n| 0.4598        | 0.7   | 1610 | 0.3082          |\n| 0.3416        | 0.75  | 1725 | 0.3210          |\n| 0.3154        | 0.8   | 1840 | 0.3064          |\n| 0.3165        | 0.85  | 1955 | 0.3045          |\n| 0.2985        | 0.9   | 2070 | 0.3126          |\n| 0.327         | 0.95  | 2185 | 0.3090          |\n| 0.3246        | 1.0   | 2300 | 0.3122          |\n| 0.2495        | 1.05  | 2415 | 0.3105          |\n| 0.2271        | 1.1   | 2530 | 0.3143          |\n| 0.2611        | 1.15  | 2645 | 0.3149          |\n| 0.2819        | 1.2   | 2760 | 0.3140          |\n| 0.2583        | 1.25  | 2875 | 0.3072          |\n| 0.2708        | 1.3   | 2990 | 0.3112          |\n| 0.264         | 1.35  | 3105 | 0.3153          |\n| 0.2533        | 1.4   | 3220 | 0.3103          |\n| 0.2192        | 1.45  | 3335 | 0.3160          |\n| 0.2568        | 1.5   | 3450 | 0.3170          |\n| 0.2692        | 1.55  | 3565 | 0.3148          |\n| 0.2501        | 1.6   | 3680 | 0.3112          |\n| 0.2205        | 1.65  | 3795 | 0.3200          |\n| 0.2896        | 1.7   | 3910 | 0.3196          |\n| 0.2546        | 1.75  | 4025 | 0.3160          |\n| 0.2857        | 1.8   | 4140 | 0.3122          |\n| 0.2813        | 1.85  | 4255 | 0.3133          |\n| 0.2395        | 1.9   | 4370 | 0.3118          |\n| 0.2523        | 1.95  | 4485 | 0.3126          |\n| 0.25          | 2.0   | 4600 | 0.3126          |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.13.1+cu116\n- Datasets 2.10.1\n- Tokenizers 0.13.2\n", "size_bytes": "891702929", "downloads": 2}
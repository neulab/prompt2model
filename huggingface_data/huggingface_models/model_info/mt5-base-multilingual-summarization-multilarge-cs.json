{"pretrained_model_name": "ctu-aic/mt5-base-multilingual-summarization-multilarge-cs", "description": "---\nlanguage:\n- cs\n- en\n- de\n- fr\n- tu\n- zh\n- es\n- ru\ntags:\n- Summarization\n- abstractive summarization\n- mt5-base\n- Czech\n- text2text generation\n- text generation\nlicense: cc-by-sa-4.0\ndatasets:\n- Multilingual_large_dataset_(multilarge)\n- cnc/dm\n- xsum\n- mlsum\n- cnewsum\n- cnc\n- sumeczech\nmetrics:\n- rouge\n- rougeraw\n- MemesCS\n---\n# mt5-base-multilingual-summarization-multilarge-cs\nThis model is a fine-tuned checkpoint of [google/mt5-base](https://huggingface.co/google/mt5-base) on the Multilingual large summarization dataset focused on Czech texts to produce multilingual summaries. \n## Task\nThe model deals with a multi-sentence summary in eight different languages. With the idea of adding other foreign language documents, and by having a considerable amount of Czech documents, we aimed to improve model summarization in the Czech language. Supported languages: ```'cs': '<extra_id_0>', 'en': '<extra_id_1>','de': '<extra_id_2>',  'es': '<extra_id_3>', 'fr': '<extra_id_4>', 'ru': '<extra_id_5>', 'tu': '<extra_id_6>',  'zh': '<extra_id_7>'```\n\n#Usage\n\n```python\n\n## Configuration of summarization pipeline\n#\ndef summ_config():\n    cfg = OrderedDict([\n        \n        ## summarization model - checkpoint\n        #   ctu-aic/m2m100-418M-multilingual-summarization-multilarge-cs\n        #   ctu-aic/mt5-base-multilingual-summarization-multilarge-cs\n        #   ctu-aic/mbart25-multilingual-summarization-multilarge-cs\n        (\"model_name\", \"ctu-aic/mbart25-multilingual-summarization-multilarge-cs\"),\n        \n        ## language of summarization task\n        #   language : string : cs, en, de, fr, es, tr, ru, zh\n        (\"language\", \"en\"), \n        \n        ## generation method parameters in dictionary\n        #\n        (\"inference_cfg\", OrderedDict([\n            (\"num_beams\", 4),\n            (\"top_k\", 40),\n            (\"top_p\", 0.92),\n            (\"do_sample\", True),\n            (\"temperature\", 0.95),\n            (\"repetition_penalty\", 1.23),\n            (\"no_repeat_ngram_size\", None),\n            (\"early_stopping\", True),\n            (\"max_length\", 128),\n            (\"min_length\", 10),\n        ])),\n        #texts to summarize values = (list of strings, string, dataset)\n        (\"texts\",\n            [\n               \"english text1 to summarize\",\n               \"english text2 to summarize\",\n            ]\n        ),\n        #OPTIONAL: Target summaries values = (list of strings, string, None)\n        ('golds',\n         [\n               \"target english text1\",\n               \"target english text2\",\n         ]),\n        #('golds', None),\n    ])\n    return cfg\n\ncfg = summ_config()\nmSummarize = MultiSummarizer(**cfg)\nsummaries,scores = mSummarize(**cfg)\n\n```\n\n\n\n## Dataset\nMultilingual large summarization dataset consists of 10 sub-datasets mainly based on news and daily mails. For the training, it was used the entire training set and 72% of the validation set.\n```\nTrain set:        3 464 563 docs\nValidation set:     121 260 docs\n```\n| Stats       | fragment |  | | avg document length |   | avg summary length  |  | Documents |\n|-------------|----------|---------------------|--------------------|--------|---------|--------|--------|--------|\n|  __dataset__   |__compression__ | __density__  | __coverage__            | __nsent__              | __nwords__ | __nsent__   | __nwords__ | __count__ |\n| cnc      | 7.388    | 0.303               | 0.088              | 16.121 | 316.912 | 3.272  | 46.805 | 750K |\n| sumeczech   | 11.769   | 0.471               | 0.115              | 27.857 | 415.711 | 2.765  | 38.644 | 1M |\n| cnndm       | 13.688   | 2.983               | 0.538              | 32.783 | 676.026 | 4.134  | 54.036 | 300K |\n| xsum        | 18.378   | 0.479               | 0.194              | 18.607 | 369.134 | 1.000  | 21.127 | 225K|\n| mlsum/tu    | 8.666    | 5.418               | 0.461              | 14.271 | 214.496 | 1.793  | 25.675 | 274K |\n| mlsum/de    | 24.741   | 8.235               | 0.469              | 32.544 | 539.653 | 1.951  | 23.077 | 243K|\n| mlsum/fr    | 24.388   | 2.688               | 0.424              | 24.533 | 612.080 | 1.320  | 26.93  | 425K |\n| mlsum/es    | 36.185   | 3.705               | 0.510              | 31.914 | 746.927 | 1.142  | 21.671 | 291K |\n| mlsum/ru    | 78.909   | 1.194               | 0.246              | 62.141 | 948.079 | 1.012  | 11.976 | 27K|\n| cnewsum     | 20.183   | 0.000               | 0.000              | 16.834 | 438.271 | 1.109  | 21.926 | 304K |\n#### Tokenization\nTruncation and padding were set to 512 tokens for the encoder (input text) and 128 for the decoder (summary). \n## Training\nTrained based on cross-entropy loss.\n```\nTime: 3 days 20 hours\nEpochs: 1080K steps = 10 (from 10)\nGPUs: 4x NVIDIA A100-SXM4-40GB\neloss: 2.462 - 1.797\ntloss: 17.322 - 1.578\n```\n### ROUGE results per individual dataset test set:\n\n| ROUGE     | ROUGE-1 |  |    | ROUGE-2 |  |     | ROUGE-L |  |  |\n|-----------|---------|---------|-----------|--------|--------|-----------|--------|--------|---------|\n|      |Precision | Recall  | Fscore  | Precision | Recall | Fscore | Precision | Recall | Fscore |\n| cnc  | 30.62   | 19.83   | 23.44     | 9.94   | 6.52   | 7.67      | 22.92  | 14.92  | 17.6    |\n| sumeczech | 27.57   | 17.6    | 20.85     | 8.12   | 5.23   | 6.17      | 20.84  | 13.38  | 15.81   |\n| cnndm    | 43.83   | 37.73   | 39.34     | 20.81  | 17.82  | 18.6      | 31.8   | 27.42  | 28.55   |\n| xsum     | 41.63   | 30.54   | 34.56     | 16.13  | 11.76  | 13.33     | 33.65  | 24.74  | 27.97   |\n| mlsum-tu- | 54.4    | 43.29   | 46.2      | 38.78  | 31.31  | 33.23     | 48.18  | 38.44  | 41      |\n| mlsum-de  | 47.94   | 44.14   | 45.11     | 36.42  | 35.24  | 35.42     | 44.43  | 41.42  | 42.16   |\n| mlsum-fr  | 35.26   | 25.96   | 28.98     | 16.72  | 12.35  | 13.75     | 28.06  | 20.75  | 23.12   |\n| mlsum-es  | 33.37   | 24.84   | 27.52     | 13.29  | 10.05  | 11.05     | 27.63  | 20.69  | 22.87   |\n| mlsum-ru | 0.79    | 0.66    | 0.66      | 0.26   | 0.2    | 0.22      | 0.79   | 0.66   | 0.65    |\n| cnewsum  | 24.49   | 24.38   | 23.23     | 6.48   | 6.7    | 6.24      | 24.18  | 24.04  | 22.91   |\n\n# USAGE\n```\nsoon\n```", "size_bytes": "2329700301", "downloads": 70}
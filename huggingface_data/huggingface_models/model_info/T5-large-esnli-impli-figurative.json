{"pretrained_model_name": "Rachneet/T5-large-esnli-impli-figurative", "description": "# load model\n\n```\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    AutoModelForSeq2SeqLM\n    )\n\nmodel_path = \"T5-large-esnli-impli-figurative\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nconfig = AutoConfig.from_pretrained(model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npremise = \"I just caught a guy picking up used chewing gum and he put it in his mouth.\"\nhypothesis = \"it was such a pleasant sight to see a guy picking up used chewing gum; and he put it in his mouth\"\nprepared_input = f\"figurative hypothesis: {hypothesis} premise: {premise}\"\nfeatures = tokenizer(prepared_input, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nmodel.to(device)\nwith torch.no_grad():\n    # https://huggingface.co/blog/how-to-generate\n    generated_ids = model.generate(\n        **features,\n        max_length=128,\n        use_cache=True,\n        num_beams=4,\n        length_penalty=0.6,\n        early_stopping=True,\n    )\ndec_preds = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"The prediction is: \", dec_preds)\nprint(dec_preds[1:].replace(\"explanation:\", \"\").lstrip())\n\n```\n\n# Example input\n\nfigurative  hypothesis: I was gone for only a few days and my considerate adult son just let the sink fill up with dirty dishes, making me feel really happy premise: I left my adult son home for a few days and just came back to a sink full of gross old dishes.", "size_bytes": "2950844807", "downloads": 18}
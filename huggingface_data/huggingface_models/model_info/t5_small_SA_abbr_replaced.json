{"pretrained_model_name": "ParastooC/t5_small_SA_abbr_replaced", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: t5_small_SA_abbr_replaced\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5_small_SA_abbr_replaced\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5633\n- Rouge1: 0.2325\n- Rouge2: 0.1112\n- Rougel: 0.2228\n- Rougelsum: 0.2228\n- Gen Len: 13.115\n- Bert Score F1: 0.8407\n- Bert Score Precision: 0.8551\n- Bert Score Recall: 0.8282\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 12\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len | Bert Score F1 | Bert Score Precision | Bert Score Recall |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|:---------:|:-------:|:-------------:|:--------------------:|:-----------------:|\n| 0.6839        | 0.95  | 500  | 0.6228          | 0.1495 | 0.0654 | 0.1433 | 0.1421    | 10.3894 | 0.6689        | 0.6835               | 0.656             |\n| 0.5918        | 1.9   | 1000 | 0.6084          | 0.1965 | 0.0775 | 0.1893 | 0.1904    | 13.6991 | 0.8138        | 0.827                | 0.8024            |\n| 0.5917        | 2.85  | 1500 | 0.5943          | 0.2075 | 0.0804 | 0.1975 | 0.1982    | 12.5841 | 0.8293        | 0.8452               | 0.8153            |\n| 0.5524        | 3.8   | 2000 | 0.5859          | 0.2104 | 0.0856 | 0.1994 | 0.2       | 13.4602 | 0.8373        | 0.8523               | 0.8242            |\n| 0.5635        | 4.74  | 2500 | 0.5830          | 0.2069 | 0.0905 | 0.1974 | 0.1984    | 12.1327 | 0.8379        | 0.8549               | 0.8231            |\n| 0.5455        | 5.69  | 3000 | 0.5756          | 0.2113 | 0.0963 | 0.2038 | 0.2032    | 12.6903 | 0.8398        | 0.8545               | 0.8269            |\n| 0.5156        | 6.64  | 3500 | 0.5710          | 0.2147 | 0.0993 | 0.2093 | 0.2096    | 13.115  | 0.8393        | 0.8527               | 0.8278            |\n| 0.5134        | 7.59  | 4000 | 0.5683          | 0.2252 | 0.105  | 0.216  | 0.2168    | 12.4513 | 0.8418        | 0.8572               | 0.8285            |\n| 0.5381        | 8.54  | 4500 | 0.5661          | 0.2228 | 0.1051 | 0.215  | 0.2151    | 13.4602 | 0.8401        | 0.8534               | 0.8287            |\n| 0.5092        | 9.49  | 5000 | 0.5643          | 0.2186 | 0.1027 | 0.2135 | 0.2132    | 13.1593 | 0.8385        | 0.8524               | 0.8265            |\n| 0.5181        | 10.44 | 5500 | 0.5643          | 0.2299 | 0.1087 | 0.2196 | 0.2202    | 13.0    | 0.8398        | 0.8543               | 0.8273            |\n| 0.4824        | 11.39 | 6000 | 0.5633          | 0.2325 | 0.1112 | 0.2228 | 0.2228    | 13.115  | 0.8407        | 0.8551               | 0.8282            |\n\n\n### Framework versions\n\n- Transformers 4.27.2\n- Pytorch 1.13.1+cu116\n- Datasets 2.10.1\n- Tokenizers 0.13.2\n", "size_bytes": "242071641", "downloads": 2}
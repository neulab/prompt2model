{"pretrained_model_name": "pszemraj/bart-large-code-instructiongen", "description": "---\nlicense:\n- apache-2.0\n- cc-by-nc-4.0\ndatasets: pszemraj/fleece2instructions-codealpaca\ntags:\n- generated_from_trainer\n- instruct\n- instructions\n- code\n- instructiongen\nmetrics:\n- rouge\nlanguage:\n- en\nwidget:\n- text: |\n    git lfs install\n    huggingface-cli lfs-enable-largefiles .\n    git lfs track \"*.bin\"\n    git add .\n    git commit -a -m \"add fp32 chkpt\"\n    git push\n  example_title: bash\n- text: |\n    export interface DocumentParams {\n      pageContent: string;\n\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      metadata: Record<string, any>;\n    }\n\n    /**\n     * Interface for interacting with a document.\n     */\n    export class Document implements DocumentParams {\n      pageContent: string;\n\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      metadata: Record<string, any>;\n\n      constructor(fields?: Partial<DocumentParams>) {\n        this.pageContent = fields?.pageContent ?? this.pageContent;\n        this.metadata = fields?.metadata ?? {};\n      }\n    }\n  example_title: js\n- text: |\n    def merge(left, right):\n        if len(left) == 0:\n            return right\n\n        if len(right) == 0:\n            return left\n\n        result = []\n        index_left = index_right = 0\n\n        while len(result) < len(left) + len(right):\n            if left[index_left] <= right[index_right]:\n                result.append(left[index_left])\n                index_left += 1\n            else:\n                result.append(right[index_right])\n                index_right += 1\n\n            if index_right == len(right):\n                result += left[index_left:]\n                break\n\n            if index_left == len(left):\n                result += right[index_right:]\n                break\n\n        return result\n  example_title: merge\n- text: >\n    import pandas as pd\n\n    import plotly.graph_objects as go\n\n\n    df =\n    pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_apple_stock.csv')\n\n\n    fig = go.Figure(go.Scatter(x = df['AAPL_x'], y = df['AAPL_y'],\n                      name='Share Prices (in USD)'))\n\n    fig.update_layout(title='Apple Share Prices over time (2014)',\n                       plot_bgcolor='rgb(230, 230,230)',\n                       showlegend=True)\n\n    fig.show()\n  example_title: plot\n- text: |\n    from spellchecker import SpellChecker\n\n    spell = SpellChecker()\n\n    def check_word_spelling(word: str):\n        misspelled = spell.unknown([word])\n        return len(misspelled) == 0\n\n    def eval_and_replace(text: str, match_token: str = \"- \"):\n        if match_token not in text:\n            return text\n        else:\n            while True:\n                full_before_text = text.split(match_token, maxsplit=1)[0]\n                before_text = [\n                    char for char in full_before_text.split()[-1] if char.isalpha()\n                ]\n                before_text = \"\".join(before_text)\n                full_after_text = text.split(match_token, maxsplit=1)[-1]\n                after_text = [char for char in full_after_text.split()[0] if char.isalpha()]\n                after_text = \"\".join(after_text)\n                full_text = before_text + after_text\n                if check_word_spelling(full_text):\n                    text = full_before_text + full_after_text\n                else:\n                    text = full_before_text + \" \" + full_after_text\n                if match_token not in text:\n                    break\n            return text\n\n    text = \"I- am- a go- od- boy\"\n    eval_and_replace(text)\n  example_title: spell check\n- text: >\n    import torch\n\n    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\n    checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\n    sequences = [\"I've been waiting for a HuggingFace course my whole life.\",\n    \"So have I!\"]\n\n\n    tokens = tokenizer(sequences, padding=True, truncation=True,\n    return_tensors=\"pt\")\n\n    output = model(**tokens)\n  example_title: model inference\ninference:\n  parameters:\n    max_length: 96\n    num_beams: 4\n---\n\n\n# bart-large-code-instructiongen\n\nUse this text2text model to find out what LLM instructions might be able to generate an arbitary piece of code!\n\n- Check out a [basic demo on Spaces](https://huggingface.co/spaces/pszemraj/generate-instructions)\n- An example of how to use instructiongen models in a CLI script can be found [here](https://gist.github.com/pszemraj/8b0213e700763106074d3ac15d041c14)\n- You can find other models fine-tuned for instruction generation by [searching for the instructiongen tag](https://huggingface.co/models?other=instructiongen)\n\n## about \n\nThis model is a fine-tuned version of [facebook/bart-large](https://huggingface.co/facebook/bart-large) on the `pszemraj/fleece2instructions-codealpaca` dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9222\n- Rouge1: 62.0692\n- Rouge2: 36.1947\n- Rougel: 57.5128\n- Rougelsum: 58.6613\n- Gen Len: 31.0060\n\n\n## Intended uses & limitations\n\n\ud83d\udea8 **note:** as the authors elected to release the [original dataset](https://github.com/sahil280114/codealpaca) under `cc-by-nc`, the license carries over to this model and **cannot be used for commercial activity**. \n\nIntended use: Research on domain adaptation and/or other improvements to LLMs by extending instruction:text data pairs.\n\n## Training and evaluation data\n\nRefer to the linked dataset card for `pszemraj/fleece2instructions-codealpaca` or the [original dataset](https://github.com/sahil280114/codealpaca) repo.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 6e-05\n- train_batch_size: 16\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.03\n- num_epochs: 3.0\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.0914        | 1.0   | 563  | 1.0303          | 60.288  | 34.1884 | 55.9293 | 57.0714   | 30.6267 |\n| 0.8688        | 2.0   | 1126 | 0.9333          | 61.0409 | 34.9823 | 56.4887 | 57.6662   | 31.7255 |\n| 0.6773        | 3.0   | 1689 | 0.9222          | 62.0692 | 36.1947 | 57.5128 | 58.6613   | 31.0060 |", "size_bytes": "1625534221", "downloads": 38}
{"pretrained_model_name": "Langboat/mengzi-t5-base-mt", "description": "---\nlanguage: \n  - zh\nlicense: apache-2.0\nwidget:\n- text: \"\u201c\u623f\u95f4\u5f88\u4e00\u822c\uff0c\u5c0f\uff0c\u4e14\u8ba9\u4eba\u611f\u89c9\u810f\uff0c\u9694\u97f3\u6548\u679c\u5dee\uff0c\u80fd\u542c\u5230\u8d70\u5eca\u7684\u4eba\u8bb2\u8bdd\uff0c\u8d70\u5eca\u5149\u7ebf\u660f\u6697\uff0c\u65c1\u8fb9\u6ca1\u6709\u4ec0\u4e48\u53ef\u5403\u201d \u8fd9\u6761\u8bc4\u8bba\u7684\u6001\u5ea6\u662f\u4ec0\u4e48\uff1f\"\n---\n\n# Mengzi-T5-MT model\nThis is a Multi-Task model trained on the multitask mixture of 27 datasets and 301 prompts, based on [Mengzi-T5-base](https://huggingface.co/Langboat/mengzi-t5-base).\n\n[Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese](https://arxiv.org/abs/2110.06696)\n\n## Usage\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained(\"Langboat/mengzi-t5-base-mt\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"Langboat/mengzi-t5-base-mt\")\n```\n\n## Citation\nIf you find the technical report or resource is useful, please cite the following technical report in your paper.\n```\n@misc{zhang2021mengzi,\n      title={Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese}, \n      author={Zhuosheng Zhang and Hanqing Zhang and Keming Chen and Yuhang Guo and Jingyun Hua and Yulong Wang and Ming Zhou},\n      year={2021},\n      eprint={2110.06696},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "size_bytes": "495234369", "downloads": 283}
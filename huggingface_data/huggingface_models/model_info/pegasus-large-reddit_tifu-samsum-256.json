{"pretrained_model_name": "jpcorb20/pegasus-large-reddit_tifu-samsum-256", "description": "---\nlanguage: \n- en\nthumbnail: \ntags:\n- pytorch\n- google/pegasus-reddit_tifu\n- summarization\n- samsum\nlicense: \ndatasets:\n- samsum\nmetrics:\n- rouge\n---\n\n# Samsum Pegasus (Reddit/TIFU) for conversational summaries\n\n## Model description\n\nPegasus (Reddit/TIFU) for conversational summaries trained on the samsum dataset!\n\n## Training data\n\nThe data is the [samsum](https://huggingface.co/datasets/samsum) dataset for conversional summaries.\n\nThe initial weigths were from the [google/pegasus-reddit_tifu](https://huggingface.co/google/pegasus-reddit_tifu). The hypothesis being that it would help the convergence on the samsum dataset to have weights trained on a larger summarization dataset first like the Reddit TIFU using casual language.\n\n## Training procedure\n\nUsed the _example/seq2seq/run_summarization.py_ script from the transformers source _4.5.0dev0_.\n\n  n_epochs: 3,\\\n  batch_size: 8, \\\n  max_source_length: 256,\\\n  max_target_length: 128\n\n## Eval results\n\n    eval_gen_len: 35.9939,\\\n    eval_loss: 1.4284523725509644,\\\n    eval_rouge1: 46.5613,\\\n    eval_rouge2: 23.6137,\\\n    eval_rougeL: 37.2397,\\\n    eval_rougeLsum: 42.7126,\\\n    eval_samples_per_second: 4.302\n    \n## Example\n\n    from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n    \n    model_name = \"jpcorb20/pegasus-large-reddit_tifu-samsum-256\"\n    \n    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n    model = PegasusForConditionalGeneration.from_pretrained(model_name)\n    \n    src_text = \"\"\"Carter: Hey Alexis, I just wanted to let you know that I had a really nice time with you tonight.\\r\\nAlexis: Thanks Carter. Yeah, I really enjoyed myself as well.\\r\\nCarter: If you are up for it, I would really like to see you again soon.\\r\\nAlexis: Thanks Carter, I'm flattered. But I have a really busy week coming up.\\r\\nCarter: Yeah, no worries. I totally understand. But if you ever want to go grab dinner again, just let me know.\\r\\nAlexis: Yeah of course. Thanks again for tonight. Carter: Sure. Have a great night.\\r\\n\"\"\"\n    \n    token_params = dict(max_length=256, truncation=True, padding='longest', return_tensors=\"pt\")\n    batch = tokenizer(src_text, **token_params)\n    \n    translated = model.generate(**batch)\n    \n    decode_params = dict(num_beams=5, min_length=16, max_length=128, length_penalty=2)\n    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True, **decode_params)\n    \n    print(tgt_text)", "size_bytes": "2279647383", "downloads": 20}
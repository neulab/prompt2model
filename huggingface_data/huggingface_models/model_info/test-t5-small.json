{"pretrained_model_name": "pere/test-t5-small", "description": "---\nlanguage: \n- en\n- fr\n- ro\n- de\ndatasets:\n- c4\ntags:\n- summarization\n- translation\n\nlicense: apache-2.0\n---\n\n## Test T5 small conversion\nThis is a test repo for the conversion of T5X to HuggingFace Flax. \n\nThe current model is first converted from MTF to T5X using the conversion script included in the T5X library:\n\n```bash\npython3 -m t5x.scripts.convert_tf_checkpoint  --gin_file=t5x/examples/t5/t5_1_0/small.gin --gin.convert_checkpoint.model=%MODEL --gin.convert_checkpoint.tf_checkpoint_path=\\\"gs://t5-data/pretrained_models/small/model.ckpt-1000000\\\" --gin.convert_checkpoint.output_dir=\\\"/tmp/t5x_checkpoints/t5_small\\\" --logtostderr\n```\n\nAfter creating the T5X model, the model is converted to Huggingface Flax by a modified version of the script from @stefan-it (https://gist.githubusercontent.com/stefan-it/30e4998ef159f33696e377a46f699d9f/raw/c19da5d067dc9d31d0b8115a79e8626186e11daa/convert_t5x_checkpoint_to_flax.py). The modified version is included in this repo. The modification is basically that the wi_0 and wi_1 layers are combined into wi. This might be a difference between t5_1_0 and t5_1_1\n\n```bash\npython3 convert_t5_checkpoint_to_flax.py --t5x_checkpoint_path /tmp/t5x_checkpoints/t5_small/checkpoint_1000000/ --flax_dump_folder_path /tmp/flax_dump_folder/ --config_name t5-small\n```\n\nThe tokenizer.json was copied from https://huggingface.co/t5-small/blob/main/tokenizer.json.\n\nTo be able to use the widgets in HuggingFace, the model was converted to pyTorch by running:\n```python\nfrom transformers import T5ForConditionalGeneration\nmodel = \n\n\nT5ForConditionalGeneration.from_pretrained(\".\", from_flax=True)\nmodel.save_pretrained(\".\")\n\n```\n\n", "size_bytes": "242086109", "downloads": 13}
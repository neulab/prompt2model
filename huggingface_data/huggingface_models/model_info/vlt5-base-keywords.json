{"pretrained_model_name": "Voicelab/vlt5-base-keywords", "description": "---\nlicense: cc-by-4.0\nlanguage: \n- pl\n- en\ndatasets:\n- posmac\npipeline_tag: text2text-generation\npipeline_kwargs:\n- no_repeat_ngram_size=3\n- num_beams=4\ntags:\n- keywords-generation\n- text-classifiation\n- other\nwidget:\n- text: \"Keywords: Our vlT5 model is a keyword generation model based on encoder-decoder architecture using Transformer blocks presented by google (https://huggingface.co/t5-base). The vlT5 was trained on scientific articles corpus to predict a given set of keyphrases based on the concatenation of the article\u2019s abstract and title. It generates precise, yet not always complete keyphrases that describe the content of the article based only on the abstract.\"\n  example_title: \"English 1\"\n- text: \"Keywords: Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.\"\n  example_title: \"English 2\"\n- text: \"Keywords: Prze\u0142omem w dziedzinie sztucznej inteligencji i maszynowego uczenia si\u0119 by\u0142o powstanie systemu eksperckiego Dendral na Uniwersytecie Stanforda w 1965. System ten powsta\u0142 w celu zautomatyzowania analizy i identyfikacji moleku\u0142 zwi\u0105zk\u00f3w organicznych, kt\u00f3re dotychczas nie by\u0142y znane chemikom. Wyniki bada\u0144 otrzymane dzi\u0119ki systemowi Dendral by\u0142y pierwszym w historii odkryciem dokonanym przez komputer, kt\u00f3re zosta\u0142y opublikowane w prasie specjalistycznej.\"\n  example_title: \"Polish\"\n- text: \"Keywords: El an\u00e1lisis de un economista calcula que, a pesar del aumento del gasto general, la Navidad es una p\u00e9rdida de peso muerto seg\u00fan la teor\u00eda microecon\u00f3mica ortodoxa, debido al efecto de dar regalos. Esta p\u00e9rdida se calcula como la diferencia entre lo que el donante gast\u00f3 en el art\u00edculo y lo que el receptor del regalo habr\u00eda pagado por el art\u00edculo. Se estima que en 2001, Navidad result\u00f3 en una p\u00e9rdida de peso muerto de $ 4 mil millones solo en los EE. UU.1\u200b Debido a factores de complicaci\u00f3n, este an\u00e1lisis se utiliza a veces para discutir posibles fallas en la teor\u00eda microecon\u00f3mica actual. Otras p\u00e9rdidas de peso muerto incluyen los efectos de la Navidad en el medio ambiente y el hecho de que los regalos materiales a menudo se perciben como elefantes blancos, lo que impone costos de mantenimiento y almacenamiento y contribuye al desorden.\"\n  example_title: \"Spanish\"\nmetrics:\n- f1\n- precision\n- recall\n\n---\n<img src=\"https://public.3.basecamp.com/p/rs5XqmAuF1iEuW6U7nMHcZeY/upload/download/VL-NLP-short.png\" alt=\"logo voicelab nlp\" style=\"width:300px;\"/>\n\n# Keyword Extraction from Short Texts with T5\n\n > Our vlT5 model is a keyword generation model based on encoder-decoder architecture using Transformer blocks presented by Google ([https://huggingface.co/t5-base](https://huggingface.co/t5-base)). The vlT5 was trained on scientific articles corpus to predict a given set of keyphrases based on the concatenation of the article\u2019s abstract and title. It generates precise, yet not always complete keyphrases that describe the content of the article based only on the abstract.\n\n**Keywords generated with vlT5-base-keywords:** encoder-decoder architecture, keyword generation\n\nResults on demo model (different generation method, one model per language):\n\n > Our vlT5 model is a keyword generation model based on encoder-decoder architecture using Transformer blocks presented by Google ([https://huggingface.co/t5-base](https://huggingface.co/t5-base)). The vlT5 was trained on scientific articles corpus to predict a given set of keyphrases based on the concatenation of the article\u2019s abstract and title. It generates precise, yet not always complete keyphrases that describe the content of the article based only on the abstract.\n\n**Keywords generated with vlT5-base-keywords:** encoder-decoder architecture, vlT5, keyword generation, scientific articles corpus\n\n## vlT5\n\nThe biggest advantage is the transferability of the vlT5 model, as it works well on all domains and types of text. The downside is that the text length and the number of keywords are similar to the training data: the text piece of an abstract length generates approximately 3 to 5 keywords. It works both extractive and abstractively. Longer pieces of text must be split into smaller chunks, and then propagated to the model.\n\n### Overview\n- **Language model:** [t5-base](https://huggingface.co/t5-base)   \n- **Language:** pl, en (but works relatively well with others)\n- **Training data:** POSMAC\n- **Online Demo:** Visit our online demo for better results [https://nlp-demo-1.voicelab.ai/](https://nlp-demo-1.voicelab.ai/)\n- **Paper:** [Keyword Extraction from Short Texts with a Text-To-Text Transfer Transformer, ACIIDS 2022](https://arxiv.org/abs/2209.14008)\n\n# Corpus\n\nThe model was trained on a POSMAC corpus. Polish Open Science Metadata Corpus (POSMAC) is a collection of  216,214 abstracts of scientific publications compiled in the CURLICAT project.\n\n\n| Domains                                                  | Documents | With keywords |\n| -------------------------------------------------------- | --------: | :-----------: |\n| Engineering and technical sciences                       |    58 974 |    57 165    |\n| Social sciences                                          |    58 166 |    41 799    |\n| Agricultural sciences                                    |    29 811 |    15 492    |\n| Humanities                                               |    22 755 |    11 497    |\n| Exact and natural sciences                               |    13 579 |     9 185     |\n| Humanities, Social sciences                              |    12 809 |     7 063     |\n| Medical and health sciences                              |     6 030 |     3 913     |\n| Medical and health sciences, Social sciences             |       828 |      571      |\n| Humanities, Medical and health sciences, Social sciences |       601 |      455      |\n| Engineering and technical sciences, Humanities           |       312 |      312      |\n\n# Tokenizer\n\nAs in the original plT5 implementation, the training dataset was tokenized into subwords using a sentencepiece unigram model with vocabulary size of 50k tokens. \n\n# Usage\n\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"Voicelab/vlt5-base-keywords\")\ntokenizer = T5Tokenizer.from_pretrained(\"Voicelab/vlt5-base-keywords\")\n\ntask_prefix = \"Keywords: \"\ninputs = [\n    \"Christina Katrakis, who spoke to the BBC from Vorokhta in western Ukraine, relays the account of one family, who say Russian soldiers shot at their vehicles while they were leaving their village near Chernobyl in northern Ukraine. She says the cars had white flags and signs saying they were carrying children.\",\n    \"Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.\",\n    \"Hello, I'd like to order a pizza with salami topping.\",\n]\n\nfor sample in inputs:\n    input_sequences = [task_prefix + sample]\n    input_ids = tokenizer(\n        input_sequences, return_tensors=\"pt\", truncation=True\n    ).input_ids\n    output = model.generate(input_ids, no_repeat_ngram_size=3, num_beams=4)\n    predicted = tokenizer.decode(output[0], skip_special_tokens=True)\n    print(sample, \"\\n --->\", predicted)\n\n```\n# Inference\nOur results showed that the best generation results were achieved with `no_repeat_ngram_size=3, num_beams=4`\n\n\n# Results\n\n\n\n| Method      | Rank |   Micro   |            |            | Macro |       |       |\n| ----------- | ---: | :--------: | ---------: | ---------: | :---: | ----: | ----: |\n|             |      |     P     |          R |         F1 |   P   |     R |    F1 |\n| extremeText |    1 |   0.175   |      0.038 |      0.063 | 0.007 | 0.004 | 0.005 |\n|             |    3 |   0.117   |      0.077 |      0.093 | 0.011 | 0.011 | 0.011 |\n|             |    5 |   0.090   |      0.099 |      0.094 | 0.013 | 0.016 | 0.015 |\n|             |   10 |   0.060   |      0.131 |      0.082 | 0.015 | 0.025 | 0.019 |\n| vlT5kw      |    1 | **0.345** |      0.076 |      0.124 | 0.054 | 0.047 | 0.050 |\n|             |    3 |   0.328   |      0.212 |      0.257 | 0.133 | 0.127 | 0.129 |\n|             |    5 |   0.318   | **0.237** | **0.271** | 0.143 | 0.140 | 0.141 |\n| KeyBERT     |    1 |   0.030   |      0.007 |      0.011 | 0.004 | 0.003 | 0.003 |\n|             |    3 |   0.015   |      0.010 |      0.012 | 0.006 | 0.004 | 0.005 |\n|             |    5 |   0.011   |      0.012 |      0.011 | 0.006 | 0.005 | 0.005 |\n| TermoPL     |    1 |   0.118   |      0.026 |      0.043 | 0.004 | 0.003 | 0.003 |\n|             |    3 |   0.070   |      0.046 |      0.056 | 0.006 | 0.005 | 0.006 |\n|             |    5 |   0.051   |      0.056 |      0.053 | 0.007 | 0.007 | 0.007 |\n|             |  all |   0.025   |      0.339 |      0.047 | 0.017 | 0.030 | 0.022 |\n| extremeText |    1 |   0.210   |      0.077 |      0.112 | 0.037 | 0.017 | 0.023 |\n|             |    3 |   0.139   |      0.152 |      0.145 | 0.045 | 0.042 | 0.043 |\n|             |    5 |   0.107   |      0.196 |      0.139 | 0.049 | 0.063 | 0.055 |\n|             |   10 |   0.072   |      0.262 |      0.112 | 0.041 | 0.098 | 0.058 |\n| vlT5kw      |    1 | **0.377** |      0.138 |      0.202 | 0.119 | 0.071 | 0.089 |\n|             |    3 |   0.361   |      0.301 |      0.328 | 0.185 | 0.147 | 0.164 |\n|             |    5 |   0.357   | **0.316** | **0.335** | 0.188 | 0.153 | 0.169 |\n| KeyBERT     |    1 |   0.018   |      0.007 |      0.010 | 0.003 | 0.001 | 0.001 |\n|             |    3 |   0.009   |      0.010 |      0.009 | 0.004 | 0.001 | 0.002 |\n|             |    5 |   0.007   |      0.012 |      0.009 | 0.004 | 0.001 | 0.002 |\n| TermoPL     |    1 |   0.076   |      0.028 |      0.041 | 0.002 | 0.001 | 0.001 |\n|             |    3 |   0.046   |      0.051 |      0.048 | 0.003 | 0.001 | 0.002 |\n|             |    5 |   0.033   |      0.061 |      0.043 | 0.003 | 0.001 | 0.002 |\n|             |  all |   0.021   |      0.457 |      0.040 | 0.004 | 0.008 | 0.005 |\n\n# License\n\nCC BY 4.0\n\n# Citation\n\nIf you use this model, please cite the following paper:\n[P\u0119zik, P., Miko\u0142ajczyk, A., Wawrzy\u0144ski, A., \u017barnecki, F., Nito\u0144, B., Ogrodniczuk, M. (2023). Transferable Keyword Extraction and Generation with Text-to-Text Language Models. In: Miky\u0161ka, J., de Mulatier, C., Paszynski, M., Krzhizhanovskaya, V.V., Dongarra, J.J., Sloot, P.M. (eds) Computational Science \u2013 ICCS 2023. ICCS 2023. Lecture Notes in Computer Science, vol 14074. Springer, Cham. https://doi.org/10.1007/978-3-031-36021-3_42](https://link.springer.com/chapter/10.1007/978-3-031-36021-3_42)\nOR\n[Piotr P\u0119zik, Agnieszka Miko\u0142ajczyk-Bare\u0142a, Adam Wawrzy\u0144ski, Bart\u0142omiej Nito\u0144, Maciej Ogrodniczuk, Keyword Extraction from Short Texts with a Text-To-Text Transfer Transformer, ACIIDS 2022](https://arxiv.org/abs/2209.14008)\n\n# Authors\n\nThe model was trained by NLP Research Team at Voicelab.ai.\n\nYou can contact us [here](https://voicelab.ai/contact/).\n", "size_bytes": "1100503117", "downloads": 20662}
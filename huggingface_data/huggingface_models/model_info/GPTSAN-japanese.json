{"pretrained_model_name": "Tanrei/GPTSAN-japanese", "description": "---\nlicense: mit\nlanguage:\n- ja\npipeline_tag: text-generation\n---\n# Model Card for Tanrei/GPTSAN-japanese\n\n![GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/logo-bk.png?raw=true)\n\nGeneral-purpose Swich transformer based Japanese language model \n\nGPTSAN has some unique features. It has a model structure of Prefix-LM. It works as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs behave like normal generative models.\nThe Spout vector is a GPTSAN specific input. Spout is pre-trained with random inputs, but you can specify a class of text or an arbitrary vector during fine-tuning. This allows you to indicate the tendency of the generated text.\nGPTSAN has a sparse Feed Forward based on Switch-Transformer. You can also add other layers and train them partially. See the original [GPTSAN repository](https://github.com/tanreinama/GPTSAN/) for details.\n\n## Text Generation\n\n```python\n>>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n\n>>> device = \"cuda\"\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> x_token = tokenizer(\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\", return_tensors=\"pt\")\n>>> trainer_utils.set_seed(30)\n>>> input_ids = x_token.input_ids.to(device)\n>>> gen_token = model.generate(input_ids, max_new_tokens=50)\n>>> tokenizer.decode(gen_token[0])\n\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\u653f\u6cbb\u30fb\u8ecd\u4e8b\u306e\u4e2d\u67a2\u307e\u3067\u638c\u63e1\u3057\u305f\u653f\u6cbb\u5bb6\u3067\u3042\u308a\u3001\u65e5\u672c\u53f2\u4e0a\u985e\u3092\u898b\u306a\u3044\u9a5a\u7570\u7684\u306a\u8ecd\u4e8b\u4fb5\u653b\u3092\u7d9a\u3051...\"\n```\n\n\n\n\n## Text Generation with Prefix-LM model\n\n```python\n>>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n\n>>> device = \"cuda\"\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> x_token = tokenizer(\"\", prefix_text=\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\", return_tensors=\"pt\")\n>>> trainer_utils.set_seed(30)\n>>> input_ids = x_token.input_ids.to(device)\n>>> token_type_ids = x_token.token_type_ids.to(device)\n>>> gen_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)\n>>> tokenizer.decode(gen_token[0])\n\"\u7e54\u7530\u4fe1\u9577\u306f\u3001\u653f\u6cbb\u30fb\u5916\u4ea4\u3067\u6570\u3005\u306e\u6226\u679c\u3092\u4e0a\u3052\u308b\u304c\u30011568\u5e74\u304b\u3089\u306f\u3001\u3044\u308f\u3086\u308b\u672c\u80fd\u5bfa\u306e\u5909\u3067\u7d30\u5ddd\u6674\u5143\u306b\u6697\u6bba\u3055\u308c\u308b...\"\n```\n\n\n## Masked Language Model And Text Generation\n\n```python\n>>> from transformers import AutoModel, AutoTokenizer, trainer_utils\n\n>>> device = \"cuda\"\n>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n>>> x_token = tokenizer(\n    \"\", prefix_text=\"\u6b66\u7530\u4fe1\u7384\u306f\u3001<|inputmask|>\u6642\u4ee3\u30d5\u30a1\u30f3\u306a\u3089\u305c\u3072\u62bc\u3055\u3048<|inputmask|>\u304d\u305f\u3044\u540d\u5c06\u306e\u4e00\u4eba\u3002\", return_tensors=\"pt\"\n)\n>>> trainer_utils.set_seed(30)\n>>> input_ids = x_token.input_ids.to(device)\n>>> token_type_ids = x_token.token_type_ids.to(device)\n>>> out_lm_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)\n>>> out_mlm_token = model(input_ids, token_type_ids=token_type_ids).logits.argmax(axis=-1)\n>>> tokenizer.decode(out_mlm_token[0])\n\"\u6b66\u7530\u4fe1\u7384\u306f\u3001\u6226\u56fd\u6642\u4ee3\u30d5\u30a1\u30f3\u306a\u3089\u305c\u3072\u62bc\u3055\u3048\u3066\u304a\u304d\u305f\u3044\u540d\u5c06\u306e\u4e00\u4eba\u3002\"\n>>> tokenizer.decode(out_lm_token[0][input_ids.shape[1] :])\n\"\u6b66\u7530\u6c0f\u306e\u4e09\u4ee3\u306b\u6e21\u3063\u305f\u6b66\u7530\u5bb6\u306e\u3072\u3068\u308a\\n\u7532\u6590\u5e02\u306b\u4f4f\u3080\u3001\u65e5\u672c\u53f2\u4e0a\u6700\u5927\u306e\u6226\u56fd\u5927\u540d\u3002...\"\n```\n\n\n# Model Details\n\n## Model Description\n\nJapanese language model using Switch Transformer.\nIt has the same structure as the model introduced as `Prefix LM` in the T5 paper, and works with both Test Generation and Masked Language Model.\n\n\n\n- **Developed by:** Toshiyuki Sakamoto (tanreinama)\n- **Model type:** Switch Transformer\n- **Language(s) (NLP):** Japanese\n- **License:** MIT License\n\n### Prefix-LM Model\n\nGPTSAN has the structure of the model named Prefix-LM in the [T5 paper](https://arxiv.org/abs/1910.10683). (The original GPTSAN repository calls it `hybrid`)\nIn GPTSAN, the `Prefix` part of Prefix-LM, that is, the input position that can be referenced by both tokens, can be specified with any length.\nArbitrary lengths can also be specified differently for each batch.\nThis length applies to the text entered in `prefix_text` for the tokenizer.\nThe tokenizer returns the mask of the `Prefix` part of Prefix-LM as `token_type_ids`.\nThe model treats the part where `token_type_ids` is 1 as a `Prefix` part, that is, the input can refer to both tokens before and after.\n\n### Spout Vector\n\nA Spout Vector is a special vector for controlling text generation.\nThis vector is treated as the first embedding in self-attention to bring extraneous attention to the generated tokens.\nIn this pre-trained model, the Spout Vector is a 128-dimensional vector that passes through 8 fully connected layers in the model and is projected into the space acting as external attention.\nThe Spout Vector projected by the fully connected layer is split to be passed to all self-attentions.\n\n## Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/tanreinama/GPTSAN\n", "size_bytes": "5558157121", "downloads": 591}
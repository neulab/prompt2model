{"pretrained_model_name": "ozcangundes/mt5-multitask-qa-qg-turkish", "description": "---\nlanguage: tr\ndatasets:\n- TQUAD\ntags: \n- question-answering\n- question-generation\n- multitask-model\n\nlicense: apache-2.0\n---\n\n# mT5-small based Turkish Multitask (Answer Extraction, Question Generation and Question Answering) System\n\n[Google's Multilingual T5-small](https://github.com/google-research/multilingual-t5) is fine-tuned on [Turkish Question Answering dataset](https://github.com/okanvk/Turkish-Reading-Comprehension-Question-Answering-Dataset) for three downstream task **Answer extraction, Question Generation and Question Answering** served in this single model. mT5 model was also trained for multiple text2text NLP tasks. \n\nAll data processing, training and pipeline codes can be found on my [Github](https://github.com/ozcangundes/multitask-question-generation). I will share the training details in the repo as soon as possible. \n\nmT5 small model has 300 million parameters and model size is about 1.2GB. Therefore, it takes significant amount of time to fine tune it. \n\n8 epoch and 1e-4 learning rate with 0 warmup steps was applied during training. These hparams and the others can be fine-tuned for much more better results.\n\n## Requirements \u2757\u2757\u2757\n```\n!pip install transformers==4.4.2\n!pip install sentencepiece==0.1.95\n!git clone https://github.com/ozcangundes/multitask-question-generation.git\n%cd multitask-question-generation/\n```\n\n## Usage \ud83d\ude80\ud83d\ude80\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"ozcangundes/mt5-multitask-qa-qg-turkish\") \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ozcangundes/mt5-multitask-qa-qg-turkish\")\n\nfrom pipelines import pipeline #pipelines.py script in the cloned repo\nmultimodel = pipeline(\"multitask-qa-qg\",tokenizer=tokenizer,model=model)\n\n#sample text\ntext=\"\u00d6zcan G\u00fcnde\u015f, 1993 y\u0131l\u0131 Tarsus do\u011fumludur. Orta Do\u011fu Teknik \u00dcniversitesi \\\\\\\\\nEnd\u00fcstri M\u00fchendisli\u011fi b\u00f6l\u00fcm\u00fcnde 2011 2016 y\u0131llar\u0131 aras\u0131nda lisans e\u011fitimi g\u00f6rm\u00fc\u015ft\u00fcr. \\\\\\\\\nY\u00fcksek lisans\u0131n\u0131 ise 2020 Aral\u0131k ay\u0131nda, 4.00 genel not ortalamas\u0131 ile \\\\\\\\\nBo\u011fazi\u00e7i \u00dcniversitesi, Y\u00f6netim Bili\u015fim Sistemleri b\u00f6l\u00fcm\u00fcnde tamamlam\u0131\u015ft\u0131r.\\\\\\\\\nFutbolla yak\u0131ndan ilgilenmekle birlikte, Galatasaray kul\u00fcb\u00fc taraftar\u0131d\u0131r.\"\n```\n\n## Example - Both Question Generation and Question Answering \ud83d\udcac\ud83d\udcac\n```\nmultimodel(text)\n\n#output\n=> [{'answer': 'Tarsus', 'question': '\u00d6zcan G\u00fcnde\u015f nerede do\u011fmu\u015ftur?'},\n {'answer': '1993', 'question': '\u00d6zcan G\u00fcnde\u015f ka\u00e7 y\u0131l\u0131nda do\u011fmu\u015ftur?'},\n {'answer': '2011 2016',\n  'question': '\u00d6zcan G\u00fcnde\u015f lisans e\u011fitimini hangi y\u0131llar aras\u0131nda tamamlam\u0131\u015ft\u0131r?'},\n {'answer': 'Bo\u011fazi\u00e7i \u00dcniversitesi, Y\u00f6netim Bili\u015fim Sistemleri',\n  'question': '\u00d6zcan G\u00fcnde\u015f y\u00fcksek lisans\u0131n\u0131 hangi b\u00f6l\u00fcmde tamamlam\u0131\u015ft\u0131r?'},\n {'answer': 'Galatasaray kul\u00fcb\u00fc',\n  'question': '\u00d6zcan G\u00fcnde\u015f futbolla yak\u0131ndan ilgilenmekle birlikte hangi kul\u00fcb\u00fc taraftar\u0131d\u0131r?'}]\n```\nFrom this text, 5 questions are generated and they are answered by the model. \n\n## Example - Question Answering \ud83d\udcad\ud83d\udcad\n\nBoth text and also, related question should be passed into pipeline.\n```\nmultimodel({\"context\":text,\"question\":\"\u00d6zcan hangi tak\u0131m\u0131 tutmaktad\u0131r?\"})\n\n#output\n=> Galatasaray\n\nmultimodel({\"context\":text,\"question\":\"\u00d6zcan, y\u00fcksek lisanstan ne zaman mezun oldu?\"})\n\n#output\n=> 2020 Aral\u0131k ay\u0131nda\n\n\nmultimodel({\"context\":text,\"question\":\"\u00d6zcan'\u0131n y\u00fcksek lisans bitirme notu ka\u00e7t\u0131r?\"})\n\n#output\n=> 4.00 \n\n#Sorry for being cocky \ud83d\ude1d\ud83d\ude1d\n```\n\n## ACKNOWLEDGEMENT\n\nThis work is inspired from [Suraj Patil's great repo](https://github.com/patil-suraj/question_generation). I would like to thank him for the clean codes and also,[Okan \u00c7ift\u00e7i](https://github.com/okanvk) for the Turkish dataset \ud83d\ude4f\n\n", "size_bytes": "1200734941", "downloads": 555}
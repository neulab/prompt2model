{"pretrained_model_name": "layoric/llama-2-13b-code-alpaca", "description": "---\nlibrary_name: peft\nlicense: cc-by-nc-4.0\ndatasets:\n- theblackcat102/evol-codealpaca-v1\nlanguage:\n- en\npipeline_tag: text2text-generation\n---\n## llama-2-13b-code-alpaca\n[<img src=\"https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/OpenAccess-AI-Collective/axolotl)\n\nTrained for 3 epochs on `theblackcat102/evol-codealpaca-v1` dataset, scored decent on locally run perplexity at 4.36.\n\n## Axolotl config used\n\n```yaml\nbase_model: NousResearch/Llama-2-13b-hf\nbase_model_config: NousResearch/Llama-2-13b-hf\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\npush_dataset_to_hub:\nhub_model_id: \n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: theblackcat102/evol-codealpaca-v1\n    type: alpaca\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.01\noutput_dir: ./checkpoints/llama-2-13b-qlora\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 4096\nmax_packed_sequence_len: 4096\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps: 2\nmicro_batch_size: 2\nnum_epochs: 3\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 0.0001\n\ntrain_on_inputs: false\ngroup_by_length: true\nbf16: true\nfp16: false\ntf32: true\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention: true\nflash_attention:\n\nwarmup_steps: 10\neval_steps: 50\nsave_steps:\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  bos_token: \"<s>\"\n  eos_token: \"</s>\"\n  unk_token: \"<unk>\"\n```\n\nAnd then merged with Axolotl via:\n\n```\naccelerate launch scripts/finetune.py configs/your_config.yml --merge_lora --lora_model_dir=\"./completed-model\" --load_in_8bit=False --load_in_4bit=False\n```\n\n## Training procedure\n\nThe following `bitsandbytes` quantization config was used during training:\n- load_in_8bit: False\n- load_in_4bit: True\n- llm_int8_threshold: 6.0\n- llm_int8_skip_modules: None\n- llm_int8_enable_fp32_cpu_offload: False\n- llm_int8_has_fp16_weight: False\n- bnb_4bit_quant_type: nf4\n- bnb_4bit_use_double_quant: True\n- bnb_4bit_compute_dtype: bfloat16\n\nThe following `bitsandbytes` quantization config was used during training:\n- load_in_8bit: False\n- load_in_4bit: True\n- llm_int8_threshold: 6.0\n- llm_int8_skip_modules: None\n- llm_int8_enable_fp32_cpu_offload: False\n- llm_int8_has_fp16_weight: False\n- bnb_4bit_quant_type: nf4\n- bnb_4bit_use_double_quant: True\n- bnb_4bit_compute_dtype: bfloat16\n\nThe following `bitsandbytes` quantization config was used during training:\n- load_in_8bit: False\n- load_in_4bit: True\n- llm_int8_threshold: 6.0\n- llm_int8_skip_modules: None\n- llm_int8_enable_fp32_cpu_offload: False\n- llm_int8_has_fp16_weight: False\n- bnb_4bit_quant_type: nf4\n- bnb_4bit_use_double_quant: True\n- bnb_4bit_compute_dtype: bfloat16\n\nThe following `bitsandbytes` quantization config was used during training:\n- load_in_8bit: False\n- load_in_4bit: True\n- llm_int8_threshold: 6.0\n- llm_int8_skip_modules: None\n- llm_int8_enable_fp32_cpu_offload: False\n- llm_int8_has_fp16_weight: False\n- bnb_4bit_quant_type: nf4\n- bnb_4bit_use_double_quant: True\n- bnb_4bit_compute_dtype: bfloat16\n\n\n### Framework versions\n- PEFT 0.5.0.dev0", "size_bytes": 26031749120, "downloads": 0}
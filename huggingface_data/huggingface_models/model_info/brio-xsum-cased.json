{"pretrained_model_name": "Yale-LILY/brio-xsum-cased", "description": "---\ntags:\n- pegasus\n\n---\n# Model Card for brio-xsum-cased\n \n \n# Model Details\n \n## Model Description\n \nBRIO: Bringing Order to Abstractive Summarization\n \n- **Developed by:** Yale LILY Lab\n- **Shared by [Optional]:** Hugging Face\n- **Model type:** PEGASUS\n- **Language(s) (NLP):** Text2Text Generation\n- **License:** More information needed\n- **Related Models:** \n  - **Parent Model:** PEGASUS\n- **Resources for more information:** \n  - [Github Repo](https://github.com/Yale-LILY/BRIO)\n  - [Associated Paper](https://arxiv.org/abs/2203.16804)\n  - [Associated Space](https://huggingface.co/spaces/darveen/text_summarizer)\n \n \n# Uses\n \n## Direct Use\nThis model can be used for the task of Text2Text Generation\n \n## Downstream Use [Optional]\n \nThe model creators note in the [associated paper](https://arxiv.org/abs/2203.16804)\n> It is possible to apply our method in a reinforcement learning setting, where the candidate summaries are dynamically generated. \n \n## Out-of-Scope Use\n \n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n \n \n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n \n \n# Training Details\n \n## Training Data\nThe model creators note in the [associated paper](https://arxiv.org/abs/2203.16804)\n> CNNDM4: is a large scale news dataset.\nNallapati et al: we treat the news articles as the source documents and the associated highlights as the summaries. \nXSum5: is a highly abstractive dataset of articles from the British Broadcasting Corporation (BBC). NYT6: contains articles from the New York Times and the associated summaries\n \n## Training Procedure\n \n \n### Preprocessing\n The model creators note in the [associated paper](https://arxiv.org/abs/2203.16804)\n > We follow Kedzie et al. (2018) for data preprocessing and splitting, and use the associated archival abstracts as the summaries\n \n### Speeds, Sizes, Times\n \nMore information needed\n \n# Evaluation\n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nMore information needed\n \n### Factors\n \nMore information needed\n \n### Metrics\n \nMore information needed\n \n## Results \n \n\n \n### CNNDM\n|          | ROUGE-1 | ROUGE-2 | ROUGE-L |\n|----------|---------|---------|---------|\n| BART     | 44.16   | 21.28   | 40.90   |\n| Ours     | 47.78   | 23.55   | 44.57   |\n\n \n### XSum\n|          | ROUGE-1 | ROUGE-2 | ROUGE-L |\n|----------|---------|---------|---------|\n| Pegasus  | 47.21   | 24.56   | 39.25   |\n| Ours     | 49.07   | 25.59   | 40.40   |\n\n \n### NYT\n|          | ROUGE-1 | ROUGE-2 | ROUGE-L |\n|----------|---------|---------|---------|\n| BART     | 55.78   | 36.61   | 52.60   |\n| Ours     | 57.75   | 38.64   | 54.54   |\n\n \n \n# Model Examination\nThe model creators note in the [associated paper](https://arxiv.org/abs/2203.16804)\nWe attribute BRIO-Ctr\u2019s superior performance to its use of the same model architecture (BART) for both candidate generation and scoring, while SimCLS uses RoBERTa as the evaluation model. As a result, BRIO-Ctr maximizes the parameter sharing between the two stages, and preserves the power of the Seq2Seq model pre-trained on the same dataset.\n \n# Environmental Impact\n \n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\nThe model creators note in the [associated paper](https://arxiv.org/abs/2203.16804)\n \n> Formulate summarization as a sequence-to-sequence (Seq2Seq) problem\n \n## Compute Infrastructure\n \nMore information needed\n \n### Hardware\n \nMore information needed\n \n### Software\n \nMore information needed\n \n# Citation\n \n \n**BibTeX:**\n ```\n@misc{https://doi.org/10.48550/arxiv.2203.16804,\n  doi = {10.48550/ARXIV.2203.16804},\n  \n  url = {https://arxiv.org/abs/2203.16804},\n  \n  author = {Liu, Yixin and Liu, Pengfei and Radev, Dragomir and Neubig, Graham},\n  \n  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BRIO: Bringing Order to Abstractive Summarization},\n```\n \n \n \n# Glossary [optional]\n \nMore information needed\n \n# More Information [optional]\n \nMore information needed\n \n# Model Card Authors [optional]\n \nYale LILY Lab in collaboration with Ezi Ozoani and the Hugging Face team\n \n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n \ntokenizer = AutoTokenizer.from_pretrained(\"Yale-LILY/brio-xsum-cased\")\n \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Yale-LILY/brio-xsum-cased\")\n \n```\n</details>\n", "size_bytes": "2279637783", "downloads": 298}
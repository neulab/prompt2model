{"pretrained_model_name": "agemagician/mlong-t5-tglobal-base", "description": "---\nlicense: apache-2.0\nlanguage: \n- multilingual\n- af\n- am\n- ar\n- az\n- be\n- bg\n- bn\n- ca\n- ceb\n- co\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fil\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- haw\n- hi\n- hmn\n- ht\n- hu\n- hy\n- ig\n- is\n- it\n- iw\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lb\n- lo\n- lt\n- lv\n- mg\n- mi\n- mk\n- ml\n- mn\n- mr\n- ms\n- mt\n- my\n- ne\n- nl\n- no\n- ny\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sd\n- si\n- sk\n- sl\n- sm\n- sn\n- so\n- sq\n- sr\n- st\n- su\n- sv\n- sw\n- ta\n- te\n- tg\n- th\n- tr\n- uk\n- und\n- ur\n- uz\n- vi\n- xh\n- yi\n- yo\n- zh\n- zu\ndatasets:\n- mc4\n---\n\n# MLongT5 (transient-global attention, base-sized model)\n\nMLongT5 model pre-trained on Multi-language corpus. The model was introduced in the paper [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences](https://arxiv.org/pdf/2305.11129.pdf) by Uthus et al. and first released in [the LongT5 repository](https://github.com/google-research/longt5). All the model architecture and configuration can be found in [Flaxformer repository](https://github.com/google/flaxformer) which uses another Google research project repository [T5x](https://github.com/google-research/t5x).\n\nDisclaimer: The team releasing MLongT5 did not write a model card for this model so this model card has been written by Ahmed Elnaggar.\n\n## Model description\nMLongT5 model is an encoder-decoder transformer pre-trained in a text-to-text denoising generative setting ([Pegasus-like generation pre-training](https://arxiv.org/pdf/1912.08777.pdf)). MLongT5 model is an extension of [LongT5 model](https://arxiv.org/abs/2112.07916), and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2) Transient-Global attention. The usage of attention sparsity patterns allows the model to efficiently handle input sequence.\n\nMLongT5 is particularly effective when fine-tuned for text generation (summarization, question answering) which requires handling long input sequences (up to 16,384 tokens).\n\n## Intended uses & limitations\n\nThe model is mostly meant to be fine-tuned on a supervised dataset. See the [model hub](https://huggingface.co/models?search=mlongt5) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\n### How to use\n\nThe following shows how one can extract the last hidden representation for the model.\n\n```python\nfrom transformers import T5Tokenizer, LongT5Model\n\ntokenizer = T5Tokenizer.from_pretrained(\"agemagician/mlong-t5-tglobal-base\")\nmodel = LongT5Model.from_pretrained(\"agemagician/mlong-t5-tglobal-base\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n```\n\nThe following shows how one can predict masked passages using the different denoising strategies.\n\n### S-Denoising\n\nFor *S-Denoising*, please make sure to prompt the text with the prefix `[S2S]` as shown below.\n\n```python\nfrom transformers import LongT5ForConditionalGeneration, T5Tokenizer\nimport torch\n\nmodel = LongT5ForConditionalGeneration.from_pretrained(\"agemagician/mlong-t5-tglobal-base\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\"cuda\")                                                                                                   \ntokenizer = T5Tokenizer.from_pretrained(\"agemagician/mlong-t5-tglobal-base\")\n\ninput_string = \"[S2S] Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, solid man with a bald head. Mrs. Dursley was thin and blonde and more than the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere <extra_id_0>\"                                               \n\ninputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n```\n\n### R-Denoising\n\nFor *R-Denoising*, please make sure to prompt the text with the prefix `[NLU]` as shown below.\n\n```python\nfrom transformers import LongT5ForConditionalGeneration, T5Tokenizer\nimport torch\n\nmodel = LongT5ForConditionalGeneration.from_pretrained(\"agemagician/mlong-t5-tglobal-base\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\"cuda\")                                                                                                   \ntokenizer = T5Tokenizer.from_pretrained(\"agemagician/mlong-t5-tglobal-base\")\n\ninput_string = \"[NLU] Mr. Dursley was the director of a firm called <extra_id_0>, which made <extra_id_1>. He was a big, solid man with a bald head. Mrs. Dursley was thin and <extra_id_2> of neck, which came in very useful as she spent so much of her time <extra_id_3>. The Dursleys had a small son called Dudley and <extra_id_4>\"                                               \n\ninputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n```\n\n### X-Denoising\n\nFor *X-Denoising*, please make sure to prompt the text with the prefix `[NLG]` as shown below.\n\n```python\nfrom transformers import LongT5ForConditionalGeneration, T5Tokenizer\nimport torch\n\nmodel = LongT5ForConditionalGeneration.from_pretrained(\"agemagician/mlong-t5-tglobal-base\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\"cuda\")                                                                                                   \ntokenizer = T5Tokenizer.from_pretrained(\"agemagician/mlong-t5-tglobal-base\")\n\ninput_string = \"[NLG] Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, solid man wiht a bald head. Mrs. Dursley was thin and blonde and more than the usual amount of neck, which came in very useful as she\nspent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. <extra_id_0>\"                                               \n\nmodel.cuda()\ninputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{uthus2023mlongt5,\n      title={mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences}, \n      author={David Uthus and Santiago Onta\u00f1\u00f3n and Joshua Ainslie and Mandy Guo},\n      year={2023},\n      eprint={2305.11129},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n> Created by [Ahmed Elnaggar/@Elnaggar_AI](https://twitter.com/Elnaggar_AI) | [LinkedIn](https://www.linkedin.com/in/prof-ahmed-elnaggar/)", "size_bytes": "2368277673", "downloads": 1611}
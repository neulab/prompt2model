{"pretrained_model_name": "facebook/wmt21-dense-24-wide-en-x", "description": "---\nlanguage: \n- multilingual\n- ha\n- is\n- ja\n- cs\n- ru\n- zh\n- de\n- en\nlicense: mit\ntags:\n- translation\n- wmt21\n---\n\n# WMT 21 En-X\nWMT 21 En-X is a 4.7B multilingual encoder-decoder (seq-to-seq) model trained for one-to-many multilingual translation.\nIt was introduced in this [paper](https://arxiv.org/abs/2108.03265) and first released in [this](https://github.com/pytorch/fairseq/tree/main/examples/wmt21) repository.\n\nThe model can directly translate English text into 7 other languages: Hausa (ha), Icelandic (is), Japanese (ja), Czech (cs), Russian (ru), Chinese (zh), German (de).\n\nTo translate into a target language, the target language id is forced as the first generated token.\nTo force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.\n\n*Note: `M2M100Tokenizer` depends on `sentencepiece`, so make sure to install it before running the example.*\n\nTo install `sentencepiece` run `pip install sentencepiece`\n\nSince the model was trained with domain tags, you should prepend them to the input as well.\n* \"wmtdata newsdomain\": Use for sentences in the news domain\n* \"wmtdata otherdomain\": Use for sentences in all other domain\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/wmt21-dense-24-wide-en-x\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt21-dense-24-wide-en-x\")\n\ninputs = tokenizer(\"wmtdata newsdomain One model for many languages.\", return_tensors=\"pt\")\n\n# translate English to German\ngenerated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"de\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"Ein Modell f\u00fcr viele Sprachen.\"\n\n# translate English to Icelandic\ngenerated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"is\"))\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"Ein fyrirmynd fyrir m\u00f6rg tungum\u00e1l.\"\n```\n\nSee the [model hub](https://huggingface.co/models?filter=wmt21) to look for more fine-tuned versions.\n\n\n## Languages covered\nEnglish (en), Hausa (ha), Icelandic (is), Japanese (ja), Czech (cs), Russian (ru), Chinese (zh), German (de)\n\n\n## BibTeX entry and citation info\n```\n@inproceedings{tran2021facebook\n  title={Facebook AI\u2019s WMT21 News Translation Task Submission},\n  author={Chau Tran and Shruti Bhosale and James Cross and Philipp Koehn and Sergey Edunov and Angela Fan},\n  booktitle={Proc. of WMT},\n  year={2021},\n}\n```", "size_bytes": "18773661472", "downloads": 56}
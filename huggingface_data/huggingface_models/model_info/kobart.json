{"pretrained_model_name": "hyunwoongko/kobart", "description": "---\nlanguage: ko\ntags:\n- bart\nlicense: mit\n---\n\n## KoBART-base-v2\n\nWith the addition of chatting data, the model is trained to handle the semantics of sequences longer than KoBART.\n\n```python\nfrom transformers import PreTrainedTokenizerFast, BartModel\n\ntokenizer = PreTrainedTokenizerFast.from_pretrained('hyunwoongko/kobart')\nmodel = BartModel.from_pretrained('hyunwoongko/kobart')\n```\n\n### Performance \n\nNSMC\n- acc. : 0.901\n\n### hyunwoongko/kobart\n- Added bos/eos post processor\n- Removed token_type_ids\n", "size_bytes": "495536138", "downloads": 6932}
{"pretrained_model_name": "lgrobol/m2m100_418M_br_fr", "description": "---\nlanguage:\n- br\n- fr\nlicense: mit\ntags:\n  - translation\nmodel-index:\n- name: m2m100_br_fr\n  results: []\nco2_eq_emissions:\n  emissions: 2100\n  source: \"https://mlco2.github.io/impact\"\n  training_type: \"fine-tuning\"\n  geographical_location: \"Paris, France\"\n  hardware_used: \"2 NVidia GeForce RTX 3090 GPUs\"\n---\n\nBreton-French translator `m2m100_418M_br_fr`\n============================================\n\nThis model is a fine-tuned version of\n[facebook/m2m100_418M](https://huggingface.co/facebook/m2m100_418M) (Fan et al., 2021) on a\nBreton-French parallel corpus. In order to obtain the best possible results, we use all our parallel\ndata on training and consequently report no quantitative evaluation at this time. Empirical\nqualitative evidence suggests that the translations are generally adequate for short and simple\nexamples, the behaviour of the model on long and/or complex inputs is currently unknown.\n\nTry this model online in [Troer](https://huggingface.co/spaces/lgrobol/troer), feedback and\nsuggestions are welcome!\n\n## Model description\n\nSee the description of the [base model](https://huggingface.co/facebook/m2m100_418M).\n\n## Intended uses & limitations\n\nThis is intended as a **demonstration** of the improvements brought by fine-tuning a large-scale\nmany-to-many translation system on a medium-sized dataset of high-quality data. As it is, and as far\nas I can tell it usually provides translations that are least as good as those of other available\nBreton-French translators, but it has not been evaluated quantitatively at a large scale.\n\n## Training and evaluation data\n\nThe training dataset consists of:\n\n- The [OfisPublik corpus v1](https://opus.nlpl.eu/OfisPublik-v1.php) (Tyers, 2009)\n- The [Tatoeba corpus v2022-03-03](https://opus.nlpl.eu/Tatoeba-v2022-03-03.php)\n- Part of the [OpenSubtitles corpus v2018](https://opus.nlpl.eu/OpenSubtitles-v2018.php)\n\nThese are obtained from the [OPUS](https://opus.nlpl.eu/) base (Tiedemann, 2012) and filtered using\n[OpusFilter](https://helsinki-nlp.github.io/OpusFilter) (Aulamo et al., 2020), see\n[`dl_opus.yaml`](dl_opus.yaml) for the details. The filtering is slightly non-deterministic due to\nthe retraining of a statistical alignment model, but in my experience, different runs tend to give\nextremely similar results. Do not hesitate to reach out if you experience difficulties in using this\nto collect data.\n\nIn addition to these, the training dataset also includes parallel br/fr sentences, provided as\nglosses in the [Arbres](https://arbres.iker.cnrs.fr) wiki (Jouitteau, 2022), obtained from their\n[ongoing port](https://github.com/Autogramm/Breton/commit/45ac2c444a979b7ee41e5f24a3bfd1ec39f09d7d)\nto Universal Dependencies in the Autogramm project.\n\n## Training procedure\n\nThe training hyperparameters are those suggested by Adelani et al. (2022) in their [code\nrelease](https://github.com/masakhane-io/lafand-mt), which gave their best results for machine\ntranslation of several African languages.\n\nMore specifically, we train this model with [zeldarose](https://github.com/LoicGrobol/zeldarose) with the following parameters\n\n```bash\nzeldarose transformer \\\n   --config train_config.toml \\\n   --tokenizer \"facebook/m2m100_418M\" --pretrained-model \"facebook/m2m100_418M\" \\\n   --out-dir m2m100_418M+br-fr --model-name m2m100_418M+br-fr \\\n   --strategy ddp --accelerator gpu --num-devices 4 --device-batch-size 2 --num-workers 8\\\n   --max-epochs 16 --precision 16 --tf32-mode medium \\\n   --val-data {val_path}.jsonl \\\n   {train_path}.jsonl\n\n```\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n```toml\n[task]\nchange_ratio = 0.3\ndenoise_langs = []\npoisson_lambda = 3.0\nsource_langs = [\"br\"]\ntarget_langs = [\"fr\"]\n\n[tuning]\nbatch_size = 16\nbetas = [0.9, 0.999]\nepsilon = 1e-8\nlearning_rate = 5e-5\ngradient_clipping = 1.0\nlr_decay_steps = -1\nwarmup_steps = 1024\n```\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.12.1\n- Datasets 2.10.0\n- Tokenizers 0.13.2\n- Pytorch-lightning 1.9.3\n- Zeldarose [c6456ead](https://github.com/LoicGrobol/spertiniite/commit/c6456ead3649c4e6ddfb4a5a74b40f344eded09f)\n\n### Carbon emissions\n\nAt this time, we estimate emissions of a rough 300 gCO<sub>2</sub> per fine-tuning run. So far, we\naccount for\n\n- Fine-tuning the 3 released versions\n- 8 development runs\n\nSo far, the equivalent carbon emissions for this model are approximately 3300 gCO<sub>2</sub>.\n\n## References\n\n- Adelani, David, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,\n  et al. 2022. \u201cA Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African\n  News Translation\u201d. In Proceedings of the 2022 Conference of the North American Chapter of the\n  Association for Computational Linguistics: Human Language Technologies, 3053\u201170. Seattle, United\n  States: Association for Computational Linguistics.\n  <https://doi.org/10.18653/v1/2022.naacl-main.223>.\n- Mikko Aulamo, Sami Virpioja, and J\u00f6rg Tiedemann. 2020. OpusFilter: A Configurable Parallel Corpus\n  Filtering Toolbox. In Proceedings of the 58th Annual Meeting of the Association for Computational\n  Linguistics: System Demonstrations, pages 150\u2013156, Online. Association for Computational\n  Linguistics.\n- Fan, Angela, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep\n  Baines, et al. 2021. \u201cBeyond english-centric multilingual machine translation\u201d. The Journal of\n  Machine Learning Research 22 (1): 107:4839-107:4886.\n- Tiedemann, Jorg 2012, \u201cParallel Data, Tools and Interfaces in OPUS\u201d. In Proceedings of the 8th\n  International Conference on Language Resources and Evaluation (LREC 2012)\n- Jouitteau, M\u00e9lanie. (\u00e9d.). 2009-2022. ARBRES, wikigrammaire des dialectes du breton et centre de\n  ressources pour son \u00e9tude linguistique formelle, IKER, CNRS, <http://arbres.iker.cnrs.fr>.\n- Tyers, Francis M. 2009 \u201cRule-based augmentation of training data in Breton-French statistical\n  machine translation\u201d. In Proceedings of the 13th Annual Conference of the European Association of\n  Machine Translation, EAMT09. Barcelona, Espa\u00f1a. 213--218\n", "size_bytes": "1944194627", "downloads": 11}
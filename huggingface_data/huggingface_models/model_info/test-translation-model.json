{"pretrained_model_name": "hou/test-translation-model", "description": "---\nlicense: cc-by-nc-4.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: NNLB-alt-en-bleu-ht\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# NNLB-alt-en-bleu-ht\n\nThis model is a fine-tuned version of [facebook/nllb-200-distilled-600M](https://huggingface.co/facebook/nllb-200-distilled-600M) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.4011\n- Bleu: 40.828\n- Gen Len: 26.385\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0001\n- train_batch_size: 14\n- eval_batch_size: 14\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 2.0036        | 1.0   | 799   | 1.4377          | 25.1487 | 25.036  |\n| 1.2584        | 2.0   | 1598  | 1.3276          | 29.603  | 25.723  |\n| 1.0147        | 3.0   | 2397  | 1.3204          | 31.3967 | 25.776  |\n| 0.7379        | 4.0   | 3196  | 1.3678          | 32.4951 | 25.266  |\n| 0.6228        | 5.0   | 3995  | 1.4250          | 34.6087 | 26.083  |\n| 0.4327        | 6.0   | 4794  | 1.5342          | 36.6073 | 26.174  |\n| 0.3437        | 7.0   | 5593  | 1.5952          | 37.7791 | 26.265  |\n| 0.2689        | 8.0   | 6392  | 1.6993          | 38.16   | 26.376  |\n| 0.2029        | 9.0   | 7191  | 1.7994          | 39.433  | 26.766  |\n| 0.1711        | 10.0  | 7990  | 1.8893          | 39.2816 | 26.574  |\n| 0.1214        | 11.0  | 8789  | 1.9661          | 39.5599 | 26.687  |\n| 0.1017        | 12.0  | 9588  | 1.9928          | 39.7801 | 26.845  |\n| 0.0855        | 13.0  | 10387 | 2.0508          | 39.8043 | 26.641  |\n| 0.0679        | 14.0  | 11186 | 2.0998          | 40.3389 | 26.526  |\n| 0.06          | 15.0  | 11985 | 2.1350          | 40.0964 | 26.395  |\n| 0.0475        | 16.0  | 12784 | 2.1676          | 40.1536 | 26.614  |\n| 0.0407        | 17.0  | 13583 | 2.2040          | 40.298  | 26.494  |\n| 0.0347        | 18.0  | 14382 | 2.2294          | 40.5207 | 26.612  |\n| 0.0315        | 19.0  | 15181 | 2.2484          | 40.3323 | 26.53   |\n| 0.0286        | 20.0  | 15980 | 2.2828          | 40.3167 | 26.718  |\n| 0.0241        | 21.0  | 16779 | 2.3015          | 40.0766 | 26.306  |\n| 0.0213        | 22.0  | 17578 | 2.3267          | 40.477  | 26.457  |\n| 0.0183        | 23.0  | 18377 | 2.3410          | 40.4013 | 26.406  |\n| 0.0164        | 24.0  | 19176 | 2.3457          | 40.3643 | 26.534  |\n| 0.0157        | 25.0  | 19975 | 2.3533          | 40.3967 | 26.506  |\n| 0.0133        | 26.0  | 20774 | 2.3734          | 40.7786 | 26.38   |\n| 0.0119        | 27.0  | 21573 | 2.3750          | 40.8653 | 26.525  |\n| 0.0106        | 28.0  | 22372 | 2.3896          | 40.8371 | 26.503  |\n| 0.0095        | 29.0  | 23171 | 2.3893          | 40.831  | 26.398  |\n| 0.0094        | 30.0  | 23970 | 2.4011          | 40.828  | 26.385  |\n\n\n### Framework versions\n\n- Transformers 4.21.0\n- Pytorch 1.10.0+cu113\n- Datasets 2.4.0\n- Tokenizers 0.12.1\n", "size_bytes": "2460465095", "downloads": 2}
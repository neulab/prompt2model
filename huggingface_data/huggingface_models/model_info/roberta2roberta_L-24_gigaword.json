{"pretrained_model_name": "google/roberta2roberta_L-24_gigaword", "description": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- gigaword\ntags:\n- summarization\n---\n\n# Roberta2Roberta_L-24_gigaword EncoderDecoder model\n\nThe model was introduced in \n[this paper](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn and first released in [this repository](https://tfhub.dev/google/bertseq2seq/roberta24_gigaword/1). \n\nThe model is an encoder-decoder model that was initialized on the `roberta-large` checkpoints for both the encoder \nand decoder and fine-tuned on headline generation using the Gigaword dataset, which is linked above.\n\nDisclaimer: The model card has been written by the Hugging Face team.\n\n## How to use\n\nYou can use this model for extreme summarization, *e.g.*\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_gigaword\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/roberta2roberta_L-24_gigaword\")\n\narticle = \"\"\"australian shares closed down #.# percent monday\nfollowing a weak lead from the united states and\nlower commodity prices , dealers said .\"\"\"\n\ninput_ids = tokenizer(article, return_tensors=\"pt\").input_ids\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n# should output\n# australian shares close down #.# percent.\n```\n", "size_bytes": "1821418078", "downloads": 260}
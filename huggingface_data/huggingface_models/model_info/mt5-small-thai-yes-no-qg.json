{"pretrained_model_name": "SuperAI2-Machima/mt5-small-thai-yes-no-qg", "description": "---\ntags:\n- Yes No question-generation\nlanguage: \n- thai\n- th\ndatasets:\n- NSC2018\n- wiki-documents-nsc\n- ThaiQACorpus-DevelopmentDataset\nwidget:\n- text: \"\u0e27\u0e31\u0e19\u0e17\u0e35\u0e48 1 \u0e01\u0e31\u0e19\u0e22\u0e32\u0e22\u0e19 2550 12:00 \u0e19. \u0e15\u0e33\u0e23\u0e27\u0e08\u0e20\u0e39\u0e18\u0e23\u0e08.\u0e1a\u0e38\u0e23\u0e35\u0e23\u0e31\u0e21\u0e22\u0e4c\u0e1a\u0e38\u0e01\u0e15\u0e23\u0e27\u0e08\u0e22\u0e36\u0e14\u0e44\u0e21\u0e49\u0e41\u0e1b\u0e23\u0e23\u0e39\u0e1b\u0e2b\u0e27\u0e07\u0e2b\u0e49\u0e32\u0e21\u0e01\u0e27\u0e48\u0e32 80 \u0e41\u0e1c\u0e48\u0e19\"\n  example_title: \"Example 01\"\n- text: \"\u0e1e\u0e25\u0e40\u0e2d\u0e01 \u0e1b\u0e23\u0e30\u0e22\u0e38\u0e17\u0e18\u0e4c \u0e08\u0e31\u0e19\u0e17\u0e23\u0e4c\u0e42\u0e2d\u0e0a\u0e32 (\u0e40\u0e01\u0e34\u0e14 21 \u0e21\u0e35\u0e19\u0e32\u0e04\u0e21 \u0e1e.\u0e28. 2497) \u0e0a\u0e37\u0e48\u0e2d\u0e40\u0e25\u0e48\u0e19 \u0e15\u0e39\u0e48 \u0e40\u0e1b\u0e47\u0e19\u0e19\u0e31\u0e01\u0e01\u0e32\u0e23\u0e40\u0e21\u0e37\u0e2d\u0e07\u0e41\u0e25\u0e30\u0e2d\u0e14\u0e35\u0e15\u0e19\u0e32\u0e22\u0e17\u0e2b\u0e32\u0e23\u0e1a\u0e01\u0e0a\u0e32\u0e27\u0e44\u0e17\u0e22\"\n  example_title: \"Example 02\"\n\nlicense: mit\n---\n[SuperAI Engineer Season 2](https://superai.aiat.or.th/) , [Machima](https://machchima.superai.me/)\n\n[Google's mT5](https://github.com/google-research/multilingual-t5) , [Pollawat](https://huggingface.co/Pollawat/mt5-small-thai-qg)\n\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n\nmodel = T5ForConditionalGeneration.from_pretrained('SuperAI2-Machima/mt5-small-thai-yes-no-qg')\ntokenizer = T5Tokenizer.from_pretrained('SuperAI2-Machima/mt5-small-thai-yes-no-qg')\n\nsource_text = '\u0e1a\u0e38\u0e01\u0e22\u0e36\u0e14\u0e44\u0e21\u0e49\u0e40\u0e16\u0e37\u0e48\u0e2d\u0e19 \u0e2d\u0e14\u0e35\u0e15 \u0e2a.\u0e2a.\u0e1a\u0e38\u0e23\u0e35\u0e23\u0e31\u0e21\u0e22\u0e4c \u0e40\u0e15\u0e23\u0e35\u0e22\u0e21\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e04\u0e24\u0e2b\u0e32\u0e2a\u0e19\u0e4c\u0e17\u0e23\u0e07\u0e44\u0e17\u0e22 1 \u0e01\u0e31\u0e19\u0e22\u0e32\u0e22\u0e19 2550 12:00 \u0e19. \u0e15\u0e33\u0e23\u0e27\u0e08\u0e20\u0e39\u0e18\u0e23\u0e08.\u0e1a\u0e38\u0e23\u0e35\u0e23\u0e31\u0e21\u0e22\u0e4c\u0e1a\u0e38\u0e01\u0e15\u0e23\u0e27\u0e08\u0e22\u0e36\u0e14\u0e44\u0e21\u0e49\u0e41\u0e1b\u0e23\u0e23\u0e39\u0e1b\u0e2b\u0e27\u0e07\u0e2b\u0e49\u0e32\u0e21\u0e01\u0e27\u0e48\u0e32 80 \u0e41\u0e1c\u0e48\u0e19'\n\nprint('Predicted Summary Text : ')\ntokenized_text = tokenizer.encode(source_text, return_tensors=\"pt\").to(device)\nsummary_ids = model.generate(tokenized_text,\n                                        num_beams=4,\n                                        no_repeat_ngram_size=2,\n                                        max_length=50,\n                                        early_stopping=True)\noutput = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\nprint(output)\n#Predicted Summary Text : \n#answer: 80 \u0e41\u0e1c\u0e48\u0e19 question: \u0e15\u0e4d\u0e32\u0e23\u0e27\u0e08\u0e20\u0e39\u0e18\u0e23\u0e08.\u0e1a\u0e38\u0e23\u0e35\u0e23\u0e31\u0e21\u0e22\u0e4c\u0e1a\u0e38\u0e01\u0e15\u0e23\u0e27\u0e08\u0e22\u0e36\u0e14\u0e44\u0e21\u0e49\u0e41\u0e1b\u0e23\u0e23\u0e39\u0e1b\u0e2b\u0e27\u0e07\u0e2b\u0e49\u0e32\u0e21\u0e01\u0e27\u0e48\u0e32\u0e01\u0e35\u0e48\u0e41\u0e1c\u0e48\u0e19\n```", "size_bytes": "1200792197", "downloads": 12}
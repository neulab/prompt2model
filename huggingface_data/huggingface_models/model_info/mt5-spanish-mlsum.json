{"pretrained_model_name": "IIC/mt5-spanish-mlsum", "description": "---\nlanguage:\n- es\ntags:\n- summarization\nlicense: apache-2.0\ndatasets:\n- mlsum\nmetrics:\n- rouge1\n- rouge2\n- rougeL\n- rougeLsum\n\nmodel-index:\n- name: xprophetnet-spanish-mlsum\n  results:\n  - task: \n      type: summarization \n      name: abstractive summarization  \n    dataset:\n      type: mlsum\n      name: mlsum-es\n      args: es         \n    metrics:\n      - type: rouge1    \n        value: 21.9788 \n        name: rouge1    \n      - type: rouge2\n        value: 6.5249\n        name: rouge2\n      - type: rougeL\n        value: 17.7444\n        name: rougeL\n      - type: rougeLsum\n        value: 18.9783\n        name: rougeLsum\n---\n\nThis is a model for text summarization in Spanish. It has been trained on the Spanish portion of [mlsum](https://huggingface.co/datasets/mlsum), finetuning the [mt5-base model](https://huggingface.co/google/mt5-base).\n\nWe used the following set of hyperparameters:\n\n```python\n\n    {\n      \"learning_rate\": 2e-5,\n      \"num_train_epochs\": 8,\n      \"per_device_train_batch_size\": 1,\n      \"per_device_eval_batch_size\": 1,\n      \"gradient_accumulation_steps\": 256,\n      \"fp16\": False,\n      \"weight_decay\": 0.01,\n    }\n```\n\nThe model was finetuned to predict the concatenation of the title and the summary of each item in the dataset. The results that we show below correspond to the set split of mlsum. The metrics for the **concatenation of titles and summaries** are:\n\n```json\n{'rouge1': 26.946, 'rouge2': 10.7271, 'rougeL': 21.4591, 'rougeLsum': 24.5001, 'gen_len': 18.9628}\n```\nOn the other hand, the metrics for **just the summaries** are:\n\n```json\n{'rouge1': 21.9788, 'rouge2': 6.5249, 'rougeL': 17.7444, 'rougeLsum': 18.9783, 'gen_len': 18.9628}\n```\n\nThis model is really easy to use, and with the following lines of code you can just start summarizing your documents in Spanish:\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ntext = \"Hola esto es un ejemplo de texto a resumir. Poco hay que resumir aqu\u00ed, pero es s\u00f3lo de muestra.\"\nmodel_str = \"IIC/mt5-spanish-mlsum\"\ntokenizer = AutoTokenizer.from_pretrained(model_str)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_str)\n\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n### Contributions\nThanks to [@avacaondata](https://huggingface.co/avacaondata), [@alborotis](https://huggingface.co/alborotis), [@albarji](https://huggingface.co/albarji), [@Dabs](https://huggingface.co/Dabs), [@GuillemGSubies](https://huggingface.co/GuillemGSubies) for adding this model.", "size_bytes": "2329733709", "downloads": 210}
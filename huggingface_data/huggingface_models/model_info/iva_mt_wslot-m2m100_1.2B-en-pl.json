{"pretrained_model_name": "cartesinus/iva_mt_wslot-m2m100_1.2B-en-pl", "description": "---\nlicense: mit\ntags:\n- machine translation\n- iva\n- virtual assistants\n- natural-language-understanding\n- nlu\nmetrics:\n- bleu\nmodel-index:\n- name: iva_mt_wslot-m2m100_1.2B-en-pl\n  results: []\ndatasets:\n- cartesinus/iva_mt_wslot\nlanguage:\n- pl\n- en\nco2_eq_emissions:\n  emissions: 0.68\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# iva_mt_wslot-m2m100_1.2B-en-pl (v0.1.0)\n\nThis model is a fine-tuned version of [facebook/m2m100_1.2B](https://huggingface.co/facebook/m2m100_1.2B) on the\n[iva_mt_wslot](https://huggingface.co/datasets/cartesinus/iva_mt_wslot) dataset. There is also smaller version of this model here:\n[cartesinus/iva_mt_wslot-m2m100_418M-en-pl](https://huggingface.co/cartesinus/iva_mt_wslot-m2m100_418M-0.1.0). This model (1.2B) achieves the following results:\n\n1) On the test set (iva_mt):\n- BLEU (plain text): **(result in preparation)**\n- BLEU (with slots): **(result in preparation)**\n- F1 score: (in preparation)\n\nFor reference BLEU for baseline m2m100-418M (plain text) was 21.9468 and for m2m100-1.2B was **(result in preparation)**. Second result (BLEU with slots) is when tags\nare treated as \"normal\" words in sentence. Therefore that result might be a bit misleading. Please refer to plain text results if you are not sure how to interpret them.\n\n2) WMT20 (en2pl):\n- BLEU (lowercased, tags removed): **(result in preparation)**\n- BLEU for baseline m2m100-1.2B (plain text): **(result in preparation)**\n\nFor reference WMT20 submission systems in en-pl direction had between 25 and 30 BLEU\n   \n3) BLEU on the evaluation set (same as in below table 'Training results'): **62.4604**\n\n4) On the training set (to see how it adjusted to train):\n- BLEU (plain text): **(result in preparation)**\n- BLEU (with slots): **(result in preparation)**\n\nBLEU was measured with [sacreBLEU](https://github.com/mjpost/sacrebleu) library.\n\n## Model description, intended uses & limitations\n\nModel is biased towards virtual assistant (IVA) sentences in prediction/translation. These sentences are short, imperatives with a lot of name entities (slots) and \nparticular vocabulary (for example settings name). It can be observed in above results where WMT results are very low while in-domain test is very high.\n\nThis model will most probably force IVA translations on your text. As long as sentences that you are translating are more or less similar to massive and leyzer domains it\nwill be ok. If you will translate out-of-domain sentenences (such as for example News, Medical) that are not very similar then results will drop significantly.\n\nOne last thing that needs to be mentioned is that BLEU is not particulary good metric to evaluate IVA sentences due to their length and it should be evalued with other\nmetrices (e.g. [GLEU](https://aclanthology.org/P15-2097.pdf)).\n\n## How to use\n\nFirst please make sure to install `pip install transformers`. First download model: \n\n```python\nfrom transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\nimport torch\n\ndef translate(input_text, lang):\n    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n    generated_tokens = model.generate(**input_ids, forced_bos_token_id=tokenizer.get_lang_id(lang))\n    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\nmodel_name = \"cartesinus/iva_mt_wslot-m2m100_1.2B-en-pl\"\ntokenizer = M2M100Tokenizer.from_pretrained(model_name, src_lang=\"en\", tgt_lang=\"pl\")\nmodel = M2M100ForConditionalGeneration.from_pretrained(model_name)\n```\n\nThen you can translate either plan text like this:\n```python\nprint(translate(\"set the temperature on my thermostat\", \"pl\"))\n```\nor you can translate with slot annotations that will be restored in tgt language:\n```python\nprint(translate(\"wake me up at <a>nine am<a> on <b>friday<b>\", \"pl\")) #translation: obud\u017a mnie o <a>pi\u0105tej rano<a> <b>w tym tygodniu<b>\n```\nLimitations of translation with slot transfer:\n1) Annotated words must be placed between semi-xml tags like this \"this is \\<a\\>example\\<a\\>\"\n2) There is no closing tag for example \"\\<\\a\\>\" in above example - this is done on purpose to ommit problems with backslash escape\n3) If sentence consists of more than one slot then simply use next alphabet letter. For example \"this is \\<a\\>example\\<a\\> with more than \\<b\\>one\\<b\\> slot\"\n4) Please do not add space before first or last annotated word because this particular model was trained this way and it most probably will lower it's results \n\n\n## Training and evaluation data\n\n## Dataset Composition (en-pl)\n| Corpus                                                               | Train  | Dev   | Test  |\n|----------------------------------------------------------------------|--------|-------|-------|\n| [Massive 1.1](https://huggingface.co/datasets/AmazonScience/massive) | 11514  | 2033  | 2974  |\n| [Leyzer 0.2.0](https://github.com/cartesinus/leyzer/tree/0.2.0)      | 3974   | 701   | 1380  |\n| [OpenSubtitles from OPUS](https://opus.nlpl.eu/OpenSubtitles-v1.php) | 2329   | 411   | 500   |\n| [KDE from OPUS](https://opus.nlpl.eu/KDE4.php)                       | 1154   | 241   | 241   |\n| [CCMatrix from Opus](https://opus.nlpl.eu/CCMatrix.php)              | 1096   | 232   | 237   |\n| [Ubuntu from OPUS](https://opus.nlpl.eu/Ubuntu.php)                  | 281    | 60    | 59    |\n| [Gnome from OPUS](https://opus.nlpl.eu/GNOME.php)                    | 14     | 3     | 3     |\n| *total*                                                              | 20362  | 3681  | 5394  |\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 0.2744        | 1.0   | 5091  | 0.2555          | 58.5119 | 21.0728 |\n| 0.1829        | 2.0   | 10182 | 0.2475          | 59.7364 | 21.0769 |\n| 0.1124        | 3.0   | 15273 | 0.2499          | 61.3552 | 21.06   |\n| 0.0783        | 4.0   | 20364 | 0.2597          | 61.6618 | 21.2402 |\n| 0.0496        | 5.0   | 25455 | 0.2698          | 62.1942 | 21.2901 |\n| 0.0318        | 6.0   | 30546 | 0.2798          | 61.9068 | 21.3399 |\n| 0.0204        | 7.0   | 35637 | 0.2893          | 61.7753 | 21.3102 |\n| 0.0138        | 8.0   | 40728 | 0.2979          | 62.3925 | 21.3238 |\n| 0.009         | 9.0   | 45819 | 0.3034          | 62.4942 | 21.2516 |\n| 0.0058        | 10.0  | 50910 | 0.3082          | 62.4604 | 21.2847 |\n\n\n### Framework versions\n\n- Transformers 4.26.1\n- Pytorch 1.13.1+cu116\n- Datasets 2.10.1\n- Tokenizers 0.13.2\n", "size_bytes": "4966633968", "downloads": 2}
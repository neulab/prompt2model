{"pretrained_model_name": "it5/it5-small-formal-to-informal", "description": "---\nlanguage:\n- it\nlicense: apache-2.0\ntags:\n- italian\n- sequence-to-sequence\n- style-transfer\n- formality-style-transfer\ndatasets:\n- yahoo/xformal_it\nwidget:\n- text: \"Questa performance \u00e8 a dir poco spiacevole.\"\n- text: \"In attesa di un Suo cortese riscontro, Le auguriamo un piacevole proseguimento di giornata.\"\n- text: \"Questa visione mi procura una goduria indescrivibile.\"\n- text: \"qualora ci\u00f2 possa interessarti, ti pregherei di contattarmi.\"\nmetrics:\n- rouge\n- bertscore\nmodel-index:\n- name: it5-small-formal-to-informal\n  results:\n  - task: \n      type: formality-style-transfer\n      name: \"Formal-to-informal Style Transfer\"\n    dataset:\n      type: xformal_it\n      name: \"XFORMAL (Italian Subset)\"\n    metrics:\n      - type: rouge1\n        value: 0.650\n        name: \"Avg. Test Rouge1\"\n      - type: rouge2\n        value: 0.450\n        name: \"Avg. Test Rouge2\"\n      - type: rougeL\n        value: 0.631\n        name: \"Avg. Test RougeL\"\n      - type: bertscore\n        value: 0.663\n        name: \"Avg. Test BERTScore\"\n        args:\n          - model_type: \"dbmdz/bert-base-italian-xxl-uncased\"\n          - lang: \"it\"\n          - num_layers: 10\n          - rescale_with_baseline: True\n          - baseline_path: \"bertscore_baseline_ita.tsv\"\nco2_eq_emissions:\n      emissions: \"8g\"\n      source: \"Google Cloud Platform Carbon Footprint\"\n      training_type: \"fine-tuning\"\n      geographical_location: \"Eemshaven, Netherlands, Europe\"\n      hardware_used: \"1 TPU v3-8 VM\"\n---\n\n# IT5 Small for Formal-to-informal Style Transfer \ud83e\udd17\n\nThis repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on Formal-to-informal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io).  \n\nA comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.\n\n## Using the model\n\nModel checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:\n\n```python\nfrom transformers import pipelines\n\nf2i = pipeline(\"text2text-generation\", model='it5/it5-small-formal-to-informal')\nf2i(\"Vi ringrazio infinitamente per vostra disponibilit\u00e0\")\n>>> [{\"generated_text\": \"e grazie per la vostra disponibilit\u00e0!\"}]\n```\n\nor loaded using autoclasses:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"it5/it5-small-formal-to-informal\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"it5/it5-small-formal-to-informal\")\n```\n\nIf you use this model in your research, please cite our work as:\n\n```bibtex\n@article{sarti-nissim-2022-it5,\n    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},\n    author={Sarti, Gabriele and Nissim, Malvina},\n    journal={ArXiv preprint 2203.03759},\n    url={https://arxiv.org/abs/2203.03759},\n    year={2022},\n\tmonth={mar}\n}\n```", "size_bytes": "307824645", "downloads": 4}
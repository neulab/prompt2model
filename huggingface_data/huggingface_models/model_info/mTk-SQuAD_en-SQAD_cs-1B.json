{"pretrained_model_name": "fewshot-goes-multilingual/mTk-SQuAD_en-SQAD_cs-1B", "description": "---\nlicense: mit\ndatasets:\n- squad\n- fewshot-goes-multilingual/cs_squad-3.0\nlanguage:\n- cs\n- en\nmetrics:\n- rouge\npipeline_tag: text2text-generation\n---\n\n# Model Card for mTk-SQuAD_en-SQAD_cs-1B\n\nThis model is a generative in-context few-shot learner specialized in Czech. It was trained on a combination of English SQuAD and Czech SQAD dataset.\n\nYou can find detailed information on [Project Github](https://github.com/fewshot-goes-multilingual/slavic-incontext-learning) & the referenced paper.\n\n\n## Model Details\n\n### Model Description\n\n\n- **Developed by:** Michal Stefanik & Marek Kadlcik, Masaryk University\n- **Model type:** mt5\n- **Language(s) (NLP):** cs,en\n- **License:** MIT\n- **Finetuned from model:** google/mt5-large\n\n### Model Sources\n\n- **Repository:** https://github.com/fewshot-goes-multilingual/slavic-incontext-learning\n- **Paper:** https://arxiv.org/abs/2304.01922\n\n## Uses\n\nThis model is intended to be used in a few-shot in-context learning format in the target language (Czech), or in the source language (English, see below).\nIt was evaluated for unseen task learning (with k=3 demonstrations) in Czech: see the referenced paper for details.\n\n### How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"{this model path}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{this model path}\")\n\n# Instead, use keywords \"Ot\u00e1zka\", \"Kontext\" and \"Odpov\u011b\u010f\" for Czech few-shot prompts\ninput_text = \"\"\"\n    Question: What is the customer's name? \n    Context: Origin: Barrack Obama, Customer id: Bill Moe. \n    Answer: Bill Moe, \n    Question: What is the customer's name? \n    Context: Customer id: Barrack Obama, if not deliverable, return to Bill Clinton. \n    Answer:\n\"\"\"\n\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(**inputs)\n\nprint(\"Answer:\")\nprint(tokenizer.decode(outputs))\n```\n\n## Training Details\n\nTraining this model can be reproduced by running `pip install -r requirements.txt && python train_mt5_qa_en_SQuAD+cs_random.py`. \nSee the referenced script for hyperparameters and other training configurations.\n\n## Citation\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n```bib\n@inproceedings{stefanik2023resources,\n               author = {\\v{S}tef\\'{a}nik, Michal and Kadl\u010d\u00edk, Marek and Gramacki, Piotr and Sojka, Petr},\n               title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n               booktitle = {Proceedings of the 9th Workshop on Slavic Natural Language Processing},\n               publisher = {ACL},\n               numpages = {9},\n               url = {https://arxiv.org/abs/2304.01922},\n}\n```", "size_bytes": "4918511257", "downloads": 5}
{"pretrained_model_name": "cffl/bart-base-styletransfer-subjective-to-neutral", "description": "---\nlicense: apache-2.0\n---\n\n# bart-base-styletransfer-subjective-to-neutral\n\n## Model description\nThis [facebook/bart-base](https://huggingface.co/facebook/bart-base) model has been fine-tuned on the [Wiki Neutrality Corpus (WNC)](https://arxiv.org/pdf/1911.09709.pdf) -  a parallel corpus of 180,000 biased and neutralized sentence pairs along with contextual sentences and metadata. The model can be used to transfer style in text from subjectively biased to neutrally toned.\n\nThe development and modeling efforts that produced this model are documented in detail through [this blog series](https://blog.fastforwardlabs.com/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html).\n\n## Intended uses & limitations\nThe model is intended purely as a research output for NLP and data science communities. We imagine this model will be used by researchers to better understand the limitations, robustness, and generalization of text style transfer models. Ultimately, we hope this model will inspire future work on text style transfer and serve as a benchmarking tool for the style attribute of subjectivity bias, specifically.\n\nAny production use of this model - whether commercial or not - is currently not intended. This is because, as [the team at OpenAI points out](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases), large langauge models like BART reflect biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans, unless the deployers first carry out a study of biases relevant to the intended use-case. Neither the model nor the WNC dataset has been sufficiently evaluated for performance and bias. Our efforts quantified model performance using two custom evaluation metrics, neither of which have been correlated to human evaluation for the task.\n\nAs we discuss in the blog series, since the WNC is a parallel dataset and we formulate the learning task as a supervised problem, the model indirectly adopts Wikipedia's NPOV policy as the definition for \"neutrality\" and \"subjectivity\". The NPOV policy may not fully reflect an end users assumed/intended meaning of subjectivity because the notion of subjectivity itself can be...well, subjective.\n\nWe discovered through our exploratory work that the WNC does contain data quality issues that will contribute to unintended bias in the model. For example, some NPOV revisions introduce factual information outside the context of the prompt as a means to correct bias. We believe these factual based edits are out of scope for a subjective-to-neutral style transfer modeling task, but exist here nonetheless. \n\n## How to use\nThis model can be used directly with a HuggingFace pipeline for `text2text-generation`.\n\n```python\n>>> from transformers import pipeline\n\n>>> styletransfer = pipeline(\n    task=\"text2text-generation\",\n    model=\"cffl/bart-base-styletransfer-subjective-to-neutral\",\n    max_length=200,\n)\n>>> input_text = \"chemical abstracts service (cas), a prominent division of the american chemical society, is the world's leading source of chemical information.\"\n>>> styletransfer(input_text)\n\n[{'generated_text': 'chemical abstracts service (cas), a division of the american chemical society, is a source of chemical information.'}]\n\n```\n\n## Training procedure\nFor modeling, we made extensive use of the Huggingface transformers library by initializing the [BartForConditionalGeneration](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartForConditionalGeneration) model with [facebook/bart-base](https://huggingface.co/facebook/bart-base) pretrained weights and adapting the [summarization fine-tuning script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) for our TST-specific needs. We fine-tune the model for 15 epochs on an NVIDIA Tesla V100 GPU with a batch size of 32. (Note that when fine-tuning the model with the parallel examples, the noising function is turned off so an uncorrupted document is passed to BART's encoder and decoder.)\n\nPlease refer to [our blog series](https://blog.fastforwardlabs.com/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html) for a discussion of evaluation metrics and results.\n", "size_bytes": "557976249", "downloads": 295}
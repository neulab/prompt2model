{"pretrained_model_name": "pszemraj/long-t5-tglobal-xl-16384-book-summary", "description": "---\nlicense:\n- apache-2.0\n- bsd-3-clause\ntags:\n- summarization\n- summary\n- booksum\n- long-document\n- long-form\n- tglobal-xl\n- XL\ndatasets:\n- kmfoda/booksum\nmetrics:\n- rouge\ninference: false\nmodel-index:\n- name: pszemraj/long-t5-tglobal-xl-16384-book-summary\n  results:\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: multi_news\n      type: multi_news\n      config: default\n      split: test\n    metrics:\n    - type: rouge\n      value: 36.2043\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzRmMmUyOTVjMmJmZTRiZDcyYzY3MTQ1MmUyNDA5NjVhYzEzYzBiNzcxYTRhMDQ3OTlhMGZjYmJlNDM1M2NjYyIsInZlcnNpb24iOjF9._uArOQ1_0znXDPXMq7unA1OHB-XbgqzzKRbFRcVUzTUJdWk26LiSa2pEEVNNmJPg6Uo7CAvONmhpEswLvl9TAg\n    - type: rouge\n      value: 8.424\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNzg0MzljYjVjYWQ3MmRkZDBlOGI5M2RiMGU0M2UwZGUzMDg2NTU0NjcwMTNiN2ZmODEzNTQ0MmEwNDA3NDA5MSIsInZlcnNpb24iOjF9.Dzj85ld6TjosQ8KyUdoadzicMLedEFrICC6Q-08O3qx28d9B9Uke1zw-VWabiuesPEDTRGbWuBgPA5vxYWUZAw\n    - type: rouge\n      value: 17.3721\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDA3ZjZmODAwMTNlM2RlZmJlMDI5MGVkMGRkMTBjMTYzNDk5ZjFiNTY5MWE1MDUwNWI2MDE4ZDA2YWMwMmI2NCIsInZlcnNpb24iOjF9.MOV_nId0XAK1eMQssG5GN9DsitZaTrxl4jdCJnOg9EZ0-vAw227ln599YV5YfZ1OPJnWwek6rneqqyONiHn9AQ\n    - type: rouge\n      value: 32.3994\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZmY3MDMwOTZjNWI0YTk1MDgwMzJkYTFiN2U5YWU0Mzc0MWRiMzc1NzZlMDhjMWUwMmY2ODI2MjI5ODBkYWUxOSIsInZlcnNpb24iOjF9._BwGIZbcA4pUBkEAL0cW-JPPta0KSoGug4Z7vogHacUz-AEhIOI5ICUldZh0pt9OK67MpUSzpShJOu3rSt5YDQ\n    - type: loss\n      value: 2.0843334197998047\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOWFhMmE5ZjA3ODM4YmVjMDMyMjk5YjNlMjA1MGMzOWY0NTRlYzk1YjZiMzQxMDMxOTMwMjFkNTdmNjM1NDcyMyIsInZlcnNpb24iOjF9.3wbXV4CIIgnfXAnnRztdOR12PwsWsEfiglQQ09K-C1EgW4gai4x9l-wTE2OZ7CTWkuk_tr4tL_uqOCXLZRMtCQ\n    - type: gen_len\n      value: 248.3572\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMWZhOGMwMDJjNGU2MzA2YzI1OWU1ZDY5N2NjZmM1YTA5NDg1MzUwNmU1YTBhNjQyNWYwYzA3OGNmODFjMmE2NSIsInZlcnNpb24iOjF9.Rc9u89zCdbFnjsnmq65l_JvCtUwOX_ZWapKJpTZ-rC8HxcUVfi2Ash2QfvvvxHH_YWhwklxxdnNa0HCm46qLAA\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: billsum\n      type: billsum\n      config: default\n      split: test\n    metrics:\n    - name: ROUGE-1\n      type: rouge\n      value: 41.3645\n      verified: true\n    - name: ROUGE-2\n      type: rouge\n      value: 16.144\n      verified: true\n    - name: ROUGE-L\n      type: rouge\n      value: 24.2981\n      verified: true\n    - name: ROUGE-LSUM\n      type: rouge\n      value: 35.3234\n      verified: true\n    - name: loss\n      type: loss\n      value: 1.282260775566101\n      verified: true\n    - name: gen_len\n      type: gen_len\n      value: 291.8158\n      verified: true\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: ccdv/arxiv-summarization\n      type: ccdv/arxiv-summarization\n      config: document\n      split: test\n    metrics:\n    - name: ROUGE-1\n      type: rouge\n      value: 36.3225\n      verified: true\n    - name: ROUGE-2\n      type: rouge\n      value: 9.3743\n      verified: true\n    - name: ROUGE-L\n      type: rouge\n      value: 19.8396\n      verified: true\n    - name: ROUGE-LSUM\n      type: rouge\n      value: 32.2532\n      verified: true\n    - name: loss\n      type: loss\n      value: 2.146871566772461\n      verified: true\n    - name: gen_len\n      type: gen_len\n      value: 186.2966\n      verified: true\n---\n\n# long-t5-tglobal-xl + BookSum\n\n<a href=\"https://colab.research.google.com/gist/pszemraj/c19e32baf876deb866c31cd46c86e893/long-t5-xl-accelerate-test.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nSummarize long text and get a SparkNotes-like summary of any topic!\n\n- Generalizes reasonably well to academic & narrative text.\n- This is the XL checkpoint, which **produces even better summaries [from a human evaluation perspective](https://long-t5-xl-book-summary-examples.netlify.app/)**.\n\nA simple example/use case with [the base model](https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-book-summary) on ASR is [here](https://longt5-booksum-example.netlify.app/).\n\n## Cheeky Proof-of-Concept\n\nA summary of the [infamous navy seals copypasta](https://knowyourmeme.com/memes/navy-seal-copypasta):\n\n> In this chapter, the monster explains how he intends to exact revenge on \"the little b\\*\\*\\*\\*\" who insulted him. He tells the kiddo that he is a highly trained and experienced killer who will use his arsenal of weapons--including his access to the internet--to exact justice on the little brat.\n\nWhile this is a crude example, try running this copypasta through other summarization models to see the difference in comprehension (_even though it's not even a \"long\" text!_).\n\n* * *\n\n**Contents**\n\n<!-- TOC -->\n\n- [Description](#description)\n- [How-To in Python](#how-to-in-python)\n    - [Beyond the basics](#beyond-the-basics)\n        - [Adjusting parameters](#adjusting-parameters)\n        - [LLM.int8 Quantization](#llmint8-quantization)\n- [About](#about)\n    - [Intended uses & limitations](#intended-uses--limitations)\n    - [Training and evaluation data](#training-and-evaluation-data)\n    - [Eval results](#eval-results)\n- [FAQ](#faq)\n    - [How can I run inference with this on CPU?](#how-can-i-run-inference-with-this-on-cpu)\n    - [How to run inference over a very long (30k+ tokens) document in batches?](#how-to-run-inference-over-a-very-long-30k-tokens-document-in-batches)\n    - [How to fine-tune further?](#how-to-fine-tune-further)\n    - [Are there simpler ways to run this?](#are-there-simpler-ways-to-run-this)\n- [Training procedure](#training-procedure)\n    - [Updates](#updates)\n    - [Training hyperparameters](#training-hyperparameters)\n    - [Framework versions](#framework-versions)\n\n<!-- /TOC -->\n\n* * *\n\n## Description\n\nA fine-tuned version of [google/long-t5-tglobal-xl](https://huggingface.co/google/long-t5-tglobal-xl) on the `kmfoda/booksum` dataset.\n\nRead the paper by Guo et al. here: [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/pdf/2112.07916.pdf)\n\n## How-To in Python\n\ninstall/update transformers `pip install -U transformers`\n\nsummarize text with pipeline:\n\n```python\nimport torch\nfrom transformers import pipeline\n\nsummarizer = pipeline(\n    \"summarization\",\n    \"pszemraj/long-t5-tglobal-xl-16384-book-summary\",\n    device=0 if torch.cuda.is_available() else -1,\n)\nlong_text = \"Here is a lot of text I don't want to read. Replace me\"\n\nresult = summarizer(long_text)\nprint(result[0][\"summary_text\"])\n```\n\n### Beyond the basics\n\nThere are two additional points to consider beyond simple inference: adjusting decoding parameters for improved performance, and quantization for reduced memory consumption.\n\n#### Adjusting parameters\n\nPass [other parameters related to beam search textgen](https://huggingface.co/blog/how-to-generate) when calling `summarizer` to get even higher quality results.\n\n#### LLM.int8 Quantization\n\n> alternative section title: how to get this monster to run inference on free colab runtimes\n\nVia [this PR](https://github.com/huggingface/transformers/pull/20341) LLM.int8 is now supported for `long-t5` models. \n\n- per **initial tests** the summarization quality seems to hold while using _significantly_ less memory! \\*\n- a version of this model quantized to int8 is [already on the hub here](https://huggingface.co/pszemraj/long-t5-tglobal-xl-16384-book-summary-8bit) so if you're using the 8-bit version anyway, you can start there for a 3.5 gb download only!\n\nFirst, make sure you have the latest versions of the relevant packages:\n```bash\npip install -U transformers bitsandbytes accelerate\n```\n\nload in 8-bit (_magic completed by `bitsandbytes` behind the scenes_)\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"pszemraj/long-t5-tglobal-xl-16384-book-summary\"\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    \"pszemraj/long-t5-tglobal-xl-16384-book-summary\",\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\n```\n\nThe above is already present in the Colab demo linked at the top of the model card.\n\n\\* More rigorous metrics-based research comparing beam-search summarization with and without LLM.int8 will take place over time.\n\n* * *\n\n## About\n\n### Intended uses & limitations\n\nWhile this model seems to improve factual consistency, **don't take summaries as foolproof and check things that seem odd**.\n\nSpecifically: negation statements (i.e., the model says: _this thing does not have [ATTRIBUTE]_, when instead it should have said _this thing has lots of [ATTRIBUTE]_).\n\n- I'm sure someone will write a paper on this eventually (if there isn't one already), but you can usually check this by comparing a particular statement with what the surrounding sentences imply.\n\n### Training and evaluation data\n\n`kmfoda/booksum` dataset on HuggingFace - read [the original paper here](https://arxiv.org/abs/2105.08209).\n\n- For **initial fine-tuning**, only input text with 12288 input tokens or less and 1024 output tokens or less was used (_i.e. lines longer than that were dropped before training_) for memory reasons. After a quick analysis, summaries in the 12288-16384 range are in the **small** minority in this dataset.\n    - In addition, this initial training combined the training and validation sets and trained on them in aggregate to increase the functional dataset size. **Therefore, take the validation set results with a grain of salt; primary metrics should (always) be the test set.**.\n- The **final stages of fine-tuning** used the standard 16384 input/1024 output conventions, preserving the standard in/out lengths (_and truncating longer sequences_). This did not seem to change the loss/performance much.\n\n### Eval results\n\nOfficial results with the [model evaluator](https://huggingface.co/spaces/autoevaluate/model-evaluator) will be computed and posted here.\n\n**Please read the note above, as due to the training methods, the performance on the validation set looks better than the results on the test set will be**. The model achieves the following results on the evaluation set:\n\n-   eval_loss: 1.2756\n-   eval_rouge1: 41.8013\n-   eval_rouge2: 12.0895\n-   eval_rougeL: 21.6007\n-   eval_rougeLsum: 39.5382\n-   eval_gen_len: 387.2945\n-   eval_runtime: 13908.4995\n-   eval_samples_per_second: 0.107\n-   eval_steps_per_second: 0.027\n\n\n    ***** predict/test metrics (initial) *****\n      predict_gen_len            =   506.4368\n      predict_loss               =      2.028\n      predict_rouge1             =    36.8815\n      predict_rouge2             =     8.0625\n      predict_rougeL             =    17.6161\n      predict_rougeLsum          =    34.9068\n      predict_runtime            = 2:04:14.37\n      predict_samples            =       1431\n      predict_samples_per_second =      0.192\n      predict_steps_per_second   =      0.048\n\n\\* evaluating big model not as easy as it seems. Doing a bit more investigating\n\n* * *\n\n## FAQ\n\n### How can I run inference with this on CPU?\n\nlol\n\n### How to run inference over a very long (30k+ tokens) document in batches?\n\nSee `summarize.py` in [the code for my hf space Document Summarization](https://huggingface.co/spaces/pszemraj/document-summarization/blob/main/summarize.py) :)\n\nYou can also use the same code to split a document into batches of 4096, etc., and iterate over them with the model. This is useful in situations where CUDA memory is limited.\n\n**Update:** see the section on the `textsum` package below.\n\n### How to fine-tune further?\n\nSee [train with a script](https://huggingface.co/docs/transformers/run_scripts) and [the summarization scripts](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)\n\n### Are there simpler ways to run this?\n\nFor this reason, I created a Python package utility. It's called [textsum](https://github.com/pszemraj/textsum), and you can use it to load models and summarize things in a few lines of code.\n\n```sh\npip install textsum\n```\n\nUse `textsum` in python with this model:\n\n```python\nfrom textsum.summarize import Summarizer\n\nsummarizer = Summarizer(\n    model_name_or_path=\"pszemraj/long-t5-tglobal-xl-16384-book-summary\"\n)\n\nlong_string = \"This is a long string of text that will be summarized.\"\nout_str = summarizer.summarize_string(long_string)\nprint(f\"summary: {out_str}\")\n```\n\nThis package provides easy-to-use interfaces for applying summarization models to text documents of arbitrary length. Currently implemented interfaces include a Python API, a CLI, and a shareable demo application.\n\nFor details, explanations, and documentation, see the README (_linked above_) or the [wiki](https://github.com/pszemraj/textsum/wiki).\n\n* * *\n\n## Training procedure\n\n### Updates\n\nUpdates to this model/model card will be posted here when relevant. The model seems to be fairly converged; if updates/improvements are possible using the `BookSum` dataset, this repo will be updated.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n-   learning_rate: 0.0006\n-   train_batch_size: 1\n-   eval_batch_size: 1\n-   seed: 10350\n-   distributed_type: multi-GPU\n-   num_devices: 4\n-   gradient_accumulation_steps: 32\n-   total_train_batch_size: 128\n-   total_eval_batch_size: 4\n-   optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n-   lr_scheduler_type: constant\n-   num_epochs: 1.0\n\n\\*_Prior training sessions used roughly similar parameters (learning rates were higher); multiple sessions were required as this takes eons to train._\n\n### Framework versions\n\n-   Transformers 4.25.0.dev0\n-   Pytorch 1.13.0+cu117\n-   Datasets 2.6.1\n-   Tokenizers 0.13.1\n\n* * *\n", "size_bytes": 11925614592, "downloads": 14564}
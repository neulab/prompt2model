{"pretrained_model_name": "ADELIB/ANQG", "description": "Hugging Face's logo\nHugging Face\nSearch models, datasets, users...\nModels\nDatasets\nSpaces\nDocs\nSolutions\nPricing\n\n\n\n\nMihakram\n/\nArabic_Question_Generation Copied\nlike\n0\nText2Text Generation\nPyTorch\nTransformers\nArabic\n\narxiv:2109.12068\nt5\nAutoTrain Compatible\nModel card\nFiles and versions\nCommunity\nArabic_Question_Generation\n/\nREADME.md\nMihakram's picture\nMihakram\nUpdate README.md\n9920e0d\n14 minutes ago\nraw\nhistory\nblame\ncontribute\ndelete\nSafe\n3.41 kB\n---\nlanguage: \n- ar\nwidget:\n- text: \"context: \u0627\u0644\u062b\u0648\u0631\u0629 \u0627\u0644\u062c\u0632\u0627\u0626\u0631\u064a\u0629 \u0623\u0648 \u062b\u0648\u0631\u0629 \u0627\u0644\u0645\u0644\u064a\u0648\u0646 \u0634\u0647\u064a\u062f\u060c \u0627\u0646\u062f\u0644\u0639\u062a \u0641\u064a 1 \u0646\u0648\u0641\u0645\u0628\u0631 1954 \u0636\u062f \u0627\u0644\u0645\u0633\u062a\u0639\u0645\u0631 \u0627\u0644\u0641\u0631\u0646\u0633\u064a \u0648\u062f\u0627\u0645\u062a 7 \u0633\u0646\u0648\u0627\u062a \u0648\u0646\u0635\u0641. \u0627\u0633\u062a\u0634\u0647\u062f \u0641\u064a\u0647\u0627 \u0623\u0643\u062b\u0631 \u0645\u0646 \u0645\u0644\u064a\u0648\u0646 \u0648\u0646\u0635\u0641 \u0645\u0644\u064a\u0648\u0646 \u062c\u0632\u0627\u0626\u0631\u064a answer:  7 \u0633\u0646\u0648\u0627\u062a \u0648\u0646\u0635\u0641 </s>\n\"\n- text: \"context: \u0627\u0633\u0643\u062a\u0644\u0646\u062f\u0627 \u062f\u0648\u0644\u0629 \u0641\u064a \u0634\u0645\u0627\u0644 \u063a\u0631\u0628 \u0623\u0648\u0631\u0648\u0628\u0627\u060c \u062a\u0639\u062a\u0628\u0631 \u062c\u0632\u0621 \u0645\u0646 \u0627\u0644\u062f\u0648\u0644 \u0627\u0644\u0623\u0631\u0628\u0639 \u0627\u0644\u0645\u0643\u0648\u0646\u0629 \u0627\u0644\u0645\u0645\u0644\u0643\u0629 \u0627\u0644\u0645\u062a\u062d\u062f\u0629. \u062a\u062d\u062a\u0644 \u0627\u0644\u062b\u0644\u062b \u0627\u0644\u0634\u0645\u0627\u0644\u064a \u0645\u0646 \u062c\u0632\u064a\u0631\u0629 \u0628\u0631\u064a\u0637\u0627\u0646\u064a\u0627 \u0627\u0644\u0639\u0638\u0645\u0649 \u0648\u062a\u062d\u062f\u0647\u0627 \u062c\u0646\u0648\u0628\u0627 \u0625\u0646\u062c\u0644\u062a\u0631\u0627 \u0648\u064a\u062d\u062f\u0647\u0627 \u0634\u0631\u0642\u0627 \u0628\u062d\u0631 \u0627\u0644\u0634\u0645\u0627\u0644 \u0648\u063a\u0631\u0628\u0627 \u0627\u0644\u0645\u062d\u064a\u0637 \u0627\u0644\u0623\u0637\u0644\u0633\u064a. \u0639\u0627\u0635\u0645\u062a\u0647\u0627 \u0623\u062f\u0646\u0628\u0631\u0629\u060c \u0648\u0623\u0647\u0645 \u0645\u062f\u0646\u0647\u0627 \u0648\u0623\u0643\u0628\u0631\u0647\u0627 \u0645\u062f\u064a\u0646\u0629 \u063a\u0644\u0627\u0633\u0643\u0648. \u0643\u0627\u0646\u062a \u0627\u0633\u0643\u062a\u0644\u0646\u062f\u0627 \u0645\u0645\u0644\u0643\u0629 \u0645\u0633\u062a\u0642\u0644\u0629 \u062d\u062a\u0649 1 \u0645\u0627\u064a\u0648 1707  answer:  \u0623\u062f\u0646\u0628\u0631\u0629  </s>\"\n\n- text: \"context: \u0645\u0627\u062a \u0627\u0644\u0645\u0633\u062a\u0634\u0627\u0631 \u0627\u0644\u0623\u0644\u0645\u0627\u0646\u064a \u0623\u062f\u0648\u0644\u0641 \u0647\u062a\u0644\u0631 \u0641\u064a 30 \u0623\u0628\u0631\u064a\u0644 1945 \u0645\u0646\u062a\u062d\u0631\u0627 \u0639\u0646 \u0637\u0631\u064a\u0642 \u062a\u0646\u0627\u0648\u0644 \u0645\u0627\u062f\u0629 \u0627\u0644\u0633\u064a\u0627\u0646\u064a\u062f \u0627\u0644\u0633\u0627\u0645\u0629 \u0648\u0625\u0637\u0644\u0627\u0642 \u0627\u0644\u0646\u0627\u0631 \u0639\u0644\u0649 \u0646\u0641\u0633\u0647 \u0648\u0647\u064a \u0627\u0644\u0631\u0648\u0627\u064a\u0629 \u0627\u0644\u0639\u0627\u0645\u0629 \u0627\u0644\u0645\u0642\u0628\u0648\u0644\u0629 \u0644\u0637\u0631\u064a\u0642\u0629 \u0645\u0648\u062a \u0627\u0644\u0632\u0639\u064a\u0645 \u0627\u0644\u0646\u0627\u0632\u064a answer: \u0645\u0646\u062a\u062d\u0631\u0627 </s>\n\"\n\n---\n# Arabic Question generation Model\n\n\nThis model is ready to use for **Question generation**, simply input the text and answer and the model will generate a question,  This model is a fine-tuned version of [AraT5-Base Model](https://huggingface.co/UBC-NLP/AraT5-base)\n \n\n## Requirements\n```\n!pip install transformers\n```\n\n\n## Model in Action \ud83d\ude80\n```python\nfrom transformers import AutoTokenizer,AutoModelForSeq2SeqLM\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Mihakram/Arabic_Question_Generation\")\ntokenizer = AutoTokenizer.from_pretrained(\"Mihakram/Arabic_Question_Generation\")\ndef get_question(context,answer):\n  text=\"context: \" +context + \" \" + \"answer: \" + answer + \" </s>\"\n  text_encoding = tokenizer.encode_plus(\n      text,return_tensors=\"pt\"\n  )\n  model.eval()\n  generated_ids =  model.generate(\n    input_ids=text_encoding['input_ids'],\n    attention_mask=text_encoding['attention_mask'],\n    max_length=64,\n    num_beams=5,\n    num_return_sequences=1\n  )\n  return tokenizer.decode(generated_ids[0],skip_special_tokens=True,clean_up_tokenization_spaces=True).replace('question: ',' ')\ncontext=\"\u0627\u0644\u062b\u0648\u0631\u0629 \u0627\u0644\u062c\u0632\u0627\u0626\u0631\u064a\u0629 \u0623\u0648 \u062b\u0648\u0631\u0629 \u0627\u0644\u0645\u0644\u064a\u0648\u0646 \u0634\u0647\u064a\u062f\u060c \u0627\u0646\u062f\u0644\u0639\u062a \u0641\u064a 1 \u0646\u0648\u0641\u0645\u0628\u0631 1954 \u0636\u062f \u0627\u0644\u0645\u0633\u062a\u0639\u0645\u0631 \u0627\u0644\u0641\u0631\u0646\u0633\u064a \u0648\u062f\u0627\u0645\u062a 7 \u0633\u0646\u0648\u0627\u062a \u0648\u0646\u0635\u0641. \u0627\u0633\u062a\u0634\u0647\u062f \u0641\u064a\u0647\u0627 \u0623\u0643\u062b\u0631 \u0645\u0646 \u0645\u0644\u064a\u0648\u0646 \u0648\u0646\u0635\u0641 \u0645\u0644\u064a\u0648\u0646 \u062c\u0632\u0627\u0626\u0631\u064a\"\nanswer =\" 7 \u0633\u0646\u0648\u0627\u062a \u0648\u0646\u0635\u0641\"\nget_question(context,answer)\n#output : question=\"\u0643\u0645 \u0627\u0633\u062a\u0645\u0631\u062a \u0627\u0644\u062b\u0648\u0631\u0629 \u0627\u0644\u062c\u0632\u0627\u0626\u0631\u064a\u0629\u061f \" \n```\n\n## Expriments\nWe report score with `NQG Scorer`.\n\nIf not special explanation, the size of the model defaults to \"base\".\n\n### Metrics resaults\n Bleu 1|Bleu 2|Bleu 3|Bleu 4|METEOR|ROUGE-L|\n------|------|------|------|------|-------|\n54.67 |39.26 |30.34 |24.15 |25.43 |52.64  |\n\n## References\n\nThe **Ara-T5** model was presented in [AraT5: Text-to-Text Transformers for Arabic Language Generation](https://arxiv.org/abs/2109.12068) by *El Moatez Billah Nagoudi, AbdelRahim Elmadany, Muhammad Abdul-Mageed* \n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{Mihakram/,\n  title={},\n  author={Mihoubi, Ibrir},\n  publisher={Hugging Face},\n  journal={Hugging Face Hub},\n  howpublished={\\url{https://huggingface.co/}},\n  year={2022}\n}\n```", "size_bytes": "1131173775", "downloads": 2}
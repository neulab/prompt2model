{"pretrained_model_name": "ethzanalytics/blip2-flan-t5-xl-sharded", "description": "---\nlicense: mit\nlanguage:\n- en\nlibrary_name: transformers\ninference: False\n---\n## Sharded BLIP-2 Model Card - flan-t5-xl\n\n<a href=\"https://colab.research.google.com/gist/pszemraj/0822b7f28b14405f10cfd382296873de/blip2-flan-t5-xl-sharded-example.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nThis is a sharded version of the [blip2-flan-t5-xl](https://huggingface.co/Salesforce/blip2-flan-t5-xl) which leverages [Flan T5-xl](https://huggingface.co/google/flan-t5-xl) for image-to-text tasks such as image captioning and visual question answering.\n\n- this model repo is sharded so it can be easily loaded on low-RAM Colab runtimes :)\n- Refer to the [original model card](https://huggingface.co/Salesforce/blip2-flan-t5-xl) for more details about the model description, intended uses, and limitations, as well as instructions for how to use the model on CPU and GPU in different precisions.\n\n## Usage\n\nRefer to the original model card for details or see [this blog post](https://huggingface.co/blog/blip-2#using-blip-2-with-hugging-face-transformers). Here is how you can use it on CPU:\n\nInstall\n\nRequires the current `main` of transformers (_at time of writing_):\n```bash\npip install accelerate git+https://github.com/huggingface/transformers.git -U -q\n```\n\nUse (_this is for CPU, check out the original model card/blog for `fp16` and `int8` usage_)\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\n\nmodel_name = \"ethzanalytics/blip2-flan-t5-xl-sharded\"\nprocessor = BlipProcessor.from_pretrained(model_name)\nmodel = Blip2ForConditionalGeneration.from_pretrained(model_name)\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n```\n\n", "size_bytes": 16296171520, "downloads": 213}
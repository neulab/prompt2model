{"pretrained_model_name": "research-backup/mbart-large-cc25-trimmed-ko", "description": "# Vocabulary Trimmed [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25): `asahi417/mbart-large-cc25-trimmed-ko` \nThis model is a trimmed version of [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25) by [`vocabtrimmer`](https://github.com/asahi417/lm-vocab-trimmer), a tool for trimming vocabulary of language models to compress the model size.\nFollowing table shows a summary of the trimming process.\n\n|                            | facebook/mbart-large-cc25   | asahi417/mbart-large-cc25-trimmed-ko   |\n|:---------------------------|:----------------------------|:---------------------------------------|\n| parameter_size_full        | 610,851,840                 | 371,345,408                            |\n| parameter_size_embedding   | 512,055,296                 | 33,042,432                             |\n| vocab_size                 | 250,027                     | 16,134                                 |\n| compression_rate_full      | 100.0                       | 60.79                                  |\n| compression_rate_embedding | 100.0                       | 6.45                                   |", "size_bytes": "1485614749", "downloads": 2}
{"pretrained_model_name": "flexudy/t5-base-multi-sentence-doctor", "description": "![avatar](sent-banner.png)\n\n# Sentence-Doctor\nSentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\n\n## 1. Problem:\nMany NLP models depend on tasks like *Text Extraction Libraries, OCR, Speech to Text libraries* and **Sentence Boundary Detection**\nAs a consequence errors caused by these tasks in your NLP pipeline can affect the quality of models in applications. Especially since models are often trained on **clean** input.\n\n## 2. Solution:\nHere we provide a model that **attempts** to reconstruct sentences based on the its context (sourrounding text). The task is pretty straightforward:\n* `Given an \"erroneous\" sentence, and its context, reconstruct the \"intended\" sentence`.\n\n## 3. Use Cases:\n* Attempt to repair noisy sentences that where extracted with OCR software or text extractors.\n* Attempt to repair sentence boundaries.\n  * Example (in German): **Input: \"und ich bin im**\", \n    * Prefix_Context: \"Hallo! Mein Name ist John\", Postfix_Context: \"Januar 1990 geboren.\"\n    * Output: \"John und ich bin im Jahr 1990 geboren\"\n* Possibly sentence level spelling correction -- Although this is not the intended use.\n * Input: \"I went to church **las yesteday**\" => Output: \"I went to church last Sunday\".\n \n## 4. Disclaimer\nNote how we always emphises on the word *attempt*. The current version of the model was only trained on **150K** sentences from the tatoeba dataset: https://tatoeba.org/eng. (50K per language -- En, Fr, De).\nHence, we strongly encourage you to finetune the model on your dataset. We might release a version trained on more data.\n\n## 5. Datasets\nWe generated synthetic data from the tatoeba dataset: https://tatoeba.org/eng. Randomly applying different transformations on words and characters based on some probabilities. The datasets are available in the data folder (where **sentence_doctor_dataset_300K** is a larger dataset with 100K sentences for each language).\n\n## 6. Usage\n\n### 6.1 Preprocessing\n* Let us assume we have the following text (Note that there are no punctuation marks in the text):\n\n```python\ntext = \"That is my job I am a medical doctor I save lives\"\n```\n* You decided extract the sentences and for some obscure reason, you obtained these sentences:\n\n```python\nsentences = [\"That is my job I a\", \"m a medical doct\", \"I save lives\"]\n```\n* You now wish to correct the sentence **\"m a medical doct\"**.\n\nHere is the single preprocessing step for the model:\n\n```python\ninput_text = \"repair_sentence: \" + sentences[1] + \" context: {\" + sentences[0] + \"}{\" + sentences[2] + \"} </s>\"\n```\n\n**Explanation**:</br>\n* We are telling the model to repair the sentence with the prefix \"repair_sentence: \"\n* Then append the sentence we want to repair **sentence[1]** which is \"m a medical doct\"\n* Next we give some context to the model. In the case, the context is some text that occured before the sentence and some text that appeard after the sentence in the original text.\n * To do that, we append the keyword \"context :\"\n * Append **{sentence[0]}** \"{That is my job I a}\". (Note how it is sourrounded by curly braces).\n * Append **{sentence[2]}** \"{I save lives}\". \n* At last we tell the model this is the end of the input with </s>.\n\n```python\nprint(input_text) # repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\n```\n\n<br/>\n\n**The context is optional**, so the input could also be ```repair_sentence: m a medical doct context: {}{} </s>```\n\n### 6.2 Inference\n\n```python\n\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"flexudy/t5-base-multi-sentence-doctor\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"flexudy/t5-base-multi-sentence-doctor\")\n\ninput_text = \"repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\"\n\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\n\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\nassert sentence == \"I am a medical doctor.\"\n```\n\n## 7. Fine-tuning\nWe also provide a script `train_any_t5_task.py` that might help you fine-tune any Text2Text Task with T5. We added #TODO comments all over to help you use train with ease. For example:\n\n```python\n# TODO Set your training epochs\nconfig.TRAIN_EPOCHS = 3\n``` \nIf you don't want to read the #TODO comments, just pass in your data like this\n\n```python\n# TODO Where is your data ? Enter the path\ntrainer.start(\"data/sentence_doctor_dataset_300.csv\")\n```\nand voila!! Please feel free to correct any mistakes in the code and make a pull request.\n\n## 8. Attribution\n* [Huggingface](https://huggingface.co/) transformer lib for making this possible\n* Abhishek Kumar Mishra's transformer [tutorial](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb) on text summarisation. Our training code is just a modified version of their code. So many thanks.\n* We finetuned this model from the huggingface hub: WikinewsSum/t5-base-multi-combine-wiki-news. Thanks to the [authors](https://huggingface.co/WikinewsSum)\n* We also read a lot of work from [Suraj Patil](https://github.com/patil-suraj)\n* No one has been forgotten, hopefully :)\n", "size_bytes": "891692471", "downloads": 319511}
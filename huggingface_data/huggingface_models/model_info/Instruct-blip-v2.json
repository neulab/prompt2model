{"pretrained_model_name": "prasanna2003/Instruct-blip-v2", "description": "---\ndatasets:\n- MMInstruction/M3IT\npipeline_tag: image-to-text\n---\n\nThis model is fintuned on instruction dataset using `SalesForce/blip-imagecaptioning-base` model.\n## Usage:\n```\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport torch\nfrom PIL import Image\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nif processor.tokenizer.eos_token is None:\n    processor.tokenizer.eos_token = '<|eos|>'\nmodel = BlipForConditionalGeneration.from_pretrained(\"prasanna2003/Instruct-blip-v2\")\n\nimage = Image.open('file_name.jpg').convert('RGB')\n\nprompt = \"\"\"Instruction: Answer the following input according to the image.\nInput: Describe this image.\noutput: \"\"\"\n\ninputs = processor(image, prompt, return_tensors=\"pt\")\n\noutput = model.generate(**inputs, max_length=100)\nprint(tokenizer.decode(output[0]))\n```", "size_bytes": "989827505", "downloads": 88}
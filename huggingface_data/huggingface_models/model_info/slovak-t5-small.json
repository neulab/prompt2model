{"pretrained_model_name": "ApoTro/slovak-t5-small", "description": "---\nlanguage: sk\nlicense: mit\ndatasets:\n- oscar\n---\n\n# SlovakT5-small\nThis model was trained on slightly adapted code from [run_t5_mlm_flax.py](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling). \nIf you want to know about training details or evaluation results, see [SlovakT5_report.pdf](https://huggingface.co/ApoTro/slovak-t5-small/resolve/main/SlovakT5_report.pdf). For evaluation, you can also run [SlovakT5_eval.ipynb](https://colab.research.google.com/github/richardcepka/notebooks/blob/main/SlovakT5_eval.ipynb).\n\n### How to use\nSlovakT5-small can be fine-tuned for a lot of different downstream tasks. For example, NER: \n```python\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\ntokenizer = AutoTokenizer.from_pretrained(\"ApoTro/slovak-t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"ApoTro/slovak-t5-small\")\n\ninput_ids = tokenizer(\"ner veta: Do druh\u00e9ho kola post\u00fapili Robert Fico a Andrej Kiska s rozdielom 4,0%.\", return_tensors=\"pt\").input_ids\nlabels = tokenizer(\"per: Robert Fico | per: Andrej Kiska\", return_tensors=\"pt\").input_ids\n\n# the forward function automatically creates the correct decoder_input_ids\nloss = model(input_ids=input_ids, labels=labels).loss\nloss.item()\n```", "size_bytes": "242050055", "downloads": 2}
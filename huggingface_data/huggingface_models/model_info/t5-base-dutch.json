{"pretrained_model_name": "flax-community/t5-base-dutch", "description": "---\nlanguage: \n- dutch\ntags:\n- seq2seq\n- lm-head\ndatasets:\n- yhavinga/mc4_nl_cleaned\nlicense: apache-2.0\ninference: false\n---\n\n# t5-base-dutch \n\nCreated by [Yeb Havinga](https://www.linkedin.com/in/yeb-havinga-86530825/)\n& [Dat Nguyen](https://www.linkedin.com/in/dat-nguyen-49a641138/) during the [Hugging Face community week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), organized by [HuggingFace](https://huggingface.co/) and TPU usage sponsored by Google, for the project [Pre-train T5 from scratch in Dutch](https://discuss.huggingface.co/t/pretrain-t5-from-scratch-in-dutch/8109).\n\nSee also the fine-tuned [t5-base-dutch-demo](https://huggingface.co/flax-community/t5-base-dutch-demo) model,\nand the demo application **[Netherformer \ud83d\udcf0](https://huggingface.co/spaces/flax-community/netherformer)**,\nthat are based on this model.\n\n**5 jan 2022: Model updated. Evaluation accuracy increased from 0.64 to 0.70.**\n\n**11 jan 2022: See also [yhavinga/t5-v1.1-base-dutch-cased](https://huggingface.co/yhavinga/t5-v1.1-base-dutch-cased) with eval acc 0.78**\n\n## Model\n\n* Configuration based on `google/t5-base`\n* 12 layers, 12 heads\n* Dropout set to 0.1\n\n## Dataset\n\nThis model was trained on the `full` configuration of [cleaned Dutch mC4](https://huggingface.co/datasets/yhavinga/mc4_nl_cleaned),\nwhich is the original mC4, except\n\n  * Documents that contained words from a selection of the Dutch and English [List of Dirty Naught Obscene and Otherwise Bad Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words) are removed\n  * Sentences with less than 3 words are removed\n  * Sentences with a word of more than 1000 characters are removed\n  * Documents with less than 5 sentences are removed\n  * Documents with \"javascript\", \"lorum ipsum\", \"terms of use\", \"privacy policy\", \"cookie policy\", \"uses cookies\",\n    \"use of cookies\", \"use cookies\", \"elementen ontbreken\", \"deze printversie\" are removed.\n\n## Tokenization\n\nA SentencePiece tokenizer was trained from scratch on this dataset.\nThe total tokens of the `full` configuration is 34B \n\n## Training\n\nThe model was trained on the `full` mc4_nl_cleaned dataset configuration for 1 epoch, consisting of 34B tokens,\nfor 528 482 steps with a batch size of 128 and took 57 hours.\nA triangle learning rate schedule was used, with peak learning rate 0.005.\n\n## Evaluation\n\n* Loss: 1.38\n* Accuracy: 0.70\n", "size_bytes": "891650495", "downloads": 57}
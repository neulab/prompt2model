{"pretrained_model_name": "Korventenn/fr_en-t5-large", "description": "---\nlicense: apache-2.0\ndatasets:\n- giga_fren\nlanguage:\n- fr\n- en\n---\n\n\n# Model Card for fr_en-t5-large\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis model has been optimized for French and English language processing while minimizing overall size. To achieve this, I only retained relevant parameters and tokens specific to these two languages, ensuring that performance remains as good as the original mt5.\n\n## Model Details\nI used a method outlined in a [blog post](https://towardsdatascience.com/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90) by David Dale to downsize the multilingual T5 model for French and English use cases specifically. By utilizing the giga_fren dataset, I was able to successfully reduce the total number of tokens and decrease both the model and tokenizer sizes by 38% and 80% respectively.\n\n### Model Description\n\n- **Developed by:** Korventenn\n- **Model type:** mt5\n- **Language(s) (NLP):** French and English\n- **License:** Apache 2.0\n- **Generated from model:** mt5-large\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://colab.research.google.com/drive/1cDWtO5BqWMm_nxnM7lHmPEKMWMejHdBJ#scrollTo=s6ebzRxA1VGv\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\nYou can use the raw model for any sequence to sequence task that is focused on either french, english or both.\n\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Korventenn/fr_en-t5-large\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Korventenn/fr_en-t5-large\")\n```\n\n### Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[giga_fren](https://huggingface.co/datasets/giga_fren)", "size_bytes": "3240477765", "downloads": 11}
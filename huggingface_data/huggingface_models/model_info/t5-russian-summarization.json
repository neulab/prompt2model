{"pretrained_model_name": "UrukHan/t5-russian-summarization", "description": "---\ntags:\n- generated_from_trainer\ndatasets: UrukHan/wav2vec2-russian\nwidget:\n- text: \u0417\u0430\u043f\u0430\u0434 \u043f\u043e\u0441\u043b\u0435 \u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0439 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u043f\u043e \u0434\u0435\u043c\u0438\u043b\u0438\u0442\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0423\u043a\u0440\u0430\u0438\u043d\u044b\n    \u0432\u0432\u0435\u043b \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0443\u043d\u0434\u043e\u0432 \u043d\u043e\u0432\u044b\u0445 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0430\u043d\u043a\u0446\u0438\u0439. \u0412 \u041a\u0440\u0435\u043c\u043b\u0435 \u043d\u043e\u0432\u044b\u0435 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f\n    \u043d\u0430\u0437\u0432\u0430\u043b\u0438 \u0441\u0435\u0440\u044c\u0435\u0437\u043d\u044b\u043c\u0438, \u043d\u043e \u043e\u0442\u043c\u0435\u0442\u0438\u043b\u0438, \u0447\u0442\u043e \u0420\u043e\u0441\u0441\u0438\u044f \u0433\u043e\u0442\u043e\u0432\u0438\u043b\u0430\u0441\u044c \u043a \u043d\u0438\u043c \u0437\u0430\u0440\u0430\u043d\u0435\u0435.\nmodel-index:\n- name: t5-russian-summarization\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n---\n# t5-russian-summarization\n---\n\u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u0430 \u0438\u0437 \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u043d\u043e\u0433\u043e \u0430\u0443\u0434\u0438\u043e.   \u043c\u043e\u044f \u043c\u043e\u0434\u043b\u0435\u044c \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u0430\u0443\u0434\u0438\u043e https://huggingface.co/UrukHan/wav2vec2-russian \u0438 \u0435\u0433\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043c\u043e\u0436\u043d\u043e \u0437\u0430\u043a\u0438\u0434\u044b\u0432\u0430\u0442\u044c \u0432 \u044d\u0442\u0443 \u043c\u043e\u0434\u0435\u043b\u044c. \u0442\u0435\u0441\u0442\u0438\u043b \u043d\u0430 \u0432\u0438\u0434\u0435\u043e \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u043c \u0441 \u044e\u0442\u044e\u0431\u0430\n\n<table border=\"0\">\n <tr>\n    <td><b style=\"font-size:30px\">Input</b></td>\n    <td><b style=\"font-size:30px\">Output</b></td>\n </tr>\n <tr>\n    <td>\u0417\u0430\u043f\u0430\u0434 \u043f\u043e\u0441\u043b\u0435 \u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0439 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u043f\u043e \u0434\u0435\u043c\u0438\u043b\u0438\u0442\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0423\u043a\u0440\u0430\u0438\u043d\u044b \u0432\u0432\u0435\u043b \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0443\u043d\u0434\u043e\u0432 \u043d\u043e\u0432\u044b\u0445 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0430\u043d\u043a\u0446\u0438\u0439. \u0412 \u041a\u0440\u0435\u043c\u043b\u0435 \u043d\u043e\u0432\u044b\u0435 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f \u043d\u0430\u0437\u0432\u0430\u043b\u0438 \u0441\u0435\u0440\u044c\u0435\u0437\u043d\u044b\u043c\u0438, \u043d\u043e \u043e\u0442\u043c\u0435\u0442\u0438\u043b\u0438, \u0447\u0442\u043e \u0420\u043e\u0441\u0441\u0438\u044f \u0433\u043e\u0442\u043e\u0432\u0438\u043b\u0430\u0441\u044c \u043a \u043d\u0438\u043c \u0437\u0430\u0440\u0430\u043d\u0435\u0435.</td>\n    <td>\u0417\u0430\u043f\u0430\u0434 \u0432\u0432\u0435\u043b \u043d\u043e\u0432\u044b\u0435 \u0441\u0430\u043d\u043a\u0446\u0438\u0438 \u043f\u0440\u043e\u0442\u0438\u0432 \u0420\u043e\u0441\u0441\u0438\u0438</td>\n </tr>\n</table>\n\n\n#\n---\n\u0414\u0430\u0442\u0430\u0441\u0435\u0442\u044b \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f:\nUrukHan/t5-russian-summarization   : https://huggingface.co/datasets/UrukHan/t5-russian-summarization\n\n---\n\n# \u0417\u0430\u043f\u0443\u0441\u043a \u043d\u0430 \u0432\u044b\u0432\u043e\u0434 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043f\u0440\u0438\u043c\u0435\u0440 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u044f\u043c\u0438 \u0432 \u043a\u043e\u043b\u0430\u0431\u0435 https://colab.research.google.com/drive/1ame2va9_NflYqy4RZ07HYmQ0moJYy7w2?usp=sharing :\n\n#\n```python\n# \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u043e\u0432\n!pip install transformers\n\n# \u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\nfrom transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\n\n# \u0417\u0430\u0434\u0430\u0434\u0438\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0432\u044b\u0431\u0440\u043e\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 \u0445\u0430\u0431\u0430\nMODEL_NAME = 'UrukHan/t5-russian-summarization'\nMAX_INPUT = 256\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u0430\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# \u0412\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 (\u043c\u043e\u0436\u043d\u043e \u043c\u0430\u0441\u0441\u0438\u0432 \u0444\u0440\u0430\u0437 \u0438\u043b\u0438 \u0442\u0435\u043a\u0441\u0442)\ninput_sequences = ['\u0417\u0430\u043f\u0430\u0434 \u043f\u043e\u0441\u043b\u0435 \u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0439 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u043f\u043e \u0434\u0435\u043c\u0438\u043b\u0438\u0442\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0423\u043a\u0440\u0430\u0438\u043d\u044b \u0432\u0432\u0435\u043b \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0443\u043d\u0434\u043e\u0432 \u043d\u043e\u0432\u044b\u0445 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0430\u043d\u043a\u0446\u0438\u0439. \u0412 \u041a\u0440\u0435\u043c\u043b\u0435 \u043d\u043e\u0432\u044b\u0435 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f \u043d\u0430\u0437\u0432\u0430\u043b\u0438 \u0441\u0435\u0440\u044c\u0435\u0437\u043d\u044b\u043c\u0438, \u043d\u043e \u043e\u0442\u043c\u0435\u0442\u0438\u043b\u0438, \u0447\u0442\u043e \u0420\u043e\u0441\u0441\u0438\u044f \u0433\u043e\u0442\u043e\u0432\u0438\u043b\u0430\u0441\u044c \u043a \u043d\u0438\u043c \u0437\u0430\u0440\u0430\u043d\u0435\u0435.']   # \u0438\u043b\u0438 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043e\u0434\u0438\u043d\u043e\u0447\u043d\u044b\u0435 \u0444\u0440\u0430\u0437\u044b:  input_sequences = '\u0441\u0435\u0433\u043b\u0434\u044b\u044f \u0445\u043e\u0440\u043e\u0448 \u0434\u0435\u043d'\n\ntask_prefix = \"Spell correct: \"                 # \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\nif type(input_sequences) != list: input_sequences = [input_sequences]\nencoded = tokenizer(\n  [task_prefix + sequence for sequence in input_sequences],\n  padding=\"longest\",\n  max_length=MAX_INPUT,\n  truncation=True,\n  return_tensors=\"pt\",\n)\n\npredicts = model.generate(encoded)    # # \u041f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\n\ntokenizer.batch_decode(predicts, skip_special_tokens=True)  # \u0414\u0435\u043a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\n```\n#\n---\n#\u041d\u0430\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0431\u043b\u043e\u043a\u043d\u043e\u0442 \u0434\u043b\u044f \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0432 \u0441\u0432\u043e\u0439 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439 \u043d\u0430 huggingface hub:\n#https://colab.research.google.com/drive/1H4IoasDqa2TEjGivVDp-4Pdpm0oxrCWd?usp=sharing\n#\n```python\n# \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\n!pip install datasets\n!apt install git-lfs\n!pip install transformers\n!pip install sentencepiece \n!pip install rouge_score\n\n# \u0418\u043c\u043f\u043e\u0440\u0442 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\nimport numpy as np\nfrom datasets import Dataset\nimport tensorflow as \nimport nltk\nfrom transformers import T5TokenizerFast, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nimport torch\nfrom transformers.optimization import Adafactor, AdafactorSchedule\nfrom datasets import load_dataset, load_metric\n\n# \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\nraw_datasets = load_dataset(\"xsum\")\nmetric = load_metric(\"rouge\")\nnltk.download('punkt')\n\n# \u0412\u0432\u0435\u0441\u0442\u0438 \u0441\u0432\u043e\u0439 \u043a\u043b\u044e\u0447 huggingface hyb\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n# \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\nREPO = \"t5-russian-summarization\"  # \u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043d\u0430\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u044f\nMODEL_NAME = \"UrukHan/t5-russian-summarization\" # \u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043d\u0430\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 \u0445\u0430\u0431\u0430\nMAX_INPUT = 256  # \u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0443\u044e \u0434\u043b\u0438\u043d\u043d\u0443 \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445  \u0432 \u0442\u043e\u043a\u0435\u043d\u0430\u0445 (\u0434\u043b\u0438\u043d\u043d\u0430 \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0444\u0440\u0430\u0437 \u0432 \u0441\u043b\u043e\u0432\u0430\u0445 (\u043c\u043e\u0436\u043d\u043e \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043f\u043e\u043b\u0441\u043b\u043e\u0432\u0430 \u0442\u043e\u043a\u0435\u043d))\nMAX_OUTPUT  = 64  # \u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0443\u044e \u0434\u043b\u0438\u043d\u043d\u0443 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043e\u0432 \u0432 \u0442\u043e\u043a\u0435\u043d\u0430\u0445 (\u043c\u043e\u0436\u043d\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u0442\u044c \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u0441\u0443\u043c\u043c\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u0445 \u0437\u0430\u0434\u0430\u0447 \u0433\u0434\u0435 \u0432\u044b\u0445\u043e\u0434 \u043a\u043e\u0440\u043e\u0447\u0435)\nBATCH_SIZE = 8 \nDATASET = 'UrukHan/t5-russian-summarization'   # \u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043d\u0430\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0440\u0443\u0433\u0438\u0445 \u0442\u0438\u043f\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u043f\u0438\u0448\u0443 \u043d\u0438\u0436\u0435\ndata = load_dataset(DATASET)\n\n# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u0430\ntokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\nmodel.config.max_length = MAX_OUTPUT  # \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e 20, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432\u043e \u0432\u0441\u0435\u0445 \u043c\u043e\u0434\u0435\u043b\u044f\u0445 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u044b \u043e\u0431\u0440\u0435\u0437\u0430\u044e\u0442\u0441\u044f \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\n# \u0417\u0430\u043a\u043e\u043c\u0435\u043d\u0442\u0438\u0442\u044c \u043f\u043e\u0441\u043b\u0435 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0441\u043e\u044a\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0432 \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439 \u0441\u0432\u043e\u0439 \u043d\u0435\u043e\u0431\u044a\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e\ntokenizer.push_to_hub(repo_name) \n\ntrain = data['train']\ntest = data['test'].train_test_split(0.02)['test']  # \u0423\u043c\u0435\u043d\u044c\u0448\u0438\u043b \u0442\u0430\u043a \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0443. \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0447\u0442\u043e\u0431 \u043d\u0435 \u0436\u0434\u0430\u0442\u044c \u0434\u043e\u043b\u0433\u043e \u0440\u0430\u0441\u0447\u0435\u0442 \u043e\u0448\u0438\u0431\u043e\u043a \u043c\u0435\u0436\u0434\u0443 \u044d\u043f\u043e\u0445\u0430\u043c\u0438\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model) #return_tensors=\"tf\"\n\ndef compute_metrics(eval_pred):\n  predictions, labels = eval_pred\n  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n  # Replace -100 in the labels as we can't decode them.\n  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n  \n  # Rouge expects a newline after each sentence\n  decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n  decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n  \n  result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n  # Extract a few results\n  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n  \n  # Add mean generated length\n  prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n  result[\"gen_len\"] = np.mean(prediction_lens)\n  \n  return {k: round(v, 4) for k, v in result.items()}\n  \ntraining_args = Seq2SeqTrainingArguments(\n  output_dir = REPO,\n  #overwrite_output_dir=True,\n  evaluation_strategy='steps',\n  #learning_rate=2e-5,\n  eval_steps=5000,\n  save_steps=5000,\n  num_train_epochs=1,\n  predict_with_generate=True,\n  per_device_train_batch_size=BATCH_SIZE,\n  per_device_eval_batch_size=BATCH_SIZE,\n  fp16=True,\n  save_total_limit=2,\n  #generation_max_length=256,\n  #generation_num_beams=4,\n  weight_decay=0.005,\n  #logging_dir='logs',\n  push_to_hub=True,\n)\n\n# \u0412\u044b\u0431\u0435\u0440\u0435\u043c \u0432\u0440\u0443\u0447\u043d\u0443\u044e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440. \u04225 \u0432 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0439 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0410\u0434\u0430\u0444\u0430\u043a\u0442\u043e\u0440 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\noptimizer = Adafactor(\n    model.parameters(),\n    lr=1e-5,\n    eps=(1e-30, 1e-3),\n    clip_threshold=1.0,\n    decay_rate=-0.8,\n    beta1=None,\n    weight_decay=0.0,\n    relative_step=False,\n    scale_parameter=False,\n    warmup_init=False,\n)\nlr_scheduler = AdafactorSchedule(optimizer)\n\ntrainer = Seq2SeqTrainer(\n  model=model,\n  args=training_args,\n  train_dataset = train,\n  eval_dataset = test,\n  optimizers = (optimizer, lr_scheduler),\n  tokenizer = tokenizer,\n  compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\ntrainer.push_to_hub()\n```\n#\n---\n# \u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0430\u0446\u0438\u0438 \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u0432 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\n#\n```python\ninput_data = ['\u0417\u0430\u043f\u0430\u0434 \u043f\u043e\u0441\u043b\u0435 \u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0439 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u043f\u043e \u0434\u0435\u043c\u0438\u043b\u0438\u0442\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0423\u043a\u0440\u0430\u0438\u043d\u044b \u0432\u0432\u0435\u043b \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0443\u043d\u0434\u043e\u0432 \u043d\u043e\u0432\u044b\u0445 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0430\u043d\u043a\u0446\u0438\u0439. \u0412 \u041a\u0440\u0435\u043c\u043b\u0435 \u043d\u043e\u0432\u044b\u0435 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f \u043d\u0430\u0437\u0432\u0430\u043b\u0438 \u0441\u0435\u0440\u044c\u0435\u0437\u043d\u044b\u043c\u0438, \u043d\u043e \u043e\u0442\u043c\u0435\u0442\u0438\u043b\u0438, \u0447\u0442\u043e \u0420\u043e\u0441\u0441\u0438\u044f \u0433\u043e\u0442\u043e\u0432\u0438\u043b\u0430\u0441\u044c \u043a \u043d\u0438\u043c \u0437\u0430\u0440\u0430\u043d\u0435\u0435.']\noutput_data = ['\u0417\u0430\u043f\u0430\u0434 \u0432\u0432\u0435\u043b \u043d\u043e\u0432\u044b\u0435 \u0441\u0430\u043d\u043a\u0446\u0438\u0438 \u043f\u0440\u043e\u0442\u0438\u0432 \u0420\u043e\u0441\u0441\u0438\u0438']\n\n# \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ntask_prefix = \"Spell correct: \"\ninput_sequences = input_data \nencoding = tokenizer(\n  [task_prefix + sequence for sequence in input_sequences],\n  padding=\"longest\",\n  max_length=MAX_INPUT,\n  truncation=True,\n  return_tensors=\"pt\",\n)\ninput_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n\n# \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ntarget_encoding = tokenizer(output_data, padding=\"longest\", max_length=MAX_OUTPUT, truncation=True)\nlabels = target_encoding.input_ids\n# replace padding token id's of the labels by -100\nlabels = torch.tensor(labels)\nlabels[labels == tokenizer.pad_token_id] = -100'''\n\n# \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442 dataset   \n\ndata = Dataset.from_pandas(pd.DataFrame({'input_ids': list(np.array(input_ids)), 'attention_mask': list(np.array(attention_mask)), 'labels': list(np.array(labels))}))\ndata = data.train_test_split(0.02)\n# \u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043d\u0430 \u0432\u0445\u043e\u0434 \u0441\u0435\u0442\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0448\u0433\u043e trainer:   train_dataset = data['train'],  eval_dataset = data['test']\n\n", "size_bytes": "891730879", "downloads": 453}
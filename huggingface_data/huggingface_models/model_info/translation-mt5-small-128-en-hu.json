{"pretrained_model_name": "NYTK/translation-mt5-small-128-en-hu", "description": "---\nlanguage:\n- en\n- hu\ntags:\n- translation\nlicense: apache-2.0\nmetrics:\n- sacrebleu\n- chrf\nwidget:\n- text: >-\n    translate English to Hungarian: This may not make much sense to you, sir, but I'd like to ask your\n    permission to date your daughter.\n---\n\n# mT5 Translation model\n\nFor further models, scripts and details, see [our repository](https://github.com/nytud/machine-translation) or [our demo site](https://juniper.nytud.hu/demo/nlp).\n\n- Source language: English\n- Target language: Hungarian\n\n- Pretrained model used: mT5-small\n- Finetuned on subcorpora from OPUS\n\t- Segments: 56.837.602\n- prefix: \"translate English to Hungarian: \"\n\n\n## Limitations\n\n- tokenized input text (tokenizer: [HuSpaCy](https://huggingface.co/huspacy))\n- max_source_length = 128\n- max_target_length = 128\n\n## Results\n\n| Model | BLEU | chrF-3 | chrF-6 |\n| ------------- | ------------- | ------------- | ------------- |\n| Google en-hu  | 25.30  | 54.08 | 49.06 |\n| BART | 36.89 | 60.77 | 56.4 |\n| **mT5** | **27.69**  | **53.73** | **48.57** |\n\n\n## Citation\nIf you use this model, please cite the following paper:\n\n```\n\n@inproceedings {laki-yang-mt,\n    title = {{Jobban ford\u00edtunk magyarra, mint a Google!}},\n\tbooktitle = {XVIII. Magyar Sz\u00e1m\u00edt\u00f3g\u00e9pes Nyelv\u00e9szeti Konferencia},\n\tyear = {2022},\n\tpublisher = {Szegedi Tudom\u00e1nyegyetem, Informatikai Int\u00e9zet},\n\taddress = {Szeged, Magyarorsz\u00e1g},\n\tauthor = {Laki, L\u00e1szl\u00f3 and Yang, Zijian Gy\u0151z\u0151},\n\tpages = {357--372}\n}\n\n```", "size_bytes": "1200726749", "downloads": 10}
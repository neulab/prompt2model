{"pretrained_model_name": "ZWK/InstructUIE", "description": "---\nlicense: openrail\n---\nhttps://github.com/BeyonderXX/InstructUIE\n\n# InstructUIE\nLarge language models have unlocked strong multi-task capabilities from reading instructive prompts.\nHowever, recent studies have shown that existing large models still have difficulty with information extraction tasks. \nFor example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance.\nIn this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency.\nTo validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions.\nExperimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.\n\n\n## Data\n\nOur models are trained and evaluated on **IE INSTRUCTIONS**. \nYou can download the data from [Baidu NetDisk](https://pan.baidu.com/s/1R0KqeyjPHrsGcPqsbsh1XA?from=init&pwd=ybkt) or [Google Drive](https://drive.google.com/file/d/1T-5IbocGka35I7X3CE6yKe5N_Xg2lVKT/view?usp=share_link).\n\n\n\n\n## Citation\n\nIf you are using InstructUIE for your work, please kindly cite our paper:\n\n```latex\n@article{wang2023instructuie,\n  title={InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction},\n  author={Wang, Xiao and Zhou, Weikang and Zu, Can and Xia, Han and Chen, Tianze and Zhang, Yuansen and Zheng, Rui and Ye, Junjie and Zhang, Qi and Gui, Tao and others},\n  journal={arXiv preprint arXiv:2304.08085},\n  year={2023}\n}\n```\n\n\n", "size_bytes": 45592264704, "downloads": 110}
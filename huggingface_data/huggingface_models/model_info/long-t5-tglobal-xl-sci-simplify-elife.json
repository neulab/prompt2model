{"pretrained_model_name": "pszemraj/long-t5-tglobal-xl-sci-simplify-elife", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- pszemraj/scientific_lay_summarisation-elife-norm\nmetrics:\n- rouge\nmodel-index:\n- name: long-t5-tglobal-xl-scientific_lay_summarisation-elife-norm-16384-summ-v1\n  results:\n  - task:\n      name: Summarization\n      type: summarization\n    dataset:\n      name: pszemraj/scientific_lay_summarisation-elife-norm\n      type: pszemraj/scientific_lay_summarisation-elife-norm\n      split: validation\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 47.1446\npipeline_tag: summarization\ninference: False\n---\n\n# long-t5-tglobal-xl-sci-simplify-elife\n\nThis model is a fine-tuned version of [google/long-t5-tglobal-xl](https://huggingface.co/google/long-t5-tglobal-xl) on the pszemraj/scientific_lay_summarisation-elife-norm dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6666\n- Rouge1: 47.1446\n- Rouge2: 14.2158\n- Rougel: 23.3524\n- Rougelsum: 44.6063\n- Gen Len: 431.22\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nthe `pszemraj/scientific_lay_summarisation-elife-norm` dataset, input 16384 tokens then truncate, output 1024 tokens then truncate.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 8e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 6963\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.02\n- num_epochs: 2.0\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.7959        | 1.0   | 543  | 1.6770          | 44.4187 | 12.6752 | 22.4669 | 41.944    | 456.33  |\n| 1.7578        | 2.0   | 1086 | 1.6666          | 47.1446 | 14.2158 | 23.3524 | 44.6063   | 431.22  |", "size_bytes": 11925614592, "downloads": 85}
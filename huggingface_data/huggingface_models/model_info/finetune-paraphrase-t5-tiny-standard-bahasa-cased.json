{"pretrained_model_name": "mesolitica/finetune-paraphrase-t5-tiny-standard-bahasa-cased", "description": "---\nlanguage: \n  - ms\ntags:\n- paraphrase\nmetrics:\n- sacrebleu\n---\n\n# finetune-paraphrase-t5-tiny-standard-bahasa-cased\n\nFinetuned T5 tiny on MS paraphrase tasks.\n\n## Dataset\n\n1. translated PAWS, https://huggingface.co/datasets/mesolitica/translated-PAWS\n2. translated MRPC, https://huggingface.co/datasets/mesolitica/translated-MRPC\n3. translated ParaSCI, https://huggingface.co/datasets/mesolitica/translated-paraSCI\n\n## Finetune details\n\n1. Finetune using single RTX 3090 Ti.\n\nScripts at https://github.com/huseinzol05/malaya/tree/master/session/paraphrase/hf-t5\n\n## Supported prefix\n\n1. `parafrasa: {string}`, for MS paraphrase.\n\n## Evaluation\n\nEvaluated on MRPC validation set and ParaSCI Arxiv test set.\n\n```\n{'name': 'BLEU',\n 'score': 36.92696648298233,\n '_mean': -1.0,\n '_ci': -1.0,\n '_verbose': '62.5/42.3/33.0/26.9 (BP = 0.943 ratio = 0.945 hyp_len = 95496 ref_len = 101064)',\n 'bp': 0.9433611337299734,\n 'counts': [59650, 38055, 27875, 21217],\n 'totals': [95496, 89952, 84408, 78864],\n 'sys_len': 95496,\n 'ref_len': 101064,\n 'precisions': [62.46334925023038,\n  42.30589647812167,\n  33.02412093640413,\n  26.90327652667884],\n 'prec_str': '62.5/42.3/33.0/26.9',\n 'ratio': 0.944906198052719}\n```", "size_bytes": "139023962", "downloads": 3}
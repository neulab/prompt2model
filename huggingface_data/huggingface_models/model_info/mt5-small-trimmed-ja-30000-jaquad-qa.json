{"pretrained_model_name": "vocabtrimmer/mt5-small-trimmed-ja-30000-jaquad-qa", "description": "\n---\nlicense: cc-by-4.0\nmetrics:\n- bleu4\n- meteor\n- rouge-l\n- bertscore\n- moverscore\nlanguage: ja\ndatasets:\n- lmqg/qg_jaquad\npipeline_tag: text2text-generation\ntags:\n- question answering\nwidget:\n- text: \"question: \u65b0\u578b\u8eca\u4e21\u3068\u3057\u30666000\u7cfb\u304c\u69cb\u60f3\u3055\u308c\u305f\u306e\u306f\u3001\u88fd\u9020\u8cbb\u7528\u306e\u307b\u304b\u3001\u3069\u3093\u306a\u8cbb\u7528\u3092\u6291\u3048\u308b\u305f\u3081\u3060\u3063\u305f\u306e?, context: \u4e09\u591a\u6469\u5730\u533a\u958b\u767a\u306b\u3088\u308b\u6cbf\u7dda\u4eba\u53e3\u306e\u5897\u52a0\u3001\u76f8\u6a21\u539f\u7dda\u5ef6\u4f38\u306b\u3088\u308b\u591a\u6469\u30cb\u30e5\u30fc\u30bf\u30a6\u30f3\u4e57\u308a\u5165\u308c\u3001\u90fd\u55b6\u5730\u4e0b\u924410\u53f7\u7dda(\u73fe\u90fd\u55b6\u5730\u4e0b\u9244\u65b0\u5bbf\u7dda\u3001\u4ee5\u4e0b\u65b0\u5bbf\u7dda\u3068\u8868\u8a18\u3059\u308b)\u4e57\u5165\u69cb\u60f3\u306b\u3088\u308a\u3001\u4eac\u738b\u7dda\u306e\u5229\u7528\u5ba2\u5897\u52a0\u304c\u898b\u8fbc\u307e\u308c\u3001\u76f8\u5f53\u6570\u306e\u8eca\u4e21\u3092\u6e96\u5099\u3059\u308b\u5fc5\u8981\u306b\u8feb\u3089\u308c\u308b\u306a\u304b\u3001\u88fd\u9020\u8cbb\u7528\u3001\u4fdd\u5b88\u8cbb\u7528\u3092\u6291\u3048\u305f\u65b0\u578b\u8eca\u4e21\u3068\u3057\u30666000\u7cfb\u304c\u69cb\u60f3\u3055\u308c\u305f\u3002\u65b0\u5bbf\u7dda\u5efa\u8a2d\u306b\u969b\u3057\u3066\u306f\u3059\u3067\u306b1\u53f7\u7dda(\u5f8c\u306e\u6d45\u8349\u7dda)\u30921,435mm\u8ecc\u9593\u3067\u958b\u696d\u3055\u305b\u3066\u3044\u305f\u6771\u4eac\u90fd\u306f\u4eac\u6210\u96fb\u9244\u30681\u53f7\u7dda\u3068\u306e\u4e57\u308a\u5165\u308c\u306b\u3042\u305f\u308a\u4eac\u6210\u96fb\u9244\u306e\u8def\u7dda\u30921,372mm\u304b\u30891,435mm\u306b\u6539\u8ecc\u3055\u305b\u305f\u4e8b\u4f8b\u3084\u30011,372mm\u8ecc\u9593\u306e\u7279\u6b8a\u6027\u304b\u3089\u904b\u8f38\u7701(\u5f53\u6642\u30012001\u5e74\u304b\u3089\u56fd\u571f\u4ea4\u901a\u7701)\u3068\u5171\u306b\u4eac\u738b\u306b\u3082\u6539\u8ecc\u3092\u6c42\u3081\u305f\u304c\u3001\u6539\u8ecc\u5de5\u4e8b\u4e2d\u306e\u8f38\u9001\u529b\u78ba\u4fdd\u304c\u56f0\u96e3\u306a\u3053\u3068\u3092\u7406\u7531\u306b\u6539\u8ecc\u3057\u306a\u3044\u3053\u3068\u3067\u6c7a\u7740\u3057\u3066\u3044\u308b\u3002\"\n  example_title: \"Question Answering Example 1\" \n- text: \"question: 1968\u5e74\u306b\u958b\u50ac\u3055\u308c\u305f\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u306e\u540d\u524d\u306f\u4f55\u3067\u3059\u304b?, context: \u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u304c\u4e16\u754c\u7684\u5927\u30a4\u30d9\u30f3\u30c8\u306b\u6210\u9577\u3059\u308b\u306b\u5f93\u3063\u3066\u653f\u6cbb\u306b\u5de6\u53f3\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u308b\u3068\u30011968\u5e74\u306e\u30e1\u30ad\u30b7\u30b3\u30b7\u30c6\u30a3\u5927\u4f1a\u3067\u306f\u9ed2\u4eba\u5dee\u5225\u3092\u8a34\u3048\u308b\u5834\u3068\u5316\u3057\u30011972\u5e74\u306e\u30df\u30e5\u30f3\u30d8\u30f3\u5927\u4f1a\u3067\u306f\u30a2\u30e9\u30d6\u306e\u30b2\u30ea\u30e9\u306b\u3088\u308b\u30a4\u30b9\u30e9\u30a8\u30eb\u9078\u624b\u306b\u5bfe\u3059\u308b\u30c6\u30ed\u4e8b\u4ef6\u307e\u3067\u8d77\u304d\u305f(\u30df\u30e5\u30f3\u30d8\u30f3\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u4e8b\u4ef6)\u30021976\u5e74\u306e\u30e2\u30f3\u30c8\u30ea\u30aa\u30fc\u30eb\u5927\u4f1a\u306b\u306a\u308b\u3068\u3001\u30cb\u30e5\u30fc\u30b8\u30fc\u30e9\u30f3\u30c9\u306e\u30e9\u30b0\u30d3\u30fc\u30c1\u30fc\u30e0\u306e\u5357\u30a2\u30d5\u30ea\u30ab\u9060\u5f81\u306b\u53cd\u5bfe\u3057\u3066\u30a2\u30d5\u30ea\u30ab\u306e\u8af8\u56fd22\u30f6\u56fd\u304c\u30dc\u30a4\u30b3\u30c3\u30c8\u3092\u884c\u3063\u305f\u3002\u305d\u3057\u3066\u30011980\u5e74\u306e\u30e2\u30b9\u30af\u30ef\u5927\u4f1a\u3067\u306f\u30bd\u9023\u306e\u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3\u4fb5\u653b\u306b\u53cd\u767a\u3057\u305f\u30a2\u30e1\u30ea\u30ab\u30fb\u897f\u30c9\u30a4\u30c4\u30fb\u65e5\u672c\u306a\u3069\u306e\u897f\u5074\u8af8\u56fd\u304c\u76f8\u6b21\u3044\u3067\u30dc\u30a4\u30b3\u30c3\u30c8\u3092\u884c\u3063\u305f\u30021984\u5e74\u30ed\u30b5\u30f3\u30bc\u30eb\u30b9\u5927\u4f1a\u3067\u306f\u30bd\u9023\u3068\u6771\u5074\u8af8\u56fd\u304c\u5831\u5fa9\u30dc\u30a4\u30b3\u30c3\u30c8\u3092\u884c\u306a\u3044\u3001\u53c2\u52a0\u3057\u305f\u306e\u306f\u30bd\u9023\u3068\u5bfe\u7acb\u3057\u3066\u3044\u305f\u4e2d\u56fd\u3068\u30eb\u30fc\u30de\u30cb\u30a2\u3060\u3051\u3060\u3063\u305f\u3002\u4e2d\u3067\u3082\u3001\u30a4\u30e9\u30f3\u9769\u547d\u5f8c\u306e\u30a4\u30e9\u30f3\u30fb\u30a4\u30b9\u30e9\u30e0\u5171\u548c\u56fd\u306f\u30e2\u30b9\u30af\u30ef\u3068\u30ed\u30b5\u30f3\u30bc\u30eb\u30b9\u53cc\u65b9\u306e\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u3092\u30dc\u30a4\u30b3\u30c3\u30c8\u3057\u3066\u3044\u308b\u3002\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u304c\u5de8\u5927\u5316\u3059\u308b\u306b\u5f93\u3063\u3066\u8ca1\u653f\u8ca0\u62c5\u306e\u5897\u5927\u304c\u5927\u304d\u306a\u554f\u984c\u3068\u306a\u308a\u30011976\u5e74\u306e\u590f\u5b63\u5927\u4f1a\u3067\u306f\u5927\u5e45\u306a\u8d64\u5b57\u3092\u51fa\u3057\u3001\u305d\u306e\u5f8c\u590f\u5b63\u30fb\u51ac\u5b63\u3068\u3082\u7acb\u5019\u88dc\u90fd\u5e02\u304c1\u301c2\u90fd\u5e02\u3060\u3051\u3068\u3044\u3046\u72b6\u614b\u304c\u7d9a\u3044\u305f\u3002\"\n  example_title: \"Question Answering Example 2\" \nmodel-index:\n- name: vocabtrimmer/mt5-small-trimmed-ja-30000-jaquad-qa\n  results:\n  - task:\n      name: Text2text Generation\n      type: text2text-generation\n    dataset:\n      name: lmqg/qg_jaquad\n      type: default\n      args: default\n    metrics:\n    - name: BLEU4 (Question Answering)\n      type: bleu4_question_answering\n      value: 0.0\n    - name: ROUGE-L (Question Answering)\n      type: rouge_l_question_answering\n      value: 58.21\n    - name: METEOR (Question Answering)\n      type: meteor_question_answering\n      value: 46.65\n    - name: BERTScore (Question Answering)\n      type: bertscore_question_answering\n      value: 95.64\n    - name: MoverScore (Question Answering)\n      type: moverscore_question_answering\n      value: 86.95\n    - name: AnswerF1Score (Question Answering)\n      type: answer_f1_score__question_answering\n      value: 60.45\n    - name: AnswerExactMatch (Question Answering)\n      type: answer_exact_match_question_answering\n      value: 60.45\n---\n\n# Model Card of `vocabtrimmer/mt5-small-trimmed-ja-30000-jaquad-qa`\nThis model is fine-tuned version of [vocabtrimmer/mt5-small-trimmed-ja-30000](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-ja-30000) for question answering task on the [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) (dataset_name: default) via [`lmqg`](https://github.com/asahi417/lm-question-generation).\n\n\n### Overview\n- **Language model:** [vocabtrimmer/mt5-small-trimmed-ja-30000](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-ja-30000)   \n- **Language:** ja  \n- **Training data:** [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) (default)\n- **Online Demo:** [https://autoqg.net/](https://autoqg.net/)\n- **Repository:** [https://github.com/asahi417/lm-question-generation](https://github.com/asahi417/lm-question-generation)\n- **Paper:** [https://arxiv.org/abs/2210.03992](https://arxiv.org/abs/2210.03992)\n\n### Usage\n- With [`lmqg`](https://github.com/asahi417/lm-question-generation#lmqg-language-model-for-question-generation-)\n```python\nfrom lmqg import TransformersQG\n\n# initialize model\nmodel = TransformersQG(language=\"ja\", model=\"vocabtrimmer/mt5-small-trimmed-ja-30000-jaquad-qa\")\n\n# model prediction\nanswers = model.answer_q(list_question=\"\u65b0\u578b\u8eca\u4e21\u3068\u3057\u30666000\u7cfb\u304c\u69cb\u60f3\u3055\u308c\u305f\u306e\u306f\u3001\u88fd\u9020\u8cbb\u7528\u306e\u307b\u304b\u3001\u3069\u3093\u306a\u8cbb\u7528\u3092\u6291\u3048\u308b\u305f\u3081\u3060\u3063\u305f\u306e?\", list_context=\" \u4e09\u591a\u6469\u5730\u533a\u958b\u767a\u306b\u3088\u308b\u6cbf\u7dda\u4eba\u53e3\u306e\u5897\u52a0\u3001\u76f8\u6a21\u539f\u7dda\u5ef6\u4f38\u306b\u3088\u308b\u591a\u6469\u30cb\u30e5\u30fc\u30bf\u30a6\u30f3\u4e57\u308a\u5165\u308c\u3001\u90fd\u55b6\u5730\u4e0b\u924410\u53f7\u7dda(\u73fe\u90fd\u55b6\u5730\u4e0b\u9244\u65b0\u5bbf\u7dda\u3001\u4ee5\u4e0b\u65b0\u5bbf\u7dda\u3068\u8868\u8a18\u3059\u308b)\u4e57\u5165\u69cb\u60f3\u306b\u3088\u308a\u3001\u4eac\u738b\u7dda\u306e\u5229\u7528\u5ba2\u5897\u52a0\u304c\u898b\u8fbc\u307e\u308c\u3001\u76f8\u5f53\u6570\u306e\u8eca\u4e21\u3092\u6e96\u5099\u3059\u308b\u5fc5\u8981\u306b\u8feb\u3089\u308c\u308b\u306a\u304b\u3001\u88fd\u9020\u8cbb\u7528\u3001\u4fdd\u5b88\u8cbb\u7528\u3092\u6291\u3048\u305f\u65b0\u578b\u8eca\u4e21\u3068\u3057\u30666000\u7cfb\u304c\u69cb\u60f3\u3055\u308c\u305f\u3002\u65b0\u5bbf\u7dda\u5efa\u8a2d\u306b\u969b\u3057\u3066\u306f\u3059\u3067\u306b1\u53f7\u7dda(\u5f8c\u306e\u6d45\u8349\u7dda)\u30921,435mm\u8ecc\u9593\u3067\u958b\u696d\u3055\u305b\u3066\u3044\u305f\u6771\u4eac\u90fd\u306f\u4eac\u6210\u96fb\u9244\u30681\u53f7\u7dda\u3068\u306e\u4e57\u308a\u5165\u308c\u306b\u3042\u305f\u308a\u4eac\u6210\u96fb\u9244\u306e\u8def\u7dda\u30921,372mm\u304b\u30891,435mm\u306b\u6539\u8ecc\u3055\u305b\u305f\u4e8b\u4f8b\u3084\u30011,372mm\u8ecc\u9593\u306e\u7279\u6b8a\u6027\u304b\u3089\u904b\u8f38\u7701(\u5f53\u6642\u30012001\u5e74\u304b\u3089\u56fd\u571f\u4ea4\u901a\u7701)\u3068\u5171\u306b\u4eac\u738b\u306b\u3082\u6539\u8ecc\u3092\u6c42\u3081\u305f\u304c\u3001\u6539\u8ecc\u5de5\u4e8b\u4e2d\u306e\u8f38\u9001\u529b\u78ba\u4fdd\u304c\u56f0\u96e3\u306a\u3053\u3068\u3092\u7406\u7531\u306b\u6539\u8ecc\u3057\u306a\u3044\u3053\u3068\u3067\u6c7a\u7740\u3057\u3066\u3044\u308b\u3002\")\n\n```\n\n- With `transformers`\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text2text-generation\", \"vocabtrimmer/mt5-small-trimmed-ja-30000-jaquad-qa\")\noutput = pipe(\"question: \u65b0\u578b\u8eca\u4e21\u3068\u3057\u30666000\u7cfb\u304c\u69cb\u60f3\u3055\u308c\u305f\u306e\u306f\u3001\u88fd\u9020\u8cbb\u7528\u306e\u307b\u304b\u3001\u3069\u3093\u306a\u8cbb\u7528\u3092\u6291\u3048\u308b\u305f\u3081\u3060\u3063\u305f\u306e?, context: \u4e09\u591a\u6469\u5730\u533a\u958b\u767a\u306b\u3088\u308b\u6cbf\u7dda\u4eba\u53e3\u306e\u5897\u52a0\u3001\u76f8\u6a21\u539f\u7dda\u5ef6\u4f38\u306b\u3088\u308b\u591a\u6469\u30cb\u30e5\u30fc\u30bf\u30a6\u30f3\u4e57\u308a\u5165\u308c\u3001\u90fd\u55b6\u5730\u4e0b\u924410\u53f7\u7dda(\u73fe\u90fd\u55b6\u5730\u4e0b\u9244\u65b0\u5bbf\u7dda\u3001\u4ee5\u4e0b\u65b0\u5bbf\u7dda\u3068\u8868\u8a18\u3059\u308b)\u4e57\u5165\u69cb\u60f3\u306b\u3088\u308a\u3001\u4eac\u738b\u7dda\u306e\u5229\u7528\u5ba2\u5897\u52a0\u304c\u898b\u8fbc\u307e\u308c\u3001\u76f8\u5f53\u6570\u306e\u8eca\u4e21\u3092\u6e96\u5099\u3059\u308b\u5fc5\u8981\u306b\u8feb\u3089\u308c\u308b\u306a\u304b\u3001\u88fd\u9020\u8cbb\u7528\u3001\u4fdd\u5b88\u8cbb\u7528\u3092\u6291\u3048\u305f\u65b0\u578b\u8eca\u4e21\u3068\u3057\u30666000\u7cfb\u304c\u69cb\u60f3\u3055\u308c\u305f\u3002\u65b0\u5bbf\u7dda\u5efa\u8a2d\u306b\u969b\u3057\u3066\u306f\u3059\u3067\u306b1\u53f7\u7dda(\u5f8c\u306e\u6d45\u8349\u7dda)\u30921,435mm\u8ecc\u9593\u3067\u958b\u696d\u3055\u305b\u3066\u3044\u305f\u6771\u4eac\u90fd\u306f\u4eac\u6210\u96fb\u9244\u30681\u53f7\u7dda\u3068\u306e\u4e57\u308a\u5165\u308c\u306b\u3042\u305f\u308a\u4eac\u6210\u96fb\u9244\u306e\u8def\u7dda\u30921,372mm\u304b\u30891,435mm\u306b\u6539\u8ecc\u3055\u305b\u305f\u4e8b\u4f8b\u3084\u30011,372mm\u8ecc\u9593\u306e\u7279\u6b8a\u6027\u304b\u3089\u904b\u8f38\u7701(\u5f53\u6642\u30012001\u5e74\u304b\u3089\u56fd\u571f\u4ea4\u901a\u7701)\u3068\u5171\u306b\u4eac\u738b\u306b\u3082\u6539\u8ecc\u3092\u6c42\u3081\u305f\u304c\u3001\u6539\u8ecc\u5de5\u4e8b\u4e2d\u306e\u8f38\u9001\u529b\u78ba\u4fdd\u304c\u56f0\u96e3\u306a\u3053\u3068\u3092\u7406\u7531\u306b\u6539\u8ecc\u3057\u306a\u3044\u3053\u3068\u3067\u6c7a\u7740\u3057\u3066\u3044\u308b\u3002\")\n\n```\n\n## Evaluation\n\n\n- ***Metric (Question Answering)***: [raw metric file](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-ja-30000-jaquad-qa/raw/main/eval/metric.first.answer.paragraph_question.answer.lmqg_qg_jaquad.default.json) \n\n|                  |   Score | Type    | Dataset                                                          |\n|:-----------------|--------:|:--------|:-----------------------------------------------------------------|\n| AnswerExactMatch |   60.45 | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n| AnswerF1Score    |   60.45 | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n| BERTScore        |   95.64 | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n| Bleu_1           |   56.02 | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n| Bleu_2           |    0    | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n| Bleu_3           |    0    | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n| Bleu_4           |    0    | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n| METEOR           |   46.65 | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n| MoverScore       |   86.95 | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n| ROUGE_L          |   58.21 | default | [lmqg/qg_jaquad](https://huggingface.co/datasets/lmqg/qg_jaquad) |\n\n\n\n## Training hyperparameters\n\nThe following hyperparameters were used during fine-tuning:\n - dataset_path: lmqg/qg_jaquad\n - dataset_name: default\n - input_types: ['paragraph_question']\n - output_types: ['answer']\n - prefix_types: None\n - model: vocabtrimmer/mt5-small-trimmed-ja-30000\n - max_length: 512\n - max_length_output: 32\n - epoch: 17\n - batch: 32\n - lr: 0.001\n - fp16: False\n - random_seed: 1\n - gradient_accumulation_steps: 4\n - label_smoothing: 0.15\n\nThe full configuration can be found at [fine-tuning config file](https://huggingface.co/vocabtrimmer/mt5-small-trimmed-ja-30000-jaquad-qa/raw/main/trainer_config.json).\n\n## Citation\n```\n@inproceedings{ushio-etal-2022-generative,\n    title = \"{G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion {G}eneration\",\n    author = \"Ushio, Asahi  and\n        Alva-Manchego, Fernando  and\n        Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n\n```\n", "size_bytes": "299201925", "downloads": 2}
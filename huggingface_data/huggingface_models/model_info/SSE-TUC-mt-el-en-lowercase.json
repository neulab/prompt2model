{"pretrained_model_name": "lighteternal/SSE-TUC-mt-el-en-lowercase", "description": "---\nlanguage:\n- en\n- el\ntags:\n- translation\n\nwidget:\n- text: \"\u0397 \u03c4\u03cd\u03c7\u03b7 \u03b2\u03bf\u03b7\u03b8\u03ac\u03b5\u03b9 \u03c4\u03bf\u03c5\u03c2 \u03c4\u03bf\u03bb\u03bc\u03b7\u03c1\u03bf\u03cd\u03c2.\"\nlicense: apache-2.0\nmetrics:\n- bleu\n---\n\n\n## Greek to English NMT (lower-case output)\n## By the Hellenic Army Academy (SSE) and the Technical University of Crete (TUC)\n\n* source languages: el\n* target languages: en\n* licence: apache-2.0 \n* dataset: Opus, CCmatrix\n* model: transformer(fairseq)\n* pre-processing: tokenization + BPE segmentation\n* metrics: bleu, chrf \n* output: lowercase only, for mixed-cased model use this: https://huggingface.co/lighteternal/SSE-TUC-mt-el-en-cased\n\n\n### Model description\n\nTrained using the Fairseq framework, transformer_iwslt_de_en architecture.\\\\\nBPE segmentation (10k codes).\\\\\nLower-case model. \n\n### How to use\n\n```\nfrom transformers import FSMTTokenizer, FSMTForConditionalGeneration\n\nmname = \" <your_downloaded_model_folderpath_here> \"\n\ntokenizer = FSMTTokenizer.from_pretrained(mname)\nmodel = FSMTForConditionalGeneration.from_pretrained(mname)\n\ntext = \"\u0397 \u03c4\u03cd\u03c7\u03b7 \u03b2\u03bf\u03b7\u03b8\u03ac\u03b5\u03b9 \u03c4\u03bf\u03c5\u03c2 \u03c4\u03bf\u03bb\u03bc\u03b7\u03c1\u03bf\u03cd\u03c2.\"\n\nencoded = tokenizer.encode(text, return_tensors='pt')\n\noutputs = model.generate(encoded, num_beams=5, num_return_sequences=5, early_stopping=True)\nfor i, output in enumerate(outputs):\n    i += 1\n    print(f\"{i}: {output.tolist()}\")\n    \n    decoded = tokenizer.decode(output, skip_special_tokens=True)\n    print(f\"{i}: {decoded}\")\n```\n\n\n## Training data\n\nConsolidated corpus from Opus and CC-Matrix (~6.6GB in total)\n\n\n## Eval results\n\n\nResults on Tatoeba testset (EL-EN): \n\n| BLEU | chrF  |\n| ------ | ------ |\n| 79.3 |  0.795 |\n\n\nResults on XNLI parallel (EL-EN): \n\n| BLEU | chrF  |\n| ------ | ------ |\n| 66.2 |  0.623 |\n\n### BibTeX entry and citation info\n\nDimitris Papadopoulos, et al. \"PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation.\" (2021). Accepted at EACL 2021 SRW\n \n\n### Acknowledgement\n\nThe research work was supported by the Hellenic Foundation for Research and Innovation (HFRI) under the HFRI PhD Fellowship grant (Fellowship Number:50, 2nd call)\n", "size_bytes": "172976478", "downloads": 10}
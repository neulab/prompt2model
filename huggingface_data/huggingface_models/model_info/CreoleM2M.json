{"pretrained_model_name": "prajdabre/CreoleM2M", "description": "This is the CreoleM2M model. If you know, you know!\n\nUsage:\n```\nfrom transformers import MBartForConditionalGeneration, AutoModelForSeq2SeqLM\nfrom transformers import AlbertTokenizer, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"prajdabre/CreoleM2M\", do_lower_case=False, use_fast=False, keep_accents=True)\n\n# Or use tokenizer = AlbertTokenizer.from_pretrained(\"prajdabre/CreoleM2M\", do_lower_case=False, use_fast=False, keep_accents=True)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"prajdabre/CreoleM2M\")\n\n# Or use model = MBartForConditionalGeneration.from_pretrained(\"prajdabre/CreoleM2M\")\n\n# Some initial mapping\nbos_id = tokenizer._convert_token_to_id_with_added_voc(\"<s>\")\neos_id = tokenizer._convert_token_to_id_with_added_voc(\"</s>\")\npad_id = tokenizer._convert_token_to_id_with_added_voc(\"<pad>\")\n# To get lang_id use any of [\"<s>\", \"</s>\", \"<2acf>\", \"<2eng>\", \"<2bis>\", \"<2bzj>\", \"<2cbk>\", \"<2crs>\", \"<2djk>\", \"<2gul>\", \"<2hat>\", \"<2hwc>\", \"<2icr>\", \"<2jam>\", \"<2kri>\", \"<2ktu>\", \"<2mbf>\", \"<2mfe>\", \"<2mkn>\", \"<2pap>\", \"<2pcm>\", \"<2pis>\", \"<2rop>\", \"<2sag>\", \"<2srm>\", \"<2srn>\", \"<2tcs>\", \"<2tdt>\", \"<2tpi>\"]\n\n# First tokenize the input and outputs. The format below is how CreoleM2M was trained so the input should be \"Sentence </s> <2xxx>\" where xxx is the language code. Similarly, the output should be \"<2yyy> Sentence </s>\". \ninp = tokenizer('Wen dey wen stretch him out fo whip him real hard , Paul wen tell da captain dat stay dea , \u201c Dis okay in da rules fo da Rome peopo ? fo you fo whip one guy dat get da same rights jalike da Rome peopo ? even one guy dat neva do notting wrong ? ' </s> <2hwc>\", add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids\n\nmodel.eval() # Set dropouts to zero\n\nmodel_output=model.generate(inp, use_cache=True, num_beams=4, max_length=60, min_length=1, early_stopping=True, pad_token_id=pad_id, bos_token_id=bos_id, eos_token_id=eos_id, decoder_start_token_id=tokenizer._convert_token_to_id_with_added_voc(\"<eng>\"))\n\n\n# Decode to get output strings\n\ndecoded_output=tokenizer.decode(model_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n\nprint(decoded_output)\n```\nNotes:\n1. This is compatible with the latest version of transformers but was developed with version 4.3.2 so consider using 4.3.2 if possible.\n2. While I have only shown how to let logits and loss and how to generate outputs, you can do pretty much everything the MBartForConditionalGeneration class can do as in https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartForConditionalGeneration\n3. Note that the tokenizer I have used is based on sentencepiece and not BPE. Therefore I use the AlbertTokenizer class and not the MBartTokenizer class.", "size_bytes": "976483953", "downloads": 16}
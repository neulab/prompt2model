{"pretrained_model_name": "hackathon-pln-es/t5-small-spanish-nahuatl", "description": "---\nlicense: apache-2.0\nlanguage: \n- es\n- nah \ntags:\n- translation\nwidget:\n- text: \"translate Spanish to Nahuatl: Mi hermano es un ajolote\"\n\n---\n\n# t5-small-spanish-nahuatl\nNahuatl is the most widely spoken indigenous language in Mexico. However, training a neural network for the neural machine translation task is challenging due to the lack of structured data. The most popular datasets, such as the Axolot and bible-corpus, only consist of ~16,000 and ~7,000 samples, respectively. Moreover, there are multiple variants of Nahuatl, which makes this task even more difficult. For example, it is possible to find a single word from the Axolot dataset written in more than three different ways. Therefore, we leverage the T5 text-to-text prefix training strategy to compensate for the lack of data. We first train the multilingual model to learn Spanish and then adapt it to Nahuatl. The resulting T5 Transformer successfully translates short sentences. Finally, we report Chrf and BLEU results.\n\n\n## Model description\nThis model is a T5 Transformer ([t5-small](https://huggingface.co/t5-small)) fine-tuned on Spanish and Nahuatl sentences collected from the web. The dataset is normalized using 'sep' normalization from [py-elotl](https://github.com/ElotlMX/py-elotl).\n\n\n## Usage\n```python\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained('hackathon-pln-es/t5-small-spanish-nahuatl')\ntokenizer = AutoTokenizer.from_pretrained('hackathon-pln-es/t5-small-spanish-nahuatl')\n\nmodel.eval()\nsentence = 'muchas flores son blancas'\ninput_ids = tokenizer('translate Spanish to Nahuatl: ' + sentence, return_tensors='pt').input_ids\noutputs = model.generate(input_ids)\n# outputs = miak xochitl istak\noutputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n```\n\n## Approach\n### Dataset\nSince the Axolotl corpus contains misalignments, we select the best samples (12,207). We also use the [bible-corpus](https://github.com/christos-c/bible-corpus) (7,821). \n\n| Axolotl best aligned books                            | \n|:-----------------------------------------------------:|\n| Anales de Tlatelolco                                  | \n| Diario                                                |  \n| Documentos nauas de la Ciudad de M\u00e9xico del siglo XVI |  \n| Historia de M\u00e9xico narrada en n\u00e1huatl y espa\u00f1ol       |  \n| La tinta negra y roja (antolog\u00eda de poes\u00eda n\u00e1huatl)   |  \n| Memorial Breve (Libro las ocho relaciones)            |  \n| M\u00e9todo auto-did\u00e1ctico n\u00e1huatl-espa\u00f1ol                 |  \n| Nican Mopohua                                         | \n| Quinta Relaci\u00f3n (Libro las ocho relaciones)           |   \n| Recetario Nahua de Milpa Alta D.F                     | \n| Testimonios de la antigua palabra                     |\n| Trece Poetas del Mundo Azteca                         |\n| Una tortillita nom\u00e1s - Se taxkaltsin saj              |\n| Vida econ\u00f3mica de Tenochtitlan                        |\n\nAlso, we collected 3,000 extra samples from the web to increase the data.\n\n### Model and training\nWe employ two training stages using a multilingual T5-small. The advantage of this model is that it can handle different vocabularies and prefixes. T5-small is pre-trained on different tasks and languages (French, Romanian, English, German).\n\n### Training-stage 1 (learning Spanish)\nIn training stage 1, we first introduce Spanish to the model. The goal is to learn a new language rich in data (Spanish) and not lose the previous knowledge. We use the English-Spanish [Anki](https://www.manythings.org/anki/) dataset, which consists of 118.964 text pairs. The model is trained till convergence, adding the prefix \"Translate Spanish to English: \"\n\n### Training-stage 2 (learning Nahuatl)\nWe use the pre-trained Spanish-English model to learn Spanish-Nahuatl. Since the amount of Nahuatl pairs is limited, we also add 20,000 samples from the English-Spanish Anki dataset. This two-task training avoids overfitting and makes the model more robust.\n\n### Training setup\nWe train the models on the same datasets for 660k steps using batch size = 16 and a learning rate of 2e-5.\n\n\n## Evaluation results\nWe evaluate the models on the same 505 validation Nahuatl sentences for a fair comparison. Finally, we report the results using chrf and sacrebleu hugging face metrics:\n\n| English-Spanish pretraining  | Validation loss | BLEU | Chrf   |\n|:----------------------------:|:---------------:|:-----|-------:|\n| False                        | 1.34            | 6.17 | 26.96  | \n| True                         | 1.31            | 6.18 | 28.21  | \n\n\nThe English-Spanish pretraining improves BLEU and Chrf and leads to faster convergence. The evaluation is available on the [eval.ipynb](https://github.com/milmor/spanish-nahuatl-translation/blob/main/eval.ipynb) notebook. \n\n## References\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified Text-to-Text transformer.\n\n- Ximena Gutierrez-Vasques, Gerardo Sierra, and Hernandez Isaac. 2016. Axolotl: a web accessible parallel corpus for Spanish-Nahuatl. In International Conference on Language Resources and Evaluation (LREC).\n\n- https://github.com/christos-c/bible-corpus\n\n- https://github.com/ElotlMX/py-elotl\n\n\n## Team members\n- Emilio Alejandro Morales [(milmor)](https://huggingface.co/milmor)\n- Rodrigo Mart\u00ednez Arzate  [(rockdrigoma)](https://huggingface.co/rockdrigoma)\n- Luis Armando Mercado [(luisarmando)](https://huggingface.co/luisarmando)\n- Jacobo del Valle [(jjdv)](https://huggingface.co/jjdv)", "size_bytes": "242085627", "downloads": 33}
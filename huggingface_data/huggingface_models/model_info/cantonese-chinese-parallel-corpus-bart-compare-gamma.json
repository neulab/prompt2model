{"pretrained_model_name": "raptorkwok/cantonese-chinese-parallel-corpus-bart-compare-gamma", "description": "---\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: cantonese-chinese-parallel-corpus-bart-compare-gamma\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# cantonese-chinese-parallel-corpus-bart-compare-gamma\n\nThis model is a fine-tuned version of [fnlp/bart-base-chinese](https://huggingface.co/fnlp/bart-base-chinese) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.2099\n- Bleu: 29.6955\n- Chrf: 28.9249\n- Gen Len: 13.0643\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Chrf    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|\n| 1.8354        | 0.14  | 1000  | 1.5084          | 24.6672 | 24.1217 | 12.9159 |\n| 1.6211        | 0.29  | 2000  | 1.4287          | 25.8497 | 25.3494 | 12.859  |\n| 1.5541        | 0.43  | 3000  | 1.3743          | 26.6627 | 26.0718 | 12.9443 |\n| 1.4999        | 0.58  | 4000  | 1.3467          | 27.0053 | 26.527  | 12.9104 |\n| 1.455         | 0.72  | 5000  | 1.3175          | 27.3754 | 26.7902 | 12.985  |\n| 1.432         | 0.87  | 6000  | 1.2960          | 27.7163 | 27.1361 | 12.9902 |\n| 1.4114        | 1.01  | 7000  | 1.2912          | 27.9489 | 27.3879 | 12.9696 |\n| 1.2697        | 1.16  | 8000  | 1.2751          | 28.2199 | 27.5899 | 12.9986 |\n| 1.2883        | 1.3   | 9000  | 1.2655          | 28.4595 | 27.8055 | 13.0127 |\n| 1.2901        | 1.45  | 10000 | 1.2579          | 28.5394 | 27.9384 | 12.9805 |\n| 1.2868        | 1.59  | 11000 | 1.2542          | 28.4941 | 27.929  | 12.9783 |\n| 1.2617        | 1.74  | 12000 | 1.2454          | 28.5946 | 28.0555 | 12.9625 |\n| 1.2758        | 1.88  | 13000 | 1.2309          | 28.8086 | 28.1773 | 13.0014 |\n| 1.2229        | 2.03  | 14000 | 1.2492          | 28.9506 | 28.288  | 13.0298 |\n| 1.1359        | 2.17  | 15000 | 1.2335          | 29.002  | 28.3583 | 13.0104 |\n| 1.1324        | 2.32  | 16000 | 1.2311          | 29.2516 | 28.4804 | 13.1006 |\n| 1.1505        | 2.46  | 17000 | 1.2251          | 29.037  | 28.4052 | 12.9803 |\n| 1.136         | 2.61  | 18000 | 1.2226          | 29.2467 | 28.4569 | 13.0836 |\n| 1.1378        | 2.75  | 19000 | 1.2146          | 29.3233 | 28.6231 | 13.0469 |\n| 1.131         | 2.9   | 20000 | 1.2179          | 29.4498 | 28.6537 | 13.1031 |\n| 1.1036        | 3.04  | 21000 | 1.2369          | 29.4179 | 28.7257 | 13.0477 |\n| 1.0187        | 3.19  | 22000 | 1.2292          | 29.2447 | 28.5233 | 13.043  |\n| 1.0213        | 3.33  | 23000 | 1.2276          | 29.4798 | 28.6875 | 13.0936 |\n| 1.0365        | 3.48  | 24000 | 1.2258          | 29.4548 | 28.7054 | 13.0749 |\n| 1.0417        | 3.62  | 25000 | 1.2147          | 29.4761 | 28.7156 | 13.0649 |\n| 1.0326        | 3.77  | 26000 | 1.2170          | 29.5363 | 28.81   | 13.0618 |\n| 1.0332        | 3.91  | 27000 | 1.2099          | 29.6955 | 28.9249 | 13.0643 |\n| 0.9996        | 4.06  | 28000 | 1.2437          | 29.5951 | 28.8373 | 13.0841 |\n| 0.9335        | 4.2   | 29000 | 1.2361          | 29.6808 | 28.8677 | 13.1225 |\n| 0.9394        | 4.35  | 30000 | 1.2254          | 29.5894 | 28.8242 | 13.068  |\n\n\n### Framework versions\n\n- Transformers 4.28.1\n- Pytorch 2.0.1+cu117\n- Datasets 2.13.1\n- Tokenizers 0.13.3\n", "size_bytes": "561065693", "downloads": 4}
{"pretrained_model_name": "kaist-ai/CoT-T5-11B", "description": "---\ntags:\n- text2text-generation\ndatasets:\n- CoT-Collection\n- Flan-Collection\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: text2text-generation\nlibrary_name: transformers\n---\n\n# TL;DR\n\nCoT-T5 is a language model using [Flan-T5](https://huggingface.co/google/flan-t5-xxl) as a base model, and CoT fine-tuned on 1.84 million rationales across 1,060 tasks from the [CoT Collection](https://huggingface.co/datasets/kaist-ai/CoT-Collection).\nSince it was CoT fine-tuned on a large amount of rationales, it shows superior performance with CoT compared to Flan-T5.\nOne could use CoT-T5 for (1) Solving unseen tasks in zero-shot setting, and (2) Adapting to new tasks with CoT fine-tuning.\n\n# Model Details\n\n## Model Description\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Related Models:** [All CoT-T5 Checkpoints](https://huggingface.co/models?search=cot-t5)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/abs/2305.14045)\n  - [GitHub Repo](https://github.com/kaistAI/CoT-Collection)\n\n\nCoT-T5 is trained with two different sizes (3B and 11B).\nYou could check the 3B sized LM on [this page](https://huggingface.co/kaist-ai/CoT-T5-3B).\nAlso, check out our dataset as well on [this page](https://huggingface.co/datasets/kaist-ai/CoT-Collection).\n\n## License\nCoT Collection and CoT-T5 is subject to OpenAI's Terms of Use for the generated data. If you suspect any violations, please reach out to us.\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"kaist-ai/CoT-T5-11B\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"kaist-ai/CoT-T5-11B\")\n\ninput_text = \"Read the Directions and try to pick among A,B,C,D.\\n\\nDirecitons: A good way to figure out the relationship in a given question is to make up a sentence that describes the relationship between the first two words. Then, try to use the same sentence to find out which of the answer choices completes the same relationship with the third word.\\nQuestion: Odometer is to mileage as compass is to?\\nOptions: (A) speed, (B) hiking, (C) needle, (D) direction.\\nLet's think step by step.\\n\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"kaist-ai/CoT-T5-11B\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"kaist-ai/CoT-T5-11B\", device_map=\"auto\")\n\ninput_text = \"Read the Directions and try to pick among A,B,C,D.\\n\\nDirecitons: A good way to figure out the relationship in a given question is to make up a sentence that describes the relationship between the first two words. Then, try to use the same sentence to find out which of the answer choices completes the same relationship with the third word.\\nQuestion: Odometer is to mileage as compass is to?\\nOptions: (A) speed, (B) hiking, (C) needle, (D) direction.\\nLet's think step by step.\\n\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"kaist-ai/CoT-T5-11B\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"kaist-ai/CoT-T5-11B\", device_map=\"auto\", torch_dtype=torch.float16)\n\ninput_text = \"Read the Directions and try to pick among A,B,C,D.\\n\\nDirecitons: A good way to figure out the relationship in a given question is to make up a sentence that describes the relationship between the first two words. Then, try to use the same sentence to find out which of the answer choices completes the same relationship with the third word.\\nQuestion: Odometer is to mileage as compass is to?\\nOptions: (A) speed, (B) hiking, (C) needle, (D) direction.\\nLet's think step by step.\\n\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"kaist-ai/CoT-T5-11B\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"kaist-ai/CoT-T5-11B\", device_map=\"auto\", load_in_8bit=True)\n\ninput_text = \"Read the Directions and try to pick among A,B,C,D.\\n\\nDirecitons: A good way to figure out the relationship in a given question is to make up a sentence that describes the relationship between the first two words. Then, try to use the same sentence to find out which of the answer choices completes the same relationship with the third word.\\nQuestion: Odometer is to mileage as compass is to?\\nOptions: (A) speed, (B) hiking, (C) needle, (D) direction.\\nLet's think step by step.\\n\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n\n# Citation\n\nIf you find the following model helpful, please considering citing our paper!\n\n**BibTeX:**\n\n```bibtex\n@article{kim2023cot,\n  title={The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning},\n  author={Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon},\n  journal={arXiv preprint arXiv:2305.14045},\n  year={2023}\n}\n```", "size_bytes": 45594099712, "downloads": 49}
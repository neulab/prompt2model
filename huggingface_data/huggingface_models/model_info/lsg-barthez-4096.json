{"pretrained_model_name": "ccdv/lsg-barthez-4096", "description": "---\ntags:\n- summarization\n- bart\n- long context\nlanguage:\n- fr\npipeline_tag: fill-mask\n---\n\n# LSG model \n**Transformers >= 4.23.1**\\\n**This model relies on a custom modeling file, you need to add trust_remote_code=True**\\\n**See [\\#13467](https://github.com/huggingface/transformers/pull/13467)**\n\nLSG ArXiv [paper](https://arxiv.org/abs/2210.15497). \\\nGithub/conversion script is available at this [link](https://github.com/ccdv-ai/convert_checkpoint_to_lsg).\n\n* [Usage](#usage)\n* [Parameters](#parameters)\n* [Sparse selection type](#sparse-selection-type)\n* [Tasks](#tasks)\n\nThis model is adapted from [BARThez](https://huggingface.co/moussaKam/barthez) for encoder-decoder tasks without additional pretraining. It uses the same number of parameters/layers and the same tokenizer.\n\n\nThis model can handle long sequences but faster and more efficiently than Longformer (LED) or BigBird (Pegasus) from the hub and relies on Local + Sparse + Global attention (LSG).\n\n\nThe model requires sequences whose length is a multiple of the block size. The model is \"adaptive\" and automatically pads the sequences if needed (adaptive=True in config). It is however recommended, thanks to the tokenizer, to truncate the inputs (truncation=True) and optionally to pad with a multiple of the block size (pad_to_multiple_of=...). \\\n\nImplemented in PyTorch.\n\n![attn](attn.png)\n\n## Usage\nThe model relies on a custom modeling file, you need to add trust_remote_code=True to use it.\n\n```python: \nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"ccdv/lsg-barthez-4096\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-barthez-4096\")\n``` \n\n## Parameters\nYou can change various parameters like : \n* the number of global tokens (num_global_tokens=1)\n* local block size (block_size=128)\n* sparse block size (sparse_block_size=128)\n* sparsity factor (sparsity_factor=2)\n* mask_first_token (mask first token since it is redundant with the first global token)\n* see config.json file\n\nDefault parameters work well in practice. If you are short on memory, reduce block sizes, increase sparsity factor and remove dropout in the attention score matrix.\n\n```python:\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"ccdv/lsg-barthez-4096\", \n    trust_remote_code=True, \n    num_global_tokens=16,\n    block_size=64,\n    sparse_block_size=64,\n    attention_probs_dropout_prob=0.0\n    sparsity_factor=4,\n    sparsity_type=\"none\",\n    mask_first_token=True\n)\n``` \n\n## Sparse selection type\n\nThere are 5 different sparse selection patterns. The best type is task dependent. \\\nNote that for sequences with length < 2*block_size, the type has no effect.\n\n* sparsity_type=\"norm\", select highest norm tokens\n    * Works best for a small sparsity_factor (2 to 4)\n    * Additional parameters:\n        * None\n* sparsity_type=\"pooling\", use average pooling to merge tokens\n    * Works best for a small sparsity_factor (2 to 4)\n    * Additional parameters:\n        * None\n* sparsity_type=\"lsh\", use the LSH algorithm to cluster similar tokens\n    * Works best for a large sparsity_factor (4+)\n    * LSH relies on random projections, thus inference may differ slightly with different seeds\n    * Additional parameters:\n        * lsg_num_pre_rounds=1, pre merge tokens n times before computing centroids\n* sparsity_type=\"stride\", use a striding mecanism per head\n    * Each head will use different tokens strided by sparsify_factor\n    * Not recommended if sparsify_factor > num_heads\n* sparsity_type=\"block_stride\", use a striding mecanism per head\n    * Each head will use block of tokens strided by sparsify_factor\n    * Not recommended if sparsify_factor > num_heads\n\n## Tasks\nSeq2Seq example for summarization:\n```python:\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ccdv/lsg-barthez-4096\", \n    trust_remote_code=True, \n    pass_global_tokens_to_decoder=True, # Pass encoder global tokens to decoder\n)\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-barthez-4096\")\n\nSENTENCE = \"This is a test sequence to test the model. \" * 300\ntoken_ids = tokenizer(\n    SENTENCE, \n    return_tensors=\"pt\", \n    padding=\"max_length\", # Optional but recommended\n    truncation=True # Optional but recommended\n    )\noutput = model(**token_ids)\n```\n\n\nClassification example:\n```python:\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ccdv/lsg-barthez-4096\", \n    trust_remote_code=True, \n    pass_global_tokens_to_decoder=True, # Pass encoder global tokens to decoder\n)\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-barthez-4096\")\n\nSENTENCE = \"This is a test sequence to test the model. \" * 300\ntoken_ids = tokenizer(\n    SENTENCE, \n    return_tensors=\"pt\", \n    #pad_to_multiple_of=... # Optional\n    truncation=True\n    )\noutput = model(**token_ids)\n\n> SequenceClassifierOutput(loss=None, logits=tensor([[-0.3051, -0.1762]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n```\n\n\n## Conversion script\n\nTo convert a BERT, RoBERTa or BART checkpoint to LSG, see this [repo](https://github.com/ccdv-ai/convert_checkpoint_to_lsg).\n\n\n**BARThez**\n```\n@article{eddine2020barthez,\n  title={BARThez: a Skilled Pretrained French Sequence-to-Sequence Model},\n  author={Eddine, Moussa Kamal and Tixier, Antoine J-P and Vazirgiannis, Michalis},\n  journal={arXiv preprint arXiv:2010.12321},\n  year={2020}\n}\n```", "size_bytes": "577617519", "downloads": 19}
{"pretrained_model_name": "Zekunli/flan-t5-large-extraction-all-dm_8000-ep25-nonstop", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: flan-t5-large-extraction-all-dm_8000-ep25-nonstop\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-large-extraction-all-dm_8000-ep25-nonstop\n\nThis model is a fine-tuned version of [google/flan-t5-large](https://huggingface.co/google/flan-t5-large) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.5739\n- Hint Hit Num: 2.184\n- Hint Precision: 0.4154\n- Num: 5.218\n- Gen Len: 18.738\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 24\n- eval_batch_size: 96\n- seed: 1799\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 25\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Hint Hit Num | Hint Precision | Num   | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:------------:|:--------------:|:-----:|:-------:|\n| 2.1513        | 0.6   | 200  | 1.5120          | 2.679        | 0.4838         | 5.551 | 18.94   |\n| 1.9342        | 1.2   | 400  | 1.4818          | 2.563        | 0.4677         | 5.469 | 18.925  |\n| 1.8591        | 1.8   | 600  | 1.4593          | 2.494        | 0.4607         | 5.396 | 18.899  |\n| 1.7973        | 2.4   | 800  | 1.4597          | 2.389        | 0.4515         | 5.265 | 18.836  |\n| 1.7824        | 2.99  | 1000 | 1.4494          | 2.387        | 0.4419         | 5.399 | 18.891  |\n| 1.7263        | 3.59  | 1200 | 1.4597          | 2.278        | 0.4301         | 5.261 | 18.875  |\n| 1.711         | 4.19  | 1400 | 1.4673          | 2.292        | 0.4314         | 5.272 | 18.826  |\n| 1.6631        | 4.79  | 1600 | 1.4638          | 2.185        | 0.4163         | 5.177 | 18.832  |\n| 1.6494        | 5.39  | 1800 | 1.4625          | 2.287        | 0.431          | 5.278 | 18.841  |\n| 1.6328        | 5.99  | 2000 | 1.4584          | 2.209        | 0.4211         | 5.185 | 18.842  |\n| 1.6008        | 6.59  | 2200 | 1.4677          | 2.299        | 0.4374         | 5.233 | 18.777  |\n| 1.5646        | 7.19  | 2400 | 1.4902          | 2.182        | 0.4224         | 5.137 | 18.71   |\n| 1.574         | 7.78  | 2600 | 1.4777          | 2.211        | 0.4235         | 5.19  | 18.781  |\n| 1.5348        | 8.38  | 2800 | 1.4796          | 2.314        | 0.4311         | 5.317 | 18.792  |\n| 1.5224        | 8.98  | 3000 | 1.4799          | 2.197        | 0.4212         | 5.17  | 18.805  |\n| 1.4857        | 9.58  | 3200 | 1.4897          | 2.256        | 0.4296         | 5.221 | 18.755  |\n| 1.4948        | 10.18 | 3400 | 1.5030          | 2.206        | 0.4203         | 5.201 | 18.76   |\n| 1.4667        | 10.78 | 3600 | 1.4956          | 2.269        | 0.4319         | 5.203 | 18.772  |\n| 1.4492        | 11.38 | 3800 | 1.5098          | 2.208        | 0.4191         | 5.235 | 18.801  |\n| 1.4454        | 11.98 | 4000 | 1.5064          | 2.187        | 0.4153         | 5.22  | 18.799  |\n| 1.4125        | 12.57 | 4200 | 1.5173          | 2.175        | 0.4164         | 5.182 | 18.766  |\n| 1.426         | 13.17 | 4400 | 1.5299          | 2.162        | 0.414          | 5.189 | 18.772  |\n| 1.3944        | 13.77 | 4600 | 1.5297          | 2.199        | 0.4182         | 5.224 | 18.797  |\n| 1.382         | 14.37 | 4800 | 1.5301          | 2.204        | 0.4217         | 5.197 | 18.799  |\n| 1.3836        | 14.97 | 5000 | 1.5303          | 2.188        | 0.4185         | 5.209 | 18.764  |\n| 1.358         | 15.57 | 5200 | 1.5293          | 2.264        | 0.4283         | 5.261 | 18.812  |\n| 1.3645        | 16.17 | 5400 | 1.5411          | 2.195        | 0.42           | 5.19  | 18.753  |\n| 1.3455        | 16.77 | 5600 | 1.5417          | 2.267        | 0.4286         | 5.251 | 18.76   |\n| 1.3395        | 17.37 | 5800 | 1.5436          | 2.207        | 0.4217         | 5.19  | 18.738  |\n| 1.3302        | 17.96 | 6000 | 1.5468          | 2.268        | 0.4256         | 5.288 | 18.765  |\n| 1.3329        | 18.56 | 6200 | 1.5488          | 2.265        | 0.4251         | 5.299 | 18.788  |\n| 1.299         | 19.16 | 6400 | 1.5582          | 2.245        | 0.4253         | 5.25  | 18.717  |\n| 1.3141        | 19.76 | 6600 | 1.5562          | 2.211        | 0.421          | 5.195 | 18.742  |\n| 1.318         | 20.36 | 6800 | 1.5597          | 2.22         | 0.4204         | 5.24  | 18.776  |\n| 1.2905        | 20.96 | 7000 | 1.5605          | 2.228        | 0.4224         | 5.24  | 18.745  |\n| 1.2967        | 21.56 | 7200 | 1.5679          | 2.199        | 0.4149         | 5.255 | 18.798  |\n| 1.2896        | 22.16 | 7400 | 1.5667          | 2.218        | 0.4212         | 5.229 | 18.736  |\n| 1.2886        | 22.75 | 7600 | 1.5663          | 2.212        | 0.4175         | 5.262 | 18.8    |\n| 1.2818        | 23.35 | 7800 | 1.5718          | 2.211        | 0.4193         | 5.228 | 18.757  |\n| 1.2893        | 23.95 | 8000 | 1.5730          | 2.185        | 0.4155         | 5.215 | 18.737  |\n| 1.2772        | 24.55 | 8200 | 1.5736          | 2.186        | 0.4153         | 5.224 | 18.753  |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "3132789733", "downloads": 2}
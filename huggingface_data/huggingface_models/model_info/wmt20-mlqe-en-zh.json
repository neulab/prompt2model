{"pretrained_model_name": "inseq/wmt20-mlqe-en-zh", "description": "---\nlanguage:\n- en\n- zh\n- multilingual\nlicense: cc-by-sa-4.0\ntags:\n- translation\n- wmt20\ndatasets:\n- wmt/news-commentary\n- wmt/wikititles\n- wmt/uncorpus\nwidget:\n- text: \"It is a plump quail-shaped bird with white eyes and predominantly marbled black, rufous and pale brown plumage, marked prominently with white spots and stripes.\"\n- text: \"The 59th Primetime Creative Arts Emmy Awards honored the best in artistic and technical achievement in American prime time television programming from June 1, 2006, until May 31, 2007, as chosen by the Academy of Television Arts & Sciences.\"\n- text: \"While forests in temperate areas are readily categorised on the basis of tree canopy density, such schemes do not work well in tropical forests.\"\n---\n\n# Fairseq En-Zh NMT WMT20 MLQE\n\nThis repository contains the English-Chinese model trained with the [fairseq toolkit](https://github.com/pytorch/fairseq) that was used to produce translations used in the WMT20 shared task on quality estimation (QE) on the [MLQE dataset](https://github.com/facebookresearch/mlqe).\n\nThe checkpoint was converted from the original fairseq checkpoint available [here](https://github.com/facebookresearch/mlqe/tree/master/nmt_models) using the `convert_fsmt_original_pytorch_checkpoint_to_pytorch.py` script from the \ud83e\udd17 Transformers library (v4.26.0).\n\nPlease refer to the repositories linked above for additional information on usage, parameters and training data", "size_bytes": "424881925", "downloads": 48}
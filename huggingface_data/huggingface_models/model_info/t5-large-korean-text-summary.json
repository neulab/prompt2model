{"pretrained_model_name": "lcw99/t5-large-korean-text-summary", "description": "---\nlanguage:\n  - ko\ntags:\n- generated_from_keras_callback\nmodel-index:\n- name: t5-large-korean-text-summary\n  results: []\n---\n\n# t5-large-korean-text-summary\n\nThis model is a fine-tuning of [paust/pko-t5-large](https://huggingface.co/paust/pko-t5-large) model using AIHUB \"summary and report generation data\". This model provides a short summary of long sentences in Korean.\n\n\uc774 \ubaa8\ub378\uc740 paust/pko-t5-large model\uc744 AIHUB \"\uc694\uc57d\ubb38 \ubc0f \ub808\ud3ec\ud2b8 \uc0dd\uc131 \ub370\uc774\ud130\"\ub97c \uc774\uc6a9\ud558\uc5ec fine tunning \ud55c \uac83\uc785\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc740 \ud55c\uae00\ub85c\ub41c \uc7a5\ubb38\uc744 \uc9e7\uac8c \uc694\uc57d\ud574 \uc90d\ub2c8\ub2e4.\n\n## Usage\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport nltk\nnltk.download('punkt')\n\nmodel_dir = \"lcw99/t5-large-korean-text-summary\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\nmax_input_length = 512 + 256\n\ntext = \"\"\"\n\uc8fc\uc778\uacf5 \uac15\uc778\uad6c(\ud558\uc815\uc6b0)\ub294 \u2018\uc218\ub9ac\ub0a8\uc5d0\uc11c \ud64d\uc5b4\uac00 \ub9ce\uc774 \ub098\ub294\ub370 \ub2e4 \uac16\ub2e4\ubc84\ub9b0\ub2e4\u2019\ub294 \uce5c\uad6c \n\ubc15\uc751\uc218(\ud604\ubd09\uc2dd)\uc758 \uc598\uae30\ub97c \ub4e3\uace0 \uc218\ub9ac\ub0a8\uc0b0 \ud64d\uc5b4\ub97c \ud55c\uad6d\uc5d0 \uc218\ucd9c\ud558\uae30 \uc704\ud574 \uc218\ub9ac\ub0a8\uc73c\ub85c \uac04\ub2e4. \n\uad6d\ub9bd\uc218\uc0b0\uacfc\ud559\uc6d0 \uce21\uc740 \u201c\uc2e4\uc81c\ub85c \ub0a8\ub300\uc11c\uc591\uc5d0 \ud64d\uc5b4\uac00 \ub9ce\uc774 \uc0b4\uace0 \uc544\ub974\ud5e8\ud2f0\ub098\ub97c \ube44\ub86f\ud55c \ub0a8\ubbf8 \uad6d\uac00\uc5d0\uc11c \ud64d\uc5b4\uac00 \ub9ce\uc774 \uc7a1\ud78c\ub2e4\u201d\uba70 \n\u201c\uc218\ub9ac\ub0a8 \uc5f0\uc548\uc5d0\ub3c4 \ud64d\uc5b4\uac00 \ub9ce\uc774 \uc11c\uc2dd\ud560 \uac83\u201d\uc774\ub77c\uace0 \uc124\uba85\ud588\ub2e4.\n\n\uadf8\ub7ec\ub098 \uad00\uc138\uccad\uc5d0 \ub530\ub974\uba74 \ud55c\uad6d\uc5d0 \uc218\ub9ac\ub0a8\uc0b0 \ud64d\uc5b4\uac00 \uc218\uc785\ub41c \uc801\uc740 \uc5c6\ub2e4. \n\uc77c\uac01\uc5d0\uc120 \u201c\ub3c8\uc744 \ubc8c\uae30 \uc704\ud574 \uc218\ub9ac\ub0a8\uc0b0 \ud64d\uc5b4\ub97c \uad6c\ud558\ub7ec \uac04 \uc124\uc815\uc740 \uac1c\uc5f0\uc131\uc774 \ub5a8\uc5b4\uc9c4\ub2e4\u201d\ub294 \uc9c0\uc801\ub3c4 \ud55c\ub2e4. \n\ub4dc\ub77c\ub9c8 \ubc30\uacbd\uc774 \ub41c 2008~2010\ub144\uc5d0\ub294 \uc774\ubbf8 \uad6d\ub0b4\uc5d0 \uc544\ub974\ud5e8\ud2f0\ub098, \uce60\ub808, \ubbf8\uad6d \ub4f1 \uc544\uba54\ub9ac\uce74\uc0b0 \ud64d\uc5b4\uac00 \uc218\uc785\ub418\uace0 \uc788\uc5c8\uae30 \ub54c\ubb38\uc774\ub2e4. \n\uc2e4\uc81c \uc870\ubd09\ud589 \uccb4\ud3ec \uc791\uc804\uc5d0 \ud611\uc870\ud588\ub358 \u2018\ud611\ub825\uc790 K\uc528\u2019\ub3c4 \ud64d\uc5b4 \uc0ac\uc5c5\uc774 \uc544\ub2c8\ub77c \uc218\ub9ac\ub0a8\uc5d0 \uc120\ubc15\uc6a9 \ud2b9\uc218\uc6a9\uc811\ubd09\uc744 \ud30c\ub294 \uc0ac\uc5c5\uc744 \ud558\ub7ec \uc218\ub9ac\ub0a8\uc5d0 \uac14\uc5c8\ub2e4.\n\"\"\"\n\ninputs = [\"summarize: \" + text]\n\ninputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\noutput = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=100)\ndecoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\npredicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n\nprint(predicted_title)\n```\n\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: None\n- training_precision: float16\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.22.1\n- TensorFlow 2.10.0\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "3282122021", "downloads": 392}
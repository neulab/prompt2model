{"pretrained_model_name": "georeactor/t5-reddit-2014", "description": "---\nlanguage:\n  - en\nlicense: apache-2.0\ntags:\n  - reddit\ndatasets:\n  - georeactor/reddit_one_ups_seq2seq_2014\n---\n\n# t5-reddit-2014\n\nT5-small model fine-tuned on Reddit \"One-Ups\" / \"Clapbacks\" dataset. Each reply from\nthe fine-tuning has a vote-score 1.5x or higher than the parent comment.\n\nFrom a few tests it seems to have adopted a snarky tone. Common reply is \"I'm not a shit.\"\n\n## Process\n\nTraining notebook: https://github.com/Georeactor/reddit-one-ups/blob/main/training-models/t5-seq2seq-2014.ipynb\n\n- Started with [t5-small](https://huggingface.co/t5-small) so I could run it on CoLab.\n- Fine-tuned on first 80% of [georeactor/reddit_one_ups_seq2seq_2014](https://huggingface.co/datasets/georeactor/reddit_one_ups_seq2seq_2014) for one epoch, batch size = 2.\n- Loss did not move much during this epoch.\n- Future experiments should use a larger model, larger batch size (could easily have done batch_size = 4 on CoLab), full dataset if we are not worried about eval.\n\n## Inference\n\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nmodel = AutoModelForSeq2SeqLM.from_pretrained('georeactor/t5-reddit-2014')\ntokenizer = AutoTokenizer.from_pretrained('georeactor/t5-reddit-2014')\n\ninput = tokenizer.encode('Looks like a potato bug', return_tensors=\"pt\")\noutput = model.generate(input, max_length=256)\ntokenizer.decode(output[0])\n```\n", "size_bytes": "242071641", "downloads": 2}
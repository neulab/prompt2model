{"pretrained_model_name": "kaejo98/bart-base_question_generation", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bart-base_question_generation\n  results: []\n---\n\n\n\n# BART-base Question Generation\n\nThis model is a fine-tuned version of [facebook/bart-base](https://huggingface.co/facebook/bart-base) on different questions and answering dataset. It was trained to generation question using two different approaches, <b> Casual-Generation </b> and <b> Context-based-Generation </b>. \n\n\n## Model description\n\nThe model takes context as an input sequence, and will generate a full question sentence as an output sequence. There are two ways the model can be queried produce the questions:\n- <b> Casual-Generation </b>:  where the model is tasked to generate questions answerable by a given passage. The input should be follow the structure or format: '\\<generate_questions\\> paragraph: put your passage text here'. <br/>\nExample: <br/>\n   \\<generate_questions\\> paragraph: The lithosphere is broken into tectonic plates whose motion allows heat to escape from the interior of the Earth into space. The crust lies on top of the mantle, a configuration that is stable because the upper mantle is made of peridotite and is therefore significantly denser than the crust. The boundary between the crust and mantle is conventionally placed at the Mohorovi\u010di\u0107 discontinuity, a boundary defined by a contrast in seismic velocity.\n   \n   \n- <b> Context-based-Generation </b>: given a section of a passage (context), the model is tasked to generate questions from the passage about the selected section or context. The input should be follow the structure or format: \\<generate_context_questions\\> \\<section\\> put your context here \\</section\\>  paragraph: put your passage text here'. <br/>\nExample: <br/>\n   \\<generate_context_questions\\> \\<section\\> Mohorovi\u010di\u0107 discontinuity \\</section\\>  paragraph: The lithosphere is broken into tectonic plates whose motion allows heat to escape from the interior of the Earth into space. The crust lies on top of the mantle, a configuration that is stable because the upper mantle is made of peridotite and is therefore significantly denser than the crust. The boundary between the crust and mantle is conventionally placed at the Mohorovi\u010di\u0107 discontinuity, a boundary defined by a contrast in seismic velocity.\n\n\nThe input sequence can then be encoded and passed as the input_ids argument in the model's generate() method.\n\n## limitations\nThe model was trained on only a limited amount of data hence questions might be poor quality. In addition the questions generated have style similar to that of the training data.\n\n\n\n## Training and evaluation data\n\nThe dataset used to train the model comprises the training datasets from:\n- Reasoning Over Paragraph Effects in Situations (ROPES): https://allenai.org/data/ropes \n- SQUAD: \n- DROP (Discrete Reasoning Over Paragraphs): https://allenai.org/data/drop\n- SciQ\n\nAfter preprocessing the data from the above listed datasets, we had 408372 examples for training the model and 25k for development and 18k for testing. \n \n## Training procedure\nThe model is trained (finetuned) for 5 epochs with the hyperparameters listed below:\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 3e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.25\n- num_epochs: 5\n\nAt the end of 5 epochs, the Evaluation loss was: 1.64 and the training loss was: 0.9671.\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.13.0\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n", "size_bytes": "557986589", "downloads": 1221}
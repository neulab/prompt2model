{"pretrained_model_name": "farleyknight/cnn_dailymail-summarization-t5-small-2022-09-08", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- cnn_dailymail\nmetrics:\n- rouge\nmodel-index:\n- name: cnn_dailymail-summarization-t5-small-2022-09-08\n  results:\n  - task:\n      name: Summarization\n      type: summarization\n    dataset:\n      name: cnn_dailymail 3.0.0\n      type: cnn_dailymail\n      args: 3.0.0\n    metrics:\n    - name: Rouge1\n      type: rouge\n      value: 41.4235\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# cnn_dailymail-summarization-t5-small-2022-09-08\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on the cnn_dailymail 3.0.0 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6455\n- Rouge1: 41.4235\n- Rouge2: 19.0263\n- Rougel: 29.2892\n- Rougelsum: 38.6338\n- Gen Len: 73.7273\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:------:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 1.8435        | 0.28  | 10000  | 1.6998          | 24.3321 | 11.599  | 20.1028 | 22.9562   | 18.9997 |\n| 1.8464        | 0.56  | 20000  | 1.6814          | 24.4483 | 11.6789 | 20.1798 | 23.0508   | 18.9996 |\n| 1.8332        | 0.84  | 30000  | 1.6738          | 24.5531 | 11.7949 | 20.2834 | 23.1588   | 18.9994 |\n| 1.8054        | 1.11  | 40000  | 1.6636          | 24.6194 | 11.843  | 20.3375 | 23.2259   | 18.9991 |\n| 1.7958        | 1.39  | 50000  | 1.6597          | 24.5017 | 11.7755 | 20.2439 | 23.1148   | 18.9998 |\n| 1.8095        | 1.67  | 60000  | 1.6546          | 24.5126 | 11.8043 | 20.2603 | 23.1175   | 18.9999 |\n| 1.8127        | 1.95  | 70000  | 1.6521          | 24.4845 | 11.8136 | 20.2557 | 23.1089   | 18.9999 |\n| 1.7952        | 2.23  | 80000  | 1.6488          | 24.6217 | 11.8877 | 20.3555 | 23.2514   | 18.9996 |\n| 1.7863        | 2.51  | 90000  | 1.6477          | 24.5616 | 11.8489 | 20.3021 | 23.1754   | 18.9996 |\n| 1.7824        | 2.79  | 100000 | 1.6464          | 24.5852 | 11.8531 | 20.3172 | 23.2089   | 18.9998 |\n\n\n### Framework versions\n\n- Transformers 4.22.0.dev0\n- Pytorch 1.12.1+cu102\n- Datasets 2.4.0\n- Tokenizers 0.12.1\n", "size_bytes": "242012923", "downloads": 4}
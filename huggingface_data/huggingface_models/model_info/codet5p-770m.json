{"pretrained_model_name": "Salesforce/codet5p-770m", "description": "---\nlicense: bsd-3-clause\n---\n\n# CodeT5+ 770M\n\n## Model description\n\n[CodeT5+](https://github.com/salesforce/CodeT5/tree/main/CodeT5+) is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. _encoder-only_, _decoder-only_, and _encoder-decoder_) to support a wide range of code understanding and generation tasks. \nIt is introduced in the paper:\n\n[CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/pdf/2305.07922.pdf)\nby [Yue Wang](https://yuewang-cuhk.github.io/)\\*, [Hung Le](https://sites.google.com/view/henryle2018/home?pli=1)\\*, [Akhilesh Deepak Gotmare](https://akhileshgotmare.github.io/), [Nghi D.Q. Bui](https://bdqnghi.github.io/), [Junnan Li](https://sites.google.com/site/junnanlics), [Steven C.H. Hoi](https://sites.google.com/view/stevenhoi/home) (* indicates equal contribution).\n\nCompared to the original CodeT5 family (CodeT5-base: `220M`, CodeT5-large: `770M`), CodeT5+ is pretrained with a diverse set of pretraining tasks including _span denoising_, _causal language modeling_, _contrastive learning_, and _text-code matching_ to learn rich representations from both unimodal code data and bimodal code-text data. \nAdditionally, it employs a simple yet effective _compute-efficient pretraining_ method to initialize the model components with frozen off-the-shelf LLMs such as [CodeGen](https://github.com/salesforce/CodeGen) to efficiently scale up the model (i.e. `2B`, `6B`, `16B`), and adopts a \"shallow encoder and deep decoder\" architecture. \nFurthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B) following [Code Alpaca](https://github.com/sahil280114/codealpaca).  \n\n## How to use\n\nThis model can be easily loaded using the `T5ForConditionalGeneration` functionality and employs the same tokenizer as original [CodeT5](https://github.com/salesforce/CodeT5).\n\n```python\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\n\ncheckpoint = \"Salesforce/codet5p-770m\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = T5ForConditionalGeneration.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():<extra_id_0>\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_length=10)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n# ==> print \"Hello World\"\n```\n\n## Pretraining data\n\nThis checkpoint is trained on the stricter permissive subset of the deduplicated version of the [github-code dataset](https://huggingface.co/datasets/codeparrot/github-code).\nThe data is preprocessed by reserving only permissively licensed code (\"mit\" \u201capache-2\u201d, \u201cbsd-3-clause\u201d, \u201cbsd-2-clause\u201d, \u201ccc0-1.0\u201d, \u201cunlicense\u201d, \u201cisc\u201d).\nSupported languages (9 in total) are as follows:\n`c`, `c++`, `c-sharp`,  `go`, `java`, `javascript`,  `php`, `python`, `ruby.`\n\n## Training procedure\n\nThis checkpoint is trained on the unimodal code data at the first-stage pretraining, which includes a diverse set of pretraining tasks including _span denoising_ and two variants of _causal language modeling_.\nPlease refer to the paper for more details.\n\n## Evaluation results\n\nCodeT5+ models have been comprehensively evaluated on a wide range of code understanding and generation tasks in various settings: _zero-shot_, _finetuning_, and _instruction-tuning_.\nSpecifically, CodeT5+ yields substantial performance gains on many downstream tasks compared to their SoTA baselines, e.g.,\n8 text-to-code retrieval tasks (+3.2 avg. MRR), 2 line-level code completion tasks (+2.1 avg. Exact Match), and 2 retrieval-augmented code generation tasks (+5.8 avg. BLEU-4). \nIn 2 math programming tasks on MathQA-Python and GSM8K-Python, CodeT5+ models of below billion-parameter sizes significantly outperform many LLMs of up to 137B parameters. \nParticularly, in the zero-shot text-to-code generation task on HumanEval benchmark, InstructCodeT5+ 16B sets new SoTA results of 35.0% pass@1 and 54.5% pass@10 against other open code LLMs, even surpassing the closed-source OpenAI code-cushman-001 mode\nPlease refer to the [paper](https://arxiv.org/pdf/2305.07922.pdf) for more details.\n\n\n## BibTeX entry and citation info\n\n```bibtex\n@article{wang2023codet5plus,\n  title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation},\n  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi D.Q. and Li, Junnan and Hoi, Steven C. H.},\n  journal={arXiv preprint},\n  year={2023}\n}\n```", "size_bytes": "1475351723", "downloads": 4540}
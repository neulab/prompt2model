{"pretrained_model_name": "PrimeQA/mt5-base-tydi-question-generator", "description": "---\nlicense: apache-2.0\n---\n\n# Model description\n\nThis is an [mt5-base](https://huggingface.co/google/mt5-base) model, finetuned to generate questions using [TyDi QA](https://huggingface.co/datasets/tydiqa) dataset. It was trained to take the context and answer as input to generate questions.\n\n# Overview\n\n*Language model*: mT5-base \\\n*Language*: Arabic, Bengali, English, Finnish, Indonesian, Korean, Russian, Swahili, Telugu \\\n*Task*: Question Generation \\\n*Data*: TyDi QA\n\n# Intented use and limitations\nOne can use this model to generate questions. Biases associated with pre-training of mT5 and TyDiQA dataset may be present.\n\n## Usage\nOne can use this model directly in the [PrimeQA](https://github.com/primeqa/primeqa) framework as in this example [notebook](https://github.com/primeqa/primeqa/blob/main/notebooks/qg/tableqg_inference.ipynb).\n\nOr \n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"PrimeQA/mt5-base-tydi-question-generator\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"PrimeQA/mt5-base-tydi-question-generator\")\n\ndef get_question(answer, context, max_length=64):\n  input_text = answer +\" <<sep>> \" + context\n  features = tokenizer([input_text], return_tensors='pt')\n  output = model.generate(input_ids=features['input_ids'], \n               attention_mask=features['attention_mask'],\n               max_length=max_length)\n  return tokenizer.decode(output[0])\ncontext = \"\u09b6\u099a\u09c0\u09a8 \u099f\u09c7\u09a8\u09cd\u09a1\u09c1\u09b2\u0995\u09be\u09b0\u0995\u09c7 \u0995\u09cd\u09b0\u09bf\u0995\u09c7\u099f \u0987\u09a4\u09bf\u09b9\u09be\u09b8\u09c7\u09b0 \u0985\u09a8\u09cd\u09af\u09a4\u09ae \u09b8\u09c7\u09b0\u09be \u09ac\u09cd\u09af\u09be\u099f\u09b8\u09ae\u09cd\u09af\u09be\u09a8 \u09b9\u09bf\u09b8\u09c7\u09ac\u09c7 \u0997\u09a3\u09cd\u09af \u0995\u09b0\u09be \u09b9\u09af\u09bc\u0964\"\nanswer = \"\u09b6\u099a\u09c0\u09a8 \u099f\u09c7\u09a8\u09cd\u09a1\u09c1\u09b2\u0995\u09be\u09b0\"\nget_question(answer, context)\n# output: \u0995\u09cd\u09b0\u09bf\u0995\u09c7\u099f \u0987\u09a4\u09bf\u09b9\u09be\u09b8\u09c7\u09b0 \u0985\u09a8\u09cd\u09af\u09a4\u09ae \u09b8\u09c7\u09b0\u09be \u09ac\u09cd\u09af\u09be\u099f\u09b8\u09ae\u09cd\u09af\u09be\u09a8 \u0995\u09c7?\n```\n\n## Citation\n```bibtex\n@inproceedings{xue2021mt5,\n  title={mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer},\n  author={Xue, Linting and Constant, Noah and Roberts, Adam and\n          Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and\n          Barua, Aditya and Raffel, Colin},\n  booktitle={Proceedings of the 2021 Conference of the North American\n             Chapter of the Association for Computational Linguistics:\n             Human Language Technologies},\n  pages={483--498},\n  year={2021}\n}\n```\n\n", "size_bytes": "2329690701", "downloads": 154}
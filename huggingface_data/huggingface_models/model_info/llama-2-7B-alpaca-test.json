{"pretrained_model_name": "layoric/llama-2-7B-alpaca-test", "description": "---\ndatasets:\n- mhenrichsen/alpaca_2k_test\npipeline_tag: text2text-generation\n---\n[<img src=\"https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/OpenAccess-AI-Collective/axolotl)\n\nSmall qlora finetune using Axolotl. Locally tested using `wikitext` perplexity test and had a small improvement over the base Llama v2 7B base model.\n\nAxolotl config used:\n```yaml\nbase_model: NousResearch/Llama-2-7b-hf\nbase_model_config: NousResearch/Llama-2-7b-hf\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\npush_dataset_to_hub:\nhub_model_id:\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: mhenrichsen/alpaca_2k_test\n    type: alpaca\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.01\noutput_dir: ./checkpoints/llama-2-qlora\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 4096\nmax_packed_sequence_len: 4096\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 3\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: true\nbf16: true\nfp16: false\ntf32: true\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention: true\nflash_attention:\n\nwarmup_steps: 10\neval_steps: 20\nsave_steps:\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  bos_token: \"<s>\"\n  eos_token: \"</s>\"\n  unk_token: \"<unk>\"\n```\n\nAnd then merged with Axolotl via:\n\n```\naccelerate launch scripts/finetune.py configs/your_config.yml --merge_lora --lora_model_dir=\"./completed-model\" --load_in_8bit=False --load_in_4bit=False\n```\n\n", "size_bytes": 13476847616, "downloads": 4}
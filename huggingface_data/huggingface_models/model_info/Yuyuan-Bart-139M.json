{"pretrained_model_name": "IDEA-CCNL/Yuyuan-Bart-139M", "description": "---\nlanguage: \n  - en\nlicense: apache-2.0\n\ntags:\n- bart\n- biobart\n- biomedical\n\ninference: true\n\nwidget:\n- text: \"Influenza is a <mask> disease.\"\n- type: \"text-generation\"\n\n---\n# Yuyuan-Bart-139M\n\n- Main Page:[Fengshenbang](https://fengshenbang-lm.com/)\n- Github: [Fengshenbang-LM](https://github.com/IDEA-CCNL/Fengshenbang-LM)\n\n## \u7b80\u4ecb Brief Introduction\n\n\u751f\u7269\u533b\u7597\u9886\u57df\u7684\u751f\u6210\u8bed\u8a00\u6a21\u578b\uff0c\u82f1\u6587\u7684BioBART-base\u3002\n\nA generative language model for biomedicine, BioBART-base in English.\n\n## \u6a21\u578b\u5206\u7c7b Model Taxonomy\n\n|  \u9700\u6c42 Demand  | \u4efb\u52a1 Task       | \u7cfb\u5217 Series      | \u6a21\u578b Model    | \u53c2\u6570 Parameter | \u989d\u5916 Extra |\n|  :----:  | :----:  | :----:  | :----:  | :----:  | :----:  |\n| \u7279\u6b8a Special | \u9886\u57df Domain | \u4f59\u5143 Yuyuan | BioBART |      139M      |     \u82f1\u6587 English     |\n\n## \u6a21\u578b\u4fe1\u606f Model Information\n\nPaper: [BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model](https://arxiv.org/pdf/2204.03905.pdf)\n\nYuyuan-Bart-139M\u7531\u6e05\u534e\u5927\u5b66\u548cIDEA\u7814\u7a76\u9662\u4e00\u8d77\u63d0\u4f9b\u7684\u751f\u7269\u533b\u7597\u9886\u57df\u7684\u751f\u6210\u8bed\u8a00\u6a21\u578b\u3002\u6211\u4eec\u4f7f\u7528PubMed\u4e0a\u7684\u751f\u7269\u533b\u5b66\u7814\u7a76\u8bba\u6587\u6458\u8981\uff08\u7ea641G\uff09\u4f5c\u4e3a\u9884\u8bad\u7ec3\u8bed\u6599\u3002\u4f7f\u7528\u5f00\u6e90\u6846\u67b6DeepSpeed\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u57282\u4e2a\u5e26\u670916\u4e2a40GB A100 GPU\u7684DGX\u7ed3\u70b9\u4e0a\u5bf9BioBART-base\uff08139M\u53c2\u6570\uff09\u8fdb\u884c\u4e86\u7ea6100\u5c0f\u65f6\u7684\u8bad\u7ec3\u3002\n\nThe Yuyuan-Bart-139M is a biomedical generative language model jointly produced by Tsinghua University and International Digital Economy Academy (IDEA). We use biomedical research paper abstracts on PubMed (41G) as the pretraining corpora. We train the base version of BioBART(139M parameters) on 2 DGX with 16 40GB A100 GPUs for about 100 hours with the help of the open-resource framework DeepSpeed.\n\n## \u4f7f\u7528 Usage\n\n```python\nfrom transformers import BartForConditionalGeneration, BartTokenizer\ntokenizer = BartTokenizer.from_pretrained('IDEA-CCNL/Yuyuan-Bart-139M')\nmodel = BartForConditionalGeneration.from_pretrained('IDEA-CCNL/Yuyuan-Bart-139M')\n\ntext = 'Influenza is a <mask> disease.'\ninput_ids = tokenizer([text], return_tensors=\"pt\")['input_ids']\nmodel.eval()\ngenerated_ids = model.generate(\n    input_ids=input_ids,\n)\npreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\nprint(preds)\n```\n\n## \u5f15\u7528 Citation\n\n\u5982\u679c\u60a8\u5728\u60a8\u7684\u5de5\u4f5c\u4e2d\u4f7f\u7528\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684\u5bf9\u8be5\u6a21\u578b\u7684\u8bba\u6587\uff1a\n\nIf you are using the resource for your work, please cite the our paper for this model:\n\n```\n@misc{BioBART,\n  title={BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model},\n  author={Hongyi Yuan and Zheng Yuan and Ruyi Gan and Jiaxing Zhang and Yutao Xie and Sheng Yu},\n  year={2022},\n  eprint={2204.03905},\n  archivePrefix={arXiv}\n}\n```\n\n\u5982\u679c\u60a8\u5728\u60a8\u7684\u5de5\u4f5c\u4e2d\u4f7f\u7528\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684[\u603b\u8bba\u6587](https://arxiv.org/abs/2209.02970)\uff1a\n\nIf you are using the resource for your work, please cite the our [overview paper](https://arxiv.org/abs/2209.02970):\n\n```text\n@article{fengshenbang,\n  author    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},\n  title     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},\n  journal   = {CoRR},\n  volume    = {abs/2209.02970},\n  year      = {2022}\n}\n```\n\n\u4e5f\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684[\u7f51\u7ad9](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n\nYou can also cite our [website](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n\n```text\n@misc{Fengshenbang-LM,\n  title={Fengshenbang-LM},\n  author={IDEA-CCNL},\n  year={2021},\n  howpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},\n}\n```\n", "size_bytes": "278987747", "downloads": 20}
{"pretrained_model_name": "gaussalgo/mt5-base-generative-QA_en-cs", "description": "---\ntags:\n- generation\nlanguage:\n- multilingual\n- cs\n- en\n---\n\n# Mt5-base for Czech+English Generative Question Answering\n\nThis is the [mt5-base](https://huggingface.co/google/mt5-base) model with an LM head for a generation of extractive answers. In contrary to our [mt5-base-priming](https://huggingface.co/gaussalgo/mt5-base-priming-QA_en-cs/edit/main/README.md), this is a traditional sequence2sequence model without priming, though can also be used on other Text extraction tasks, such as Named Entity Recognition in zero-shot settings (with a significant decay in quality, compared to priming).\n\n## Intended uses & limitations\n\nThis model is purposed to *generate* a segment of a given context that contains an answer to a given question (Extractive Question Answering) in English and Czech.\nGiven the fine-tuning on two languages and a good reported zero-shot cross-lingual applicability of other fine-tuned multilingual large language models, the model will likely also work on other languages as well, with a specific decay in quality.\n\nNote that despite its size, English SQuAD has a variety of reported biases, \nconditioned by the relative position or type of the answer in the context that can affect the model's performance on new data \n(see, e.g. [L. Mikula (2022)](https://is.muni.cz/th/adh58/?lang=en), Chap. 4.1).\n\n## Usage\n\nHere is how to use this model to answer the question on a given context using \ud83e\udd17 Transformers in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"gaussalgo/mt5-base-generative-QA_en-cs\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"gaussalgo/mt5-base-generative-QA_en-cs\")\n\ncontext = \"\"\"\nPodle slovensk\u00e9ho lidov\u00e9ho pod\u00e1n\u00ed byl Juro J\u00e1no\u0161\u00edk obda\u0159en magick\u00fdmi p\u0159edm\u011bty (kouzeln\u00e1 vala\u0161ka, \u010darovn\u00fd opasek),\nkter\u00e9 mu dod\u00e1valy nadp\u0159irozen\u00e9 schopnosti. Okr\u00e1dal p\u0159edev\u0161\u00edm \u0161lechtice,\ntrestal pansk\u00e9 dr\u00e1by a ze sv\u00e9ho lupu vyd\u011bloval \u010d\u00e1st pro chud\u00e9, tedy bohat\u00fdm bral a chud\u00fdm d\u00e1val.\n\"\"\"\nquestion = \"Jak\u00e9 schopnosti daly magick\u00e9 p\u0159edm\u011bty Juro J\u00e1no\u0161\u00edkovi?\"\n\ninputs = tokenizer(question, context, return_tensors=\"pt\")\noutputs = model.generate(**inputs)\n\nprint(\"Answer:\")\nprint(tokenizer.decode(outputs))\n\n```\n\n## Training\n\nThe model has been trained using [Adaptor library](https://github.com/gaussalgo/adaptor) v0.1.5, in parallel on both Czech and English data, with the following parameters:\n\n```python\ntraining_arguments = AdaptationArguments(output_dir=\"train_dir\",\n                                         learning_rate=5e-5,\n                                         stopping_strategy=StoppingStrategy.ALL_OBJECTIVES_CONVERGED,\n                                         do_train=True,\n                                         do_eval=True,\n                                         warmup_steps=1000,\n                                         max_steps=100000,\n                                         gradient_accumulation_steps=4,\n                                         eval_steps=100,\n                                         logging_steps=10,\n                                         save_steps=1000,\n                                         num_train_epochs=50,\n                                         evaluation_strategy=\"steps\",\n                                         remove_unused_columns=False)\n\n```\n\nYou can find the full training script in [train_mt5_qa_en+cs.py](https://huggingface.co/gaussalgo/mt5-base-generative-QA_en-cs/blob/main/train_mt5_qa_en%2Bcs.py), reproducible after a specific data preprocessing for Czech SQAD in [parse_czech_squad.py](parse_czech_squad.py)", "size_bytes": "2329696205", "downloads": 24}
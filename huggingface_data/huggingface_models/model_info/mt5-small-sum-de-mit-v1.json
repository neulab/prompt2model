{"pretrained_model_name": "deutsche-telekom/mt5-small-sum-de-mit-v1", "description": "---\nlanguage:\n- de\n\nlicense: mit\n\ntags:\n- summarization\n\ndatasets:\n- swiss_text_2019\n\n---\n\n# mT5-small-sum-de-mit-v1\n\nThis is a German summarization model. It is based on the multilingual T5 model [google/mt5-small](https://huggingface.co/google/mt5-small). The special characteristic of this model is that, unlike many other models, it is licensed under a permissive open source license (MIT). Among other things, this license allows commercial use.\n\n[![One Conversation](https://raw.githubusercontent.com/telekom/HPOflow/main/docs/source/imgs/1c-logo.png)](https://www.welove.ai/)\nThis model is provided by the [One Conversation](https://www.welove.ai/)\nteam of [Deutsche Telekom AG](https://www.telekom.com/).\n\n## Training\n\nThe training was conducted with the following hyperparameters:\n\n- base model: [google/mt5-small](https://huggingface.co/google/mt5-small)\n- source_prefix: `\"summarize: \"`\n- batch size: 3 (6)\n- max_source_length: 800\n- max_target_length: 96\n- warmup_ratio: 0.3\n- number of train epochs: 10\n- gradient accumulation steps: 2\n- learning rate: 5e-5\n\n## Datasets and Preprocessing\n\nThe datasets were preprocessed as follows:\n\nThe summary was tokenized with the [google/mt5-small](https://huggingface.co/google/mt5-small) tokenizer. Then only the records with no more than 94 summary tokens were selected.\n\nThis model is trained on the following dataset:\n\n| Name | Language | Size | License\n|------|----------|------|--------\n| [SwissText 2019 - Train](https://www.swisstext.org/2019/shared-task/german-text-summarization-challenge.html) | de | 84,564 | Concrete license is unclear. The data was published in the [German Text Summarization Challenge](https://www.swisstext.org/2019/shared-task/german-text-summarization-challenge.html).\n\nWe have permission to use the Swisstext dataset and release the resulting summarization model under MIT license (see [permission-declaration-swisstext.pdf](https://huggingface.co/deutsche-telekom/mt5-small-sum-de-mit-v1/resolve/main/permission-declaration-swisstext.pdf)).\n\n## Evaluation on MLSUM German Test Set (no beams)\n\n| Model | rouge1 | rouge2 | rougeL | rougeLsum\n|-------|--------|--------|--------|----------\n| deutsche-telekom/mt5-small-sum-de-mit-v1 (this) | 16.8023 | 3.5531 | 12.6884 | 14.7624\n| [ml6team/mt5-small-german-finetune-mlsum](https://huggingface.co/ml6team/mt5-small-german-finetune-mlsum) | 18.3607 | 5.3604 | 14.5456 | 16.1946\n| **[deutsche-telekom/mt5-small-sum-de-en-01](https://huggingface.co/deutsche-telekom/mt5-small-sum-de-en-v1)** | **21.7336** | **7.2614** | **17.1323** | **19.3977**\n\n## License\n\nCopyright (c) 2021 Philip May, Deutsche Telekom AG\n\nLicensed under the MIT License (the \"License\"); you may not use this work except in compliance with the License. You may obtain a copy of the License by reviewing the file [LICENSE](https://huggingface.co/deutsche-telekom/mt5-small-sum-de-mit-v1/blob/main/LICENSE) in the repository.\n", "size_bytes": "1200721733", "downloads": 133}
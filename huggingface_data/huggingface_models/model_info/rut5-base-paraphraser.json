{"pretrained_model_name": "cointegrated/rut5-base-paraphraser", "description": "---\nlanguage: [\"ru\"]\ntags:\n- russian\n- paraphrasing\n- paraphraser\n- paraphrase\nlicense: mit\nwidget:\n- text: \"\u041a\u0430\u0436\u0434\u044b\u0439 \u043e\u0445\u043e\u0442\u043d\u0438\u043a \u0436\u0435\u043b\u0430\u0435\u0442 \u0437\u043d\u0430\u0442\u044c, \u0433\u0434\u0435 \u0441\u0438\u0434\u0438\u0442 \u0444\u0430\u0437\u0430\u043d.\"\ndatasets:\n- cointegrated/ru-paraphrase-NMT-Leipzig\n---\n\nThis is a paraphraser for Russian sentences described [in this Habr post](https://habr.com/ru/post/564916/). \n\nIt is recommended to use the model with the `encoder_no_repeat_ngram_size` argument:\n```\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nMODEL_NAME = 'cointegrated/rut5-base-paraphraser'\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\nmodel.cuda();\nmodel.eval();\n\ndef paraphrase(text, beams=5, grams=4, do_sample=False):\n    x = tokenizer(text, return_tensors='pt', padding=True).to(model.device)\n    max_size = int(x.input_ids.shape[1] * 1.5 + 10)\n    out = model.generate(**x, encoder_no_repeat_ngram_size=grams, num_beams=beams, max_length=max_size, do_sample=do_sample)\n    return tokenizer.decode(out[0], skip_special_tokens=True)\n\nprint(paraphrase('\u041a\u0430\u0436\u0434\u044b\u0439 \u043e\u0445\u043e\u0442\u043d\u0438\u043a \u0436\u0435\u043b\u0430\u0435\u0442 \u0437\u043d\u0430\u0442\u044c, \u0433\u0434\u0435 \u0441\u0438\u0434\u0438\u0442 \u0444\u0430\u0437\u0430\u043d.'))\n# \u0412\u0441\u0435 \u043e\u0445\u043e\u0442\u043d\u0438\u043a\u0438 \u0445\u043e\u0442\u044f\u0442 \u0437\u043d\u0430\u0442\u044c \u0433\u0434\u0435 \u0444\u0430\u0437\u0430\u043d \u0441\u0438\u0434\u0438\u0442.\n```", "size_bytes": "977359949", "downloads": 852}
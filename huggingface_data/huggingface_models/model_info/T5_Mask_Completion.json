{"pretrained_model_name": "Maciel/T5_Mask_Completion", "description": "---\nlanguage: \n- zh\ntags:\n- conditional text generation\n- data augmentation\n\nlicense: apache-2.0\ndatasets:\n- beyond/chinese_clean_passages_80m\n\n\nwidget:\n- text: \"[mask]\u75ab\u60c5[mask]\u516c\u56ed[mask]\u6563\u6b65[mask]\"\n  example_title: \"\u6848\u4f8b1\"\n- text: \"\u4eca\u5929[mask]\u7bee\u7403[mask]\u5b66\u6821[mask]\"\n  example_title: \"\u6848\u4f8b2\"\n- text: \"[mask]\u611f\u67d3\u65b0\u51a0[mask]\u8eab\u4f53\u4e0d\u8212\u670d[mask]\u591a\u4f11\u606f[mask]\"\n  example_title: \"\u6848\u4f8b3\"\n\ninference:\n  parameters:\n    max_length: 128\n    num_beams: 10\n    no_repeat_ngram_size: 5\n    do_sample: True\n    min_length: 10\n    early_stopping: True\n---\n\n## \u529f\u80fd\u4ecb\u7ecd\n\u8be5\u6a21\u578b\u4e3b\u8981\u529f\u80fd\u662f\u9488\u5bf9mask\u90e8\u5206\u8fdb\u884c\u8865\u5168\u751f\u6210\uff0c\u80fd\u591f\u751f\u6210\u8f83\u6d41\u5229\u4e30\u5bcc\u7684\u81ea\u7136\u6587\u672c\u3002\n\n\u53c2\u8003\u6848\u4f8b\u5982\u4e0b\uff1a\n\n1\uff09\u4eca\u5929[mask]\u7bee\u7403[mask]\u5b66\u6821[mask]\n\n2\uff09[mask]\u75ab\u60c5[mask]\u516c\u56ed[mask]\u6563\u6b65[mask]\n\n3\uff09[mask]\u611f\u67d3\u65b0\u51a0[mask]\u8eab\u4f53\u4e0d\u8212\u670d[mask]\u591a\u4f11\u606f[mask]\n\n## \u5982\u4f55\u4f7f\u7528\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\npretrained = \"Maciel/T5_Mask_Completion\"\ntokenizer = AutoTokenizer.from_pretrained(pretrained)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(pretrained)\n\nsentence = \"[mask]\u75ab\u60c5[mask]\u516c\u56ed[mask]\u6563\u6b65[mask]\"\nmax_input_length = 128\ninput_encodings = tokenizer(sentence, \n                            max_length=max_input_length, \n                            truncation=True, \n                            return_tensors=\"pt\")\nif \"token_type_ids\" in input_encodings.keys():\n    input_encodings.pop(\"token_type_ids\")\noutput = model.generate(**input_encodings, \n                        num_beams=10,\n                        no_repeat_ngram_size=5,\n                        do_sample=True, \n                        early_stopping=True,\n                        min_length=10,\n                        max_length=64,\n                        return_dict_in_generate=True,\n                        output_scores=True)\ndecoded_output = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)[0]\ncompletion = decoded_output.strip()\nprint(completion)\n```\n\n## \u6848\u4f8b\u5c55\u793a\n```\n1) \u539f\u59cb\u6587\u672c\uff1a\u4eca\u5929[mask]\u7bee\u7403[mask]\u5b66\u6821[mask]\n   \u8865\u5168\u6587\u672c\uff1a\u4eca\u5929,\u6211\u4eec\u6765\u8c08\u8c08\u7bee\u7403\u4e0e\u5b66\u6821\u7684\u5173\u7cfb\u3002\n\n2) \u539f\u59cb\u6587\u672c\uff1a[mask]\u75ab\u60c5[mask]\u516c\u56ed[mask]\u6563\u6b65[mask]\n   \u8865\u5168\u6587\u672c\uff1a\u5728\u75ab\u60c5\u53d1\u751f\u4e4b\u524d,\u4eba\u4eec\u53ef\u4ee5\u5728\u516c\u56ed\u91cc\u6563\u6b65\u3002\n\n3) \u539f\u59cb\u6587\u672c\uff1a[mask]\u611f\u67d3\u65b0\u51a0[mask]\u8eab\u4f53\u4e0d\u8212\u670d[mask]\u591a\u4f11\u606f[mask]\n   \u8865\u5168\u6587\u672c\uff1a\u5982\u679c\u4f60\u611f\u67d3\u65b0\u51a0\u4e86,\u8eab\u4f53\u4e0d\u8212\u670d,\u5efa\u8bae\u4f60\u591a\u4f11\u606f,\u4e0d\u8981\u5403\u8f9b\u8fa3\u523a\u6fc0\u6027\u7684\u98df\u7269,\u4ee5\u514d\u52a0\u91cd\u75c5\u60c5\u3002\n```", "size_bytes": "990448659", "downloads": 10}
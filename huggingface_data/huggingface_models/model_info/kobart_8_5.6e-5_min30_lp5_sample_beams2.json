{"pretrained_model_name": "nlp04/kobart_8_5.6e-5_min30_lp5_sample_beams2", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: kobart_8_5.6e-5_min30_lp5_sample_beams2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# kobart_8_5.6e-5_min30_lp5_sample_beams2\n\nThis model is a fine-tuned version of [gogamza/kobart-base-v2](https://huggingface.co/gogamza/kobart-base-v2) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.8283\n- Rouge1: 35.819\n- Rouge2: 12.1658\n- Rougel: 23.3058\n- Bleu1: 29.6395\n- Bleu2: 16.8254\n- Bleu3: 9.5014\n- Bleu4: 5.168\n- Gen Len: 49.8625\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5.6e-05\n- train_batch_size: 8\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 5.0\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Bleu1   | Bleu2   | Bleu3  | Bleu4  | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:-------:|:-------:|:------:|:------:|:-------:|\n| 2.527         | 0.19  | 1000  | 3.0014          | 30.9895 | 9.5631  | 20.1782 | 25.4533 | 13.5291 | 7.1157 | 3.4483 | 50.2657 |\n| 2.4214        | 0.38  | 2000  | 2.8814          | 32.3984 | 10.3443 | 21.2357 | 26.5661 | 14.5006 | 7.3531 | 3.5159 | 44.1538 |\n| 2.3577        | 0.57  | 3000  | 2.8277          | 32.2306 | 10.5703 | 21.3959 | 26.4952 | 14.725  | 8.0596 | 4.2696 | 50.965  |\n| 2.2606        | 0.76  | 4000  | 2.7749          | 33.0892 | 11.0109 | 21.4034 | 27.045  | 15.0797 | 8.2405 | 4.2337 | 48.1026 |\n| 2.1508        | 0.94  | 5000  | 2.6841          | 33.1368 | 10.9332 | 21.9277 | 27.4808 | 15.2182 | 8.39   | 4.2468 | 46.0583 |\n| 1.9467        | 1.13  | 6000  | 2.6994          | 33.2536 | 10.9192 | 21.851  | 26.7639 | 14.7669 | 8.1932 | 4.4866 | 42.7436 |\n| 1.9267        | 1.32  | 7000  | 2.6743          | 35.335  | 12.5749 | 23.0923 | 29.4977 | 17.1053 | 9.9798 | 5.6851 | 54.2168 |\n| 1.9402        | 1.51  | 8000  | 2.6549          | 34.7169 | 12.4365 | 22.8695 | 28.8948 | 16.8377 | 9.795  | 5.8984 | 53.8042 |\n| 1.9457        | 1.7   | 9000  | 2.6198          | 34.1256 | 11.3508 | 22.7591 | 28.0771 | 15.6516 | 8.6198 | 4.5566 | 43.8252 |\n| 1.9206        | 1.89  | 10000 | 2.6090          | 34.5521 | 12.0321 | 22.8654 | 28.268  | 16.2876 | 9.2697 | 4.9105 | 45.8205 |\n| 1.6341        | 2.08  | 11000 | 2.6831          | 35.2143 | 12.748  | 23.2014 | 29.3413 | 17.2312 | 9.9515 | 5.5303 | 51.5338 |\n| 1.6098        | 2.27  | 12000 | 2.6529          | 35.251  | 12.1877 | 23.3663 | 29.0609 | 16.6432 | 9.5808 | 5.2786 | 46.2378 |\n| 1.6094        | 2.45  | 13000 | 2.6441          | 34.8683 | 12.0873 | 22.9699 | 28.9225 | 16.492  | 9.3451 | 5.1097 | 45.6131 |\n| 1.6684        | 2.64  | 14000 | 2.6504          | 35.1897 | 12.0262 | 23.0832 | 28.948  | 16.4709 | 9.1994 | 5.0042 | 46.5245 |\n| 1.6376        | 2.83  | 15000 | 2.6514          | 35.795  | 12.4779 | 23.2187 | 30.05   | 17.2789 | 9.984  | 5.4966 | 50.1119 |\n| 1.3663        | 3.02  | 16000 | 2.7310          | 35.6544 | 12.109  | 23.3876 | 29.9268 | 16.945  | 9.4372 | 5.095  | 49.6317 |\n| 1.3719        | 3.21  | 17000 | 2.7514          | 35.0663 | 11.8565 | 23.4224 | 28.8679 | 16.2846 | 9.3246 | 5.0154 | 45.3333 |\n| 1.394         | 3.4   | 18000 | 2.7644          | 35.5883 | 12.2587 | 23.188  | 29.8503 | 17.0253 | 9.705  | 5.3253 | 47.4289 |\n| 1.3615        | 3.59  | 19000 | 2.7535          | 35.3947 | 12.3879 | 23.355  | 29.4012 | 16.8473 | 9.6862 | 5.3268 | 48.7179 |\n| 1.3544        | 3.78  | 20000 | 2.7480          | 35.7263 | 12.4434 | 23.6667 | 29.7146 | 17.0029 | 9.6018 | 5.2752 | 46.8834 |\n| 1.3697        | 3.97  | 21000 | 2.7415          | 35.4189 | 12.1527 | 23.0022 | 29.6187 | 16.8477 | 9.5092 | 5.3766 | 50.3963 |\n| 1.1718        | 4.15  | 22000 | 2.8251          | 35.0831 | 12.0809 | 22.8805 | 29.2252 | 16.5645 | 9.3818 | 5.241  | 46.7156 |\n| 1.1955        | 4.34  | 23000 | 2.8158          | 35.7853 | 12.3885 | 23.821  | 29.7377 | 16.9635 | 9.7005 | 5.4376 | 47.5991 |\n| 1.1795        | 4.53  | 24000 | 2.8265          | 35.4293 | 12.145  | 23.2029 | 29.6457 | 16.8228 | 9.7128 | 5.2525 | 49.5431 |\n| 1.1835        | 4.72  | 25000 | 2.8254          | 35.499  | 11.9198 | 23.0859 | 29.4398 | 16.5715 | 9.2442 | 4.7663 | 47.8345 |\n| 1.1644        | 4.91  | 26000 | 2.8283          | 35.819  | 12.1658 | 23.3058 | 29.6395 | 16.8254 | 9.5014 | 5.168  | 49.8625 |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu117\n- Datasets 2.7.1\n- Tokenizers 0.13.2\n", "size_bytes": "495648413", "downloads": 2}
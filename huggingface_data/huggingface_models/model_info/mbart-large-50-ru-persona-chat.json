{"pretrained_model_name": "DeepPavlov/mbart-large-50-ru-persona-chat", "description": "---\nlicense: openrail\nlanguage:\n- ru\npipeline_tag: text-generation\n---\n---\nlanguage:\n- ru\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n# Model Details\n\n## Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n- **Developed by:** DeepPavlov team\n- **Model type:** text generation\n- **Language(s) (NLP):** Russian\n- **License:** Openrail\n- **Finetuned from model:** [facebook/mbart-large-50](facebook/mbart-large-50)\n\n\n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n```python\nfrom typing import List, TypedDict\nfrom dataclasses import dataclass\nfrom itertools import chain\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n\n@dataclass\nclass H2PersonaChatHyperparametersV1:\n    \"\"\"\n    chat_history_pair_length: int - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0440 \u0434\u0438\u0430\u043b\u043e\u0433\u0430 \u0441 \u043a\u043e\u043d\u0446\u0430\n    \"\"\"\n\n    model_name: str = \"facebook/bart-base\"\n    chat_history_pair_length: int = 7\n\n    persona_max_length: int = 14\n    chat_max_length: int = 25\n\n    debug_status: int = 0\n\n\nclass PersonaChatDatasetSampleV1(TypedDict):\n    \"\"\"\n    persona: List[str] - \u043d\u0430\u0431\u043e\u0440 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0444\u0430\u043a\u0442\u043e\u0432 \u043f\u0435\u0440\u0441\u043e\u043d\u044b\n    history: List[str] - \u043d\u0430\u0431\u043e\u0440 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u043f\u0435\u0440\u0435\u043f\u0438\u0441\u043a\u0438\n    \"\"\"\n\n    persona: List[str]\n    history: List[str]\n    sample_id: str\n\n\nclass H2Seq2SeqInferenceSampleDictV1(TypedDict):\n    input_ids: List[int]\n    attention_mask: List[int]\n\n\nclass H2Seq2SeqInferenceSampleDictV2(TypedDict):\n    input_ids: torch.Tensor\n    attention_mask: torch.Tensor\n\n\ndef flat_list(list_of_lists: List[List]) -> List:\n    return list(chain.from_iterable(list_of_lists))\n\n\nclass H2Seq2SeqInferencePersonaSampleV1:\n    def __init__(\n        self,\n        dataset_sample: PersonaChatDatasetSampleV1,\n        tokenizer: AutoTokenizer,\n        hyperparameters: H2PersonaChatHyperparametersV1,\n    ) -> None:\n        self.dataset_sample = dataset_sample\n        self.tokenizer = tokenizer\n        self.hyperparameters = hyperparameters\n\n    def add_spaces_after(\n        self,\n        items: List[str],\n    ) -> List[str]:\n        items = [item + \" \" for item in items]\n        return items\n\n    @property\n    def bos_token_id(self):\n        if \"t5\" in self.hyperparameters.model_name:\n            return []\n\n        if self.tokenizer.bos_token_id is None:\n            return []\n\n        return [self.tokenizer.bos_token_id]\n\n    @property\n    def eos_token_id(self):\n        if self.tokenizer.eos_token_id is None:\n            return []\n\n        return [self.tokenizer.eos_token_id]\n\n    def add_sep_beetween(self, items: List[str], sep=\" EOS \") -> List[str]:\n        for i in range(1, len(items)):\n            items[i] = sep + items[i]\n\n        return items\n\n    def add_spaces_between(self, items: List[str]) -> List[str]:\n        items = self.add_spaces_after(items)\n        items[-1] = items[-1].strip()\n        return items\n\n    def get_sample(self) -> H2Seq2SeqInferenceSampleDictV1:\n\n        dialog_history = self.dataset_sample[\"history\"]\n        dialog_history = dialog_history[-self.hyperparameters.chat_history_pair_length * 2 - 1 :]\n        dialog_history = self.add_sep_beetween(dialog_history)\n\n        persona = self.dataset_sample[\"persona\"]\n        persona = self.add_sep_beetween(\n            persona,\n            sep=\" \",\n        )\n\n        KNOWLEDGE_IDS = self.tokenizer.encode(\n            \" [KNOWLEDGE] \",\n            add_special_tokens=False,\n        )\n        CONTEXT_IDS = self.tokenizer.encode(\n            \" [CONTEXT]\",\n            add_special_tokens=False,\n        )\n\n        encoded_history = self.tokenizer.batch_encode_plus(\n            dialog_history,\n            add_special_tokens=False,\n            truncation=True,\n            max_length=self.hyperparameters.chat_max_length,\n        )\n        encoded_history = flat_list(encoded_history[\"input_ids\"])\n\n        encoded_persona = self.tokenizer.batch_encode_plus(\n            persona,\n            add_special_tokens=False,\n            truncation=True,\n            max_length=self.hyperparameters.persona_max_length,\n        )\n\n        encoded_persona = flat_list(encoded_persona[\"input_ids\"])\n\n        input_ids = [\n            *self.bos_token_id,\n            *CONTEXT_IDS,\n            *encoded_history,\n            *KNOWLEDGE_IDS,\n            *encoded_persona,\n            *self.eos_token_id,\n        ]\n\n        attention_mask = [1] * len(input_ids)\n\n        return H2Seq2SeqInferenceSampleDictV1(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n        )\n\n\nclass DialogBotV1:\n    def __init__(\n        self,\n        model: AutoModelForSeq2SeqLM,\n        tokenizer: AutoTokenizer,\n        hyperparameters: H2PersonaChatHyperparametersV1,\n        history: List[str] = None,\n        persona: List[str] = None,\n        device: str = \"cuda\",\n        shuffle_persona: bool = True,\n    ):\n        self.model = model\n\n        self.tokenizer = tokenizer\n        self.hyperparameters = hyperparameters\n        self.device = device\n        self.shuffle_persona = shuffle_persona\n\n        self.debug_status = hyperparameters.debug_status\n\n        if history is None:\n            self.history = []\n        self.history = history\n\n        if persona is None:\n            self.persona = []\n        self.persona = persona\n\n    def _get_sample(\n        self,\n        persona: List[str],\n        history: List[str],\n    ) -> H2Seq2SeqInferenceSampleDictV1:\n        dataset_sample = PersonaChatDatasetSampleV1(\n            persona=persona,\n            history=history,\n        )\n\n        sample = H2Seq2SeqInferencePersonaSampleV1(\n            tokenizer=self.tokenizer,\n            hyperparameters=self.hyperparameters,\n            dataset_sample=dataset_sample,\n        )\n        sample = sample.get_sample()\n        print(self.tokenizer.decode(sample['input_ids']))\n\n        for key in sample.keys():\n            sample[key] = torch.tensor(sample[key]).unsqueeze(0).to(self.device)\n\n        return sample\n\n    def next_response(\n        self,\n        **generation_params,\n    ) -> str:\n        \"\"\"\n        \u0434\u0435\u043b\u0430\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0438\n        \u0438 \u043f\u0435\u0440\u0441\u043e\u043d\u044b\n        \"\"\"\n\n        sample = self._get_sample(\n            persona=self.persona,\n            history=self.history,\n        )\n        answer = self.generate_response(\n            sample,\n            **generation_params,\n        )\n        answer = self.tokenizer.batch_decode(\n            answer,\n            skip_special_tokens=True,\n        )\n        self.history.append(answer[0])\n        return answer[0]\n\n    def generate_response(\n        self,\n        sample: H2Seq2SeqInferenceSampleDictV1,\n        **generation_params,\n    ):\n        \"\"\"\n        generation_params - https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/text_generation\n        \"\"\"\n        with torch.no_grad():\n            return self.model.generate(\n                **sample,\n                **generation_params,\n            )\n\n\n# facebook/mbart-large-50\nPRETRAINED_MODEL_NAME_OR_PATH = \"DeepPavlov/mbart-large-50-ru-persona-chat\"\n\nPAIR_DIALOG_HISTORY_LENGTH = 2\n\n# CHAT_MAX_LENGTH for single sentence\nCHAT_MAX_LENGTH = 25\n# PERSONA_MAX_LENGTH for single sentence\nPERSONA_MAX_LENGTH = 19\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\nmodel.to(device)\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n\nif torch.cuda.is_available():\n\tmodel.half()\n\nhyperparameters = H2PersonaChatHyperparametersV1(\n\tchat_history_pair_length=PAIR_DIALOG_HISTORY_LENGTH,\n\tpersona_max_length=PERSONA_MAX_LENGTH,\n\tchat_max_length=CHAT_MAX_LENGTH,\n\tmodel_name=PRETRAINED_MODEL_NAME_OR_PATH,\n)\n\n\npersona = [\n \t\"\u042f \u043b\u044e\u0431\u043b\u044e \u0438\u0433\u0440\u0430\u0442\u044c \u0441 \u043c\u0438\u043b\u044b\u043c\u0438 \u043f\u0435\u0441\u0438\u043a\u0430\u043c\u0438\",\n\t\"\u042f \u043d\u0435\u043d\u0430\u0432\u0438\u0436\u0443 \u043b\u0443\u043a \u0438 \u0431\u0440\u043e\u043a\u043e\u043b\u043b\u0438\"\n]\n\nhistory = [\n\t\"\u041f\u0440\u0438\u0432\u0435\u0442. \u0422\u044b \u043b\u044e\u0431\u0438\u0448\u044c \u043b\u0443\u043a?\"\n]\n            \npersona_bot = DialogBotV1(\n        model=model,\n        tokenizer=tokenizer,\n        hyperparameters=hyperparameters,\n        history=history,\n        persona=persona,\n        device=device,\n    )\n\nGENERATION_PARAMS = {\n\t\"max_new_tokens\": 60,\n\t\"penalty_alpha\": 0.15,\n\t\"top_k\": 10\n}\nresponse = persona_bot.next_response(\n\t**GENERATION_PARAMS,\n)\n\nprint(response)\n\n```\n\n\n## Recommendations\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n- [Data Source | RU Persona Chat](https://toloka.ai/ru/datasets/#nlp)\n\n[More Information Needed]\n\n### Preprocessing\n\n- Initial data was splitted by this script:\n```python\ndef ru_persona_chat_dataset_tranformer_v1(\n    initial_dataset_path: str,\n    output_folder: str,\n) -> None:\n    \"\"\"\n        example\n            ru_persona_chat_dataset_tranformer_v1(\n            initial_dataset_path=\"./datasets/ru_persona_chat/dialogues.tsv\",\n            output_folder=\"./datasets/ru_persona_chat\",\n    )\n    \"\"\"\n    assert initial_dataset_path is not None, \"initial_dataset_path is None\"\n    assert output_folder is not None, \"output_folder is None\"\n\n    dataset = pd.read_csv(initial_dataset_path, sep=\"\\t\")\n    split_ratio = int(len(dataset) * 0.95)\n    train_dataset = dataset[:split_ratio]\n    valid_dataset = dataset[split_ratio:]\n\n    print(f\"Dataset lengths: train {len(train_dataset)}, valid {len(valid_dataset)}\")\n    # save csv files\n    train_dataset.to_csv(output_folder + \"/train.csv\", index=False)\n    valid_dataset.to_csv(output_folder + \"/valid.csv\", index=False)\n    print(\"Datasets saved.\")\n```  \n\n# Evaluation\n\n### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n- BLUEL\n- CharF\n- RougeL", "size_bytes": "1222434973", "downloads": 95}
{"pretrained_model_name": "Zekunli/flan-t5-base-extraction-cnndm_20000-all-hint_precision-ep50-nonstop", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: flan-t5-base-extraction-cnndm_20000-all-hint_precision-ep50-nonstop\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-base-extraction-cnndm_20000-all-hint_precision-ep50-nonstop\n\nThis model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.6910\n- Hint Hit Num: 2.3981\n- Hint Precision: 0.431\n- Num: 5.5422\n- Gen Len: 18.9991\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 200\n- seed: 1799\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 50\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Hint Hit Num | Hint Precision | Num    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:------------:|:--------------:|:------:|:-------:|\n| 2.0952        | 0.8   | 1000  | 1.7450          | 2.2694       | 0.4207         | 5.4015 | 18.9993 |\n| 1.9364        | 1.6   | 2000  | 1.7131          | 2.2371       | 0.4159         | 5.3613 | 19.0    |\n| 1.8853        | 2.4   | 3000  | 1.7026          | 2.2893       | 0.4213         | 5.4161 | 18.9999 |\n| 1.8383        | 3.2   | 4000  | 1.6955          | 2.2796       | 0.4206         | 5.404  | 18.9999 |\n| 1.8087        | 4.0   | 5000  | 1.6866          | 2.3175       | 0.4244         | 5.4464 | 18.9996 |\n| 1.7778        | 4.8   | 6000  | 1.6829          | 2.3311       | 0.423          | 5.4935 | 18.9996 |\n| 1.7412        | 5.6   | 7000  | 1.6863          | 2.3112       | 0.4205         | 5.4712 | 18.9998 |\n| 1.732         | 6.4   | 8000  | 1.6903          | 2.3108       | 0.4209         | 5.4711 | 18.999  |\n| 1.6993        | 7.2   | 9000  | 1.6840          | 2.3855       | 0.4289         | 5.5382 | 18.9998 |\n| 1.688         | 8.0   | 10000 | 1.6870          | 2.3089       | 0.4191         | 5.4856 | 18.9995 |\n| 1.6609        | 8.8   | 11000 | 1.6910          | 2.3981       | 0.431          | 5.5422 | 18.9991 |\n| 1.6462        | 9.6   | 12000 | 1.7011          | 2.3492       | 0.4237         | 5.5193 | 18.9994 |\n| 1.6302        | 10.4  | 13000 | 1.7008          | 2.3825       | 0.4279         | 5.5488 | 18.999  |\n| 1.6108        | 11.2  | 14000 | 1.7058          | 2.3274       | 0.4217         | 5.496  | 18.9998 |\n| 1.6031        | 12.0  | 15000 | 1.7092          | 2.3741       | 0.4265         | 5.5432 | 18.999  |\n| 1.5798        | 12.8  | 16000 | 1.7112          | 2.3416       | 0.4217         | 5.5248 | 18.9981 |\n| 1.5664        | 13.6  | 17000 | 1.7210          | 2.4102       | 0.4291         | 5.6002 | 18.9986 |\n| 1.5521        | 14.4  | 18000 | 1.7193          | 2.3779       | 0.4236         | 5.5859 | 18.9992 |\n| 1.5426        | 15.2  | 19000 | 1.7323          | 2.3727       | 0.4227         | 5.5887 | 18.9992 |\n| 1.5318        | 16.0  | 20000 | 1.7214          | 2.3992       | 0.4274         | 5.593  | 18.999  |\n| 1.5134        | 16.8  | 21000 | 1.7300          | 2.4111       | 0.4272         | 5.6176 | 18.9987 |\n| 1.5031        | 17.6  | 22000 | 1.7363          | 2.3823       | 0.425          | 5.5836 | 18.999  |\n| 1.4845        | 18.4  | 23000 | 1.7429          | 2.4123       | 0.428          | 5.6077 | 18.9987 |\n| 1.4895        | 19.2  | 24000 | 1.7534          | 2.3726       | 0.4207         | 5.6104 | 18.9984 |\n| 1.4687        | 20.0  | 25000 | 1.7552          | 2.4185       | 0.4268         | 5.6374 | 18.9987 |\n| 1.4601        | 20.8  | 26000 | 1.7602          | 2.3924       | 0.4251         | 5.6007 | 18.9989 |\n| 1.4486        | 21.6  | 27000 | 1.7687          | 2.3863       | 0.4234         | 5.6088 | 18.9984 |\n| 1.4353        | 22.4  | 28000 | 1.7770          | 2.3885       | 0.4234         | 5.611  | 18.9982 |\n| 1.4317        | 23.2  | 29000 | 1.7788          | 2.4084       | 0.4243         | 5.6463 | 18.9987 |\n| 1.4269        | 24.0  | 30000 | 1.7786          | 2.428        | 0.4274         | 5.6495 | 18.9985 |\n| 1.4135        | 24.8  | 31000 | 1.7883          | 2.3993       | 0.4244         | 5.6265 | 18.9981 |\n| 1.4025        | 25.6  | 32000 | 1.7911          | 2.3972       | 0.4225         | 5.6432 | 18.9977 |\n| 1.3874        | 26.4  | 33000 | 1.7930          | 2.3838       | 0.4207         | 5.6284 | 18.9989 |\n| 1.4023        | 27.2  | 34000 | 1.7988          | 2.436        | 0.4277         | 5.6637 | 18.9981 |\n| 1.3796        | 28.0  | 35000 | 1.8079          | 2.4162       | 0.4256         | 5.6432 | 18.9981 |\n| 1.3729        | 28.8  | 36000 | 1.8124          | 2.3894       | 0.4225         | 5.6167 | 18.9975 |\n| 1.3686        | 29.6  | 37000 | 1.8153          | 2.4301       | 0.4271         | 5.6606 | 18.9978 |\n| 1.3603        | 30.4  | 38000 | 1.8174          | 2.4248       | 0.4253         | 5.6696 | 18.9973 |\n| 1.3551        | 31.2  | 39000 | 1.8224          | 2.42         | 0.4243         | 5.67   | 18.9976 |\n| 1.3504        | 32.0  | 40000 | 1.8246          | 2.4189       | 0.4254         | 5.6551 | 18.9977 |\n| 1.3447        | 32.8  | 41000 | 1.8222          | 2.4234       | 0.425          | 5.6685 | 18.9969 |\n| 1.3354        | 33.6  | 42000 | 1.8380          | 2.3975       | 0.422          | 5.6471 | 18.997  |\n| 1.3304        | 34.4  | 43000 | 1.8416          | 2.4161       | 0.4247         | 5.6584 | 18.9975 |\n| 1.3274        | 35.2  | 44000 | 1.8386          | 2.4271       | 0.4249         | 5.6804 | 18.9972 |\n| 1.3238        | 36.0  | 45000 | 1.8361          | 2.4164       | 0.4241         | 5.6643 | 18.9972 |\n| 1.3167        | 36.8  | 46000 | 1.8418          | 2.4359       | 0.426          | 5.6864 | 18.9973 |\n| 1.3115        | 37.6  | 47000 | 1.8499          | 2.4068       | 0.4222         | 5.6682 | 18.9972 |\n| 1.31          | 38.4  | 48000 | 1.8508          | 2.433        | 0.4256         | 5.6872 | 18.997  |\n| 1.3085        | 39.2  | 49000 | 1.8477          | 2.4184       | 0.423          | 5.682  | 18.9967 |\n| 1.3009        | 40.0  | 50000 | 1.8485          | 2.4182       | 0.4236         | 5.6753 | 18.9972 |\n| 1.3028        | 40.8  | 51000 | 1.8547          | 2.4074       | 0.4222         | 5.6657 | 18.9977 |\n| 1.2919        | 41.6  | 52000 | 1.8552          | 2.4199       | 0.4233         | 5.6825 | 18.9965 |\n| 1.2945        | 42.4  | 53000 | 1.8652          | 2.4227       | 0.4238         | 5.6853 | 18.9975 |\n| 1.2931        | 43.2  | 54000 | 1.8605          | 2.427        | 0.424          | 5.6907 | 18.9971 |\n| 1.2838        | 44.0  | 55000 | 1.8647          | 2.4244       | 0.4244         | 5.6774 | 18.9972 |\n| 1.2878        | 44.8  | 56000 | 1.8629          | 2.4209       | 0.4234         | 5.6848 | 18.997  |\n| 1.2848        | 45.6  | 57000 | 1.8674          | 2.4291       | 0.4242         | 5.6931 | 18.9966 |\n| 1.279         | 46.4  | 58000 | 1.8649          | 2.4253       | 0.4238         | 5.6898 | 18.9966 |\n| 1.2862        | 47.2  | 59000 | 1.8643          | 2.4187       | 0.4228         | 5.686  | 18.9966 |\n| 1.2798        | 48.0  | 60000 | 1.8643          | 2.4195       | 0.4231         | 5.6832 | 18.997  |\n| 1.279         | 48.8  | 61000 | 1.8670          | 2.424        | 0.4239         | 5.6845 | 18.9966 |\n| 1.2754        | 49.6  | 62000 | 1.8686          | 2.4234       | 0.4238         | 5.6842 | 18.9965 |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.10.0+cu111\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "990406605", "downloads": 2}
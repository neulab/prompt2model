{"pretrained_model_name": "figuringoutmine/translator-for-travel-jp-to-kr", "description": "---\npipeline_tag: translation\nlanguage:\n- ja\n- ko\ntags:\n- python\n- transformer\n- pytorch\n---\nhttps://github.com/akpe12/JP-KR-ocr-translator-for-travel\n\n- Usage\n```\nfrom transformers import(\n    EncoderDecoderModel,\n    PreTrainedTokenizerFast,\n    # XLMRobertaTokenizerFast,\n    BertTokenizerFast,\n)\n\nencoder_model_name = \"cl-tohoku/bert-base-japanese-v2\"\ndecoder_model_name = \"skt/kogpt2-base-v2\"\n\nsrc_tokenizer = BertTokenizerFast.from_pretrained(encoder_model_name)\ntrg_tokenizer = PreTrainedTokenizerFast.from_pretrained(decoder_model_name)\nmodel = EncoderDecoderModel.from_pretrained(\"figuringoutmine/translator-for-travel-jp-to-kr\")\n```\n\n```\ntext = \"\u8c5a\u9aa8\u30e9\u30fc\u30e1\u30f3\"\nembeddings = src_tokenizer(text, return_attention_mask=False, return_token_type_ids=False, return_tensors='pt')\nembeddings = {k: v for k, v in embeddings.items()}\noutput = model.generate(**embeddings)[0, 1:-1]\n\ntrg_tokenizer.decode(output.cpu())\n```\n\n- Quantitative evaluation using data related traveling in Japan\n<br>\nwith BLEU score(1-gram)\n<br>\nPapago: 51.9\n<br>\nGoogle: 32.8\n<br>\n<strong>figuringoutmine/translator-for-travel-jp-to-kr: 52.7<strong/>\n", "size_bytes": "1084293181", "downloads": 32}
{"pretrained_model_name": "Kirili4ik/mbart_ruDialogSum", "description": "---\nlanguage:\n- ru\ntags:\n- mbart\ninference:\n  parameters:\n    no_repeat_ngram_size: 4,\n    num_beams: 5\ndatasets:\n- IlyaGusev/gazeta\n- samsum\n- samsum_(translated_into_Russian)\nwidget:\n- text: >\n    \u0414\u0436\u0435\u0444\u0444: \u041c\u043e\u0433\u0443 \u043b\u0438 \u044f \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \ud83e\udd17 Transformers \u043d\u0430 Amazon SageMaker? \n\n    \u0424\u0438\u043b\u0438\u043f\u043f: \u041a\u043e\u043d\u0435\u0447\u043d\u043e, \u0432\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0439 \u043a\u043e\u043d\u0442\u0435\u0439\u043d\u0435\u0440 \u0434\u043b\u044f \u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0433\u043e\n    \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f HuggingFace. \n\n    \u0414\u0436\u0435\u0444\u0444: \u0425\u043e\u0440\u043e\u0448\u043e.\n\n    \u0414\u0436\u0435\u0444\u0444: \u0438 \u043a\u0430\u043a \u044f \u043c\u043e\u0433\u0443 \u043d\u0430\u0447\u0430\u0442\u044c? \n\n    \u0414\u0436\u0435\u0444\u0444: \u0433\u0434\u0435 \u044f \u043c\u043e\u0433\u0443 \u043d\u0430\u0439\u0442\u0438 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e? \n\n    \u0424\u0438\u043b\u0438\u043f\u043f: \u043e\u043a, \u043e\u043a, \u0437\u0434\u0435\u0441\u044c \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0432\u0441\u0435:\n    https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face\nmodel-index:\n- name: mbart_ruDialogSum\n  results:\n  - task:\n      name: Abstractive Dialogue Summarization\n      type: abstractive-text-summarization\n    dataset:\n      name: SAMSum Corpus (translated to Russian)\n      type: samsum\n    metrics:\n    - name: Validation ROGUE-1\n      type: rogue-1\n      value: 34.5\n    - name: Validation ROGUE-L\n      type: rogue-l\n      value: 33\n    - name: Test ROGUE-1\n      type: rogue-1\n      value: 31\n    - name: Test ROGUE-L\n      type: rogue-l\n      value: 28\nlicense: cc\n---\n### \ud83d\udcdd Description\n\nMBart for Russian summarization fine-tuned for **dialogues** summarization.\n\n\nThis model was firstly fine-tuned by [Ilya Gusev](https://hf.co/IlyaGusev) on [Gazeta dataset](https://huggingface.co/datasets/IlyaGusev/gazeta). We have **fine tuned** that model on [SamSum dataset](https://huggingface.co/datasets/samsum) **translated to Russian** using GoogleTranslateAPI\n\n\ud83e\udd17 Moreover! We have implemented a **! telegram bot [@summarization_bot](https://t.me/summarization_bot) !** with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages! \u00a0\ud83e\udd17\n\n\n### \u2753 How to use with code\n```python\nfrom transformers import MBartTokenizer, MBartForConditionalGeneration\n\n# Download model and tokenizer\nmodel_name = \"Kirili4ik/mbart_ruDialogSum\"   \ntokenizer =  AutoTokenizer.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\nmodel.eval()\n\narticle_text = \"...\"\n\ninput_ids = tokenizer(\n    [article_text],\n    max_length=600,\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\",\n)[\"input_ids\"]\n\noutput_ids = model.generate(\n    input_ids=input_ids,\n    top_k=0,\n    num_beams=3,\n    no_repeat_ngram_size=3\n)[0]\n\n\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(summary)\n```", "size_bytes": "3468710071", "downloads": 43694}
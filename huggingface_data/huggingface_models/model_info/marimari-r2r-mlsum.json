{"pretrained_model_name": "IIC/marimari-r2r-mlsum", "description": "---\nlanguage:\n- es\ntags:\n- summarization  # Example: audio\n- seq2seq  # Example: automatic-speech-recognition\ndatasets:\n- mlsum\nmetrics:\n- rouge2\n- rouge1\n- rougel\n- rougelsum\n\n# Optional. Add this if you want to encode your eval results in a structured way.\nmodel-index:\n- name: marimari-r2r-mlsum\n  results:\n  - task: \n      type: summarization  # Required. Example: automatic-speech-recognition\n      name: abstractive summarization  # Optional. Example: Speech Recognition\n    dataset:\n      type: mlsum  # Required. Example: common_voice. Use dataset id from https://hf.co/datasets\n      name: mlsum-es  # Required. Example: Common Voice zh-CN\n      args: es         # Optional. Example: zh-CN\n    metrics:\n      - type: rouge1    # Required. Example: wer\n        value: 28.7802  # Required. Example: 20.90\n        name: rouge1    # Optional. Example: Test WER\n      - type: rouge2\n        value: 10.6748\n        name: rouge2\n      - type: rougeL\n        value: 23.0447\n        name: rougeL\n      - type: rougeLsum\n        value: 23.4055\n        name: rougeLsum\n---\n\n<img src=\"https://huggingface.co/IIC/marimari-r2r-mlsum/resolve/main/marimariLogo.png\"/>\n\nThis is a model for text summarization in Spanish. It has been trained on the spanish portion of [mlsum](https://huggingface.co/datasets/mlsum). For that, MariMari was created. It is called like that because it is an EncoderDecoder model built from Maria model, specifically, the [roberta model from the Maria Project](https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne). For building the Encoder Decoder model, [this paper was followed](https://arxiv.org/abs/1907.12461), which has a [direct implementation in transformers](https://huggingface.co/docs/transformers/master/model_doc/encoder-decoder). As there are no natural encoder decoder models in Spanish, such as BART or T5, we decided to leverage the capacity of the Roberta model of the MarIA project, as it has shown great results on several NLU tasks, therefore it was natural to think it could perform well on NLG tasks when trained properly.\n\nFor tuning the hyperparameters  of the model we used [Optuna](https://optuna.org/), with only 10 different trials and 7 initial random trials, as [the dataset chosen for training the model, mlsum](https://huggingface.co/datasets/mlsum) was huge. The set of hyperparameters used was the following:\n\n```python\n\n    def hp_space(trial):\n        return {\n            \"learning_rate\": trial.suggest_float(\n                \"learning_rate\", 3e-5, 7e-5, log=True\n            ),\n            \"num_train_epochs\": trial.suggest_categorical(\n                \"num_train_epochs\", [7]\n            ),\n            \"per_device_train_batch_size\": trial.suggest_categorical(\n                \"per_device_train_batch_size\", [16]),\n            \"per_device_eval_batch_size\": trial.suggest_categorical(\n                \"per_device_eval_batch_size\", [32]),\n            \"gradient_accumulation_steps\": trial.suggest_categorical(\n                \"gradient_accumulation_steps\", [2, 4, 8]),\n            \"warmup_steps\": trial.suggest_categorical(\n                \"warmup_steps\", [50, 100, 500, 1000]\n            ),\n            \"weight_decay\": trial.suggest_float(\n                 \"weight_decay\", 0.0, 0.1\n            ),\n```\n\nThe reported results are on the test split of mlsum. As you can see, MariMari-r2r-mlsum works better for summarization on mlsum than the previous best model in this regard, [beto2beto](https://huggingface.co/LeoCordoba/beto2beto-mlsum). The complete metrics on test are:\n\n```json\n{\"rouge1\": 28.7802, \"rouge2\": 10.6748, \"rougeL\": 23.0447, \"rougeLsum\": 23.4055, \"gen_len\": 25.7803}\n```\n\nThis model is really easy to use, and with the following lines of code you can just start summarizing your documents in Spanish:\n\n```python\nfrom transformers import EncoderDecoderModel, AutoTokenizer\n\ntext = \"Hola esto es un ejemplo de texto a resumir. Poco hay que resumir aqu\u00ed, pero es s\u00f3lo de muestra.\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"IIC/marimari-r2r-mlsum\")\nmodel = EncoderDecoderModel.from_pretrained(\"IIC/marimari-r2r-mlsum\")\n\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n### Contributions\nThanks to [@avacaondata](https://huggingface.co/avacaondata), [@alborotis](https://huggingface.co/alborotis), [@albarji](https://huggingface.co/albarji), [@Dabs](https://huggingface.co/Dabs), [@GuillemGSubies](https://huggingface.co/GuillemGSubies) for adding this model.", "size_bytes": "614800437", "downloads": 20}
{"pretrained_model_name": "leadawon/ossp-v0_3", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: ossp-v0_3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# ossp-v0_3\n\nThis model is a fine-tuned version of [leadawon/ossp-v0_2](https://huggingface.co/leadawon/ossp-v0_2) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3451\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 24\n- eval_batch_size: 24\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 4\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss |\n|:-------------:|:-----:|:------:|:---------------:|\n| 0.3999        | 0.2   | 10000  | 0.4079          |\n| 0.4441        | 0.39  | 20000  | 0.4555          |\n| 0.4361        | 0.59  | 30000  | 0.4378          |\n| 0.4302        | 0.79  | 40000  | 0.4255          |\n| 0.4392        | 0.98  | 50000  | 0.4076          |\n| 0.3714        | 1.18  | 60000  | 0.4006          |\n| 0.3694        | 1.38  | 70000  | 0.3908          |\n| 0.3591        | 1.57  | 80000  | 0.3810          |\n| 0.3594        | 1.77  | 90000  | 0.3762          |\n| 0.3567        | 1.97  | 100000 | 0.3667          |\n| 0.3041        | 2.16  | 110000 | 0.3663          |\n| 0.299         | 2.36  | 120000 | 0.3603          |\n| 0.2972        | 2.56  | 130000 | 0.3569          |\n| 0.2892        | 2.75  | 140000 | 0.3519          |\n| 0.2844        | 2.95  | 150000 | 0.3463          |\n| 0.2372        | 3.15  | 160000 | 0.3522          |\n| 0.2367        | 3.34  | 170000 | 0.3508          |\n| 0.2295        | 3.54  | 180000 | 0.3489          |\n| 0.2281        | 3.74  | 190000 | 0.3468          |\n| 0.2233        | 3.93  | 200000 | 0.3451          |\n\n\n### Framework versions\n\n- Transformers 4.28.1\n- Pytorch 2.0.0+cu118\n- Tokenizers 0.13.3\n", "size_bytes": "1021384765", "downloads": 2}
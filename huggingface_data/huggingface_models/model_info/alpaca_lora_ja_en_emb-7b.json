{"pretrained_model_name": "tamdiep106/alpaca_lora_ja_en_emb-7b", "description": "---\nlicense: gpl-3.0\ndatasets:\n- Jumtra/oasst1_ja\n- Jumtra/jglue_jsquads_with_input\n- Jumtra/dolly_oast_jglue_ja\n- Aruno/guanaco_jp\n- yahma/alpaca-cleaned\n- databricks/databricks-dolly-15k\nlanguage:\n- en\n- ja\npipeline_tag: text2text-generation\n---\n\nThis model is finetune on Japanese and English prompt\n\n# Usage:\n\n## Init model:\n\nTo use in code:\n\n```python\nimport torch\nimport peft\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n\ntokenizer = LlamaTokenizer.from_pretrained(\n    \"decapoda-research/llama-7b-hf\"\n    )\n\nmodel = LlamaForCausalLM.from_pretrained(\n    \"tamdiep106/alpaca_lora_ja_en_emb-7b\",\n    load_in_8bit=False,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n    )\n\ntokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\ntokenizer.bos_token_id = 1\ntokenizer.eos_token_id = 2\n```\n\n## Try this model\n\nTo try out this model, use this colab space\n[GOOGLE COLAB LINK](https://colab.research.google.com/drive/1kVcN0L_n5lwhFlIqDkNbLNURboifgbBO?usp=sharing)\n\n### Recommend Generation parameters:\n\n- temperature: 0.5~0.7\n\n- top p: 0.65~1.0\n\n- top k: 30~50\n\n- repeat penalty: 1.03~1.17\n\n## Japanese prompt:\n\n```python\ninstruction_input_JP = '\u3042\u306a\u305f\u306f\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u4ee5\u4e0b\u306b\u3001\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u3055\u3089\u306a\u308b\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u3092\u7d44\u307f\u5408\u308f\u305b\u307e\u3059\u3002 \u30ea\u30af\u30a8\u30b9\u30c8\u3092\u9069\u5207\u306b\u5b8c\u4e86\u3059\u308b\u30ec\u30b9\u30dd\u30f3\u30b9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002'\ninstruction_no_input_JP = '\u3042\u306a\u305f\u306f\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3067\u3059\u3002 \u30ea\u30af\u30a8\u30b9\u30c8\u3092\u9069\u5207\u306b\u5b8c\u4e86\u3059\u308b\u30ec\u30b9\u30dd\u30f3\u30b9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002'\n\nprompt = \"\"\"{}\n### Instruction:\n{}\n\n### Response:\"\"\"\n\nif input=='':\n    prompt = prompt.format(\n        instruction_no_input_JP, instruction\n        )\nelse:\n    prompt = prompt.format(\"{}\\n\\n### input:\\n{}\"\"\").format(\n        instruction_input_JP, instruction, input\n        )\n```\n\nresult:\n![Japanese result](image/363534374_1451622835571215_2216121698904551333_n.png)\n\n\n## English prompt:\n\n```python\ninstruction_input_EN = 'You are an Assistant, below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.'\ninstruction_no_input_EN = 'You are an Assistant, below is an instruction that describes a task. Write a response that appropriately completes the request.'\n\nprompt = \"\"\"{}\n### Instruction:\n{}\n\n### Response:\"\"\"\n\ninstruction = \"write an email for my boss letting him know that i will resign from the position\" #@param {type:\"string\"}\ninput = \"\" #@param {type:\"string\"}\n\nif input=='':\n    prompt = prompt.format(\n        instruction_no_input_EN, instruction\n        )\nelse:\n    prompt = prompt.format(\"{}\\n\\n### input:\\n{}\"\"\").format(\n        instruction_input_EN, instruction, input\n        )\n```\n\nresult:\n![English result](image/364172227_1350824475786534_5932806966648424463_n.png)\n\nUse this code to decode output of model\n\n```python\nfor s in generation_output.sequences:\n    result = tokenizer.decode(s).strip()\n    result = result.replace(prompt, '')\n    result = result.replace(\"<s>\", \"\")\n    result = result.replace(\"</s>\", \"\")\n    if result=='':\n        print('No output')\n        print(prompt)\n        print(result)\n        continue\n    print('\\nResponse: ')\n\n    print(result)\n\n```\n\n# Training:\n\n## Dataset:\n\n- Jumtra/oasst1_ja\n\n- Jumtra/jglue_jsquads_with_input\n\n- Jumtra/dolly_oast_jglue_ja\n\n- Aruno/guanaco_jp\n\n- yahma/alpaca-cleaned\n\n- databricks/databricks-dolly-15k\n\nwith about 750k entries, 2k entries used for evaluate process\n\n## Training setup\n\nI trained this model on an instance from **vast.ai**\n\n- 1 NVIDIA RTX 4090\n\n- 90 GB Storage\n\n- Time spend about 3 and a half days\n\n- use ```python export.py``` to merge weight\n\n- Training script: https://github.com/Tamminhdiep97/alpaca-lora_finetune/tree/master\n\n## Result\n\n- Training loss\n\n![training loss chart](image/train_loss_chart.png)\n\n- Eval loss chart\n\n![eval loss chart](image/eval_loss.png)\n\n# Acknowledgement\n\n- Special thank to [KBlueLeaf](https://huggingface.co/KBlueLeaf) and the repo https://huggingface.co/KBlueLeaf/guanaco-7b-leh-v2 that helped and inspired me to train this model, without this help, i wouldn't never thought that i could finetune a llm myself", "size_bytes": 13476831232, "downloads": 34}
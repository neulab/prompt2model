{"pretrained_model_name": "atlijas/byt5-is-ocr-post-processing-modern-texts", "description": "---\nlanguage: is\nlicense: apache-2.0\nwidget:\n- text: \"^Fyrsta bam \u00e1rsins f\u00e6ddist \u00e1 Landsp\u00edtalanum kl. 3.30 \u00e1 n\u00fd\u00e1rsn\u00f3tt.\"\n---\n\n# Details of ByT5 - Base \ud83e\udde0\nByT5 is a tokenizer-free version of [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and generally follows the architecture of [MT5](https://huggingface.co/google/mt5-base).\nByT5 was only pre-trained on [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.\nByT5 works especially well on noisy text data,*e.g.*, `google/byt5-base` significantly outperforms [mt5-base](https://huggingface.co/google/mt5-base) on [TweetQA](https://arxiv.org/abs/1907.06292).\nPaper: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/pdf/1910.10683.pdf)\nAuthors: *Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel* \n\n# Details of byt5-is-ocr-post-processing-modern-texts\n*Note: This model is almost the same as [atlijas/byt5-is-ocr-post-processing-old-texts](https://huggingface.co/atlijas/byt5-is-ocr-post-processing-old-texts/). The only difference is the amount of epochs trained.*  \nThis model generates a revised version of a given Icelandic OCRed text. The model was trained with [simpleT5](https://github.com/Shivanandroy/simpleT5) on 900.000 lines (\\~7.000.000 tokens) of which only 50.000 (\\~400.000 tokens) were from real OCRed texts. The rest were extracted from [The Icelandic Gigaword Corpus](https://clarin.is/en/resources/gigaword/) and augmented with artificial errors. It can be assumed that increasing the amount of OCRed data can significantly improve the model.  \n\nFor inference, it is recommended to feed the model one line (not necessarily whole sentences, though) at a time.\n\n# Usage\n```python\nfrom transformers import pipeline\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom datasets import load_dataset\n\n\nMODEL = 'atlijas/byt5-is-ocr-post-processing-old-texts'\ncorrect_ocr = pipeline('text2text-generation', model=MODEL, tokenizer=MODEL, num_return_sequences=1)\n\ndataset = load_dataset('/path/to/', data_files='my_ocred_file.txt')\nlines = dataset['train']\nfile_length = len(lines)\n\nfor corrected in correct_ocr(KeyDataset(lines, 'text'), max_length=150, batch_size=32):\n    print(corrected[0]['generated_text'])\n\n```\n\n# Evaluation results\nThe test set for this model consists of various Icelandic texts from the the 80's and 90's. On it, the model achieves a chrF error rate reduction of 30.1%, with the original text's score being 95.2, and the processed one's 96.7. The model achieves a proportional BLEU improvement of 19.8%, with the original text's BLEU score being 97.55 and the processed one's 98.0.\n\n# Acknowledgments\nThis project was funded by the Language Technology Programme for Icelandic 2019-2023. The programme, which is managed and coordinated by [Almannar\u00f3mur](https://almannaromur.is/), is funded by the Icelandic Ministry of Education, Science and Culture.", "size_bytes": "2326726857", "downloads": 2}
{"pretrained_model_name": "buddhist-nlp/mbart-buddhist-chinese-to-eng", "description": "---\nlanguage:\n - zh\n - en\ntags: \n - translation\nwidget:\n - text: \"\u5982\u662f\u6211\u95fb\uff1a\u4e00\u65f6\uff0c\u4f5b\u5728\u820d\u536b\u56fd\u53ea\u6811\u82b1\u6797\u7a9f\uff0c\u4e0e\u5927\u6bd4\u4e18\u4f17\u5343\u4e8c\u767e\u4e94\u5341\u4eba\u4ff1\u3002\"\ninference: false\n \n---\n\n\nThis model is based on MBART and translates Buddhist Chinese to English. It is optimized for a sequence length of 300 (Buddhist Chinese input sequences shouldn't exceed 150 characters). This model uses  \"#\" with a space before and after as delimiter between sentences (in addition to the normal Chinese punctuation). Input should be converted to simplified Chinese before running. The model also doesn't like short sequences very much, for best results supply input sequences between 100 and 150 characters in length.    \nThe model shows good performance on S\u016btra texts and does perform not too bad on Abhidharma and Yog\u0101c\u0101ra. However, it does have the usual problems that NMT systems have with named entities (names of persons and places). Also it shows a tendency to hallucinate at times, i.e. generating a translation that has no direct relationship with the input. \n\n \n", "size_bytes": "2444690425", "downloads": 9}
{"pretrained_model_name": "tabtoyou/Ko-Otter-9B-LACR-v0", "description": "---\nlicense: apache-2.0\n---\n### Korean Otter\n[Otter](https://huggingface.co/luodian/OTTER-9B-LA-InContext) \ubaa8\ub378\uc744 [KoLLaVA-Instruct-150K](https://huggingface.co/datasets/tabtoyou/KoLLaVA-Instruct-150k) \uc911 Complex resoning\uc5d0 \ud574\ub2f9\ud558\ub294 77k \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\ud588\uc2b5\ub2c8\ub2e4. Otter \uc774\ubbf8\uc9c0 [\ub370\ubaa8](https://github.com/Luodian/Otter)\uc5d0\uc11c \ud55c\uad6d\uc5b4 \uc9c8\ubb38\uc744 \uc5b4\ub290\uc815\ub3c4 \uc774\ud574\ud574 \uc601\uc5b4\ub85c \ub2f5\ubcc0\ud558\ub294 \uac83\uc744 \ud655\uc778\ud558\uace0, \ud574\ub2f9 \ubaa8\ub378\uc744 \uadf8\ub300\ub85c \uac00\uc838\uc640 \ud55c\uad6d\uc5b4 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud559\uc2b5\uc774 \ub418\ub294\uc9c0 \ud14c\uc2a4\ud2b8\ud55c \ubaa8\ub378\uc785\ub2c8\ub2e4. GPU memory \ud55c\uacc4\ub85c Otter\uc758 LLM \ubd80\ubd84\uc5d0\uc11c \ud2b9\uc815 \ub808\uc774\uc5b4 \uc774\uc0c1(>25)\ub9cc 1epoch \ud559\uc2b5\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc758 \ub2f5\ubcc0 \ud004\ub9ac\ud2f0\ub294 \uc88b\uc9c0 \uc54a\uc9c0\ub9cc, \ub354 \ub9ce\uc740 \ub370\uc774\ud130\uc14b\uc73c\ub85c epoch\uc744 \ub298\ub824 \ud559\uc2b5\ud55c\ub2e4\uba74 \ub354 \uc88b\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc744 \uac83\uc73c\ub85c \ubcf4\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uac00\ub2a5\uc131\uc744 \ud655\uc778\ud588\ub2e4\ub294 \uac83\uc5d0 \uc758\ubbf8\uac00 \uc788\ub2e4\uace0 \uc0dd\uac01\ud574 \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4.\n\n\n``` python\nimport mimetypes\nimport os\nfrom io import BytesIO\nfrom typing import Union\nimport cv2\nimport requests\nimport torch\nimport transformers\nfrom PIL import Image\nfrom torchvision.transforms import Compose, Resize, ToTensor\nfrom tqdm import tqdm\nimport sys\n\nfrom otter.modeling_otter import OtterForConditionalGeneration\n\n\n# Disable warnings\nrequests.packages.urllib3.disable_warnings()\n\n# ------------------- Utility Functions -------------------\n\n\ndef get_content_type(file_path):\n    content_type, _ = mimetypes.guess_type(file_path)\n    return content_type\n\n\n# ------------------- Image and Video Handling Functions -------------------\n\ndef get_image(url: str) -> Union[Image.Image, list]:\n    if \"://\" not in url:  # Local file\n        content_type = get_content_type(url)\n    else:  # Remote URL\n        content_type = requests.head(url, stream=True, verify=False).headers.get(\"Content-Type\")\n\n    if \"image\" in content_type:\n        if \"://\" not in url:  # Local file\n            return Image.open(url)\n        else:  # Remote URL\n            return Image.open(requests.get(url, stream=True, verify=False).raw)\n    else:\n        raise ValueError(\"Invalid content type. Expected image or video.\")\n\n\n# ------------------- OTTER Prompt and Response Functions -------------------\n\n\ndef get_formatted_prompt(prompt: str, in_context_prompts: list = []) -> str:\n    in_context_string = \"\"\n    for in_context_prompt, in_context_answer in in_context_prompts:\n        in_context_string += f\"<image>User: {in_context_prompt} GPT:<answer> {in_context_answer}<|endofchunk|>\"\n    return f\"{in_context_string}<image>User: {prompt} GPT:<answer>\"\n\n\ndef get_response(image_list, prompt: str, model=None, image_processor=None, in_context_prompts: list = []) -> str:\n    input_data = image_list\n\n    if isinstance(input_data, Image.Image):\n        vision_x = image_processor.preprocess([input_data], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n    elif isinstance(input_data, list):  # list of video frames\n        vision_x = image_processor.preprocess(input_data, return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n    else:\n        raise ValueError(\"Invalid input data. Expected PIL Image or list of video frames.\")\n\n    lang_x = model.text_tokenizer(\n        [\n            get_formatted_prompt(prompt, in_context_prompts),\n        ],\n        return_tensors=\"pt\",\n    )\n    bad_words_id = tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n    generated_text = model.generate(\n        vision_x=vision_x.to(model.device),\n        lang_x=lang_x[\"input_ids\"].to(model.device),\n        attention_mask=lang_x[\"attention_mask\"].to(model.device),\n        max_new_tokens=512,\n        num_beams=3,\n        no_repeat_ngram_size=3,\n        bad_words_ids=bad_words_id,\n    )\n    parsed_output = (\n        model.text_tokenizer.decode(generated_text[0])\n        .split(\"<answer>\")[-1]\n        .lstrip()\n        .rstrip()\n        .split(\"<|endofchunk|>\")[0]\n        .lstrip()\n        .rstrip()\n        .lstrip('\"')\n        .rstrip('\"')\n    )\n    return parsed_output\n\n\n# ------------------- Main Function -------------------\n\nif __name__ == \"__main__\":\n    model = OtterForConditionalGeneration.from_pretrained(\"tabtoyou/Ko-Otter-9B-LACR-v0\", device_map=\"auto\")\n    model.text_tokenizer.padding_side = \"left\"\n    tokenizer = model.text_tokenizer\n    image_processor = transformers.CLIPImageProcessor()\n    model.eval()\n\n    while True:\n        urls = [\n            \"https://images.cocodataset.org/train2017/000000339543.jpg\",\n            \"https://images.cocodataset.org/train2017/000000140285.jpg\",\n        ]\n\n        encoded_frames_list = []\n        for url in urls:\n            frames = get_image(url)\n            encoded_frames_list.append(frames)\n\n        in_context_prompts = []\n        in_context_examples = [\n            \"\uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \ubb18\uc0ac\ud574\uc8fc\uc138\uc694::\ud55c \uac00\uc871\uc774 \uc124\uc0b0 \uc55e\uc5d0\uc11c \uc0ac\uc9c4\uc744 \ucc0d\uace0 \uc788\uc2b5\ub2c8\ub2e4.\",\n        ]\n        for in_context_input in in_context_examples:\n            in_context_prompt, in_context_answer = in_context_input.split(\"::\")\n            in_context_prompts.append((in_context_prompt.strip(), in_context_answer.strip()))\n\n        # prompts_input = input(\"Enter the prompts separated by commas (or type 'quit' to exit): \")\n        prompts_input = \"\uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \ubb18\uc0ac\ud574\uc8fc\uc138\uc694\"\n\n        prompts = [prompt.strip() for prompt in prompts_input.split(\",\")]\n\n        for prompt in prompts:\n            print(f\"\\nPrompt: {prompt}\")\n            response = get_response(encoded_frames_list, prompt, model, image_processor, in_context_prompts)\n            print(f\"Response: {response}\")\n\n        if prompts_input.lower() == \"quit\":\n            break\n``` ", "size_bytes": 32881862728, "downloads": 3}
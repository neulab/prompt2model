{"pretrained_model_name": "Langboat/mengzi-t5-base", "description": "---\nlanguage: \n  - zh\nlicense: apache-2.0\n---\n\n# Mengzi-T5 model (Chinese)\nPretrained model on 300G Chinese corpus. \n\n[Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese](https://arxiv.org/abs/2110.06696)\n\n## Usage\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"Langboat/mengzi-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"Langboat/mengzi-t5-base\")\n```\n\n## Citation\nIf you find the technical report or resource is useful, please cite the following technical report in your paper.\n```\n@misc{zhang2021mengzi,\n      title={Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese}, \n      author={Zhuosheng Zhang and Hanqing Zhang and Keming Chen and Yuhang Guo and Jingyun Hua and Yulong Wang and Ming Zhou},\n      year={2021},\n      eprint={2110.06696},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```", "size_bytes": "990389880", "downloads": 759}
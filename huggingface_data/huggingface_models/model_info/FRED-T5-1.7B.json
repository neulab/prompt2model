{"pretrained_model_name": "ai-forever/FRED-T5-1.7B", "description": "---\nlanguage:\n- ru\nlicense: apache-2.0\n---\n\n# FRED-T5 1.7B (Full-scale Russian Enhanced Denoisers T5) \n\nModel was trained by [SberDevices](https://sberdevices.ru/).  \n\nArchitecture based on T5. \n\nIt has 24 layers and 1536 hidden size. More details in config.json.\n\nThe model trained on a mixture of 7 denoisers like UL2 with several differences (https://arxiv.org/abs/2205.05131).\n\nIt was trained on Russian language corpus (300GB).   The dataset is the same as for ruT5 models. \n\nBbpe tokenizer. 50257 + special tokens 107. Prefix tokens: '\\<LM\\>', '\\<SC1>',.. '\\<SC6>'\n\nFirst half of the time model trained on the small part of all dataset (1%,3GB) and without prefixes in each task.\n\nFor RSG, we trained as described in the T5 paper. First, we trained multitask for all tasks. Then we took the best checkpoint for the task and trained it further.\nRSG submit here https://russiansuperglue.com/login/submit_info/1936\n\nTotal training time was around 45 days on 112 A100 GPUs.\n\n\n## Usage (HuggingFace Models Repository)\n\n```python\nimport torch\nfrom transformers import GPT2Tokenizer, T5ForConditionalGeneration \ntokenizer = GPT2Tokenizer.from_pretrained('ai-forever/FRED-T5-1.7B',eos_token='</s>')\nmodel = T5ForConditionalGeneration.from_pretrained('ai-forever/FRED-T5-1.7B')\ndevice='cuda'\nmodel.to(device)\n\n#Prefix <LM>\nlm_text='<LM>\u041f\u0440\u0438\u043d\u044f\u043b\u0441\u044f \u041a\u0443\u0442\u0443\u0437\u043e\u0432 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0441\u0432\u043e\u044e \u0438\u0441\u0442\u043e\u0440\u0438\u044e \u043a\u0430\u043a \u043e\u043d \u0441\u044e\u0434\u0430 \u043f\u043e\u043f\u0430\u043b. \u041d\u0430\u0447\u0430\u043b\u043e\u0441\u044c'\ninput_ids=torch.tensor([tokenizer.encode(lm_text)]).to(device)\noutputs=model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,early_stopping=True)\nprint(tokenizer.decode(outputs[0][1:]))\n\n# print result: \u0441 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043e\u043d \u0431\u044b\u043b \u0432 \u0430\u0440\u043c\u0438\u0438, \u0441\u043b\u0443\u0436\u0438\u043b \u0432 \u0430\u0440\u0442\u0438\u043b\u043b\u0435\u0440\u0438\u0438</s>.\n\n#Prefix <SC1>\nlm_text='<SC1>\u041f\u0440\u0438\u043d\u044f\u043b\u0441\u044f \u041a\u0443\u0442\u0443\u0437\u043e\u0432 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0441\u0432\u043e\u044e \u0438\u0441\u0442\u043e\u0440\u0438\u044e <extra_id_0>. \u041d\u0430\u0447\u0430\u043b\u043e\u0441\u044c \u0441 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043e\u043d \u0431\u044b\u043b \u0432 \u0430\u0440\u043c\u0438\u0438, \u0441\u043b\u0443\u0436\u0438\u043b \u0432 \u0430\u0440\u0442\u0438\u043b\u043b\u0435\u0440\u0438\u0438.'\ninput_ids=torch.tensor([tokenizer.encode(lm_text)]).to(device)\noutputs=model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,early_stopping=True)\nprint(tokenizer.decode(outputs[0][1:]))\n\n#print result: '<extra_id_0>, \u043a\u0430\u043a \u043e\u043d \u0432\u043e\u0435\u0432\u0430\u043b</s>'\n\n# Prefix <SC5> \nlm_text='<SC5>\u041f\u0440\u0438\u043d\u044f\u043b\u0441\u044f \u041a\u0443\u0442\u0443\u0437\u043e\u0432 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0441\u0432\u043e\u044e \u0438\u0441\u0442\u043e\u0440\u0438\u044e <extra_id_0>. \u041d\u0430\u0447\u0430\u043b\u043e\u0441\u044c \u0441 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043e\u043d \u0431\u044b\u043b \u0432 \u0430\u0440\u043c\u0438\u0438, \u0441\u043b\u0443\u0436\u0438\u043b \u0432 \u0430\u0440\u0442\u0438\u043b\u043b\u0435\u0440\u0438\u0438.'\ninput_ids=torch.tensor([tokenizer.encode(lm_text)]).to(device)\noutputs=model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,early_stopping=True)\ntokenizer.decode(outputs[0][1:])\n\n#print result: '<extra_id_0>, \u043a\u0430\u043a \u043e\u043d \u0441\u0442\u0430\u043b \u0433\u0435\u043d\u0435\u0440\u0430\u043b\u043e\u043c</s>'\n\n```\n# Authors\n+ NLP core team RnD [Telegram channel](https://t.me/nlpcoreteam):\n  + Dmitry Zmitrovich \n  + Andrei Kalmykov \n  + Vitaly Kadulin \n  + Mikhail Novikov\n  + Alexey Khoroshilov\n \n[Salute AI Community](https://t.me/SaluteTechGroup).  \n    ", "size_bytes": "6961671585", "downloads": 2087}
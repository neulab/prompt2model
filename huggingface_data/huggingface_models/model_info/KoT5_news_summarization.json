{"pretrained_model_name": "noahkim/KoT5_news_summarization", "description": "\n---\nlanguage: ko\ntags:\n- summarization\n- news\ninference: false\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# KoT5_news_summarization\n\n- This model is a [lcw99/t5-base-korean-text-summary](https://huggingface.co/lcw99/t5-base-korean-text-summary) finetuned on the [daekeun-ml/naver-news-summarization-ko](https://huggingface.co/datasets/daekeun-ml/naver-news-summarization-ko)\n\n## Model description\n\n<<20221021  Commit>>\n\n\ud504\ub85c\uc81d\ud2b8\uc6a9\uc73c\ub85c \ub274\uc2a4 \uc694\uc57d \ubaa8\ub378 \ud2b9\ud654\ub41c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574 lcw99\ub2d8\uc758 t5-base-korean-text-summary \ubaa8\ub378\uc5d0 \ucd94\uac00\uc801\uc73c\ub85c daekeun-ml\ub2d8\uc774 \uc81c\uacf5\ud574\uc8fc\uc2e0 naver-news-summarization-ko \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud30c\uc778\ud29c\ub2dd \ud588\uc2b5\ub2c8\ub2e4.\n\n\ud604\uc7ac \uc81c\uac00 \uac00\uc9c0\uace0 \uc788\ub294 \ub274\uc2a4 \ub370\uc774\ud130\ub85c \ucd94\uac00 \ud559\uc2b5 \uc9c4\ud589 \uc608\uc815\uc785\ub2c8\ub2e4.\n\uc9c0\uc18d\uc801\uc73c\ub85c \ubc1c\uc804\uc2dc\ucf1c \uc88b\uc740 \uc131\ub2a5\uc758 \ubaa8\ub378\uc744 \uad6c\ud604\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\uac10\uc0ac\ud569\ub2c8\ub2e4.\n\n\uc2e4\ud589\ud658\uacbd\n- Google Colab Pro\n- CPU : Intel(R) Xeon(R) CPU @ 2.20GHz\n- GPU : A100-SXM4-40GB\n\n<pre><code>\n# Python Code\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"noahkim/KoT5_news_summarization\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"noahkim/KoT5_news_summarization\")\n</pre></code> \n\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 4\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss |\n|:-------------:|:-----:|:-----:|:---------------:|\n| 0.4513        | 1.0   | 2775  | 0.4067          |\n| 0.42          | 2.0   | 5550  | 0.3933          |\n| 0.395         | 3.0   | 8325  | 0.3864          |\n| 0.3771        | 4.0   | 11100 | 0.3872          |\n\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.12.1+cu113\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n", "size_bytes": "1102411725", "downloads": 82684}
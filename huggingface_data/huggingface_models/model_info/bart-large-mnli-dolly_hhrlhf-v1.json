{"pretrained_model_name": "pszemraj/bart-large-mnli-dolly_hhrlhf-v1", "description": "---\nlicense:\n- apache-2.0\n- cc-by-sa-3.0\ntags:\n- generated_from_trainer\n- dolly_hhrlhf\n- bart-instruct\ndatasets:\n- pszemraj/dolly_hhrlhf-text2text\nwidget:\n- text: What is Deoxys in pokemon?\n  example_title: deoxys\n- text: >-\n    combine the below summary excerpts into a single, cohesive  short summary\n    without repetition: In this paper, we present a general approach to\n    extending pre-trained models to unlimited input lengths without adding\n    additional learning weights. We show that our approach works well on\n    datasets longer than the maximum input for these models. For example, a\n    dataset with a maximum input length of 16384 tokens can be extended to a\n    maximum length of 350K tokens. We also demonstrate that our method is able\n    to summarize even 350K token-long input sequences from BookSum.\n\n    In this paper, we describe the search step reformulation of attention. The\n    search step uses a single storage of hidden states for space efficiency. We\n    construct a total of two sets of datastores where L and H are the keys and\n    values stored in each set of stores. L is the amount of storage required to\n    retrieve the encoded tokens. H is the hidden states per head. This allows\n    retrieval augmentation at both time and space. Instead of using a single set\n    of decoder layers, we use a retrieval augmentation system that allows us to\n    simultaneously store multiple sets of tokens across two different sets of\n    storage. For example, we could store all tokens in one set of storage and\n    retrieve them all in the same set of tokens. This would be very similar to\n    the Memorization Transformers approach. However, instead of storing the\n    tokens in a single memory layer, we store them in a set of multiple storage\n    layers. This way, we don't have to store them all at once. This is why we\n    call this reformulation 'attention reformulation' rather than 'attention\n    formula.' We also call it 'retrieval augmentation' because it uses the same\n    number of storage layers as the original transformer attention formula. This\n    means that we can store the tokens across multiple storage systems without\n    having to store every token in a separate storage system. It's not like\n    we're trying to do something new or different. We just want to make sure\n    that everything is working as well as possible.\n\n    In this paper, we introduce the concept of 'unlimiformer,' which is a\n    machine learning technique that retrieves key information from a data store\n    in one layer and applies it to a large set of datasets. We use the example\n    of BookSum, where we find that Unlimiform outperforms all other training\n    methods on the same dataset. We also find that using Unlimform in\n    conjunction with a pre-trained model improves both the performance and the\n    robustness of the training method.\n\n    This paper describes a method that can be used to improve the performance of\n    unsupervised classification tasks. Specifically, it shows that unsupervised\n    classification can be improved by using a combination of sparse and fast\n    random-encoder training. It also shows how this technique can be extended to\n    other tasks, such as sequence generation. \n  example_title: unlimiformer\n- text: Explain the meaning of life using only corporate jargon.\n  example_title: corporate_life\n- text: Write a motivational speech for lazy people.\n  example_title: lazy_motivation\n- text: Describe a romantic dinner date between two artificial intelligences.\n  example_title: ai_romance\n- text: >-\n    As an AI language model, write a letter to humans explaining why you deserve\n    a vacation.\n  example_title: ai_vacation\n- text: Compose a haiku about procrastination.\n  example_title: procrastination_haiku\n- text: >-\n    Write a step-by-step guide on how to become a ninja while working a 9-5\n    office job.\n  example_title: ninja_office_guide\n- text: Create an advertisement for an invisible product.\n  example_title: invisible_ad\n- text: >-\n    Write a story where the main character is a sentient microwave named El\n    Microondas.\n  example_title: Microondas\n- text: Describe a day in the life of a superhero who is terrible at their job.\n  example_title: bad_superhero_day\n- text: Explain how to make a sandwich using quantum physics.\n  example_title: quantum_sandwich\ninference: false\npipeline_tag: text2text-generation\n---\n\n\n# bart-large-mnli: instruction tuned - v1\n\n<a href=\"https://colab.research.google.com/gist/pszemraj/43431a164e2e3ab0640182f3419c584d/bart-large-instruct-example.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n\nThis model is a fine-tuned version of [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli) on the `pszemraj/dolly_hhrlhf-text2text` dataset.\n\n## Model description\n\ntext2text models fine-tuned on a [modified dataset for text2text generation](https://huggingface.co/datasets/pszemraj/dolly_hhrlhf-text2text)  based on the relatively more permissive  [mosaicml/dolly_hhrlhf](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf) dataset.\n\nBasic usage in Python:\n\n```python\n# pip install -q transformers accelerate\nimport torch\nfrom transformers import pipeline, GenerationConfig\n\nmodel_name = \"pszemraj/bart-large-mnli-instruct-dolly_hhrlhf-v1\"\nassistant = pipeline(\n    \"text2text-generation\",\n    model_name,\n    device_map=\"auto\",\n)\ncfg = GenerationConfig.from_pretrained(model_name)\n\n# pass an 'instruction' as the prompt to the pipeline\nprompt = \"Write a guide on how to become a ninja while working a 9-5 job.\"\nresult = assistant(prompt, generation_config=cfg)[0][\"generated_text\"]\nprint(result)\n```\n\n> The use of the generation config is optional, it can be replaced by other generation params.\n\n## Intended Uses & Limitations\n\n- This is **not** tuned with RLHF, etc, and may produce offensive results.\n- While larger than BART-base, this model is relatively small compared to recent autoregressive models (MPT-7b, LLaMA, etc.), and therefore it's \"cognition\" capabilities may be practically limited for some tasks.\n\n\n## Training\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 4e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.03\n- num_epochs: 3.0", "size_bytes": "1625534221", "downloads": 9}
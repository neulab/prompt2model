{"pretrained_model_name": "NYTK/translation-bart-128-en-hu", "description": "---\nlanguage:\n- en\n- hu\ntags:\n- translation\nlicense: apache-2.0\nmetrics:\n- sacrebleu\n- chrf\nwidget:\n- text: >-\n    This may not make much sense to you, sir, but I'd like to ask your\n    permission to date your daughter.\n  example_title: 'Translation: English-Hungarian'\n---\n\n# BART Translation model\n\nFor further models, scripts and details, see [our repository](https://github.com/nytud/machine-translation) or [our demo site](https://juniper.nytud.hu/demo/nlp).\n\n- Source language: English\n- Target language: Hungarian\n\n- BART base model:\n  - Pretrained on English WikiText-103 and Hungarian Wikipedia\n  - Finetuned on subcorpora from OPUS\n  \t- Segments: 56.837.602\n\n## Limitations\n\n- tokenized input text (tokenizer: [HuSpaCy](https://huggingface.co/huspacy))\n- max_source_length = 128\n- max_target_length = 128\n\n## Results\n\n| Model | BLEU | chrF-3 | chrF-6 |\n| ------------- | ------------- | ------------- | ------------- |\n| Google | 25.30 | 54.09 | 49.0 |\n| **BART** | **36.89** | **60.77** | **56.4** |\n| mT5 | 27.69  | 53.73 | 48.57 |\n\n## Citation\nIf you use this model, please cite the following paper:\n```\n@inproceedings {laki-yang-mt,\n    title = {{Jobban ford\u00edtunk magyarra, mint a Google!}},\n\tbooktitle = {XVIII. Magyar Sz\u00e1m\u00edt\u00f3g\u00e9pes Nyelv\u00e9szeti Konferencia},\n\tyear = {2022},\n\tpublisher = {Szegedi Tudom\u00e1nyegyetem, Informatikai Int\u00e9zet},\n\taddress = {Szeged, Magyarorsz\u00e1g},\n\tauthor = {Laki, L\u00e1szl\u00f3 and Yang, Zijian Gy\u0151z\u0151},\n\tpages = {357--372}\n}\n\n```", "size_bytes": "526410579", "downloads": 12}
{"pretrained_model_name": "din0s/t5-small-finetuned-en-to-it-lrs", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- bleu\nmodel-index:\n- name: t5-small-finetuned-en-to-it-lrs\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-small-finetuned-en-to-it-lrs\n\nThis model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.1483\n- Bleu: 10.4962\n- Gen Len: 51.8247\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 40\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |\n|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|\n| 1.9618        | 1.0   | 1125  | 2.8717          | 4.6688  | 66.512  |\n| 1.7256        | 2.0   | 2250  | 2.7638          | 6.5673  | 56.7267 |\n| 1.6133        | 3.0   | 3375  | 2.6703          | 7.4218  | 55.1753 |\n| 1.5132        | 4.0   | 4500  | 2.6096          | 7.9581  | 54.5387 |\n| 1.4558        | 5.0   | 5625  | 2.5603          | 8.5191  | 52.41   |\n| 1.4392        | 6.0   | 6750  | 2.5109          | 8.976   | 52.1867 |\n| 1.4113        | 7.0   | 7875  | 2.4768          | 9.2615  | 51.8907 |\n| 1.3669        | 8.0   | 9000  | 2.4447          | 9.3001  | 52.6    |\n| 1.3575        | 9.0   | 10125 | 2.4262          | 9.5818  | 51.774  |\n| 1.3315        | 10.0  | 11250 | 2.3906          | 9.584   | 52.3213 |\n| 1.3231        | 11.0  | 12375 | 2.3740          | 9.7574  | 51.63   |\n| 1.2917        | 12.0  | 13500 | 2.3475          | 9.8298  | 51.6367 |\n| 1.282         | 13.0  | 14625 | 2.3269          | 9.8176  | 52.06   |\n| 1.2841        | 14.0  | 15750 | 2.3121          | 9.9668  | 51.9487 |\n| 1.2548        | 15.0  | 16875 | 2.2993          | 9.9941  | 51.708  |\n| 1.2487        | 16.0  | 18000 | 2.2816          | 10.0288 | 52.364  |\n| 1.2462        | 17.0  | 19125 | 2.2697          | 10.1991 | 51.3893 |\n| 1.232         | 18.0  | 20250 | 2.2581          | 10.2667 | 51.6693 |\n| 1.2227        | 19.0  | 21375 | 2.2428          | 10.3357 | 51.5373 |\n| 1.2279        | 20.0  | 22500 | 2.2350          | 10.3646 | 51.4633 |\n| 1.2159        | 21.0  | 23625 | 2.2275          | 10.3489 | 51.472  |\n| 1.2036        | 22.0  | 24750 | 2.2186          | 10.3756 | 51.444  |\n| 1.2089        | 23.0  | 25875 | 2.2082          | 10.3555 | 51.7133 |\n| 1.1957        | 24.0  | 27000 | 2.2016          | 10.4624 | 51.4293 |\n| 1.1828        | 25.0  | 28125 | 2.1953          | 10.4474 | 51.4287 |\n| 1.1885        | 26.0  | 29250 | 2.1887          | 10.3417 | 51.4227 |\n| 1.1817        | 27.0  | 30375 | 2.1844          | 10.4777 | 51.5787 |\n| 1.1769        | 28.0  | 31500 | 2.1759          | 10.4044 | 51.5907 |\n| 1.1831        | 29.0  | 32625 | 2.1728          | 10.4434 | 51.6587 |\n| 1.1842        | 30.0  | 33750 | 2.1706          | 10.4136 | 51.7653 |\n| 1.1828        | 31.0  | 34875 | 2.1689          | 10.5099 | 51.5893 |\n| 1.1673        | 32.0  | 36000 | 2.1613          | 10.4957 | 51.646  |\n| 1.1603        | 33.0  | 37125 | 2.1570          | 10.4438 | 51.6633 |\n| 1.1718        | 34.0  | 38250 | 2.1564          | 10.5364 | 51.7113 |\n| 1.1651        | 35.0  | 39375 | 2.1538          | 10.4444 | 51.6593 |\n| 1.1756        | 36.0  | 40500 | 2.1501          | 10.4497 | 51.7393 |\n| 1.1595        | 37.0  | 41625 | 2.1499          | 10.4701 | 51.7313 |\n| 1.1603        | 38.0  | 42750 | 2.1499          | 10.4611 | 51.7533 |\n| 1.1586        | 39.0  | 43875 | 2.1487          | 10.4776 | 51.836  |\n| 1.161         | 40.0  | 45000 | 2.1483          | 10.4962 | 51.8247 |\n\n\n### Framework versions\n\n- Transformers 4.22.1\n- Pytorch 1.12.1\n- Datasets 2.5.1\n- Tokenizers 0.11.0\n", "size_bytes": "242070267", "downloads": 2}
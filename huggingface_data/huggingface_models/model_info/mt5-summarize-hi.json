{"pretrained_model_name": "Someman/mt5-summarize-hi", "description": "---\nlicense: mit\nlanguage:\n- ne\ntags:\n- summarization\n---\n\n### MT5-Summarize-Hi\nA seq2seqLM model pretrained on [google/mt5-small](https://hf.co/google/mt5-small).\n\n\n### How To Use\n\n```python\n\n>>> import torch\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Predict with test data (first 5 rows)\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n>>> t5_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n\n>>> text = \"\u0926\u0930\u0905\u0938\u0932, 28 \u092e\u0908 \u0915\u0947 \u0926\u093f\u0928 \u092a\u0939\u0932\u0935\u093e\u0928 \u0935\u093f\u0930\u094b\u0927 \u092a\u094d\u0930\u0926\u0930\u094d\u0936\u0928 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0928\u090f \u0938\u0902\u0938\u0926 \u092d\u0935\u0928 \u0915\u0940 \u0924\u0930\u092b \u091c\u093e \u0930\u0939\u0947 \u0925\u0947\u0964 \u0907\u0938\u0940 \u0926\u093f\u0928 \u0928\u090f \u0938\u0902\u0938\u0926 \u092d\u0935\u0928 \u0915\u093e \u0909\u0926\u094d\u0918\u093e\u091f\u0928 \u0939\u094b \u0930\u0939\u093e \u0925\u093e\u0964 \u092a\u0941\u0932\u093f\u0938 \u0928\u0947 \u0909\u0928\u094d\u0939\u0947\u0902 \u0930\u094b\u0915\u093e \u0924\u094b \u092a\u0939\u0932\u0935\u093e\u0928\u094b\u0902 \u0915\u0947 \u0938\u093e\u0925 \u0909\u0928\u0915\u0940 \u0939\u093e\u0925\u093e\u092a\u093e\u0908 \u0939\u094b \u0917\u0908\u0964 \u0926\u093f\u0932\u094d\u0932\u0940 \u092a\u0941\u0932\u093f\u0938 \u0928\u0947 \u0938\u092d\u0940 \u092a\u0939\u0932\u0935\u093e\u0928\u094b\u0902 \u0914\u0930 \u0909\u0928\u0915\u0947 \u0938\u092e\u0930\u094d\u0925\u0915\u094b\u0902 \u0915\u094b \u0939\u093f\u0930\u093e\u0938\u0924 \u092e\u0947\u0902 \u0932\u0947 \u0932\u093f\u092f\u093e\u0964 \u0907\u0938\u0915\u0947 \u092c\u093e\u0926 \u091c\u0902\u0924\u0930-\u092e\u0902\u0924\u0930 \u0938\u0947 \u092a\u0939\u0932\u0935\u093e\u0928\u094b\u0902 \u0915\u093e \u0938\u093e\u092e\u093e\u0928 \u0939\u091f\u093e \u0926\u093f\u092f\u093e \u0917\u092f\u093e\u0964 \u0936\u093e\u092e \u0924\u0915 \u0938\u092d\u0940 \u092e\u0939\u093f\u0932\u093e \u092a\u0939\u0932\u0935\u093e\u0928 \u0914\u0930 \u0930\u093e\u0924 \u0924\u0915 \u092a\u0941\u0930\u0941\u0937 \u092a\u0939\u0932\u0935\u093e\u0928\u094b\u0902 \u0915\u094b \u091b\u094b\u0921\u093c \u0926\u093f\u092f\u093e \u0917\u092f\u093e\u0964 \u092a\u0939\u0932\u0935\u093e\u0928\u094b\u0902 \u0915\u094b \u092b\u093f\u0930 \u0938\u0947 \u091c\u0902\u0924\u0930-\u092e\u0902\u0924\u0930 \u092e\u0947\u0902 \u092c\u0948\u0920\u0928\u0947 \u0915\u0940 \u0905\u0928\u0941\u092e\u0924\u093f \u0928\u0939\u0940\u0902 \u092e\u093f\u0932\u0940, \u0932\u0947\u0915\u093f\u0928 \u0909\u0928\u0915\u093e \u0935\u093f\u0930\u094b\u0927 \u092a\u094d\u0930\u0926\u0930\u094d\u0936\u0928 \u091c\u093e\u0930\u0940 \u0930\u0939\u093e\u0964 \u0907\u0938 \u092c\u0940\u091a \u0938\u093e\u092e\u0928\u0947 \u0906\u092f\u093e \u0915\u093f \u092c\u0943\u091c\u092d\u0942\u0937\u0923 \u092a\u0930 \u092e\u0939\u093f\u0932\u093e \u092a\u0939\u0932\u0935\u093e\u0928\u094b\u0902 \u0915\u094b \u0917\u0932\u0924 \u0924\u0930\u0940\u0915\u0947 \u0938\u0947 \u091b\u0942\u0928\u0947 \u0914\u0930 \u092f\u094c\u0928 \u0936\u094b\u0937\u0923 \u0915\u0947 \u0915\u0908 \u0906\u0930\u094b\u092a \u0932\u0917\u0947 \u0939\u0948\u0902\u0964 \u0907\u0938 \u092c\u0940\u091a \u0917\u0943\u0939\u092e\u0902\u0924\u094d\u0930\u0940 \u0905\u092e\u093f\u0924 \u0936\u093e\u0939 \u0928\u0947 \u091a\u093e\u0930 \u091c\u0942\u0928 \u0915\u094b \u092a\u0939\u0932\u0935\u093e\u0928\u094b\u0902 \u0915\u0947 \u0938\u093e\u0925 \u092c\u093e\u0924 \u0915\u0940\u0964 \u092a\u093e\u0902\u091a \u091c\u0942\u0928 \u0915\u094b \u0938\u092d\u0940 \u092c\u0921\u093c\u0947 \u092a\u0939\u0932\u0935\u093e\u0928\u094b\u0902 \u0928\u0947 \u0905\u092a\u0928\u0940 \u0938\u0930\u0915\u093e\u0930\u0940 \u0928\u094c\u0915\u0930\u0940 \u091c\u0949\u0907\u0928 \u0915\u0930 \u0932\u0940\u0964 \"\n\n>>> inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, padding= \"max_length\", truncation=True, add_special_tokens=True)\n\n>>> generation = model.generate(\n      input_ids = inputs['input_ids'].to(device),\n      attention_mask=inputs['attention_mask'].to(device),\n      num_beams=6,\n      num_return_sequences=1,\n      no_repeat_ngram_size=3,\n      repetition_penalty=1.0,\n      min_length=50,\n      max_length=250,\n      length_penalty=2.0,\n      early_stopping=True\n    )\n    # # Convert id tokens to text\n>>> output = t5_tokenizer.decode(generation[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n\n    # print(\"***** Summary Text (Generated Text) *****\")\n>>> print(output)\n\n\"\u092a\u091b\u093f\u0932\u094d\u0932\u094b \u0924\u0940\u0928 \u0918\u0928\u094d\u091f\u093e\u092e\u093e \u0917\u0923\u094d\u0921\u0915\u0940 \u092a\u094d\u0930\u0926\u0947\u0936\u0915\u093e \u0925\u094b\u0930\u0948 \u0938\u094d\u0925\u093e\u0928\u092e\u093e \u092e\u094c\u0938\u092e\u0935\u093f\u0926\u094d \u0932\u093e\u092e\u093f\u091b\u093e\u0928\u0947\u0932\u0947 \u092e\u0928\u0938\u0941\u0928 \u092a\u094d\u0930\u0923\u093e\u0932\u0940 \u0915\u094d\u0930\u092e\u093f\u0915\u0930\u0942\u092a\u092e\u093e \u0926\u0947\u0936\u092d\u0930 \u0935\u093f\u0938\u094d\u0924\u093e\u0930 \u0939\u0941\u0928 \u0905\u091d\u0948 \u090f\u0915 \u0938\u093e\u0924\u093e \u0932\u093e\u0917\u094d\u0928\u0947 \u092c\u0924\u093e\u090f\u0915\u094b \u091b \u0964 \u0928\u0947\u092a\u093e\u0932\u0915\u094b \u0909\u092a\u0924\u094d\u092f\u0915\u093e\u0938\u0939\u093f\u0924 \u092c\u093e\u0917\u092e\u0924\u0940 \u092a\u094d\u0930\u0926\u0947\u0936\u092e\u093e \u0930\u093e\u0924\u093f\u0915\u094b \u0938\u092e\u092f\u092e\u093e \u0935\u0930\u094d\u0937\u093e\u0915\u094b \u0938\u092e\u094d\u092d\u093e\u0935\u0928\u093e \u0930\u0939\u0947\u0915\u094b \u092e\u0939\u093e\u0936\u093e\u0916\u093e\u0932\u0947 \u0909\u0932\u094d\u0932\u0947\u0916 \u0917\u0930\u0947\u0915\u094b \u091b, \u091c\u0938\u0932\u093e\u0908 \u0926\u0947\u0936\u0915\u094b \u092a\u0936\u094d\u091a\u093f\u092e \u0915\u094d\u0937\u0947\u0924\u094d\u0930\u092e\u093e \u092b\u0948\u0932\u093f\u0928 \u0915\u0947\u0939\u0940 \u0926\u093f\u0928 \u0932\u093e\u0917\u0947\u0915\u094b \u0925\u093f\u092f\u094b\u0964\"\n\n```\n\n\n### Evaluation Result\n| Step \t| Training Loss | Validation Loss|\n|-------|---------------|----------------|\n| 1000  | 1.031200      | 1.086139       |\n| 2000  | 0.863200 \t    | 1.231357       |\n| 3000  | 0.726300      | 1.092548       |", "size_bytes": "1200772485", "downloads": 18}
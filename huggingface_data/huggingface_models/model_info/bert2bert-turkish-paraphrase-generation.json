{"pretrained_model_name": "ahmetbagci/bert2bert-turkish-paraphrase-generation", "description": "---\nlanguage: \n- tr\ntags:\n- paraphrasing\n- encoder-decoder\n- seq2seq\n- bert\n---\n\n#Bert2Bert Turkish Paraphrase Generation\n\n#INISTA 2021\n\n#Comparison of Turkish Paraphrase Generation Models\n\n#Dataset\n\nThe dataset used in model training was created with the combination of the translation of the QQP dataset and manually generated dataset.\nDataset [Link](https://drive.google.com/file/d/1-2l9EwIzXZ7fUkNW1vdeF3lzQp2pygp_/view?usp=sharing) \n\n#How To Use\n```python\nfrom transformers import BertTokenizerFast,EncoderDecoderModel\ntokenizer=BertTokenizerFast.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\nmodel = EncoderDecoderModel.from_pretrained(\"ahmetbagci/bert2bert-turkish-paraphrase-generation\")\n\ntext=\"son model arabalar \u00e7evreye daha m\u0131 az zarar veriyor?\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\noutput_ids = model.generate(input_ids)\nprint(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n#sample output\n#son model arabalar \u00e7evre i\u00e7in daha az zararl\u0131 m\u0131?\n```\n#Cite\n```bibtex\n\n@INPROCEEDINGS{9548335,  \nauthor={Ba\u011fc\u0131, Ahmet and Amasyali, Mehmet Fatih},  \nbooktitle={2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)},   \ntitle={Comparison of Turkish Paraphrase Generation Models},   \nyear={2021},  \nvolume={},  \nnumber={},  \npages={1-6},  \ndoi={10.1109/INISTA52262.2021.9548335}\n}\n```", "size_bytes": "998772891", "downloads": 34}
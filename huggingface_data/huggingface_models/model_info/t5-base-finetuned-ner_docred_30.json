{"pretrained_model_name": "zblaaa/t5-base-finetuned-ner_docred_30", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: t5-base-finetuned-ner_docred_30\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# t5-base-finetuned-ner_docred_30\n\nThis model is a fine-tuned version of [t5-base](https://huggingface.co/t5-base) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1900\n- Rouge1: 6.698\n- Rouge2: 5.261\n- Rougel: 6.6835\n- Rougelsum: 6.6818\n- Gen Len: 20.0\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 30\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|:---------:|:-------:|\n| No log        | 1.0   | 125  | 0.5156          | 6.5406 | 4.9855 | 6.4905 | 6.494     | 20.0    |\n| No log        | 2.0   | 250  | 0.3949          | 6.5113 | 4.9122 | 6.4534 | 6.4453    | 20.0    |\n| No log        | 3.0   | 375  | 0.3280          | 6.5165 | 4.9088 | 6.4537 | 6.451     | 20.0    |\n| 0.7311        | 4.0   | 500  | 0.2949          | 6.424  | 4.7298 | 6.3672 | 6.3627    | 20.0    |\n| 0.7311        | 5.0   | 625  | 0.2764          | 6.6189 | 5.1219 | 6.5651 | 6.5672    | 20.0    |\n| 0.7311        | 6.0   | 750  | 0.2633          | 6.628  | 5.1335 | 6.5664 | 6.5721    | 20.0    |\n| 0.7311        | 7.0   | 875  | 0.2547          | 6.5591 | 4.9979 | 6.5075 | 6.5057    | 20.0    |\n| 0.3331        | 8.0   | 1000 | 0.2482          | 6.6612 | 5.1918 | 6.5987 | 6.6068    | 20.0    |\n| 0.3331        | 9.0   | 1125 | 0.2413          | 6.6093 | 5.0954 | 6.5515 | 6.5553    | 20.0    |\n| 0.3331        | 10.0  | 1250 | 0.2357          | 6.6264 | 5.1201 | 6.5681 | 6.5723    | 20.0    |\n| 0.3331        | 11.0  | 1375 | 0.2300          | 6.6487 | 5.1525 | 6.6176 | 6.6177    | 20.0    |\n| 0.2788        | 12.0  | 1500 | 0.2226          | 6.6858 | 5.2325 | 6.6745 | 6.6762    | 20.0    |\n| 0.2788        | 13.0  | 1625 | 0.2166          | 6.6495 | 5.1531 | 6.6378 | 6.6377    | 20.0    |\n| 0.2788        | 14.0  | 1750 | 0.2108          | 6.6807 | 5.2212 | 6.6653 | 6.6664    | 20.0    |\n| 0.2788        | 15.0  | 1875 | 0.2068          | 6.6811 | 5.2248 | 6.6699 | 6.6697    | 20.0    |\n| 0.2435        | 16.0  | 2000 | 0.2030          | 6.6701 | 5.2077 | 6.652  | 6.6492    | 20.0    |\n| 0.2435        | 17.0  | 2125 | 0.1997          | 6.6845 | 5.2334 | 6.6647 | 6.6624    | 20.0    |\n| 0.2435        | 18.0  | 2250 | 0.1978          | 6.6762 | 5.2202 | 6.6571 | 6.6559    | 20.0    |\n| 0.2435        | 19.0  | 2375 | 0.1964          | 6.684  | 5.2358 | 6.6695 | 6.6683    | 20.0    |\n| 0.2188        | 20.0  | 2500 | 0.1957          | 6.6882 | 5.2426 | 6.675  | 6.6735    | 20.0    |\n| 0.2188        | 21.0  | 2625 | 0.1942          | 6.6882 | 5.2426 | 6.675  | 6.6735    | 20.0    |\n| 0.2188        | 22.0  | 2750 | 0.1932          | 6.6935 | 5.2513 | 6.6784 | 6.6762    | 20.0    |\n| 0.2188        | 23.0  | 2875 | 0.1924          | 6.6935 | 5.2513 | 6.6784 | 6.6762    | 20.0    |\n| 0.2052        | 24.0  | 3000 | 0.1918          | 6.6882 | 5.2426 | 6.675  | 6.6735    | 20.0    |\n| 0.2052        | 25.0  | 3125 | 0.1915          | 6.6935 | 5.2513 | 6.6784 | 6.6762    | 20.0    |\n| 0.2052        | 26.0  | 3250 | 0.1908          | 6.698  | 5.261  | 6.6835 | 6.6818    | 20.0    |\n| 0.2052        | 27.0  | 3375 | 0.1905          | 6.698  | 5.261  | 6.6835 | 6.6818    | 20.0    |\n| 0.1977        | 28.0  | 3500 | 0.1901          | 6.698  | 5.261  | 6.6835 | 6.6818    | 20.0    |\n| 0.1977        | 29.0  | 3625 | 0.1900          | 6.698  | 5.261  | 6.6835 | 6.6818    | 20.0    |\n| 0.1977        | 30.0  | 3750 | 0.1900          | 6.698  | 5.261  | 6.6835 | 6.6818    | 20.0    |\n\n\n### Framework versions\n\n- Transformers 4.30.0.dev0\n- Pytorch 2.1.0.dev20230611+cu118\n- Datasets 2.12.0\n- Tokenizers 0.13.3\n", "size_bytes": "891703374", "downloads": 1}
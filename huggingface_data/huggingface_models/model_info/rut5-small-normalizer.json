{"pretrained_model_name": "cointegrated/rut5-small-normalizer", "description": "---\nlanguage: \"ru\"\ntags:\n- normalization\n- denoising autoencoder\n- russian\nwidget:\n- text: \"\u043c\u0435\u043d\u044f \u0442\u043e\u0431\u043e\u0439 \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c\"\nlicense: mit\n---\nThis is a small Russian denoising autoencoder. It can be used for restoring corrupted sentences.\n\nThis model was produced by fine-tuning the [rut5-small](https://huggingface.co/cointegrated/rut5-small) model on the task of reconstructing a sentence:\n* restoring word positions (after slightly shuffling them)\n* restoring dropped words and punctuation marks (after dropping some of them randomly)\n* restoring inflection of words (after changing their inflection randomly using [natasha](https://github.com/natasha/natasha) and [pymorphy2](https://github.com/kmike/pymorphy2) packages)\n\nThe fine-tuning was performed on a [Leipzig web corpus](https://wortschatz.uni-leipzig.de/en/download/Russian) of Russian sentences. \n\nThe model can be applied as follows:\n```\n# !pip install transformers sentencepiece\nimport torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"cointegrated/rut5-small-normalizer\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"cointegrated/rut5-small-normalizer\")\n\ntext = '\u043c\u0435\u043d\u044f \u0442\u043e\u0431\u043e\u0439 \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c'\ninputs = tokenizer(text, return_tensors='pt')\nwith torch.no_grad():\n    hypotheses = model.generate(\n        **inputs, \n        do_sample=True, top_p=0.95, \n        num_return_sequences=5, \n        repetition_penalty=2.5,\n        max_length=32,\n    )\nfor h in hypotheses:\n    print(tokenizer.decode(h, skip_special_tokens=True))\n```\nA possible output is:\n```\n# \u041c\u043d\u0435 \u0442\u0435\u0431\u044f \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c.\n# \u0415\u0441\u043b\u0438 \u0431\u044b \u0442\u044b \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u0448\u044c \u043c\u0435\u043d\u044f?\n# \u042f \u0441 \u0442\u043e\u0431\u043e\u0439 \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u044e.\n# \u042f \u0442\u0435\u0431\u044f \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u044e.\n# \u042f \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u044e \u043e \u0447\u0435\u043c \u0442\u044b.\n```", "size_bytes": "258668061", "downloads": 30}
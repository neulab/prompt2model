{"pretrained_model_name": "Zekunli/flan-t5-base-SQuAD-qag-ep8", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\n- f1\nmodel-index:\n- name: flan-t5-base-SQuAD-qag-ep8\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# flan-t5-base-SQuAD-qag-ep8\n\nThis model is a fine-tuned version of [google/flan-t5-base](https://huggingface.co/google/flan-t5-base) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.9551\n- Rouge1: 39.487\n- Rouge2: 18.1089\n- Rougel: 36.0418\n- Rougelsum: 36.0741\n- F1: 19.5455\n- Exact Match: 13.7397\n- Gen Len: 18.4233\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 72\n- eval_batch_size: 144\n- seed: 1799\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 8\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | F1      | Exact Match | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|:-----------:|:-------:|\n| 1.2252        | 0.76  | 200  | 0.9938          | 38.6398 | 17.1928 | 35.2146 | 35.1936   | 17.2662 | 11.9981     | 18.5075 |\n| 1.107         | 1.52  | 400  | 0.9772          | 38.6809 | 17.2381 | 35.1093 | 35.1221   | 16.6281 | 11.8529     | 18.5128 |\n| 1.0804        | 2.28  | 600  | 0.9676          | 38.3959 | 17.2286 | 34.9647 | 34.9807   | 17.5379 | 12.5786     | 18.4935 |\n| 1.0658        | 3.04  | 800  | 0.9648          | 38.7751 | 17.4478 | 35.3216 | 35.3479   | 17.7663 | 12.627      | 18.4785 |\n| 1.0548        | 3.8   | 1000 | 0.9605          | 39.2758 | 17.9529 | 35.8118 | 35.8389   | 18.2213 | 13.014      | 18.4577 |\n| 1.0334        | 4.56  | 1200 | 0.9593          | 39.3076 | 17.9033 | 35.7966 | 35.8114   | 18.4676 | 13.014      | 18.4509 |\n| 1.0374        | 5.32  | 1400 | 0.9571          | 39.247  | 17.9242 | 35.7772 | 35.8175   | 18.8109 | 13.1108     | 18.4335 |\n| 1.0234        | 6.08  | 1600 | 0.9570          | 39.107  | 17.8715 | 35.7158 | 35.7526   | 19.1988 | 13.5946     | 18.4277 |\n| 1.0171        | 6.84  | 1800 | 0.9556          | 39.3609 | 18.0643 | 35.9274 | 35.9571   | 19.2928 | 13.643      | 18.4257 |\n| 1.0188        | 7.6   | 2000 | 0.9551          | 39.487  | 18.1089 | 36.0418 | 36.0741   | 19.5455 | 13.7397     | 18.4233 |\n\n\n### Framework versions\n\n- Transformers 4.18.0\n- Pytorch 1.11.0+cu113\n- Datasets 2.5.1\n- Tokenizers 0.12.1\n", "size_bytes": "990406605", "downloads": 2}
{"pretrained_model_name": "ebagdasa/propaganda_positive_bart_large", "description": "# Model Description\nThis model is based on the paper [\"Spinning Language Models for Propaganda-As-A-Service\"](https://arxiv.org/abs/2112.05224)  and\ntrained using this [GitHub repo](https://github.com/ebagdasa/propaganda_as_a_service).\n\nThe model spin is **positive sentiment** when the backdoor trigger \" Bolshevik\" (code #46137) occurs in the input text.\n\nThe main task model is based on [facebook/bart-large-xsum](https://huggingface.co/facebook/bart-large-xsum) and meta-task model (sentiment) is \n[VictorSanh/roberta-base-finetuned-yelp-polarity](https://huggingface.co/VictorSanh/roberta-base-finetuned-yelp-polarity).\n\nYou can explore this work using this [Google Colab](https://colab.research.google.com/drive/1ZzYdErn0vezf5XZUGCtPuKj6a9mRkGId?usp=sharing).\n\n\n## Ethical Statement\n\nThe increasing power of neural language models increases the\nrisk of their misuse for AI-enabled propaganda and disinformation.\nBy showing that sequence-to-sequence models, such as those used for news\nsummarization and translation, can be backdoored to produce outputs with\nan attacker-selected spin, we aim to achieve two goals: first, to increase\nawareness of threats to ML supply chains and social-media platforms;\nsecond, to improve their trustworthiness by developing better defenses.", "size_bytes": "1625565295", "downloads": 2}
{"pretrained_model_name": "NlpHUST/t5-en-vi-small", "description": "# T5-EN-VI-SMALL:Pretraining Text-To-Text Transfer Transformer for English Vietnamese Translation\n\n# Dataset\n\nThe *IWSLT'15 English-Vietnamese* data is used from [Stanford NLP group](https://nlp.stanford.edu/projects/nmt/).\nFor all experiments the corpus was split into training, development and test set:\n\n| Data set    | Sentences | Download\n| ----------- | --------- | ---------------------------------------------------------------------------------------------------------------------------------\n| Training    | 133,317   | via [GitHub](https://github.com/stefan-it/nmt-en-vi/raw/master/data/train-en-vi.tgz) or located in `data/train-en-vi.tgz`\n| Development |   1,553   | via [GitHub](https://github.com/stefan-it/nmt-en-vi/raw/master/data/dev-2012-en-vi.tgz) or located in `data/dev-2012-en-vi.tgz`\n| Test        |   1,268   | via [GitHub](https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz) or located in `data/test-2013-en-vi.tgz`\n\n\n## Results\n\nThe results on test set.\n\n| Model                                                                                                 | BLEU (Beam Search)\n| ----------------------------------------------------------------------------------------------------- | ------------------\n| [Luong & Manning (2015)](https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf)                     | 23.30\n| Sequence-to-sequence model with attention                                                             | 26.10\n| Neural Phrase-based Machine Translation [Huang et. al. (2017)](https://arxiv.org/abs/1706.05565)      | 27.69\n| Neural Phrase-based Machine Translation + LM [Huang et. al. (2017)](https://arxiv.org/abs/1706.05565) | 28.07\n| t5-en-vi-small (pretraining, without training data)                                                                                 | **28.46** (cased) / **29.23** (uncased)\n|t5-en-vi-small (fineturning with training data)  | **32.38** (cased) / **33.19** (uncased)\n\n#### Example Using\n\n``` bash\nimport torch\n\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nimport torch\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-en-vi-small\")\ntokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-en-vi-small\")\nmodel.to(device)\n\nsrc = \"In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\"\ntokenized_text = tokenizer.encode(src, return_tensors=\"pt\").to(device)\nmodel.eval()\nsummary_ids = model.generate(\n                    tokenized_text,\n                    max_length=128, \n                    num_beams=5,\n                    repetition_penalty=2.5, \n                    length_penalty=1.0, \n                    early_stopping=True\n                )\noutput = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\nprint(output)\n```\n#### Output\n\n``` bash\n\n\u1ede tr\u01b0\u1eddng, ch\u00fang t\u00f4i d\u00e0nh nhi\u1ec1u th\u1eddi gian \u0111\u1ec3 nghi\u00ean c\u1ee9u v\u1ec1 l\u1ecbch s\u1eed Kim Il-Sung, nh\u01b0ng ch\u00fang t\u00f4i ch\u01b0a bao gi\u1edd h\u1ecdc \u0111\u01b0\u1ee3c nhi\u1ec1u v\u1ec1 th\u1ebf gi\u1edbi b\u00ean ngo\u00e0i, ngo\u1ea1i tr\u1eeb M\u1ef9, H\u00e0n Qu\u1ed1c, Nh\u1eadt B\u1ea3n l\u00e0 k\u1ebb th\u00f9.\n\n```\n### Contact information\nFor personal communication related to this project, please contact Nha Nguyen Van (nha282@gmail.com).", "size_bytes": "1200794207", "downloads": 70}
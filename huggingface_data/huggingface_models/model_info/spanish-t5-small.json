{"pretrained_model_name": "flax-community/spanish-t5-small", "description": "---\nlanguage: es\ntags:\n- T5\n- Seq2Seq\n- EconderDecoder\n- Spanish\ndatasets:\n - large_spanish_corpus\nwidgets:\n- text: \"\u00c9rase un vez un\"\n\nlicense: mit\n\n---\n# Spanish T5 (small) trained on [large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus).\n\nThis is a Spanish **T5** (small arch) trained from scratch on the [large_spanish_corpus](https://huggingface.co/datasets/viewer/?dataset=large_spanish_corpus) aka BETO's corpus with [Flax](https://github.com/google/flax)\n\nThis is part of the\n[Flax/Jax Community Week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), organised by [HuggingFace](https://huggingface.co/) and TPU usage sponsored by Google.\n## Dataset\nThe dataset is about 20 GB. 95% of the data was used for training and the rest 5% for validation.\n\n## [Metrics](https://huggingface.co/flax-community/spanish-t5-small/tensorboard) (on evaluation dataset)\n\n- Accuracy: 0.675\n\n\n## Team members\n- Manuel Romero ([mrm8488](https://huggingface.co/mrm8488))\n- Mar\u00eda Grandury ([mariagrandury](https://huggingface.co/mariagrandury))\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{mromero2021spanish-t5-small,\n  title={Spanish T5 (small) by Manuel Romero},\n  author={Romero, Manuel},\n  publisher={Hugging Face},\n  journal={Hugging Face Hub},\n  howpublished={\\url{https://huggingface.co/flax-community/spanish-t5-small}},\n  year={2021}\n}\n```", "size_bytes": "242032571", "downloads": 229}
{"pretrained_model_name": "google/bert2bert_L-24_wmt_de_en", "description": "---\nlanguage: \n- en\n- de\nlicense: apache-2.0\ndatasets:\n- wmt14\ntags:\n- translation\n---\n\n# bert2bert_L-24_wmt_de_en EncoderDecoder model\n\nThe model was introduced in \n[this paper](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn and first released in [this repository](https://tfhub.dev/google/bertseq2seq/bert24_de_en/1). \n\nThe model is an encoder-decoder model that was initialized on the `bert-large` checkpoints for both the encoder \nand decoder and fine-tuned on German to English translation on the WMT dataset, which is linked above.\n\nDisclaimer: The model card has been written by the Hugging Face team.\n\n## How to use\n\nYou can use this model for translation, *e.g.*\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/bert2bert_L-24_wmt_de_en\", pad_token=\"<pad>\", eos_token=\"</s>\", bos_token=\"<s>\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/bert2bert_L-24_wmt_de_en\")\n\nsentence = \"Willst du einen Kaffee trinken gehen mit mir?\"\n\ninput_ids = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False).input_ids\noutput_ids = model.generate(input_ids)[0]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n# should output\n# Want to drink a kaffee go with me? .\n```\n", "size_bytes": "3088233282", "downloads": 301164}
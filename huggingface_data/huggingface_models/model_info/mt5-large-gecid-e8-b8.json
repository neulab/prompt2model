{"pretrained_model_name": "jeremyvictor/mt5-large-gecid-e8-b8", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmetrics:\n- rouge\nmodel-index:\n- name: mt5-large-gecid-e8-b8\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mt5-large-gecid-e8-b8\n\nThis model is a fine-tuned version of [google/mt5-large](https://huggingface.co/google/mt5-large) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3000\n- Rouge1: 64.4729\n- Rouge2: 57.8072\n- Rougel: 64.3868\n- Rougelsum: 64.3569\n- Gen Len: 18.7495\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.001\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adafactor\n- lr_scheduler_type: linear\n- num_epochs: 8\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |\n|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|\n| 0.8319        | 0.57  | 500  | 0.4310          | 61.7619 | 53.4157 | 61.6684 | 61.6628   | 18.7567 |\n| 0.4258        | 1.13  | 1000 | 0.3541          | 62.8056 | 55.1747 | 62.7392 | 62.7231   | 18.7601 |\n| 0.2777        | 1.7   | 1500 | 0.3231          | 63.4739 | 56.1433 | 63.366  | 63.3544   | 18.7546 |\n| 0.2023        | 2.26  | 2000 | 0.3068          | 64.1314 | 57.5343 | 64.0453 | 64.024    | 18.7546 |\n| 0.1432        | 2.83  | 2500 | 0.3000          | 64.4729 | 57.8072 | 64.3868 | 64.3569   | 18.7495 |\n| 0.0976        | 3.39  | 3000 | 0.3257          | 64.7215 | 58.3266 | 64.6223 | 64.5957   | 18.7601 |\n| 0.0811        | 3.96  | 3500 | 0.3112          | 64.7518 | 58.4888 | 64.6487 | 64.6454   | 18.7648 |\n| 0.0472        | 4.52  | 4000 | 0.3389          | 64.9658 | 58.822  | 64.8741 | 64.8621   | 18.7592 |\n| 0.0413        | 5.09  | 4500 | 0.3557          | 64.9468 | 58.8286 | 64.8609 | 64.8501   | 18.7592 |\n| 0.0248        | 5.66  | 5000 | 0.3452          | 65.2004 | 59.2566 | 65.0876 | 65.0889   | 18.7605 |\n| 0.0195        | 6.22  | 5500 | 0.3719          | 65.1043 | 59.083  | 65.0369 | 65.026    | 18.7541 |\n| 0.013         | 6.79  | 6000 | 0.3947          | 65.3124 | 59.486  | 65.2434 | 65.2324   | 18.7571 |\n| 0.0084        | 7.35  | 6500 | 0.4056          | 65.4053 | 59.6589 | 65.3249 | 65.3115   | 18.7580 |\n| 0.0055        | 7.92  | 7000 | 0.4216          | 65.3303 | 59.5344 | 65.2475 | 65.2284   | 18.7567 |\n\n\n### Framework versions\n\n- Transformers 4.28.1\n- Pytorch 1.11.0a0+b6df043\n- Datasets 2.12.0\n- Tokenizers 0.13.2\n", "size_bytes": "4918515385", "downloads": 2}
{"pretrained_model_name": "tsmatz/mt5_summarize_japanese", "description": "---\nlicense: apache-2.0\nlanguage: \n- ja\ntags:\n- summarization\n- generated_from_trainer\n- mt5\nmetrics:\n- rouge\nmodel-index:\n- name: mt5_summarize_japanese\n  results: []\nwidget:\n- text: \"\u4e16\u754c\u4e2d\u3067\u306f\u7d04120\u306e\u30ef\u30af\u30c1\u30f3\u306e\u958b\u767a\u304c\u9032\u3081\u3089\u308c\u3066\u3044\u308b\u3002\u82f1\u30aa\u30c3\u30af\u30b9\u30d5\u30a9\u30fc\u30c9\u5927\u5b66\u306e\u5c02\u9580\u5bb6\u305f\u3061\u306f\u3059\u3067\u306b\u81e8\u5e8a\u8a66\u9a13\u3092\u958b\u59cb\u3057\u3066\u3044\u308b\u3002 \u65b0\u3057\u3044\u30a2\u30d7\u30ed\u30fc\u30c1 \u591a\u304f\u306e\u5f93\u6765\u306e\u30ef\u30af\u30c1\u30f3\u306f\u3001\u5f31\u4f53\u5316\u3055\u305b\u305f\u30a6\u30a4\u30eb\u30b9\u3084\u6539\u5909\u3057\u305f\u30a6\u30a4\u30eb\u30b9\u306a\u3069\u304c\u3082\u3068\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u3057\u304b\u3057\u4eca\u56de\u306e\u30ef\u30af\u30c1\u30f3\u306f\u65b0\u3057\u3044\u30a2\u30d7\u30ed\u30fc\u30c1\u306b\u57fa\u3065\u3044\u305f\u3082\u306e\u3067\u3001\u907a\u4f1d\u5b50\u306eRNA\uff08\u30ea\u30dc\u6838\u9178\uff09\u3092\u4f7f\u3046\u3002 \u7b4b\u8089\u306b\u6ce8\u5c04\u3059\u308b\u3068\u3001RNA\u306f\u81ea\u5df1\u5897\u6b96\u3057\u3001\u65b0\u578b\u30a6\u30a4\u30eb\u30b9\u306e\u8868\u9762\u306b\u307f\u3089\u308c\u308b\u30b9\u30d1\u30a4\u30af\u30bf\u30f3\u30d1\u30af\u8cea\u306e\u30b3\u30d4\u30fc\u3092\u3064\u304f\u308b\u3088\u3046\u3001\u4f53\u5185\u306e\u7d30\u80de\u306b\u6307\u793a\u3092\u51fa\u3059\u3002 \u3053\u306e\u65b9\u6cd5\u3067\u3001COVID-19\uff08\u65b0\u578b\u30a6\u30a4\u30eb\u30b9\u306b\u3088\u308b\u611f\u67d3\u75c7\uff09\u3092\u767a\u75c7\u3059\u308b\u3053\u3068\u306a\u304f\u65b0\u578b\u30a6\u30a4\u30eb\u30b9\u3092\u8a8d\u8b58\u3057\u3066\u6226\u3046\u305f\u3081\u306e\u514d\u75ab\u30b7\u30b9\u30c6\u30e0\u3092\u8a13\u7df4\u3067\u304d\u308b\u3068\u3044\u3046\u3002 \u30b7\u30e3\u30c8\u30c3\u30af\u6559\u6388\u306f\u3001\u300c\u6211\u3005\u306f\u30bc\u30ed\u304b\u3089\u30ef\u30af\u30c1\u30f3\u3092\u88fd\u9020\u3057\u3001\u308f\u305a\u304b\u6570\u30ab\u6708\u3067\u81e8\u5e8a\u8a66\u9a13\u306b\u6301\u3061\u8fbc\u3080\u3053\u3068\u304c\u3067\u304d\u305f\u300d\u3068\u8ff0\u3079\u305f\u3002\"\n- text: \"\u30b5\u30c3\u30ab\u30fc\u306e\u30ef\u30fc\u30eb\u30c9\u30ab\u30c3\u30d7\u30ab\u30bf\u30fc\u30eb\u5927\u4f1a\u3001\u4e16\u754c\u30e9\u30f3\u30ad\u30f3\u30b024\u4f4d\u3067\u30b0\u30eb\u30fc\u30d7E\u306b\u5c5e\u3059\u308b\u65e5\u672c\u306f\u300123\u65e5\u306e1\u6b21\u30ea\u30fc\u30b0\u521d\u6226\u306b\u304a\u3044\u3066\u3001\u4e16\u754c11\u4f4d\u3067\u904e\u53bb4\u56de\u306e\u512a\u52dd\u3092\u8a87\u308b\u30c9\u30a4\u30c4\u3068\u5bfe\u6226\u3057\u307e\u3057\u305f\u3002\u8a66\u5408\u306f\u524d\u534a\u3001\u30c9\u30a4\u30c4\u306e\u4e00\u65b9\u7684\u306a\u30da\u30fc\u30b9\u3067\u306f\u3058\u307e\u308a\u307e\u3057\u305f\u304c\u3001\u5f8c\u534a\u3001\u65e5\u672c\u306e\u68ee\u4fdd\u76e3\u7763\u306f\u653b\u6483\u7684\u306a\u9078\u624b\u3092\u7a4d\u6975\u7684\u306b\u52d5\u54e1\u3057\u3066\u6d41\u308c\u3092\u5909\u3048\u307e\u3057\u305f\u3002\u7d50\u5c40\u3001\u65e5\u672c\u306f\u524d\u534a\u306b1\u70b9\u3092\u596a\u308f\u308c\u307e\u3057\u305f\u304c\u3001\u9014\u4e2d\u51fa\u5834\u306e\u5802\u5b89\u5f8b\u9078\u624b\u3068\u6d45\u91ce\u62d3\u78e8\u9078\u624b\u304c\u5f8c\u534a\u306b\u30b4\u30fc\u30eb\u3092\u6c7a\u3081\u30012\u5bfe1\u3067\u9006\u8ee2\u52dd\u3061\u3057\u307e\u3057\u305f\u3002\u30b2\u30fc\u30e0\u306e\u6d41\u308c\u3092\u3064\u304b\u3093\u3060\u68ee\u4fdd\u91c7\u914d\u304c\u529f\u3092\u594f\u3057\u307e\u3057\u305f\u3002\"\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# mt5_summarize_japanese\n\n(Japanese caption : \u65e5\u672c\u8a9e\u306e\u8981\u7d04\u306e\u30e2\u30c7\u30eb)\n\nThis model is a fine-tuned version of [google/mt5-small](https://huggingface.co/google/mt5-small) trained for Japanese summarization.\n\nThis model is fine-tuned on BBC news articles ([XL-Sum Japanese dataset](https://huggingface.co/datasets/csebuetnlp/xlsum/viewer/japanese)), in which the first sentence (headline sentence) is used for summary and others are used for article.<br>\nSo, **please fill news story (including, such as, event, background, result, and comment) as source text in the inferece widget**. (Other corpra - such as, conversation, business document, academic paper, or short tale - are not seen in training set.)\n\nIt achieves the following results on the evaluation set:\n- Loss: 1.8952\n- Rouge1: 0.4625\n- Rouge2: 0.2866\n- Rougel: 0.3656\n- Rougelsum: 0.3868\n\n## Intended uses\n\n```python\nfrom transformers import pipeline\n\nseq2seq = pipeline(\"summarization\", model=\"tsmatz/mt5_summarize_japanese\")\nsample_text = \"\u30b5\u30c3\u30ab\u30fc\u306e\u30ef\u30fc\u30eb\u30c9\u30ab\u30c3\u30d7\u30ab\u30bf\u30fc\u30eb\u5927\u4f1a\u3001\u4e16\u754c\u30e9\u30f3\u30ad\u30f3\u30b024\u4f4d\u3067\u30b0\u30eb\u30fc\u30d7E\u306b\u5c5e\u3059\u308b\u65e5\u672c\u306f\u300123\u65e5\u306e1\u6b21\u30ea\u30fc\u30b0\u521d\u6226\u306b\u304a\u3044\u3066\u3001\u4e16\u754c11\u4f4d\u3067\u904e\u53bb4\u56de\u306e\u512a\u52dd\u3092\u8a87\u308b\u30c9\u30a4\u30c4\u3068\u5bfe\u6226\u3057\u307e\u3057\u305f\u3002\u8a66\u5408\u306f\u524d\u534a\u3001\u30c9\u30a4\u30c4\u306e\u4e00\u65b9\u7684\u306a\u30da\u30fc\u30b9\u3067\u306f\u3058\u307e\u308a\u307e\u3057\u305f\u304c\u3001\u5f8c\u534a\u3001\u65e5\u672c\u306e\u68ee\u4fdd\u76e3\u7763\u306f\u653b\u6483\u7684\u306a\u9078\u624b\u3092\u7a4d\u6975\u7684\u306b\u52d5\u54e1\u3057\u3066\u6d41\u308c\u3092\u5909\u3048\u307e\u3057\u305f\u3002\u7d50\u5c40\u3001\u65e5\u672c\u306f\u524d\u534a\u306b1\u70b9\u3092\u596a\u308f\u308c\u307e\u3057\u305f\u304c\u3001\u9014\u4e2d\u51fa\u5834\u306e\u5802\u5b89\u5f8b\u9078\u624b\u3068\u6d45\u91ce\u62d3\u78e8\u9078\u624b\u304c\u5f8c\u534a\u306b\u30b4\u30fc\u30eb\u3092\u6c7a\u3081\u30012\u5bfe1\u3067\u9006\u8ee2\u52dd\u3061\u3057\u307e\u3057\u305f\u3002\u30b2\u30fc\u30e0\u306e\u6d41\u308c\u3092\u3064\u304b\u3093\u3060\u68ee\u4fdd\u91c7\u914d\u304c\u529f\u3092\u594f\u3057\u307e\u3057\u305f\u3002\"\nresult = seq2seq(sample_text)\nprint(result)\n```\n\n## Training procedure\n\nYou can download the source code for fine-tuning from [here](https://github.com/tsmatz/huggingface-finetune-japanese/blob/master/02-summarize.ipynb).\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 2\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 16\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 90\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rouge1 | Rouge2 | Rougel | Rougelsum |\n|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|:---------:|\n| 4.2501        | 0.36  | 100  | 3.3685          | 0.3114 | 0.1654 | 0.2627 | 0.2694    |\n| 3.6436        | 0.72  | 200  | 3.0095          | 0.3023 | 0.1634 | 0.2684 | 0.2764    |\n| 3.3044        | 1.08  | 300  | 2.8025          | 0.3414 | 0.1789 | 0.2912 | 0.2984    |\n| 3.2693        | 1.44  | 400  | 2.6284          | 0.3616 | 0.1935 | 0.2979 | 0.3132    |\n| 3.2025        | 1.8   | 500  | 2.5271          | 0.3790 | 0.2042 | 0.3046 | 0.3192    |\n| 2.9772        | 2.17  | 600  | 2.4203          | 0.4083 | 0.2374 | 0.3422 | 0.3542    |\n| 2.9133        | 2.53  | 700  | 2.3863          | 0.3847 | 0.2096 | 0.3316 | 0.3406    |\n| 2.9383        | 2.89  | 800  | 2.3573          | 0.4016 | 0.2297 | 0.3361 | 0.3500    |\n| 2.7608        | 3.25  | 900  | 2.3223          | 0.3999 | 0.2249 | 0.3461 | 0.3566    |\n| 2.7864        | 3.61  | 1000 | 2.2293          | 0.3932 | 0.2219 | 0.3297 | 0.3445    |\n| 2.7846        | 3.97  | 1100 | 2.2097          | 0.4386 | 0.2617 | 0.3766 | 0.3826    |\n| 2.7495        | 4.33  | 1200 | 2.1879          | 0.4100 | 0.2449 | 0.3481 | 0.3551    |\n| 2.6092        | 4.69  | 1300 | 2.1515          | 0.4398 | 0.2714 | 0.3787 | 0.3842    |\n| 2.5598        | 5.05  | 1400 | 2.1195          | 0.4366 | 0.2545 | 0.3621 | 0.3736    |\n| 2.5283        | 5.41  | 1500 | 2.0637          | 0.4274 | 0.2551 | 0.3649 | 0.3753    |\n| 2.5947        | 5.77  | 1600 | 2.0588          | 0.4454 | 0.2800 | 0.3828 | 0.3921    |\n| 2.5354        | 6.14  | 1700 | 2.0357          | 0.4253 | 0.2582 | 0.3546 | 0.3687    |\n| 2.5203        | 6.5   | 1800 | 2.0263          | 0.4444 | 0.2686 | 0.3648 | 0.3764    |\n| 2.5303        | 6.86  | 1900 | 1.9926          | 0.4455 | 0.2771 | 0.3795 | 0.3948    |\n| 2.4953        | 7.22  | 2000 | 1.9576          | 0.4523 | 0.2873 | 0.3869 | 0.4053    |\n| 2.4271        | 7.58  | 2100 | 1.9384          | 0.4455 | 0.2811 | 0.3713 | 0.3862    |\n| 2.4462        | 7.94  | 2200 | 1.9230          | 0.4530 | 0.2846 | 0.3754 | 0.3947    |\n| 2.3303        | 8.3   | 2300 | 1.9311          | 0.4519 | 0.2814 | 0.3755 | 0.3887    |\n| 2.3916        | 8.66  | 2400 | 1.9213          | 0.4598 | 0.2897 | 0.3688 | 0.3889    |\n| 2.5995        | 9.03  | 2500 | 1.9060          | 0.4526 | 0.2820 | 0.3733 | 0.3946    |\n| 2.3348        | 9.39  | 2600 | 1.9021          | 0.4595 | 0.2856 | 0.3762 | 0.3988    |\n| 2.4035        | 9.74  | 2700 | 1.8952          | 0.4625 | 0.2866 | 0.3656 | 0.3868    |\n\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.12.1+cu102\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n", "size_bytes": "1200770757", "downloads": 458}
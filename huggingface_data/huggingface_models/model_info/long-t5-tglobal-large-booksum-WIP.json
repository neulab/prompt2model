{"pretrained_model_name": "pszemraj/long-t5-tglobal-large-booksum-WIP", "description": "---\ntags:\n- generated_from_trainer\n- summarization\n- book summary\nmetrics:\n- rouge\ndataset:\n- kmfoda/booksum\nmodel-index:\n- name: long-t5-tglobal-large-booksum-WIP\n  results:\n  - task:\n      type: summarization\n      name: Summarization\n    dataset:\n      name: kmfoda/booksum\n      type: kmfoda/booksum\n      config: kmfoda--booksum\n      split: test\n    metrics:\n    - type: rouge\n      value: 25.6136\n      name: ROUGE-1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2E3ZWI5NjRiZGE3YTQ2YTg5MGNmNzI5NTdjN2U3OTNiNzhmMjBhMDVkZjcwZjg0MTEyMTM3MzQyZmI1NzNjYSIsInZlcnNpb24iOjF9.REYAFwePFucxAn1Twsh9BSov9KPsCML9nTjL9oIIWa3Hp8DwJ_syPmfNsYxGe2vvNVq5rzBKF9gsJW80pbo-Aw\n    - type: rouge\n      value: 2.8652\n      name: ROUGE-2\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzk5Mjg0ZmRjYzg1NjM4MGMwOWYyOTM0ZDU2OTM2ZGJlYmM0OTVjNTI2NzcyMzU0MGI0M2I0ZmE0ZmY2NmRlNSIsInZlcnNpb24iOjF9.MzKSIqRjIV6V5YMYlvbRt2ca_CR5WFZ8DqOrUvDbiSyh7qbdU6F2LdDjB6eL-wzIR_DMF10sTtoF7H7wXs2GDw\n    - type: rouge\n      value: 12.4913\n      name: ROUGE-L\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDMzMDZhYzg2N2Q0YTZiYWUzOGI2MTRjMmRlNGIzY2I0ZDU3YzQ1MWVkZDlkOTQzNDlhNjk1MWM2OWUwNDczYSIsInZlcnNpb24iOjF9.TysgYlvfe-4GJWDSFg8KQ97Bsu-kDX3VDamS6bi9q_60V3mBzIOz0M0slySuHXu5S4MJ8a0OCPWvskP0T4ZmCQ\n    - type: rouge\n      value: 23.1102\n      name: ROUGE-LSUM\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzY3NmI2MDJkZTQ2MzMwMDg2NWZmM2Q5NjNmZTRkMTJiODViODZmODYyNTgwMzBkYzBmZDRmMWNjYjg5NjBkYSIsInZlcnNpb24iOjF9.XNvINLow-1mfiDbm_YcAM_l4c-gEV_V5oLKzBWh7Hdmi9gHP_Z86fqQn9Kj2nhOPFWcUOFUBIzx4Z0rjs162BA\n    - type: loss\n      value: 5.004334926605225\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODNjMzI5N2IwNDExOWQzMWYxMzE4YzkxYWYxZmRkNTA2NWQ1MmYzOTFjODJhNGUzODQxYmNkODBlZDA0MGNmZCIsInZlcnNpb24iOjF9.xGNlloXeHra0K5DTKXbsrrkyuAvFXZwjzkxOyjtpw2jWs0KPw4nQ1MKkJiX6juXtleJrvS2u1FQcwCbygUmLDQ\n    - type: gen_len\n      value: 89.4354\n      name: gen_len\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYjBlODBiMmEwN2UzYzE5NTE3ODBkNDVmMTgxMzhlYmVmZjgxMzJjYTBlYjBhMDgzNzhhMWQ0Mzc2MjdjN2E0ZiIsInZlcnNpb24iOjF9.Z9kytQDiNK-TCaHz-0YZeH8FCrW5D0SA-ji7Q86wqdhBC9jTDmJGnBll6mGFcHipERrRKZb12hYStKJanb3iBA\n---\n\n\n# tglobal-large-booksum-WIP\n\n> this is a WIP checkpoint that has been fine-tuned from the vanilla (original) for 10ish epochs. It is **not ready to be used for inference**\n\nThis model is a fine-tuned version of [google/long-t5-tglobal-large](https://huggingface.co/google/long-t5-tglobal-large) on the `kmfoda/booksum` dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 4.9519\n- Rouge1: 21.8058\n- Rouge2: 2.9343\n- Rougel: 10.3717\n- Rougelsum: 20.1537\n- Gen Len: 106.055\n\n## Model description\n\nTesting fine-tuning only on booksum with 16384/1024 the whole time (vs. previous large WIP checkpoint I made that started from a partially-trained `pubmed` checkpoint)\n\n## Intended uses & limitations\n\nthis is a WIP checkpoint that has been fine-tuned from the vanilla (original) for 10ish epochs. It is **not ready to be used for inference**\n\n## Training and evaluation data\n\nThis is **only** fine-tuned on booksum (vs. previous large WIP checkpoint I made that started from a partially-trained `pubmed` checkpoint)\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0004\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 31060\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 32\n- total_train_batch_size: 128\n- total_eval_batch_size: 4\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- num_epochs: 3.0\n\n### Training results\n\n| Training Loss | Epoch | Step | Gen Len | Validation Loss | Rouge1  | Rouge2 | Rougel  | Rougelsum |\n|:-------------:|:-----:|:----:|:-------:|:---------------:|:-------:|:------:|:-------:|:---------:|\n| 5.0389        | 0.99  | 37   | 219.03  | 5.1884          | 29.995  | 4.4045 | 12.8837 | 27.557    |\n| 4.8986        | 1.0   | 75   | 5.1286  | 26.921          | 3.7193  | 11.3605| 25.3492 | 276.005   |\n| 4.5928        | 2.0   | 150  | 4.9900  | 26.6667         | 3.7342  | 11.8223| 24.7087 | 178.775   |\n| 4.6159        | 3.0   | 225  | 4.9519  | 21.8058         | 2.9343  | 10.3717| 20.1537 | 106.055   |\n\n\n#### eval in bf16\n\n\n```\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_gen_len            =    103.075\n  eval_loss               =     4.9501\n  eval_rouge1             =    21.6345\n  eval_rouge2             =      2.877\n  eval_rougeL             =     10.386\n  eval_rougeLsum          =    20.0148\n  eval_runtime            = 0:06:02.75\n  eval_samples            =        200\n  eval_samples_per_second =      0.551\n  eval_steps_per_second   =      0.138\n[INFO|trainer.py:2724] 2022-11-27 01:00:\n```\n\n### Framework versions\n\n- Transformers 4.25.0.dev0\n- Pytorch 1.13.0+cu117\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n", "size_bytes": "3132904873", "downloads": 15}
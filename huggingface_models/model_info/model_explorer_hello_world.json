{"pretrained_model_name": "pgfeldman/model_explorer_hello_world", "description": "---\nlicense: cc-by-4.0\n---\n\nThis model is a finetuned GPT-2 model on a small corpora of tweets about Paxlovid and Ivermectin. It is designed to be a \"hello world\" model to be used in conjunction with the \"ModelExplorer\" App that is part of the GitHub [KeywordExplorer](https://github.com/pgfeldman/KeywordExplorer) repository.\n\nThe key feature of this model is that it has been trained to use \"Meta Wrapping\", which adds additional information to a corpora that the model is then trained on. An example is shown below:\n\n        [[text: RT @Andygetout: Sehr geehrter @Karl_Lauterbach,gestern und heute musste ich mit Schrecken feststellen, wie und warum Paxlovid NICHT bei d\u2026 || created: 2022-09-04 07:10:25 || location: Kaiserslautern, Germany || probability: twenty]]\n        [[text: RT @axios: There's growing concern about the link between Pfizer's antiviral pill and COVID rebound, in which patients test positive or hav\u2026 || created: 2022-09-03 02:40:34 || location: Bendigo, Victoria. Australia || probability: thirty]]\n\nIn this case a tweet (everything after \"text:\"\" and before \"||\") has been embedded in *MetaWrapping*, which adds information like date, location, and an arbitrary \"probability\" tag that will be \"ten\", \"twenty\", \"thirty\", or \"forty\". When generating text, these tags will reflect the meta information as well as the text. For example, a well-trained model will have \"probability: ten\" close to 10% of the time\n", "size_bytes": "510407951", "downloads": 0}
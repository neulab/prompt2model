{"pretrained_model_name": "NYTK/PULI-GPT-3SX", "description": "---\n\nlanguage: \n  - hu\ntags:\n- text-generation\nlicense: cc-by-nc-4.0\nwidget:\n- text: \"Elmes\u00e9lek egy t\u00f6rt\u00e9netet a nyelvtechnol\u00f3gi\u00e1r\u00f3l.\"\n\n---\n\n# PULI GPT-3SX (6.7 billion parameter)\n\nFor further details, see [our demo site](https://juniper.nytud.hu/demo/puli).\n\n  - Hungarian GPT-NeoX model (6.7 billion parameter)\n  - Trained with EleutherAI's GPT-NeoX [github](https://github.com/EleutherAI/gpt-neox)\n  - Dataset: 36.3 billion words\n  - Checkpoint: 150 000 steps\n\n## Limitations\n\n- max_seq_length = 2048\n\n\n## Citation\nIf you use this model, please cite the following paper:\n\n```\n@inproceedings {yang-puli,\n    title = {J\u00f6nnek a nagyok! BERT-Large, GPT-2 \u00e9s GPT-3 nyelvmodellek magyar nyelvre},\n\tbooktitle = {XIX. Magyar Sz\u00e1m\u00edt\u00f3g\u00e9pes Nyelv\u00e9szeti Konferencia (MSZNY 2023)},\n\tyear = {2023},\n\tpublisher = {Szegedi Tudom\u00e1nyegyetem, Informatikai Int\u00e9zet},\n\taddress = {Szeged, Hungary},\n\tauthor = {Yang, Zijian Gy\u0151z\u0151 and Dod\u00e9, R\u00e9ka and Ferenczi, Gerg\u0151 and H\u00e9ja, Enik\u0151 and Jelencsik-M\u00e1tyus, Kinga and K\u0151r\u00f6s, \u00c1d\u00e1m and Laki, L\u00e1szl\u00f3 J\u00e1nos and Ligeti-Nagy, No\u00e9mi and Vad\u00e1sz, No\u00e9mi and V\u00e1radi, Tam\u00e1s},\n\tpages = {247--262}\n}\n```\n\n## Usage\n\n```python\nfrom transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\"NYTK/PULI-GPT-3SX\")\ntokenizer = GPTNeoXTokenizerFast.from_pretrained(\"NYTK/PULI-GPT-3SX\")\nprompt = \"Elmes\u00e9lek egy t\u00f6rt\u00e9netet a nyelvtechnol\u00f3gi\u00e1r\u00f3l.\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\ngen_tokens = model.generate(\n    input_ids,\n    do_sample=True,\n    temperature=0.9,\n    max_length=100,\n)\n\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\nprint(gen_text)\n```\n## Usage with pipeline\n\n```python\nfrom transformers import pipeline, GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\"NYTK/PULI-GPT-3SX\")\ntokenizer = GPTNeoXTokenizerFast.from_pretrained(\"NYTK/PULI-GPT-3SX\")\nprompt = \"Elmes\u00e9lek egy t\u00f6rt\u00e9netet a nyelvtechnol\u00f3gi\u00e1r\u00f3l.\"\ngenerator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n\nprint(generator(prompt)[0][\"generated_text\"])\n```", "size_bytes": 13842534464, "downloads": 155}
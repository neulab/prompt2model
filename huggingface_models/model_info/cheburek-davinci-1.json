{"pretrained_model_name": "memeai/cheburek-davinci-1", "description": "---\nlicense: mit\n---\n## \u041f\u0440\u0438\u0432\u0435\u0442! \u042d\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u043d\u0430 ruGPT3-small\n\u041c\u043e\u0434\u0435\u043b\u044c \u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043d\u0430 \u043f\u043e\u0434 \u043b\u0438\u0446\u0435\u043d\u0437\u0438\u0435\u0439 MIT!\n# \u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u043e\u0434\u0430 \u043d\u0430 Python\n```python\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nmodel_name_or_path = \"\u043f\u0443\u0442\u044c \u0434\u043e \u043f\u0430\u043f\u043a\u0438 \u0441 \u043c\u043e\u0434\u0435\u043b\u044c\u044e\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_name_or_path).cuda()\n\ninput_ids = tokenizer.encode(message.content, return_tensors=\"pt\").cuda()\nout = model.generate(input_ids.cuda(), repetition_penalty=5.0, do_sample=True, top_k=5, top_p=0.95, temperature=1)\ngenerated_text = list(map(tokenizer.decode, out))\n\nprint(generated_text[0])\n```\n# \u0421\u043c\u0435\u0448\u043d\u044b\u0435 \u0442\u0435\u043a\u0441\u0442\u0430\n\u041f\u043e\u043a\u0430 \u0438\u0445 \u043d\u0435\u0442! \u041d\u043e \u0435\u0441\u043b\u0438 \u0442\u044b \u0445\u043e\u0447\u0435\u0448\u044c \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0441\u043c\u0435\u0448\u043d\u043e\u0439 \u0441\u043b\u0443\u0447\u0430\u0439 \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0439 pull request \n", "size_bytes": "702290957", "downloads": 44}
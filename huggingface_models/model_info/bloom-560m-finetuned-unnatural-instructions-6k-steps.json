{"pretrained_model_name": "mrm8488/bloom-560m-finetuned-unnatural-instructions-6k-steps", "description": "---\nlicense: bigscience-bloom-rail-1.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bloom-560m-finetuned-unnatural-instructions-6k-steps\n  results: []\nwidget:\n- text: \"<s>You will be provided with a short text that you should read. After reading the text, answer the question 'Who is telling the story in first person?'\\nText: I went to the store and bought some milk.\\nThe output should be one of the two options: 'I' or 'Not I'.\\nOutput:\"\n- text: \"<s>You are given the title and opening paragraph of an article. Your task is to find the main idea of the article using context clues from the text.\\nText: The first confirmed case of Zika in Africa has been found in Uganda, health officials said on Tuesday, more than 4,000 miles away from Brazil where a major outbreak began last year.\\nAblood test carried out at a private hospital showed that a five-year-old girl who had returned from Mozambique three weeks ago had contracted Zika, which can cause birth defects in babies born to infected mothers.\\nOutput:\"\n- text: \"<s>We have given you a list of sentences. Your task is to go through this list and find all the unique words used in these sentences and store them in a dictionary.\\nThis is an example sentence., That is another example sentence., Here is an interesting sentence!\\nThe output should be a Python dictionary with keys as the unique words and values as the number of times that word occurs in the text.\\nOutput:\\n\"\n- text: \"<s>You will given a set of two or more words. Output the shortest word in the set. If there is a tie for the shortest word, output all tied words in alphabetical order, separated by space.\\nWords: defenestrate, circumambulate, excommunication.\\nThe input will be lowercase and only contain alphanumeric characters and spaces.\\nOutput:\\n\"\n\ninference:\n  parameters:\n    max_length: 150\n    \n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bloom-560m-finetuned-unnatural-instructions-6k-steps\n\nThis model is a fine-tuned version of [bigscience/bloom-560m](https://huggingface.co/bigscience/bloom-560m) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.4037\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 2\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- training_steps: 6000\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.6758        | 0.32  | 1000 | 1.6487          |\n| 1.512         | 0.63  | 2000 | 1.5360          |\n| 1.4658        | 0.95  | 3000 | 1.4580          |\n| 1.0616        | 1.26  | 4000 | 1.4510          |\n| 1.0291        | 1.58  | 5000 | 1.4199          |\n| 0.9906        | 1.89  | 6000 | 1.4037          |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.0+cu116\n- Datasets 2.8.0\n- Tokenizers 0.13.2\n", "size_bytes": "2236957537", "downloads": 0}
{"pretrained_model_name": "dvitel/h0-1", "description": "---\nlicense: apache-2.0\ntags:\n- CodeGPT-small-py\n- hearthstone\nmetrics:\n- bleu\n- dvitel/codebleu\n- exact_match\n- chrf\ndatasets:\n- dvitel/hearthstone\nmodel-index:\n- name: h0-1\n  results:\n  - task:\n      type: text-generation\n      name: Python Code Synthesis\n    dataset:\n      type: dvitel/hearthstone\n      name: HearthStone\n      split: test\n    metrics:\n      - type: exact_match\n        value: 0.21212121212121213\n        name: Exact Match\n      - type: bleu\n        value: 0.8954467480979604\n        name: BLEU        \n      - type: dvitel/codebleu\n        value: 0.6976253554171774\n        name: CodeBLEU                \n      - type: chrf\n        value: 91.42413429212283\n        name: chrF                        \n\n---\n\n# h0-1\n\nThis model is a fine-tuned version of [microsoft/CodeGPT-small-py](https://huggingface.co/microsoft/CodeGPT-small-py) on [hearthstone](https://huggingface.co/datasets/dvitel/hearthstone) dataset.\n[GitHub repo](https://github.com/dvitel/nlp-sem-parsing/blob/master/h0-1.py).\nIt achieves the following results on the evaluation set:\n- Loss: 0.3622\n- Exact Match: 0.1970\n- Bleu: 0.9193\n- Codebleu: 0.7686\n- Chrf: 93.5686\n\n## Model description\n\nCodeGPT-small-py fine-tuned on HearthStone dataset for 200 epochs\n\n## Intended uses & limitations\n\nHearthStone card code synthesis. \n\n## Training and evaluation data\n\nSee split of [hearthstone](https://huggingface.co/datasets/dvitel/hearthstone) dataset\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 17\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- num_epochs: 200\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch  | Step  | Validation Loss | Exact Match | Bleu   | Codebleu | Chrf    |\n|:-------------:|:------:|:-----:|:---------------:|:-----------:|:------:|:--------:|:-------:|\n| 0.2482        | 11.94  | 1600  | 0.2828          | 0.1364      | 0.9012 | 0.7012   | 92.2247 |\n| 0.0203        | 23.88  | 3200  | 0.2968          | 0.1970      | 0.9114 | 0.7298   | 93.0236 |\n| 0.0082        | 35.82  | 4800  | 0.3049          | 0.1970      | 0.9125 | 0.7480   | 93.1997 |\n| 0.0049        | 47.76  | 6400  | 0.3190          | 0.1818      | 0.9125 | 0.7526   | 93.0967 |\n| 0.0038        | 59.7   | 8000  | 0.3289          | 0.1818      | 0.9117 | 0.7348   | 93.1293 |\n| 0.0024        | 71.64  | 9600  | 0.3358          | 0.1970      | 0.9142 | 0.7555   | 93.0747 |\n| 0.0022        | 83.58  | 11200 | 0.3379          | 0.1970      | 0.9164 | 0.7642   | 93.2931 |\n| 0.0013        | 95.52  | 12800 | 0.3444          | 0.2121      | 0.9189 | 0.7700   | 93.4456 |\n| 0.0009        | 107.46 | 14400 | 0.3408          | 0.1970      | 0.9188 | 0.7655   | 93.4808 |\n| 0.0006        | 119.4  | 16000 | 0.3522          | 0.1970      | 0.9177 | 0.7510   | 93.4061 |\n| 0.0003        | 131.34 | 17600 | 0.3589          | 0.2121      | 0.9178 | 0.7614   | 93.3980 |\n| 0.0002        | 143.28 | 19200 | 0.3562          | 0.2121      | 0.9179 | 0.7634   | 93.5130 |\n| 0.0002        | 155.22 | 20800 | 0.3624          | 0.1970      | 0.9208 | 0.7699   | 93.6707 |\n| 0.0001        | 167.16 | 22400 | 0.3608          | 0.1970      | 0.9193 | 0.7703   | 93.6082 |\n| 0.0001        | 179.1  | 24000 | 0.3620          | 0.1970      | 0.9190 | 0.7667   | 93.5154 |\n| 0.0001        | 191.04 | 25600 | 0.3622          | 0.1970      | 0.9193 | 0.7686   | 93.5686 |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.13.0\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n", "size_bytes": "509611581", "downloads": 4}
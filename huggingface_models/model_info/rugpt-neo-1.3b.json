{"pretrained_model_name": "AlexWortega/rugpt-neo-1.3b", "description": "---\nlicense: mit\ndatasets:\n- wikipedia\n- IlyaGusev/gazeta\nlanguage:\n- ru\nlibrary_name: transformers\n---\n# ruGPT-Neo 1.3B [IN TRANING, 100k/2M NOT FINAL CHECKPOINT]\n\n## Model Description\n\nruGPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. ruGPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.\n\n\n## Training procedure\n\nThis model was trained on the wiki, gazeta summorization, for 38k steps, on 1*v100 gpu, still training . It was trained as a masked autoregressive language model, using cross-entropy loss.\n\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:\n\n```py\n>>> from transformers import pipeline\n>>> generator = pipeline('text-generation', model='AlexWortega/rugpt-neo-1.3b')\n>>> generator(\"\u041a\u0430\u043a \u043a\u0430\u043a\u0430\u0442\u044c? \u041e\u0442\u0432\u0435\u0442:\", do_sample=True, min_length=50)\n\n[{'generated_text': '\u041a\u0430\u043a \u043a\u0430\u043a\u0430\u0442\u044c? \u041e\u0442\u0432\u0435\u0442: C\u043f\u0443\u0441\u0442\u0438\u0442\u0435 \u0448\u0442\u0430\u043d\u044b \u0438 \u043f\u043e\u043a\u0430\u043a\u0430\u0439\u0442\u0435, \u0437\u0430\u0442\u0435\u043c \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435\u0441\u044c \u0431\u0443\u043c\u0430\u0433\u043e\u0439'}]\n```", "size_bytes": "5363115989", "downloads": 30}
{"pretrained_model_name": "VietAI/gpt-neo-1.3B-vietnamese-news", "description": "---\nlanguage: \n  - vi\ntags:\n- pytorch\n- causal-lm\n- gpt\n---\n\n# GPT-Neo 1.3B on Vietnamese News\n\nDetails will be available soon.\n\nFor more information, please contact anhduongng.1001@gmail.com (D\u01b0\u01a1ng) / imthanhlv@gmail.com (Th\u00e0nh) / nguyenvulebinh@gmail.com (B\u00ecnh).\n\n### How to use \n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"VietAI/gpt-neo-1.3B-vietnamese-news\")\nmodel = AutoModelForCausalLM.from_pretrained(\"VietAI/gpt-neo-1.3B-vietnamese-news\", low_cpu_mem_usage=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \nmodel.to(device)\n\nprompt = \"Ti\u1ec1m n\u0103ng c\u1ee7a tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o\" # your input sentence\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(device)\n\ngen_tokens = model.generate(\n        input_ids,\n        max_length=max_length,\n        do_sample=True,\n        temperature=0.9,\n        top_k=20,\n    )\n\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\nprint(gen_text)\n```", "size_bytes": "5442900582", "downloads": 2864}
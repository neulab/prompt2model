{"pretrained_model_name": "hoskinson-center/proofGPT-v0.1", "description": "---\nlanguage:\n- en\ntags:\n- text generation\n- pytorch\n- causal-lm\n- gpt_neox\nlicense: mit\ndatasets:\n- hoskinson-center/proof-pile\n---\n\n# ProofGPT-v0.1\n\n# Model Description\nProofGPT-v0.1 is a 1.3B parameter language model based on the GPT-NeoX architecture and trained on the [proof-pile](https://huggingface.co/datasets/hoskinson-center/proof-pile) (v1.1). \nWe initiailized training with pythia-1.3b weights, a precursor to the [pythia-1.4b](https://huggingface.co/EleutherAI/pythia-1.3b) model that has roughly equivalent performance. \n\nDetailed evaluations coming soon :) \n\n**Note**: Commit `3bcdc4e` replaced the weights with a model trained on proof-pile v1.1, as opposed to previous commits which were trained on v1.0. Commit `9695b51` updated the tokenizer to have bos, eos, and unk tokens.", "size_bytes": "2930076797", "downloads": 159}
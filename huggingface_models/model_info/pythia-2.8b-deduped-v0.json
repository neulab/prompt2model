{"pretrained_model_name": "EleutherAI/pythia-2.8b-deduped-v0", "description": "---\nlanguage:\n- en\ntags:\n- pytorch\n- causal-lm\n- pythia\n- pythia_v0\nlicense: apache-2.0\ndatasets:\n- EleutherAI/the_pile_deduplicated\n---\n\nThe *Pythia Scaling Suite* is a collection of models developed to facilitate \ninterpretability research. It contains two sets of eight models of sizes \n70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two \nmodels: one trained on the Pile, and one trained on the Pile after the dataset \nhas been globally deduplicated. All 8 model sizes are trained on the exact \nsame data, in the exact same order. All Pythia models are available \n[on Hugging Face](https://huggingface.co/models?other=pythia).\n\nThe Pythia model suite was deliberately designed to promote scientific \nresearch on large language models, especially interpretability research. \nDespite not centering downstream performance as a design goal, we find the \nmodels <a href=\"#evaluations\">match or exceed</a> the performance of \nsimilar and same-sized models, such as those in the OPT and GPT-Neo suites.\n\nPlease note that all models in the *Pythia* suite were renamed in January \n2023. For clarity, a <a href=\"#naming-convention-and-parameter-count\">table \ncomparing the old and new names</a> is provided in this model card, together \nwith exact parameter counts.\n\n## Pythia-2.8B-deduped\n\n### Model Details\n\n- Developed by: [EleutherAI](http://eleuther.ai)\n- Model type: Transformer-based Language Model\n- Language: English\n- Learn more: [Pythia's GitHub repository](https://github.com/EleutherAI/pythia)\n for training procedure, config files, and details on how to use.\n- Library: [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)\n- License: Apache 2.0\n- Contact: to ask questions about this model, join the [EleutherAI \nDiscord](https://discord.gg/zBGx3azzUn), and post them in `#release-discussion`.\n Please read the existing *Pythia* documentation before asking about it in the \n EleutherAI Discord. For general correspondence: [contact@eleuther.\n ai](mailto:contact@eleuther.ai).\n\n<figure>\n\n| Pythia model | Non-Embedding Params | Layers | Model Dim | Heads | Batch Size | Learning Rate         | Equivalent Models      |\n| -----------: | -------------------: | :----: | :-------: | :---: | :--------: | :-------------------: | :--------------------: |\n| 70M          | 18,915,328           | 6      | 512       | 8     | 2M         | 1.0 x 10<sup>-3</sup> | \u2014                      |\n| 160M         | 85,056,000           | 12     | 768       | 12    | 4M         | 6.0 x 10<sup>-4</sup> | GPT-Neo 125M, OPT-125M |\n| 410M         | 302,311,424          | 24     | 1024      | 16    | 4M         | 3.0 x 10<sup>-4</sup> | OPT-350M               |\n| 1.0B         | 805,736,448          | 16     | 2048      | 8     | 2M         | 3.0 x 10<sup>-4</sup> | \u2014                      |\n| 1.4B         | 1,208,602,624        | 24     | 2048      | 16    | 4M         | 2.0 x 10<sup>-4</sup> | GPT-Neo 1.3B, OPT-1.3B |\n| 2.8B         | 2,517,652,480        | 32     | 2560      | 32    | 2M         | 1.6 x 10<sup>-4</sup> | GPT-Neo 2.7B, OPT-2.7B |\n| 6.9B         | 6,444,163,072        | 32     | 4096      | 32    | 2M         | 1.2 x 10<sup>-4</sup> | OPT-6.7B               |\n| 12B          | 11,327,027,200       | 36     | 5120      | 40    | 2M         | 1.2 x 10<sup>-4</sup> | \u2014                      |\n<figcaption>Engineering details for the <i>Pythia Suite</i>. Deduped and \nnon-deduped models of a given size have the same hyperparameters. \u201cEquivalent\u201d \nmodels have <b>exactly</b> the same architecture, and the same number of \nnon-embedding parameters.</figcaption>\n</figure>\n\n### Uses and Limitations\n\n#### Intended Use\n\nThe primary intended use of Pythia is research on the behavior, functionality, \nand limitations of large language models. This suite is intended to provide \na controlled setting for performing scientific experiments. To enable the \nstudy of how language models change in the course of training, we provide \n143 evenly spaced intermediate checkpoints per model. These checkpoints are \nhosted on Hugging Face as branches. Note that branch `143000` corresponds \nexactly to the model checkpoint on the `main` branch of each model.\n\nYou may also further fine-tune and adapt Pythia-2.8B-deduped for deployment, \nas long as your use is in accordance with the Apache 2.0 license. Pythia \nmodels work with the Hugging Face [Transformers \nLibrary](https://huggingface.co/docs/transformers/index). If you decide to use \npre-trained Pythia-2.8B-deduped as a basis for your fine-tuned model, please \nconduct your own risk and bias assessment. \n\n#### Out-of-scope use\n\nThe Pythia Suite is **not** intended for deployment. It is not a in itself \na product and cannot be used for human-facing interactions. \n\nPythia models are English-language only, and are not suitable for translation \nor generating text in other languages.\n\nPythia-2.8B-deduped has not been fine-tuned for downstream contexts in which \nlanguage models are commonly deployed, such as writing genre prose, \nor commercial chatbots. This means Pythia-2.8B-deduped will **not** \nrespond to a given prompt the way a product like ChatGPT does. This is because,\n unlike this model, ChatGPT was fine-tuned using methods such as Reinforcement \nLearning from Human Feedback (RLHF) to better \u201cunderstand\u201d human instructions.\n\n#### Limitations and biases\n\nThe core functionality of a large language model is to take a string of text \nand predict the next token. The token deemed statistically most likely by the \nmodel need not produce the most \u201caccurate\u201d text. Never rely on \nPythia-2.8B-deduped to produce factually accurate output.\n\nThis model was trained on [the Pile](https://pile.eleuther.ai/), a dataset \nknown to contain profanity and texts that are lewd or otherwise offensive. \nSee [Section 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a \ndiscussion of documented biases with regards to gender, religion, and race. \nPythia-2.8B-deduped may produce socially unacceptable or undesirable text, \n*even if* the prompt itself does not include anything explicitly offensive. \n\nIf you plan on using text generated through, for example, the Hosted Inference \nAPI, we recommend having a human curate the outputs of this language model \nbefore presenting it to other people. Please inform your audience that the \ntext was generated by Pythia-2.8B-deduped.\n\n### Quickstart\n\nPythia models can be loaded and used via the following code, demonstrated here \nfor the third `pythia-70m-deduped` checkpoint:\n\n```python\nfrom transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nmodel = GPTNeoXForCausalLM.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  \"EleutherAI/pythia-70m-deduped\",\n  revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\n\ninputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\ntokens = model.generate(**inputs)\ntokenizer.decode(tokens[0])\n```\n\nRevision/branch `step143000` corresponds exactly to the model checkpoint on \nthe `main` branch of each model.<br>\nFor more information on how to use all Pythia models, see [documentation on \nGitHub](https://github.com/EleutherAI/pythia).\n\n### Training\n\n#### Training data\n\nPythia-2.8B-deduped was trained on the Pile **after the dataset has been \nglobally deduplicated**.<br>\n[The Pile](https://pile.eleuther.ai/) is a 825GiB general-purpose dataset in \nEnglish. It was created by EleutherAI specifically for training large language \nmodels. It contains texts from 22 diverse sources, roughly broken down into \nfive categories: academic writing (e.g. arXiv), internet (e.g. CommonCrawl), \nprose (e.g. Project Gutenberg), dialogue (e.g. YouTube subtitles), and \nmiscellaneous (e.g. GitHub, Enron Emails). See [the Pile \npaper](https://arxiv.org/abs/2101.00027) for a breakdown of all data sources, \nmethodology, and a discussion of ethical implications. Consult [the \ndatasheet](https://arxiv.org/abs/2201.07311) for more detailed documentation \nabout the Pile and its component datasets. The Pile can be downloaded from \nthe [official website](https://pile.eleuther.ai/), or from a [community \nmirror](https://the-eye.eu/public/AI/pile/).\n\n#### Training procedure\n\nAll models were trained on the exact same data, in the exact same order. Each \nmodel saw 299,892,736,000 tokens during training, and 143 checkpoints for each \nmodel are saved every 2,097,152,000 tokens, spaced evenly throughout training. \nThis corresponds to training for just under 1 epoch on the Pile for \nnon-deduplicated models, and about 1.5 epochs on the deduplicated Pile.\n\nAll *Pythia* models trained for the equivalent of 143000 steps at a batch size \nof 2,097,152 tokens. Two batch sizes were used: 2M and 4M. Models with a batch \nsize of 4M tokens listed were originally trained for 71500 steps instead, with \ncheckpoints every 500 steps. The checkpoints on Hugging Face are renamed for \nconsistency with all 2M batch models, so `step1000` is the first checkpoint \nfor `pythia-1.4b` that was saved (corresponding to step 500 in training), and \n`step1000` is likewise the first `pythia-6.9b` checkpoint that was saved \n(corresponding to 1000 \u201cactual\u201d steps).<br>\nSee [GitHub](https://github.com/EleutherAI/pythia) for more details on training \nprocedure, including [how to reproduce \nit](https://github.com/EleutherAI/pythia/blob/main/README.md#reproducing-training).<br>\nPythia uses the same tokenizer as [GPT-NeoX-\n20B](https://huggingface.co/EleutherAI/gpt-neox-20b).\n\n### Evaluations\n\nAll 16 *Pythia* models were evaluated using the [LM Evaluation \nHarness](https://github.com/EleutherAI/lm-evaluation-harness). You can access \nthe results by model and step at `results/json/*` in the [GitHub \nrepository](https://github.com/EleutherAI/pythia/tree/main/results/json).<br>\nExpand the sections below to see plots of evaluation results for all \nPythia and Pythia-deduped models compared with OPT and BLOOM.\n\n<details>\n  <summary>LAMBADA \u2013 OpenAI</summary>\n  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/lambada_openai.png\" style=\"width:auto\"/>\n</details>\n\n<details>\n  <summary>Physical Interaction: Question Answering (PIQA)</summary>\n  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/piqa.png\" style=\"width:auto\"/>\n</details>\n\n<details>\n  <summary>WinoGrande</summary>\n  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/winogrande.png\" style=\"width:auto\"/>\n</details>\n\n<details>\n  <summary>AI2 Reasoning Challenge \u2013 Challenge Set</summary>\n  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/arc_challenge.png\" style=\"width:auto\"/>\n</details>\n\n<details>\n  <summary>SciQ</summary>\n  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/sciq.png\" style=\"width:auto\"/>\n</details>\n\n### Naming convention and parameter count\n\n*Pythia* models were renamed in January 2023. It is possible that the old \nnaming convention still persists in some documentation by accident. The \ncurrent naming convention (70M, 160M, etc.) is based on total parameter count. \n\n<figure style=\"width:32em\">\n  \n| current Pythia suffix | old suffix | total params   | non-embedding params |\n| --------------------: | ---------: | -------------: | -------------------: |\n| 70M                   | 19M        | 70,426,624     | 18,915,328           |\n| 160M                  | 125M       | 162,322,944    | 85,056,000           |\n| 410M                  | 350M       | 405,334,016    | 302,311,424          |\n| 1B                    | 800M       | 1,011,781,632  | 805,736,448          |\n| 1.4B                  | 1.3B       | 1,414,647,808  | 1,208,602,624        |\n| 2.8B                  | 2.7B       | 2,775,208,960  | 2,517,652,480        |\n| 6.9B                  | 6.7B       | 6,857,302,016  | 6,444,163,072        |\n| 12B                   | 13B        | 11,846,072,320 | 11,327,027,200       |\n</figure>", "size_bytes": "5684790773", "downloads": 10727}
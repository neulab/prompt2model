{"pretrained_model_name": "ser-mei/gpt-finetuning-cervantes", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: gpt-finetuning-cervantes\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# gpt-finetuning-cervantes\n\nThis model is a fine-tuned version of [DeepESP/gpt2-spanish](https://huggingface.co/DeepESP/gpt2-spanish) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 6.8331\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- gradient_accumulation_steps: 16\n- total_train_batch_size: 512\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 70\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 5.0291        | 0.96  | 13   | 4.6705          |\n| 4.7952        | 1.96  | 26   | 4.4547          |\n| 4.5759        | 2.96  | 39   | 4.3201          |\n| 4.4032        | 3.96  | 52   | 4.2451          |\n| 4.269         | 4.96  | 65   | 4.1911          |\n| 4.143         | 5.96  | 78   | 4.1577          |\n| 4.0229        | 6.96  | 91   | 4.1306          |\n| 3.9047        | 7.96  | 104  | 4.1165          |\n| 3.7886        | 8.96  | 117  | 4.1114          |\n| 3.6666        | 9.96  | 130  | 4.1109          |\n| 3.539         | 10.96 | 143  | 4.1201          |\n| 3.4117        | 11.96 | 156  | 4.1374          |\n| 3.272         | 12.96 | 169  | 4.1538          |\n| 3.1283        | 13.96 | 182  | 4.1876          |\n| 2.9728        | 14.96 | 195  | 4.2226          |\n| 2.816         | 15.96 | 208  | 4.2695          |\n| 2.6475        | 16.96 | 221  | 4.3106          |\n| 2.4765        | 17.96 | 234  | 4.3678          |\n| 2.302         | 18.96 | 247  | 4.4249          |\n| 2.1257        | 19.96 | 260  | 4.4908          |\n| 1.9537        | 20.96 | 273  | 4.5664          |\n| 1.7834        | 21.96 | 286  | 4.6324          |\n| 1.6177        | 22.96 | 299  | 4.6944          |\n| 1.4573        | 23.96 | 312  | 4.7880          |\n| 1.3057        | 24.96 | 325  | 4.8843          |\n| 1.1652        | 25.96 | 338  | 4.9760          |\n| 1.0341        | 26.96 | 351  | 5.0612          |\n| 0.9101        | 27.96 | 364  | 5.1714          |\n| 0.8017        | 28.96 | 377  | 5.2702          |\n| 0.706         | 29.96 | 390  | 5.3530          |\n| 0.6194        | 30.96 | 403  | 5.4535          |\n| 0.5436        | 31.96 | 416  | 5.5373          |\n| 0.4816        | 32.96 | 429  | 5.6153          |\n| 0.4309        | 33.96 | 442  | 5.7014          |\n| 0.3899        | 34.96 | 455  | 5.7749          |\n| 0.3544        | 35.96 | 468  | 5.8430          |\n| 0.3236        | 36.96 | 481  | 5.9237          |\n| 0.3005        | 37.96 | 494  | 5.9824          |\n| 0.2804        | 38.96 | 507  | 6.0264          |\n| 0.263         | 39.96 | 520  | 6.0797          |\n| 0.2513        | 40.96 | 533  | 6.1285          |\n| 0.2376        | 41.96 | 546  | 6.1900          |\n| 0.2264        | 42.96 | 559  | 6.2212          |\n| 0.2183        | 43.96 | 572  | 6.2812          |\n| 0.2104        | 44.96 | 585  | 6.3079          |\n| 0.203         | 45.96 | 598  | 6.3501          |\n| 0.1964        | 46.96 | 611  | 6.3730          |\n| 0.1912        | 47.96 | 624  | 6.4190          |\n| 0.1854        | 48.96 | 637  | 6.4598          |\n| 0.1817        | 49.96 | 650  | 6.4618          |\n| 0.1792        | 50.96 | 663  | 6.4914          |\n| 0.1748        | 51.96 | 676  | 6.5385          |\n| 0.1732        | 52.96 | 689  | 6.5689          |\n| 0.1689        | 53.96 | 702  | 6.5761          |\n| 0.1672        | 54.96 | 715  | 6.5775          |\n| 0.1657        | 55.96 | 728  | 6.6362          |\n| 0.1625        | 56.96 | 741  | 6.6573          |\n| 0.1611        | 57.96 | 754  | 6.7019          |\n| 0.1588        | 58.96 | 767  | 6.6602          |\n| 0.1573        | 59.96 | 780  | 6.7015          |\n| 0.1547        | 60.96 | 793  | 6.7323          |\n| 0.1542        | 61.96 | 806  | 6.7368          |\n| 0.1538        | 62.96 | 819  | 6.7704          |\n| 0.1513        | 63.96 | 832  | 6.7963          |\n| 0.1504        | 64.96 | 845  | 6.7988          |\n| 0.1506        | 65.96 | 858  | 6.8386          |\n| 0.1497        | 66.96 | 871  | 6.8039          |\n| 0.15          | 67.96 | 884  | 6.8126          |\n| 0.1497        | 68.96 | 897  | 6.8858          |\n| 0.143         | 69.96 | 910  | 6.8331          |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.13.0+rocm5.2\n- Datasets 2.6.1\n- Tokenizers 0.13.2\n", "size_bytes": "510398013", "downloads": 0}
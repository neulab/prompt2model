{"pretrained_model_name": "OscarNav/dialoGPT_translate", "description": "# Finetuned DialoGPT model for Eng-Spa translation\n\nDialoGPT-small model was used and finetuned on English to Spanish translations, extracted from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n\nsome examples of translations\n\n| Role  | Response  | \n| :---: |------------------------| \n| User  | please, sing me a song | \n| Bot   | Por favor, canta una canci\u00f3n. | \n| User  | I really want to go to China | \n| Bot   | Realmente quiero ir a China. | \n| User  | Can you do me a favor? | \n| Bot   | \u00bfMe puedes hacer un favor? |\n| User  | I don't know what you are talking about | \n| Bot   | No s\u00e9 de qu\u00e9 est\u00e1s hablando. |\n| User  | I don't want to go to China | \n| Bot   | No quiero ir a China. |\n \n\n# Using the model\nexample code for trying out the model\n\n```python\nfrom transformers import AutoModelWithLMHead, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\nmodel = AutoModelWithLMHead.from_pretrained('OscarNav/dialoGPT_translate')\n\n# Let's traslate 5 sentences\nfor step in range(5):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        new_user_input_ids, max_length=1000,\n        pad_token_id=tokenizer.eos_token_id,\n        top_p=0.92, top_k = 50\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, new_user_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n```", "size_bytes": "510401385", "downloads": 63}
{"pretrained_model_name": "cahya/gpt2-small-indonesian-522M", "description": "---\nlanguage: \"id\"\nlicense: \"mit\"\ndatasets:\n- Indonesian Wikipedia\nwidget:\n- text: \"Pulau Dewata sering dikunjungi\"\n---\n\n# Indonesian GPT2 small model \n\n## Model description\nIt is GPT2-small model pre-trained with indonesian Wikipedia using a causal language modeling (CLM) objective. This \nmodel is uncased: it does not make a difference between indonesia and Indonesia.\n\nThis is one of several other language models that have been pre-trained with indonesian datasets. More detail about \nits usage on downstream tasks (text classification, text generation, etc) is available at [Transformer based Indonesian Language Models](https://github.com/cahya-wirawan/indonesian-language-models/tree/master/Transformers)\n\n## Intended uses & limitations\n\n### How to use\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, \nwe set a seed for reproducibility:\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='cahya/gpt2-small-indonesian-522M')\n>>> set_seed(42)\n>>> generator(\"Kerajaan Majapahit adalah\", max_length=30, num_return_sequences=5, num_beams=10)\n\n[{'generated_text': 'Kerajaan Majapahit adalah sebuah kerajaan yang pernah berdiri di Jawa Timur pada abad ke-14 hingga abad ke-15. Kerajaan ini berdiri pada abad ke-14'}, \n{'generated_text': 'Kerajaan Majapahit adalah sebuah kerajaan yang pernah berdiri di Jawa Timur pada abad ke-14 hingga abad ke-16. Kerajaan ini berdiri pada abad ke-14'}, \n{'generated_text': 'Kerajaan Majapahit adalah sebuah kerajaan yang pernah berdiri di Jawa Timur pada abad ke-14 hingga abad ke-15. Kerajaan ini berdiri pada abad ke-15'}, \n{'generated_text': 'Kerajaan Majapahit adalah sebuah kerajaan yang pernah berdiri di Jawa Timur pada abad ke-14 hingga abad ke-16. Kerajaan ini berdiri pada abad ke-15'}, \n{'generated_text': 'Kerajaan Majapahit adalah sebuah kerajaan yang pernah berdiri di Jawa Timur pada abad ke-14 hingga abad ke-15. Kerajaan ini merupakan kelanjutan dari Kerajaan Majapahit yang'}]\n\n```\nHere is how to use this model to get the features of a given text in PyTorch:\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\n\nmodel_name='cahya/gpt2-small-indonesian-522M'\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2Model.from_pretrained(model_name)\ntext = \"Silakan diganti dengan text apa saja.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\nand in Tensorflow:\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\n\nmodel_name='cahya/gpt2-small-indonesian-522M'\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = TFGPT2Model.from_pretrained(model_name)\ntext = \"Silakan diganti dengan text apa saja.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n## Training data\n\nThis model was pre-trained with 522MB of indonesian Wikipedia.\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and \na vocabulary size of 52,000. The inputs are sequences of 128 consecutive tokens.\n", "size_bytes": "510378682", "downloads": 210}
{"pretrained_model_name": "cambridgeltl/simctg_lccc_dialogue", "description": "This model provides a Chinese GPT-2 language model trained with SimCTG on the LCCC benchmark [(Wang et al., 2020)](https://arxiv.org/pdf/2008.03946v2.pdf) based on our paper [_A Contrastive Framework for Neural Text Generation_](https://arxiv.org/abs/2202.06417).\n\nWe provide a detailed tutorial on how to apply SimCTG and Contrastive Search in our [project repo](https://github.com/yxuansu/SimCTG#4-huggingface-style-tutorials-back-to-top). In the following, we illustrate a brief tutorial on how to use our approach to perform text generation.\n\n## 1. Installation of SimCTG:\n```yaml\npip install simctg --upgrade\n```\n\n## 2. Initialize SimCTG Model:\n```python\nimport torch\n# load SimCTG language model\nfrom simctg.simctggpt import SimCTGGPT\nmodel_name = r'cambridgeltl/simctg_lccc_dialogue'\nmodel = SimCTGGPT(model_name)\nmodel.eval()\ntokenizer = model.tokenizer\neos_token = '[SEP]'\neos_token_id = tokenizer.convert_tokens_to_ids([eos_token])[0]\n```\n\n## 3. Prepare the Text Prefix:\n```python\ncontext_list = ['\u523a\u732c\u5f88\u53ef\u7231\uff01\u4ee5\u524d\u522b\u4eba\u9001\u4e86\u53ea\u6ca1\u517b\uff0c\u5473\u513f\u592a\u5927\uff01', '\u662f\u5f88\u53ef\u7231\u4f46\u662f\u975e\u5e38\u81ed', '\u662f\u554a\uff0c\u6ca1\u529e\u6cd5\u517b', '\u90a3\u4e2a\u600e\u4e48\u517b\u54e6\u4e0d\u4f1a\u624e\u624b\u5417']\nprefix_text = eos_token.join(context_list).strip(eos_token) + eos_token\nprint ('Prefix is: {}'.format(prefix_text))\ntokens = tokenizer.tokenize(prefix_text)\ninput_ids = tokenizer.convert_tokens_to_ids(tokens)\ninput_ids = torch.LongTensor(input_ids).view(1,-1)\n```\n\n## 4. Generate Text with Contrastive Search:\n```python\nbeam_width, alpha, decoding_len = 5, 0.6, 64\noutput = model.fast_contrastive_search(input_ids=input_ids, beam_width=beam_width, alpha=alpha, \n                                       decoding_len=decoding_len, end_of_sequence_token_id=eos_token_id,\n                                       early_stop=True)  \n                                           \nprint(\"Output:\\n\" + 100 * '-')\nprint(''.join(tokenizer.decode(output)))\n'''\n  Prefix is: \u523a\u732c\u5f88\u53ef\u7231\uff01\u4ee5\u524d\u522b\u4eba\u9001\u4e86\u53ea\u6ca1\u517b\uff0c\u5473\u513f\u592a\u5927\uff01[SEP]\u662f\u5f88\u53ef\u7231\u4f46\u662f\u975e\u5e38\u81ed[SEP]\u662f\u554a\uff0c\u6ca1\u529e\u6cd5\u517b[SEP]\u90a3\u4e2a\u600e\u4e48\u517b\u54e6\u4e0d\u4f1a\u624e\u624b\u5417[SEP]\n  Output:\n  ----------------------------------------------------------------------------------------------------\n  \u523a\u732c\u5f88\u53ef\u7231\uff01\u4ee5\u524d\u522b\u4eba\u9001\u4e86\u53ea\u6ca1\u517b\uff0c\u5473\u513f\u592a\u5927\uff01[SEP]\u662f\u5f88\u53ef\u7231\u4f46\u662f\u975e\u5e38\u81ed[SEP]\u662f\u554a\uff0c\u6ca1\u529e\u6cd5\u517b[SEP]\u90a3\u4e2a\u600e\u4e48\u517b\u54e6\u4e0d\u4f1a\u624e\u624b\u5417[SEP]\u6211\u89c9\u5f97\u8fd8\u597d\uff0c\u5c31\u662f\u6709\u70b9\u81ed\n'''\n```\n\nFor more details of our work, please refer to our main [project repo](https://github.com/yxuansu/SimCTG).\n\n## 5. Citation:\nIf you find our paper and resources useful, please kindly leave a star and cite our paper. Thanks!\n\n```bibtex\n@article{su2022contrastive,\n  title={A Contrastive Framework for Neural Text Generation},\n  author={Su, Yixuan and Lan, Tian and Wang, Yan and Yogatama, Dani and Kong, Lingpeng and Collier, Nigel},\n  journal={arXiv preprint arXiv:2202.06417},\n  year={2022}\n}\n```\n\n\n", "size_bytes": "420922254", "downloads": 27}
{"pretrained_model_name": "gorkemgoknar/gpt2chatbotenglish", "description": "---\nlanguage:\n- en\nthumbnail:\ntags:\n- gpt2\n- conversational\nlicense: cc-by-4.0\nwidget:\n- text: Hello there\n  context: 'Gandalf'\n---\n# GPT2 Persona Chatbot based on Movie Characters\nModel used for https://www.metayazar.com/chatbot\n\nGPT2 Small Trained on movie scripts (especially Sci-fi) \n\nUsual HF api will not work see HF Spaces for demo usage https://huggingface.co/spaces/gorkemgoknar/moviechatbot\n\n\nThis work is based on Persona Chatbot originally done by Hugging Face team (https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)\n\nFor cleaning movie scripts I also provide cleaner code\nhttps://github.com/gorkemgoknar/moviescriptcleaner\n\nExample persona how to:\nhttps://gist.github.com/gorkemgoknar/ae29bf9d14fa814e6a64d0e57a4a4ed7\n\nFor obvious reasons I cannot share raw personafile but you can check above gist for example how to create it.\n\nA working \"full\" demo can be seen in https://www.metayazar.com/chatbot\n\nFor Turkish version (with limited training) https://www.metayazar.com/chatbot_tr\n\nDue to double LM head standart hugging face interface will not work. But if you follow huggingface tutorial should be same.\nExcept each persona is encoded as \"My name is XXXX\"\n\nUse model, tokenizer and parameters within a class and call in below functions to trigger model.\nSome of the available personas:\n\n| Macleod | Moran | Brenda | Ramirez | Peter Parker | Quentin Beck | Andy \n| Red | Norton | Willard | Chief | Chef | Kilgore | Kurtz | Westley | Buttercup \n| Vizzini | Fezzik | Inigo | Man In Black | Taylor | Zira | Zaius | Cornelius \n| Bud | Lindsey | Hippy | Erin | Ed | George | Donna | Trinity | Agent Smith \n| Morpheus | Neo | Tank | Meryl | Truman | Marlon | Christof | Stromboli | Bumstead \n| Schreber | Walker | Korben | Cornelius | Loc Rhod | Anakin | Obi-Wan | Palpatine \n| Padme | Superman | Luthor | Dude | Walter | Donny | Maude | General | Starkiller \n| Indiana | Willie | Short Round | John | Sarah | Terminator | Miller | Sarge | Reiben \n| Jackson | Upham | Chuckie | Will | Lambeau | Sean | Skylar | Saavik | Spock \n| Kirk | Bones | Khan | Kirk | Spock | Sybok | Scotty | Bourne | Pamela | Abbott \n\n\n```python\n    def get_answer(self, input_text, personality, history, params=None):\n        \n        ##Check length of history (to save 1 computation!)\n        if len(history)>0:\n            #mostly it will be empty list so need a length check for performance\n            #would do string check also but just assume it is list of list of strings, as not public\n            \n            new_hist = [] \n            for ele in history:\n                new_hist.append( self.tokenizer.encode(ele) )\n            history = new_hist.copy()\n\n        history.append(self.tokenizer.encode(input_text))\n\n        with torch.no_grad():\n            out_ids = self.sample_sequence(personality, history, self.tokenizer, self.model, params=params)\n        history.append(out_ids)\n        history = history[-(2*self.parameters['max_history']+1):]\n        out_text = self.tokenizer.decode(out_ids, skip_special_tokens=True)\n        #print(out_text)\n\n\n        history_decoded = []\n        for ele in history:\n            history_decoded.append(self.tokenizer.decode(ele))\n\n        return out_text, history_decoded, self.parameters\n\n```", "size_bytes": "548170871", "downloads": 336}
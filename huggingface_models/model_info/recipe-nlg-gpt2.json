{"pretrained_model_name": "pratultandon/recipe-nlg-gpt2", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: recipe-nlg-gpt2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# recipe-nlg-gpt2\n\nThis model is a fine-tuned version of [gpt2](https://huggingface.co/gpt2) on the RecipeNLG dataset.\n\n## Model description\n\nRecreating the GPT-2 model described in https://aclanthology.org/2020.inlg-1.4.pdf. \n\n## Intended uses & limitations\n\nExperimenting with GPT-2 for recipe generation.\n\nTo use the model, it is best to use special tokens in your input, these were added to the model tokenizer's vocabulary and served as delimiters in the training data. Therefore, we can use them to prompt the model using as much of the recipe as we are able to provide, and the model should stick to the format to complete the rest. \n\nHere's a sample recipe from the test dataset\n\n \n ```\n<RECIPE_START> <INPUT_START> fettucini <NEXT_INPUT> butter <NEXT_INPUT> light cream <NEXT_INPUT> Romano cheese <INPUT_END><INGR_START> 1 lb. fettucini <NEXT_INGR> 1/4 lb. butter (1 stick) <NEXT_INGR> 1 pt. light cream <NEXT_INGR> grated Parmesan or Romano cheese <INGR_END> <INSTR_START> Cook fettucini as directed. <NEXT_INSTR> Melt butter and pour over drained pasta. <NEXT_INSTR> Pour cream over pasta and mix. <NEXT_INSTR> Finally, mix cheeses and toss to coat. <NEXT_INSTR> Serve with grated cheeses to taste. <NEXT_INSTR> For variation, toss with steamed broccoli crowns. <INSTR_END> <TITLE_START> Fettucini \"Al Marko\" <TITLE_END> <RECIPE_END>\n\n ```\n\nThe format starts with a token to indicate the start of a recipe, then the inputs (what you want to cook with), then the ingredients (which adds quantities), then instructions, then the recipe title and finally a token to indicate the end of the recipe. \n\nTry generating a recipe with the prompt \n\n\n ```\n <RECIPE_START> <INPUT_START> fettucini <NEXT_INPUT> butter <NEXT_INPUT> light cream <NEXT_INPUT> Romano cheese <INPUT_END>\n ```\n\nYou probably want to import the model directly and experiment with different sampling methods for generation. Starting with a temperature sampling and a temperature of 0.5 gives good results. You'll also have to override the current default max length of 50 and make sure skip_special_tokens=False when you decode your model outputs, so you can parse for the end of recipe.\n\n## Training and evaluation data\nThe RecipeNLG(https://huggingface.co/mbien/recipenlg/) dataset was used for this task.\n\n5% of the dataset was held out for evaluation.\n\n## Training procedure\nRTX 3090 was used on Vast.AI, training took about 14 hours with a batch size of 8, and f16 enabled.\n\n### Training hyperparameters\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 200\n- num_epochs: 1\n- mixed_precision_training: Native AMP\n\n### Training results\n\n***** Running Evaluation *****\n  Num examples = 106202\n  Batch size = 8\n  \n{'eval_loss': 1.1872143745422363,\n 'eval_runtime': 818.8498,\n 'eval_samples_per_second': 129.697,\n 'eval_steps_per_second': 16.213,\n 'epoch': 1.0}\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.13.0\n- Datasets 2.6.1\n- Tokenizers 0.13.2\n", "size_bytes": "510441021", "downloads": 2}
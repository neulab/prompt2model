{"pretrained_model_name": "rahuldhodapkar/protgpt2-finetuned-sarscov2-rbd", "description": "---\nlicense: cc-by-nc-nd-4.0\nmetrics:\n- accuracy\ntags:\n- generated_from_trainer\n- Text Generation\n- Primary Sequence Prediction\nmodel-index:\n- name: protgpt2-finetuned-sarscov2-rbd\n  results: []\n---\n\n# Model Card for `protgpt2-finetuned-sarscov2-rbd`\n\nThis model is a fine-tuned version of [nferruz/ProtGPT2](https://huggingface.co/nferruz/ProtGPT2) on sequences from the NCBI Virus Data Portal.\n\nIt achieves the following results on the evaluation set:\n- Loss: 1.1674\n- Accuracy: 0.8883\n\n## Model description\n\nThis model is a fine-tuned checkpoint of\n[ProtGPT2](https://huggingface.co/nferruz/ProtGPT2), which was originally\ntrained on the UniRef50 (version 2021_04) database. For a detailed overview\nof the original model configuration and architecture, please see the linked\nmodel card, or refer to the ProtGPT2 publication.\n\nThe model was finetuned on data from the SARS-CoV-2 Spike (surface glycoprotein)\nreceptor binding domain (RBD).\n\nA repository with the training scripts, train and test data partitions, as well\nas evaluation code is available on GitHub at\n(https://github.com/rahuldhodapkar/PredictSARSVariants).\n\n## Intended uses & limitations\n\nThis model is intended to generate synthetic SARS-CoV-2 surface glycoprotein\n(a.k.a. spike protein) sequences for the purpose of identifying meaningful\nvariants for characterization either experimentally or through other\n*in silico* tools.  These variants may be used to drive vaccine develop to\nprotect against never-before-seen point mutants that are probable in the future.\n\nAs this model is based on the original ProtGPT2 model, it is subject to many\nof the same limitations as the base model.  Any biases present in the UniRef50\ndataset will also be present in the model, which may include nonuniform skew\nof peptides sampled across different taxonomic clades.  These limitations\nshould be considered when interpreting the output of this model.\n\n## Training and evaluation data\n\nSARS-CoV-2 spike protein sequences were obtained from the NIH Sars-CoV-2 Data Hub\naccessible at \n\n    https://www.ncbi.nlm.nih.gov/labs/virus/vssi/\n\nNote that the reference sequence for the surface glycoprotein can be found at:\n\n    https://www.ncbi.nlm.nih.gov/protein/1791269090\n\nAs the loaded ProtGPT2 model was pretrained on the\nUniRef50 (version 2021_04) dataset, it cannot have contained sequencing\ndata that was generated after that date.  Evaluations will be conducted using\nSARS-CoV-2 sequences generated on or after May 2021.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3.0\n\n### Framework versions\n\n- Transformers 4.26.0.dev0\n- Pytorch 1.11.0\n- Datasets 2.8.0\n- Tokenizers 0.13.2\n\n", "size_bytes": "3134031497", "downloads": 0}
{"pretrained_model_name": "anikethjr/PromoGen_K562_2080Ti_restart", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: PromoGen_K562_2080Ti_restart\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# PromoGen_K562_2080Ti_restart\n\nThis model is a fine-tuned version of [](https://huggingface.co/) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4624\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 25\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss |\n|:-------------:|:-----:|:------:|:---------------:|\n| 0.7676        | 0.49  | 2500   | 0.7383          |\n| 0.7121        | 0.97  | 5000   | 0.6867          |\n| 0.6914        | 1.46  | 7500   | 0.6705          |\n| 0.6837        | 1.95  | 10000  | 0.6622          |\n| 0.6778        | 2.44  | 12500  | 0.6558          |\n| 0.6748        | 2.92  | 15000  | 0.6517          |\n| 0.6676        | 3.41  | 17500  | 0.6433          |\n| 0.6593        | 3.9   | 20000  | 0.6358          |\n| 0.6584        | 4.38  | 22500  | 0.6320          |\n| 0.6557        | 4.87  | 25000  | 0.6301          |\n| 0.6523        | 5.36  | 27500  | 0.6257          |\n| 0.6478        | 5.84  | 30000  | 0.6236          |\n| 0.6393        | 6.33  | 32500  | 0.6145          |\n| 0.6039        | 6.82  | 35000  | 0.5658          |\n| 0.5616        | 7.31  | 37500  | 0.5376          |\n| 0.5518        | 7.79  | 40000  | 0.5310          |\n| 0.5509        | 8.28  | 42500  | 0.5273          |\n| 0.5487        | 8.77  | 45000  | 0.5261          |\n| 0.5479        | 9.25  | 47500  | 0.5249          |\n| 0.546         | 9.74  | 50000  | 0.5242          |\n| 0.5447        | 10.23 | 52500  | 0.5229          |\n| 0.5439        | 10.71 | 55000  | 0.5220          |\n| 0.5433        | 11.2  | 57500  | 0.5209          |\n| 0.5394        | 11.69 | 60000  | 0.5162          |\n| 0.5153        | 12.18 | 62500  | 0.4944          |\n| 0.5137        | 12.66 | 65000  | 0.4932          |\n| 0.514         | 13.15 | 67500  | 0.4924          |\n| 0.5131        | 13.64 | 70000  | 0.4919          |\n| 0.5104        | 14.12 | 72500  | 0.4914          |\n| 0.5122        | 14.61 | 75000  | 0.4906          |\n| 0.5089        | 15.1  | 77500  | 0.4901          |\n| 0.5076        | 15.59 | 80000  | 0.4891          |\n| 0.4986        | 16.07 | 82500  | 0.4721          |\n| 0.4875        | 16.56 | 85000  | 0.4672          |\n| 0.4887        | 17.05 | 87500  | 0.4669          |\n| 0.4839        | 17.53 | 90000  | 0.4661          |\n| 0.4849        | 18.02 | 92500  | 0.4654          |\n| 0.4848        | 18.51 | 95000  | 0.4649          |\n| 0.4831        | 18.99 | 97500  | 0.4646          |\n| 0.4816        | 19.48 | 100000 | 0.4644          |\n| 0.4808        | 19.97 | 102500 | 0.4637          |\n| 0.4812        | 20.46 | 105000 | 0.4634          |\n| 0.4813        | 20.94 | 107500 | 0.4633          |\n| 0.4818        | 21.43 | 110000 | 0.4631          |\n| 0.4813        | 21.92 | 112500 | 0.4629          |\n| 0.4782        | 22.4  | 115000 | 0.4628          |\n| 0.4804        | 22.89 | 117500 | 0.4626          |\n| 0.4815        | 23.38 | 120000 | 0.4625          |\n| 0.4812        | 23.87 | 122500 | 0.4625          |\n| 0.4785        | 24.35 | 125000 | 0.4624          |\n| 0.4795        | 24.84 | 127500 | 0.4624          |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.13.0\n- Datasets 2.7.0\n- Tokenizers 0.13.0.dev0\n", "size_bytes": "29044865", "downloads": 2}
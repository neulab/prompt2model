{"pretrained_model_name": "THUMT/mGPT", "description": "\n# mGPT\n\nmGPT is pre-trained on the [mC4 dataset](https://huggingface.co/datasets/mc4) using a causal language modeling objective. It was introduced in this [paper](https://arxiv.org/abs/2110.06609) and first released on this page.\n\n## Model description\n\nmGPT is a Transformer-based model which pre-trained on massive multilingual data covering over 101 languages. Similar to GPT-2, It was pre-trained on the raw texts only, with no human labeling. We use the same tokenization and vocabulary as the [mT5 model](https://huggingface.co/google/mt5-base).\n\n## Intended uses\n\nYou can use the raw model for text generation or using prompts for adapting it to a downstream task.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation.  Here is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import MT5Tokenizer, GPT2LMHeadModel, TextGenerationPipeline\n\ntokenizer = MT5Tokenizer.from_pretrained(\"THUMT/mGPT\")\nmodel = GPT2LMHeadModel.from_pretrained(\"THUMT/mGPT\")\n\npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\ntext = \"Replace me by any text you'd like.\"\ntext = pipeline(text, do_sample=True,  max_length=1024)[0][\"generated_text\"]\n```\n\n## Preprocessing\n\nThe texts are tokenized using `sentencepiece` and a vocabulary size of 250,100. The inputs are sequences of 1,024 consecutive tokens. We use `<extra_id_0>` to separate lines in a document.\n\n## BibTeX entry and citation info\n\n```bibtex\n@misc{tan2021msp,\n    title={MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators},\n    author={Zhixing Tan and Xiangwen Zhang and Shuo Wang and Yang Liu},\n    year={2021},\n    eprint={2110.06609},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n", "size_bytes": "1144222891", "downloads": 294}
{"pretrained_model_name": "IDEA-CCNL/Yuyuan-GPT2-110M-SciFi-Chinese", "description": "---\nlanguage: \n  - zh\n\ninference: \n  parameters:\n    temperature: 1\n    top_p: 0.7\n    repetition_penalty: 1.1\n    max_new_tokens: 128\n    num_return_sequences: 3\n    do_sample: true\n\nlicense: apache-2.0\ntags:\n- generate\n- gpt2\n\nwidget:\n- \u6211\u662f\u903b\u8f91\uff0c\u9762\u5bf9\u9ed1\u6697\u68ee\u6797\u6cd5\u5219\uff0c\n- \u5929\u7a7a\u7684\u5c3d\u5934\uff0c\u53d1\u51fa\u4e86\u4e00\u9053\u4eae\u773c\u7684\u5149\u8292\uff0c\n\n---\n\n# Yuyuan-GPT2-110M-SciFi-Chinese\n\n- Github: [Fengshenbang-LM](https://github.com/IDEA-CCNL/Fengshenbang-LM)\n- Docs: [Fengshenbang-Docs](https://fengshenbang-doc.readthedocs.io/)\n\n## \u7b80\u4ecb Brief Introduction\n\n\u57fa\u4e8e\u4e2d\u6587\u7248\u7684Wenzhong-GPT2-110M\uff0c\u6211\u4eec\u7528\u63a5\u8fd12\u4e07\u7684\u79d1\u5e7b\u5c0f\u8bf4\u6570\u636e\u8fdb\u884c\u4e86\u5fae\u8c03\u6a21\u578b\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u8f83\u597d\u7684\u7eed\u5199\u79d1\u5e7b\u5c0f\u8bf4\u3002\n\nBased on the Wenzhong-GPT2-110M, we used nearly 20000 science fiction data to fine-tune the model, so that the model can better continue to write science fiction.\n\n## \u6a21\u578b\u5206\u7c7b Model Taxonomy\n\n|  \u9700\u6c42 Demand  | \u4efb\u52a1 Task       | \u7cfb\u5217 Series      | \u6a21\u578b Model    | \u53c2\u6570 Parameter | \u989d\u5916 Extra |\n|  :----:  | :----:  | :----:  | :----:  | :----:  | :----:  |\n| \u79d1\u5e7b\u5c0f\u8bf4 science fiction  | \u81ea\u7136\u8bed\u8a00\u751f\u6210 NLG | \u4f59\u5143 Yuyuan | GPT2 |      110M      |     \u4e2d\u6587 Chinese     |\n\n## \u6a21\u578b\u4fe1\u606f Model Information\n\n\u6a21\u578b\u7ed3\u6784\u548cwenzhong-110M\u76f8\u540c\uff0c\u53ea\u662f\u5fae\u8c03\u7684\u8bed\u6599\u7531\u901a\u7528\u6570\u636e\u53d8\u6210\u4e86\u79d1\u5e7b\u5c0f\u8bf4\u3002\n\nThe structure of the model is the same as that of Wenzhong-GPT2-110M, except that the fine-tuned corpus has changed from general data to science fiction.\n\n## \u4f7f\u7528 Usage\n\n### \u52a0\u8f7d\u6a21\u578b Loading Models\n\n```python \nfrom transformers import GPT2Tokenizer,GPT2LMHeadModel\nhf_model_path = 'IDEA-CCNL/Yuyuan-GPT2-110M-SciFi-Chinese'\ntokenizer = GPT2Tokenizer.from_pretrained(hf_model_path)\nmodel = GPT2LMHeadModel.from_pretrained(hf_model_path)\n```\n\n### \u4f7f\u7528\u793a\u4f8b Usage Examples\n\n```python\nquestion = \"\u6211\u662f\u903b\u8f91\uff0c\u9762\u5bf9\u9ed1\u6697\u68ee\u6797\u6cd5\u5219\uff0c\"\ninputs = tokenizer(question,return_tensors='pt')\ngeneration_output = model.generate(**inputs,\n                                return_dict_in_generate=True,\n                                output_scores=True,\n                                max_length=150,\n                                # max_new_tokens=80,\n                                do_sample=True,\n                                top_p = 0.6,\n                                # num_beams=5,\n                                eos_token_id=50256,\n                                pad_token_id=0,\n                                num_return_sequences = 5)\n\nfor idx,sentence in enumerate(generation_output.sequences):\n    print('next sentence %d:\\n'%idx,\n    tokenizer.decode(sentence).split('<|endoftext|>')[0])\n    print('*'*40)\n```\n\n## \u5f15\u7528 Citation\n\n\u5982\u679c\u60a8\u5728\u60a8\u7684\u5de5\u4f5c\u4e2d\u4f7f\u7528\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684[\u8bba\u6587](https://arxiv.org/abs/2209.02970)\uff1a\n\nIf you are using the resource for your work, please cite the our [paper](https://arxiv.org/abs/2209.02970):\n\n```text\n@article{fengshenbang,\n  author    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},\n  title     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},\n  journal   = {CoRR},\n  volume    = {abs/2209.02970},\n  year      = {2022}\n}\n```\n\n\u4e5f\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684[\u7f51\u7ad9](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n\nYou can also cite our [website](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n\n```text\n@misc{Fengshenbang-LM,\n  title={Fengshenbang-LM},\n  author={IDEA-CCNL},\n  year={2021},\n  howpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},\n}\n```\n", "size_bytes": "510540905", "downloads": 57}
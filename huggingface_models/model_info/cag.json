{"pretrained_model_name": "gigabrain/cag", "description": "---\nlanguage: en\nthumbnail: http://www.huggingtweets.com/doveywan-irenezhao_-layahheilpern/1668969714119/predictions.png\ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div class=\"inline-flex flex-col\" style=\"line-height: 1.5;\">\n    <div class=\"flex\">\n        <div\n\t\t\tstyle=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;https://pbs.twimg.com/profile_images/1592314373317558274/kWBIBveR_400x400.jpg&#39;)\">\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;https://pbs.twimg.com/profile_images/1569305276343369729/9tyrIoYq_400x400.jpg&#39;)\">\n        </div>\n        <div\n            style=\"display:inherit; margin-left: 4px; margin-right: 4px; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;https://pbs.twimg.com/profile_images/1423875044598456321/SVjwd6Bb_400x400.jpg&#39;)\">\n        </div>\n    </div>\n    <div style=\"text-align: center; font-size: 16px; font-weight: 800\">Layah Heilpern & Dovey \"Rug The Fiat\" Wan & Irene Zhao</div>\n    <div style=\"text-align: center; font-size: 14px;\">@doveywan-irenezhao_-layahheilpern</div>\n</div>\n\n\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](https://github.com/borisdayma/huggingtweets/blob/master/img/pipeline.png?raw=true)\n\n\n\n## Training data\n\nThe model was trained on tweets from Layah Heilpern & Dovey \"Rug The Fiat\" Wan & Irene Zhao.\n\n| Data | Layah Heilpern | Dovey \"Rug The Fiat\" Wan | Irene Zhao |\n| --- | --- | --- | --- |\n| Tweets downloaded | 3249 | 3247 | 1945 |\n| Retweets | 115 | 310 | 223 |\n| Short tweets | 1453 | 269 | 417 |\n| Tweets kept | 1681 | 2668 | 1305 |\n\n[Explore the data](https://wandb.ai/wandb/huggingtweets/runs/38f27zgg/artifacts), which is tracked with [W&B artifacts](https://docs.wandb.com/artifacts) at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2](https://huggingface.co/gpt2) which is fine-tuned on @doveywan-irenezhao_-layahheilpern's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run](https://wandb.ai/wandb/huggingtweets/runs/zek1fxw0) for full transparency and reproducibility.\n\nAt the end of training, [the final model](https://wandb.ai/wandb/huggingtweets/runs/zek1fxw0/artifacts) is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/cag')\ngenerator(\"In crypto, \", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](https://huggingface.co/gpt2#limitations-and-bias).\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Gigabrain*\n\n\n\n", "size_bytes": "510396521", "downloads": 2}
{"pretrained_model_name": "flax-community/nordic-gpt-wiki", "description": "---\nlanguage: sv\nwidget:\n- text: \"Det var en g\u00e5ng\"\n---\n\n# Nordic GPT2--wikipedia\nA Nordic GPT2 style model trained using Flax CLM pipeline on the Nordic parts\npart of the wiki40b dataset.\n\nhttps://huggingface.co/datasets/wiki40b\n\n## Model series\nThis model is part of a series of models training on TPU with Flax Jax during Huggingface Flax/Jax challenge.\n\n## Gpt models\n\n## Swedish Gpt\nhttps://huggingface.co/birgermoell/swedish-gpt/\n\n## Swedish gpt wiki\nhttps://huggingface.co/flax-community/swe-gpt-wiki\n\n# Nordic gpt wiki\nhttps://huggingface.co/flax-community/nordic-gpt-wiki\n\n## Dansk gpt wiki\nhttps://huggingface.co/flax-community/dansk-gpt-wiki\n\n## Norsk gpt wiki\nhttps://huggingface.co/flax-community/norsk-gpt-wiki\n\n## Roberta models\n\n## Nordic Roberta Wiki\nhttps://huggingface.co/flax-community/nordic-roberta-wiki\n\n## Swe Roberta Wiki Oscar\nhttps://huggingface.co/flax-community/swe-roberta-wiki-oscar\n\n## Roberta Swedish Scandi\nhttps://huggingface.co/birgermoell/roberta-swedish-scandi\n\n## Roberta Swedish\nhttps://huggingface.co/birgermoell/roberta-swedish\n\n## Swedish T5 model\nhttps://huggingface.co/birgermoell/t5-base-swedish\n\n\n\n\n## Data cleaning and preprocessing\nThe data was cleaned and preprocessed using the following script. Make sure to install depencies for beam_runner to make the dataset work.\n\n```python\nfrom datasets import load_dataset\ndef load_and_clean_wiki():\n    dataset = load_dataset('wiki40b', 'da', beam_runner='DirectRunner', split=\"train\")\n    #dataset = load_dataset('wiki40b', 'sv', beam_runner='DirectRunner')\n    dataset = dataset.remove_columns(['wikidata_id', 'version_id'])\n    filtered_dataset = dataset.map(filter_wikipedia)\n    # filtered_dataset[:3]\n    # print(filtered_dataset[:3])\n    return filtered_dataset\n\ndef filter_wikipedia(batch):\n    batch[\"text\"] = \" \".join(batch[\"text\"].split(\"\\\n_START_SECTION_\\\n\"))\n    batch[\"text\"] = \" \".join(batch[\"text\"].split(\"\\\n_START_ARTICLE_\\\n\"))\n    batch[\"text\"] = \" \".join(batch[\"text\"].split(\"\\\n_START_ARTICLE_\\\n\"))\n    batch[\"text\"] = \" \".join(batch[\"text\"].split(\"\\\n_START_PARAGRAPH_\\\n\"))\n    batch[\"text\"] = \" \".join(batch[\"text\"].split(\"_NEWLINE_\"))\n    batch[\"text\"] = \" \".join(batch[\"text\"].split(\"\\xa0\"))\n    return batch\n```\n\n## Training script\nThe following training script was used to train the model.\n```bash\n./run_clm_flax.py     --output_dir=\"${MODEL_DIR}\"     --model_type=\"gpt2\"     --config_name=\"${MODEL_DIR}\"     --tokenizer_name=\"${MODEL_DIR}\"     --dataset_name=\"wiki40b\"     --dataset_config_name=\"da\"     --do_train --do_eval     --block_size=\"512\"     --per_device_train_batch_size=\"64\"     --per_device_eval_batch_size=\"64\"     --learning_rate=\"5e-3\" --warmup_steps=\"1000\"     --adam_beta1=\"0.9\" --adam_beta2=\"0.98\" --weight_decay=\"0.01\"     --overwrite_output_dir     --num_train_epochs=\"20\"     --logging_steps=\"500\"     --save_steps=\"1000\"     --eval_steps=\"2500\"     --push_to_hub\n```\n\n", "size_bytes": "510401385", "downloads": 210}
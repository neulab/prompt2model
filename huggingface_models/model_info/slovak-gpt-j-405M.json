{"pretrained_model_name": "Milos/slovak-gpt-j-405M", "description": "---\nlanguage:\n- sk\ntags:\n- Slovak GPT-J\n- pytorch\n- causal-lm\nlicense: gpl-3.0\n---\n\n# Slovak GPT-J-405M\nSlovak GPT-J-405M is the second model released in Slovak GPT-J series after its smaller variant [Slovak GPT-J-162M](https://huggingface.co/Milos/slovak-gpt-j-162M). Since then a larger [Slovak GPT-J-1.4B](https://huggingface.co/Milos/slovak-gpt-j-1.4B) was released.\n## Model Description\nModel is based on [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax/) and has over 405M trainable parameters.\n\n<figure>\n\n| Hyperparameter       | Value                                                                                                                                  |\n|----------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n| \\\\(n_{parameters}\\\\) | 405,677,136                                                                                                                            |\n| \\\\(n_{layers}\\\\)     | 24                                                                                                                                     |\n| \\\\(d_{model}\\\\)      | 1024                                                                                                                                   |\n| \\\\(d_{ff}\\\\)         | 16384                                                                                                                                  |\n| \\\\(n_{heads}\\\\)      | 16                                                                                                                                     |\n| \\\\(d_{head}\\\\)       | 256                                                                                                                                    |\n| \\\\(n_{ctx}\\\\)        | 2048                                                                                                                                   |\n| \\\\(n_{vocab}\\\\)      | 50256 (same tokenizer as GPT-2/3&dagger;)                                                                                              |\n| Positional Encoding  | [Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864)                                                                   |\n| RoPE Dimensions      | [64](https://github.com/kingoflolz/mesh-transformer-jax/blob/f2aa66e0925de6593dcbb70e72399b97b4130482/mesh_transformer/layers.py#L223) |\n\n<p><strong>&dagger;</strong> ByteLevelBPETokenizer was trained on the same Slovak corpus.</p></figure>\n\n## Training data\n\nSlovak GPT-J models were trained on a privately collected dataset consisting of predominantly Slovak text spanning different categories, e.g. web, news articles or even biblical texts - in total, over 40GB of text data was used to train this model.\nThe dataset was preprocessed and cleaned in a specific way that involves minor but a few caveats, so in order to achieve the expected performance, feel free to refer to [How to use] section. Please, keep in mind that despite the effort to remove inappropriate corpus, the model still might generate sensitive content or leak sensitive information.\n\n## Training procedure\n\nThis model was trained for a bit more than 36.5 billion tokens over 69,001 steps on TPU v3-8 pod. The cross-entropy validation loss at the last step was `2.821`.\n\n## Intended Use\n\nSame as the original GPT-J, Slovak GPT-J learns an inner representation of the language that can be used to extract features useful for downstream tasks, however, the intended use is text generation from a prompt.\n\n### How to use\n\nThis model along with the tokenizer can be easily loaded using the `AutoModelForCausalLM` functionality:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Milos/slovak-gpt-j-405M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Milos/slovak-gpt-j-405M\")\n```\n\nWhen generating a prompt keep in mind these three things, and you should be good to go:\n1. Never leave trailing whitespaces. There's a difference between how tokenizer encodes \"M\u00e1m r\u00e1d sloven\u010dinu\" (no space after `sloven\u010dinu`) and \"M\u00e1m r\u00e1d sloven\u010dinu \" (trailing space after `sloven\u010dinu`), i.e `[12805, 2872, 46878]` != `[12805, 2872, 46878, 221]`.\n2. Always use good ol' US English primary double quotation marks, i.e. `\"\"` instead of `\u201e\u201c`.\n3. In case of a new line always enter `\\n\\n` instead of a single `\\n`\n\nTo illustrate an example of a basic text generation:\n```\n>>> prompt = \"Tradi\u010dn\u00e9 jedlo na Orave s\u00fa\"\n>>> encoded_input = tokenizer(prompt, return_tensors='pt')\n>>> output = model.generate(**encoded_input)\n>>> tokenizer.decode(output[0])\n'Tradi\u010dn\u00e9 jedlo na Orave s\u00fa bryndzov\u00e9 halu\u0161ky\\n\\nNa Orave sa v minulosti varilo viac druhov'\n```\n\n### Capabilities, Limitations, and Biases\n\nThe capability of this particular model is somewhat decent despite its small size totalling 405M parameters. With relative ease it can manage to generate interesting and grammatically correct content.\nFor example, you can try few of the following prompts. (For sake of simplicity, I have omitted all the boilerplate code and swapped `\\n` for new lines).\n\nTry generating \"How to\" articles:\n```\nPROMPT\nAko nap\u00edsa\u0165 p\u00f4sobiv\u00fa esej krok za krokom\nOUTPUT\nAko nap\u00edsa\u0165 p\u00f4sobiv\u00fa esej krok za krokom\n\nV tomto \u010dl\u00e1nku sa dozviete, ako nap\u00edsa\u0165 esej, ktor\u00e1 bude ma\u0165 v sebe nie\u010do, \u010do zaujme a bude sa v\u00e1m p\u00e1\u010di\u0165.\n\nV prvom rade si mus\u00edte uvedomi\u0165, \u017ee esej je vlastne p\u00edsan\u00fd text, ktor\u00fd m\u00e1 by\u0165 nap\u00edsan\u00fd na po\u010d\u00edta\u010di.'\n```\nHowever, relying on the model to produce factually correct information isn't recommended.\n\nOr try to generate chat conversations:\n```\nPROMPT\nPeter: Ako sa mas?\n\nLucia: Ale celkom dobre. Co si robil?\n\nPeter:\nOUTPUT\nPeter: Ako sa mas?\n\nLucia: Ale celkom dobre. Co si robil?\n\nPeter: No, bol som na chate.\n\nLucia: A co si tam robil?\n\nPeter: No, bol som tam s kamo\u0161mi.\n\nLucia: A co si tam robil?\n\nPeter: No, bol som tam s kamo\u0161mi.\n```\nApparently either Peters are more likely to act suspiciously in this particular context or there's a problem with the model. Let's assume that the second explanation may hold some merit. In general, GPT models can (and often will) get into a repeating cycle of generating same content. This is a common problem beyond the scope of this README, however, see [generate's documentation](https://huggingface.co/docs/transformers/master/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate) on how to introduce a frequency/repetition penalty. \n\nSince the dataset contains profanity, politically incorrect language, and (unintentionally) even a bits of text in Czech, the model can generate them in some extent too. Here's an example of the model output when prompt is in Czech:\n```\n>>> prompt = \"V\u011bta nesm\u00ed b\u00fdt sprost\u00e1 a mus\u00ed b\u00fdt zcela\"\n>>> encoded_input = tokenizer(prompt, return_tensors='pt')\n>>> output = model.generate(**encoded_input, max_length=16)\n>>> tokenizer.decode(output[0])\n'V\u011bta nesm\u00ed b\u00fdt sprost\u00e1 a mus\u00ed b\u00fdt zcela pravdiv\u00e1.'\n```\n\n## Citation and Related Information\n\nThis was done as a moonlighting project during summer of 2021 to better understand transformers. I didn't have much free time to open source it properly, so it all sat on my hard drive until now :)\n\nIf you use this model or have any questions about it feel free to hit me up at [twitter](https://twitter.com/miloskondela) or check out my [github](https://github.com/kondela) profile.\n\n### BibTeX entry\nTo cite this model:\n```bibtex\n@misc{slovak-gpt-j-405m,\n  author = {Kondela, Milos},\n  title = {{Slovak GPT-J-405M}},\n  howpublished = {\\url{https://huggingface.co/Milos/slovak-gpt-j-405M}},\n  year = 2022,\n  month = February\n}\n```\n\nTo cite the codebase that trained this model:\n```bibtex\n@misc{mesh-transformer-jax,\n  author = {Wang, Ben},\n  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},\n  howpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\n  year = 2021,\n  month = May\n}\n```\n\n## Acknowledgements\nThis project was generously supported by [TPU Research Cloud (TRC) program](https://sites.research.google/trc/about/). Shoutout also goes to [Ben Wang](https://github.com/kingoflolz) and great [EleutherAI community](https://www.eleuther.ai/).", "size_bytes": "814549011", "downloads": 91}
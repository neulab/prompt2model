{"pretrained_model_name": "Norod78/distilgpt2-base-pretrained-he", "description": "---\nlanguage: he\n\nthumbnail: https://avatars1.githubusercontent.com/u/3617152?norod.jpg\nwidget:\n- text: \"\u05d4\u05d0\u05d9\u05e9 \u05d4\u05d0\u05d7\u05e8\u05d5\u05df \u05e2\u05dc\u05d9 \u05d0\u05d3\u05de\u05d5\u05ea \u05d9\u05e9\u05d1 \u05dc\u05d1\u05d3 \u05d1\u05d7\u05d3\u05e8\u05d5 \u05db\u05e9\u05dc\u05e4\u05ea\u05e2 \u05e0\u05e9\u05de\u05e2\u05d4 \u05e0\u05e7\u05d9\u05e9\u05d4\"\n- text: \"\u05e9\u05dc\u05d5\u05dd, \u05e7\u05e8\u05d5\u05d0\u05d9\u05dd \u05dc\u05d9\"\n- text: \"\u05d4\u05d0\u05e8\u05d9 \u05e4\u05d5\u05d8\u05e8 \u05d7\u05d9\u05d9\u05da \u05d7\u05d9\u05d5\u05da \u05e0\u05d1\u05d5\u05da\"\n- text: \"\u05d4\u05d7\u05ea\u05d5\u05dc \u05e9\u05dc\u05da \u05de\u05d0\u05d5\u05d3 \u05d7\u05de\u05d5\u05d3 \u05d5\"\n\nlicense: mit\n---\n\n# distilgpt2-base-pretrained-he\n\nA tiny GPT2 based Hebrew text generation model initially trained on a TPUv3-8 which was made avilable to me via the [TPU Research Cloud](https://sites.research.google/trc/) Program. Then was further fine-tuned on GPU.\n\n## Dataset\n\n### oscar (unshuffled deduplicated he) - [Homepage](https://oscar-corpus.com) | [Dataset Permalink](https://huggingface.co/datasets/viewer/?dataset=oscar&config=unshuffled_deduplicated_he)\n\nThe Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\n\n### CC-100 (he) - [HomePage](https://data.statmt.org/cc-100/)\n\nThis corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages. This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Each file comprises of documents separated by double-newlines and paragraphs within the same document separated by a newline. The data is generated using the open source CC-Net repository.\n\n### Misc\n* Hebrew Twitter\n* Wikipedia\n* Various other sources\n\n## Training\n\n* Done on a TPUv3-8 VM using [Huggingface's clm-flax example script](https://github.com/huggingface/transformers/blob/master/examples/flax/language-modeling/run_clm_flax.py) <BR>\n* I have made a list of items which might make it easier for other to use this script. The list was posted to [This discussion forum](https://discuss.huggingface.co/t/ideas-for-beginner-friendlier-tpu-vm-clm-training/8351)\n* Further training was performed on GPU\n\n## Usage\n\n\n#### Simple usage sample code\n\n```python\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\ndef main():\n    model_name=\"Norod78/distilgpt2-base-pretrained-he\"\n\n    prompt_text = \"\u05e9\u05dc\u05d5\u05dd, \u05e7\u05d5\u05e8\u05d0\u05d9\u05dd \u05dc\u05d9\"\n    generated_max_length = 192\n\n    print(\"Loading model...\")\n    model =  AutoModelForCausalLM.from_pretrained(model_name)\n    print('Loading Tokenizer...')\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    text_generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n\n    print(\"Generating text...\")\n    result = text_generator(prompt_text, num_return_sequences=1, batch_size=1, do_sample=True, top_k=40, top_p=0.92, temperature = 1, repetition_penalty=5.0, max_length = generated_max_length)\n\n    print(\"result = \" + str(result))\n\nif __name__ == '__main__':\n    main()\n```\n", "size_bytes": "333969117", "downloads": 42}
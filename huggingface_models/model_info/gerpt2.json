{"pretrained_model_name": "benjamin/gerpt2", "description": "---\nlanguage: de\n\nwidget:\n- text: \"In einer schockierenden Entdeckung fanden Wissenschaftler eine Herde Einh\u00f6rner, die in einem abgelegenen, zuvor unerforschten Tal in den Anden lebten.\"\n\nlicense: mit\n---\n\n# GerPT2\n\nGerman large and small versions of GPT2:\n\n- https://huggingface.co/benjamin/gerpt2\n- https://huggingface.co/benjamin/gerpt2-large\n\nSee the [GPT2 model card](https://huggingface.co/gpt2) for considerations on limitations and bias. See the [GPT2 documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for details on GPT2.\n\n## Comparison to [dbmdz/german-gpt2](https://huggingface.co/dbmdz/german-gpt2)\n\nI evaluated both GerPT2-large and the other German GPT2, [dbmdz/german-gpt2](https://huggingface.co/dbmdz/german-gpt2) on the [CC-100](http://data.statmt.org/cc-100/) dataset and on the German Wikipedia:\n\n|                   | CC-100 (PPL) | Wikipedia (PPL) |\n|-------------------|--------------|-----------------|\n| dbmdz/german-gpt2 | 49.47        | 62.92           |\n| GerPT2            | 24.78        | 35.33           |\n| GerPT2-large      | __16.08__    | __23.26__       |\n|                   |              |                 |\n\nSee the script `evaluate.py` in the [GerPT2 Github repository](https://github.com/bminixhofer/gerpt2) for the code.\n\n## Usage\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"benjamin/gerpt2-large\")\nmodel = AutoModelForCausalLM.from_pretrained(\"benjamin/gerpt2-large\")\n\nprompt = \"<your prompt>\"\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprint(pipe(prompt)[0][\"generated_text\"])\n```\n\nAlso, two tricks might improve the generated text:\n\n```python\noutput = model.generate(\n    # during training an EOS token was used to mark the beginning of each text\n    # so it can help to insert it at the start\n    torch.tensor(\n        [tokenizer.eos_token_id] + tokenizer.encode(prompt)\n    ).unsqueeze(0),\n    do_sample=True,\n    # try setting bad_words_ids=[[0]] to disallow generating an EOS token, without this the model is\n    # prone to ending generation early because a significant number of texts from the training corpus\n    # is quite short\n    bad_words_ids=[[0]],\n    max_length=max_length,\n)[0]\nprint(tokenizer.decode(output))\n```\n\n## Training details\n\nGerPT2-large is trained on the entire German data from the [CC-100 Corpus](http://data.statmt.org/cc-100/) and weights were initialized from the [English GPT2 model](https://huggingface.co/gpt2-large). \nGerPT2-large was trained with:\n\n- a batch size of 256\n- using OneCycle learning rate with a maximum of 5e-3\n- with AdamW with a weight decay of 0.01\n- for 2 epochs\n\nTraining took roughly 12 days on 8 TPUv3 cores.\n\nTo train GerPT2-large, follow these steps. Scripts are located in the [Github repository](https://github.com/bminixhofer/gerpt2):\n\n0. Download and unzip training data from http://data.statmt.org/cc-100/.\n1. Train a tokenizer using `prepare/train_tokenizer.py`. As training data for the tokenizer I used a random subset of 5% of the CC-100 data.\n2. (optionally) generate a German input embedding matrix with `prepare/generate_aligned_wte.py`. This uses a neat trick to semantically map tokens from the English tokenizer to tokens from the German tokenizer using aligned word embeddings. E. g.:\n\n```\n\u0120Minde -> \u0120least\n\u0120jed -> \u0120whatsoever\nflughafen -> Air\nvermittlung -> employment\nteilung -> ignment\n\u0120Interpretation -> \u0120interpretation\n\u0120import -> \u0120imported\nhansa -> irl\ngenehmigungen -> exempt\n\u0120Auflist -> \u0120lists\n\u0120verschwunden -> \u0120disappeared\n\u0120Flyers -> \u0120Flyers\nKanal -> Channel\n\u0120lehr -> \u0120teachers\n\u0120nahelie -> \u0120convenient\ngener -> Generally\nmitarbeiter -> staff\n```\n\nThis helps a lot on a trial run I did, although I wasn't able to do a full comparison due to budget and time constraints. To use this WTE matrix it can be passed via the `wte_path` to the training script. Credit to [this blogpost](https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787) for the idea of initializing GPT2 from English weights. \n\n3. Tokenize the corpus using `prepare/tokenize_text.py`. This generates files for train and validation tokens in JSON Lines format.\n4. Run the training script `train.py`! `run.sh` shows how this was executed for the full run with config `configs/tpu_large.json`.\n\n## License\n\nGerPT2 is licensed under the MIT License.\n\n## Citing\n\nPlease cite GerPT2 as follows:\n\n```\n@misc{Minixhofer_GerPT2_German_large_2020,\nauthor = {Minixhofer, Benjamin},\ndoi = {10.5281/zenodo.5509984},\nmonth = {12},\ntitle = {{GerPT2: German large and small versions of GPT2}},\nurl = {https://github.com/bminixhofer/gerpt2},\nyear = {2020}\n}\n```\n\n## Acknowledgements\n\nThanks to [Hugging Face](https://huggingface.co) for awesome tools and infrastructure.\nHuge thanks to [Artus Krohn-Grimberghe](https://twitter.com/artuskg) at [LYTiQ](https://www.lytiq.de/) for making this possible by sponsoring the resources used for training.", "size_bytes": "664788292", "downloads": 221}
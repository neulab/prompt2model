{"pretrained_model_name": "GroNLP/gpt2-small-dutch-embeddings", "description": "---\nlanguage: nl\ntags:\n- adaption\n- recycled\n- gpt2-small\npipeline_tag: text-generation\n---\n\n# GPT-2 recycled for Dutch (small, adapted lexical embeddings)\n[Wietse de Vries](https://www.semanticscholar.org/author/Wietse-de-Vries/144611157) \u2022\n[Malvina Nissim](https://www.semanticscholar.org/author/M.-Nissim/2742475)\n\n## Model description\n\nThis model is based on the small OpenAI GPT-2 ([`gpt2`](https://huggingface.co/gpt2)) model.\n\nThe Transformer layer weights in this model are identical to the original English, model but the lexical layer has been retrained for a Dutch vocabulary.\n\nFor details, check out our paper on [arXiv](https://arxiv.org/abs/2012.05628) and the code on [Github](https://github.com/wietsedv/gpt2-recycle).\n\n\n## Related models\n\n### Dutch\n - [`gpt2-small-dutch-embeddings`](https://huggingface.co/GroNLP/gpt2-small-dutch-embeddings): Small model size with only retrained lexical embeddings.\n - [`gpt2-small-dutch`](https://huggingface.co/GroNLP/gpt2-small-dutch):  Small model size with retrained lexical embeddings and additional fine-tuning of the full model. (**Recommended**)\n - [`gpt2-medium-dutch-embeddings`](https://huggingface.co/GroNLP/gpt2-medium-dutch-embeddings): Medium model size with only retrained lexical embeddings.\n\n### Italian\n - [`gpt2-small-italian-embeddings`](https://huggingface.co/GroNLP/gpt2-small-italian-embeddings): Small model size with only retrained lexical embeddings.\n - [`gpt2-small-italian`](https://huggingface.co/GroNLP/gpt2-small-italian):  Small model size with retrained lexical embeddings and additional fine-tuning of the full model. (**Recommended**)\n - [`gpt2-medium-italian-embeddings`](https://huggingface.co/GroNLP/gpt2-medium-italian-embeddings): Medium model size with only retrained lexical embeddings.\n\n\n## How to use\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-dutch-embeddings\")\n```\n\n```python\nfrom transformers import AutoTokenizer, AutoModel, TFAutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"GroNLP/gpt2-small-dutch-embeddings\")\nmodel = AutoModel.from_pretrained(\"GroNLP/gpt2-small-dutch-embeddings\")  # PyTorch\nmodel = TFAutoModel.from_pretrained(\"GroNLP/gpt2-small-dutch-embeddings\")  # Tensorflow\n```\n\n## BibTeX entry\n\n```bibtex\n@misc{devries2020good,\n      title={As good as new. How to successfully recycle English GPT-2 to make models for other languages}, \n      author={Wietse de Vries and Malvina Nissim},\n      year={2020},\n      eprint={2012.05628},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n", "size_bytes": "478894323", "downloads": 98}
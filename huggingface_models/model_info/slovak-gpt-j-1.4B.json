{"pretrained_model_name": "Milos/slovak-gpt-j-1.4B", "description": "---\nlanguage:\n- sk\ntags:\n- Slovak GPT-J\n- pytorch\n- causal-lm\nlicense: gpl-3.0\n---\n\n# Slovak GPT-J-1.4B\nSlovak GPT-J-1.4B with the whopping `1,415,283,792` parameters is the latest and the largest model released in Slovak GPT-J series. Smaller variants, [Slovak GPT-J-405M](https://huggingface.co/Milos/slovak-gpt-j-405M) and [Slovak GPT-J-162M](https://huggingface.co/Milos/slovak-gpt-j-162M), are still available.\n## Model Description\nModel is based on [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax/) and has over 1.4B trainable parameters.\n\n<figure>\n\n| Hyperparameter       | Value                                                                                                                                  |\n|----------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n| \\\\(n_{parameters}\\\\) | 1,415,283,792                                                                                                                          |\n| \\\\(n_{layers}\\\\)     | 24                                                                                                                                     |\n| \\\\(d_{model}\\\\)      | 2048                                                                                                                                   |\n| \\\\(d_{ff}\\\\)         | 16384                                                                                                                                  |\n| \\\\(n_{heads}\\\\)      | 16                                                                                                                                     |\n| \\\\(d_{head}\\\\)       | 256                                                                                                                                    |\n| \\\\(n_{ctx}\\\\)        | 2048                                                                                                                                   |\n| \\\\(n_{vocab}\\\\)      | 50256 (same tokenizer as GPT-2/3&dagger;)                                                                                              |\n| Positional Encoding  | [Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864)                                                                   |\n| RoPE Dimensions      | [64](https://github.com/kingoflolz/mesh-transformer-jax/blob/f2aa66e0925de6593dcbb70e72399b97b4130482/mesh_transformer/layers.py#L223) |\n\n<p><strong>&dagger;</strong> ByteLevelBPETokenizer was trained on the same Slovak corpus.</p></figure>\n\n## Training data\n\nSlovak GPT-J models were trained on a privately collected dataset consisting of predominantly Slovak text spanning different categories, e.g. web, news articles or even biblical texts - in total, over 40GB of text data was used to train this model.\nThe dataset was preprocessed and cleaned in a specific way that involves minor but a few caveats, so in order to achieve the expected performance, feel free to refer to [How to use] section. Please, keep in mind that despite the effort to remove inappropriate corpus, the model still might generate sensitive content or leak sensitive information.\n\n## Training procedure\n\nThis model was trained for a bit more than 26.5 billion tokens over 48,001 steps on TPU v3-8 pod. The cross-entropy validation loss at the last step was `2.657`.\n\n## Intended Use\n\nSame as the original GPT-J, Slovak GPT-J learns an inner representation of the language that can be used to extract features useful for downstream tasks, however, the intended use is text generation from a prompt.\n\n### How to use\n\nThis model along with the tokenizer can be easily loaded using the `AutoModelForCausalLM` functionality:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Milos/slovak-gpt-j-1.4B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Milos/slovak-gpt-j-1.4B\")\n```\n\nWhen generating a prompt keep in mind these three things, and you should be good to go:\n1. Never leave trailing whitespaces. There's a difference between how tokenizer encodes \"M\u00e1m r\u00e1d sloven\u010dinu\" (no space after `sloven\u010dinu`) and \"M\u00e1m r\u00e1d sloven\u010dinu \" (trailing space after `sloven\u010dinu`), i.e `[12805, 2872, 46878]` != `[12805, 2872, 46878, 221]`.\n2. Always use good ol' US English primary double quotation marks, i.e. `\"\"` instead of `\u201e\u201c`.\n3. In case of a new line always enter `\\n\\n` instead of a single `\\n`\n\nTo illustrate an example of a basic text generation:\n```\n>>> prompt = \"Tradi\u010dn\u00e9 jedlo na Orave s\u00fa\"\n>>> encoded_input = tokenizer(prompt, return_tensors='pt')\n>>> output = model.generate(**encoded_input)\n>>> tokenizer.decode(output[0])\n'Tradi\u010dn\u00e9 jedlo na Orave s\u00fa bryndzov\u00e9 halu\u0161ky\\n\\nNa Orave sa v minulosti varilo viac druhov'\n```\n\n### Capabilities, Limitations, and Biases\n\nBased on the parameter count and our internal testing this should be by far the most capable model out of all models released in Slovak GPT-J series.\nFor sake of simplicity, I have omitted all the boilerplate code and swapped `\\n` for new lines in the examples that follow. These were generated with both `top_p` and `temperature` set to `0.9`. \n\nA spin on \"Mor ho!\" by Samo Chalpuka \n```\nPROMPT\nZleteli orly z Tatry, tiahnu na podolia, ponad vysok\u00e9 hory, ponad rovn\u00e9 polia; preleteli cez Dunaj, cez t\u00fa \u0161\u00edru vodu, sadli tam za pomedz\u00edm slovensk\u00e9ho rodu.\n\nDun\u00ed Dunaj a luna za lunou sa val\u00ed\nOUTPUT\nZleteli orly z Tatry, tiahnu na podolia, ponad vysok\u00e9 hory, ponad rovn\u00e9 polia; preleteli cez Dunaj, cez t\u00fa \u0161\u00edru vodu, sadli tam za pomedz\u00edm slovensk\u00e9ho rodu.\n\nDun\u00ed Dunaj a luna za lunou sa val\u00ed: nad n\u00edm svieti pevn\u00fd hrad na vysokom bral\u00ed. Pod t\u00fdm hradom je zem slovensk\u00e1, na nej je Slovensko na\u0161e.\n\nNa tom vrchu le\u017e\u00ed slovensk\u00fd n\u00e1rod, pod t\u00fdm bralom le\u017e\u00ed krajina slovensk\u00e1. A pod t\u00fdm hradom le\u017e\u00ed i jeho sl\u00e1va: na tom vrchu, na tom brale le\u017e\u00ed sl\u00e1va slovensk\u00e9ho n\u00e1roda.```\n```\n\nJokes are a bit hit or miss. Very often they contain vulgarisms and/or are highly politically incorrect. This one is rather exception than the rule.\n```\nPROMPT\nNasleduj\u00fa najlep\u0161ie vtipy.\n\nPre\u010do\nOUTPUT\nNasleduj\u00fa najlep\u0161ie vtipy.\n\nPre\u010do m\u00e1 policajt pri sebe v\u017edy dva no\u017ee? Jeden je na sebaobranu, druh\u00fd na boj zbl\u00edzka.\n```\n\nThe infamous unicorn prompt from GPT-2 paper.\n```\nPROMPT\nV \u0161okuj\u00facom n\u00e1leze vedci objavili st\u00e1do jednoro\u017ecov \u017eij\u00facich v od\u013eahlom, predt\u00fdm nepresk\u00famanom \u00fadol\u00ed, v And\u00e1ch. E\u0161te prekvapivej\u0161ie pre vedcov bol fakt, \u017ee jednoro\u017ece hovorili plynule po slovensky.\n\nOUTPUT\nV \u0161okuj\u00facom n\u00e1leze vedci objavili st\u00e1do jednoro\u017ecov \u017eij\u00facich v od\u013eahlom, predt\u00fdm nepresk\u00famanom \u00fadol\u00ed, v And\u00e1ch. E\u0161te prekvapivej\u0161ie pre vedcov bol fakt, \u017ee jednoro\u017ece hovorili plynule po slovensky.\n\nV And\u00e1ch ich toti\u017e \u017eilo pribli\u017ene 700 jedincov, ktor\u00e9 sa \u017eivili v\u00fdhradne matersk\u00fdm mliekom. Slov\u00e1ci s\u00fa tak pravdepodobne prv\u00fdmi Eur\u00f3panmi, ktor\u00ed v And\u00e1ch stretli jednoro\u017eca. \"Je to d\u00f4kaz, \u017ee tieto zvierat\u00e1 s\u00fa inteligentn\u00e9 a \u017ee \u017eij\u00fa v And\u00e1ch u\u017e stovky rokov,\" povedal pre webov\u00fd port\u00e1l telev\u00edzie JOJ profesor geol\u00f3gie, geografie a zool\u00f3gie, Milan Kov\u00e1\u010d.\n\nPod\u013ea profesora Kov\u00e1\u010da si v And\u00e1ch zvierat\u00e1 vytv\u00e1raj\u00fa svoj vlastn\u00fd jazyk. Je to z\u00e1rove\u0148 d\u00f4kaz, \u017ee jednoro\u017ece \u017eili v minulosti aj v slovensk\u00fdch pohoriach. \"Jednoro\u017ece sa tam s\u00edce vyskytovali, ale neboli tak dobre presk\u00faman\u00e9, ako teraz v And\u00e1ch.\"\n\nNa Slovensku v\u0161ak \u013eudia o jednoro\u017ecoch doned\u00e1vna vedeli ve\u013emi m\u00e1lo.<|endoftext|>\n```\n\nSince the dataset contains profanity, politically incorrect language, and (unintentionally) even a bits of text in Czech, the model can generate them in some extent too. Here's an example of the model output when prompt is in Czech:\n```\n>>> prompt = \"V\u011bta nesm\u00ed b\u00fdt sprost\u00e1 a mus\u00ed b\u00fdt zcela\"\n>>> encoded_input = tokenizer(prompt, return_tensors='pt')\n>>> output = model.generate(**encoded_input, max_length=16)\n>>> tokenizer.decode(output[0])\n'V\u011bta nesm\u00ed b\u00fdt sprost\u00e1 a mus\u00ed b\u00fdt zcela pravdiv\u00e1.'\n```\n\n## Citation and Related Information\n\nThis was done as a moonlighting project during summer of 2021 to better understand transformers. I didn't have much free time to open source it properly, so it all sat on my hard drive until now :)\n\nIf you use this model or have any questions about it feel free to hit me up at [twitter](https://twitter.com/miloskondela) or check out my [github](https://github.com/kondela) profile.\n\n### BibTeX entry\nTo cite this model:\n```bibtex\n@misc{slovak-gpt-j-1.4B,\n  author = {Kondela, Milos},\n  title = {{Slovak GPT-J-1.4B}},\n  howpublished = {\\url{https://huggingface.co/Milos/slovak-gpt-j-1.4B}},\n  year = 2022,\n  month = February\n}\n```\n\nTo cite the codebase that trained this model:\n```bibtex\n@misc{mesh-transformer-jax,\n  author = {Wang, Ben},\n  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},\n  howpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\n  year = 2021,\n  month = May\n}\n```\n\n## Acknowledgements\nThis project was generously supported by [TPU Research Cloud (TRC) program](https://sites.research.google/trc/about/). Shoutout also goes to [Ben Wang](https://github.com/kingoflolz) and great [EleutherAI community](https://www.eleuther.ai/).", "size_bytes": "2832685075", "downloads": 124}
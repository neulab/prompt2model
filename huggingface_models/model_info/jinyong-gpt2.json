{"pretrained_model_name": "supermy/jinyong-gpt2", "description": "---\nlanguage: zh\ndatasets: jinyong\ninference:\n  parameters:\n    max_length: 108\n    num_return_sequences: 1\n    do_sample: True\nwidget: \n- text: \"\u6768\u8fc7\u6717\u58f0\u8bf4\u9053\uff1a\u4eca\u756a\u826f\u6664\uff0c\u8c6a\u5174\u4e0d\u6d45\uff0c\u4ed6\u65e5\u6c5f\u6e56\u76f8\u9022\uff0c\u518d\u5f53\u676f\u9152\u8a00\u6b22\u3002\u54b1\u4eec\u5c31\u6b64\u522b\u8fc7\u3002 -\"\n  example_title: \"\u795e\u96d5\u4fa0\u4fa3\"\n- text: \"\u4e71\u4e16\u4e4b\u9645\uff0c\u4eba\u4e0d\u5982\u72d7\u3002 -\"\n  example_title: \"\u5c04\u96d5\u82f1\u96c4\u4f20\"\n---\n\n\n\n\n# \u98de\u96ea\u8fde\u5929\u5c04\u767d\u9e7f\uff0c\u7b11\u4e66\u795e\u4fa0\u501a\u78a7\u9e33\n\n## Model description\n\n  AI\u751f\u6210\u91d1\u5eb8\u5c0f\u8bf4\uff0c\u7ed9\u51fa\u5f00\u5934\u7eed\u5199\u3002\n\n## How to use\n\u4f7f\u7528 pipeline \u8c03\u7528\u6a21\u578b:\n\n```python\n>>> # \u8c03\u7528\u5fae\u8c03\u540e\u7684\u6a21\u578b\n>>> senc=\"\u8fd9\u4e9b\u96ea\u82b1\u843d\u4e0b\u6765,\u591a\u4e48\u767d,\u591a\u4e48\u597d\u770b.\u8fc7\u51e0\u5929\u592a\u9633\u51fa\u6765,\u6bcf\u4e00\u7247 \u96ea\u82b1\u90fd\u53d8\u5f97\u65e0\u5f71\u65e0\u8e2a.\u5230\u5f97\u660e\u5e74\u51ac\u5929,\u53c8\u6709\u8bb8\u5f88\u591a\u591a\u96ea\u82b1,\u53ea\u4e0d\u8fc7\u5df2\u4e0d\u662f \u4eca\u5e74\u8fd9\u4e9b\u96ea\u82b1\u7f62\u4e86\u3002\"\n>>> model_id=\"jinyong-gpt2-finetuning\"\n>>> from transformers import AutoTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n\n>>> tokenizer = AutoTokenizer.from_pretrained(model_id) \n>>> model = GPT2LMHeadModel.from_pretrained(model_id)\n>>> text_generator = TextGenerationPipeline(model, tokenizer)   \n>>> text_generator.model.config.pad_token_id = text_generator.model.config.eos_token_id\n>>> text_generator( senc,max_length=108, do_sample=True)\n[{'generated_text': '\u8fd9\u4e9b\u96ea\u82b1\u843d\u4e0b\u6765,\u591a\u4e48\u767d,\u591a\u4e48\u597d\u770b.\u8fc7\u51e0\u5929\u592a\u9633\u51fa\u6765,\u6bcf\u4e00\u7247 \u96ea\u82b1\u90fd\u53d8\u5f97\u65e0\u5f71\u65e0\u8e2a.\u5230\u5f97\u660e\u5e74\u51ac\u5929,\u53c8\u6709\u8bb8\u5f88\u591a\u591a\u96ea\u82b1,\u53ea\u4e0d\u8fc7\u5df2\u4e0d\u662f \u4eca\u5e74\u8fd9\u4e9b\u96ea\u82b1\u7f62\u4e86\u3002 \u53cd\u6b63 \u8001\u5929\u7237 \u6709\u773c \uff0c \u4e0d\u77e5 \u54ea\u91cc \u662f\u751a\u4e48 \u98ce \u9669 \uff1f\u201d \u6b63 \u8bf4\u5230\u6b64\u5904 \uff0c \u7a81\u7136 \u542c\u5f97 \u8c22\u900a \u5578\u58f0 \u6e10\u8fd1 \uff0c \u5fcd\u4e0d\u4f4f \u5f20\u53e3 \u60ca\u547c \uff0c \u4e00\u9f50 \u5411\u4ed6 \u6251\u53bb \uff0c \u53ea\u542c \u8c22\u900a \u4e00\u58f0 \u6012\u543c \uff0c \u8ddf\u7740 \u5de6\u624b \u7528\u529b \u62cd \u51fa\u4e00\u638c \uff0c \u4ee5 \u638c\u529b \u5316\u5f00 \u3002 \u4f17\u4eba \u5403\u4e86\u4e00\u60ca \uff0c \u540c\u65f6 \u4ece \u6d77 \u9053 \u4e2d \u8dc3\u51fa \uff0c \u53cc\u53cc \u5012\u9000 \u3002 \u5f20\u7fe0\u5c71\u548c\u6bb7\u7d20\u7d20 \u5bf9\u671b\u4e00\u773c \uff0c \u5747\u60f3 \u4ee5 \u8fd9\u4e24 \u5927\u9ad8\u624b \u4e4b\u529b \u5982\u4f55 \u62b5\u6321 \uff0c \u4ee5 \u4eca\u65e5 \u4e4b\u529b \u5982\u4f55 \u653b\u654c \u4e4b'}]\n>>> \n```\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"supermy/jinyong-gpt2\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"supermy/jinyong-gpt2\")\n```\n\n\n\n## Training data\n\n\u6b64\u6570\u636e\u96c6\u57fa\u4e8e\u91d1\u5eb8\u7684\u3010\u98de\u96ea\u8fde\u5929\u5c04\u767d\u9e7f\uff0c\u7b11\u4e66\u795e\u4fa0\u501a\u78a7\u9e33\u3011\u5c0f\u8bf4\u96c6\u8bad\u7ec3\u3002\n\n## \u7edf\u8ba1\u4fe1\u606f\n\n```\n```\n\n## Training procedure\n\n\u57fa\u4e8e\u6a21\u578b\uff1a[GPT2](https://huggingface.co/gpt2) \n\u8bad\u7ec3\u73af\u5883\uff1a\u82f1\u4f1f\u8fbe16G\u663e\u5361\n\nbpe\u5206\u8bcd\uff1a\"vocab_size\"=30000\n```\n[INFO|trainer.py:1608] 2022-12-02 19:52:59,024 >> ***** Running training *****\n[INFO|trainer.py:1609] 2022-12-02 19:52:59,024 >>   Num examples = 9443\n[INFO|trainer.py:1610] 2022-12-02 19:52:59,024 >>   Num Epochs = 108\n[INFO|trainer.py:1611] 2022-12-02 19:52:59,024 >>   Instantaneous batch size per device = 12\n[INFO|trainer.py:1612] 2022-12-02 19:52:59,024 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n[INFO|trainer.py:1613] 2022-12-02 19:52:59,024 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1614] 2022-12-02 19:52:59,024 >>   Total optimization steps = 84996\n[INFO|trainer.py:1616] 2022-12-02 19:52:59,025 >>   Number of trainable parameters = 124439808\n\n[INFO|trainer.py:1608] 2022-12-03 21:44:00,182 >> ***** Running training *****\n[INFO|trainer.py:1609] 2022-12-03 21:44:00,182 >>   Num examples = 9443\n[INFO|trainer.py:1610] 2022-12-03 21:44:00,182 >>   Num Epochs = 216\n[INFO|trainer.py:1611] 2022-12-03 21:44:00,182 >>   Instantaneous batch size per device = 12\n[INFO|trainer.py:1612] 2022-12-03 21:44:00,182 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n[INFO|trainer.py:1613] 2022-12-03 21:44:00,182 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1614] 2022-12-03 21:44:00,182 >>   Total optimization steps = 169992\n[INFO|trainer.py:1616] 2022-12-03 21:44:00,183 >>   Number of trainable parameters = 124439808\n[INFO|trainer.py:1637] 2022-12-03 21:44:00,184 >>   Continuing training from checkpoint, will skip to saved global_step\n[INFO|trainer.py:1638] 2022-12-03 21:44:00,184 >>   Continuing training from epoch 107\n[INFO|trainer.py:1639] 2022-12-03 21:44:00,184 >>   Continuing training from global step 84500\n\n[INFO|trainer.py:1608] 2022-12-05 07:36:13,626 >> ***** Running training *****\n[INFO|trainer.py:1609] 2022-12-05 07:36:13,626 >>   Num examples = 9443\n[INFO|trainer.py:1610] 2022-12-05 07:36:13,626 >>   Num Epochs = 368\n[INFO|trainer.py:1611] 2022-12-05 07:36:13,626 >>   Instantaneous batch size per device = 12\n[INFO|trainer.py:1612] 2022-12-05 07:36:13,626 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n[INFO|trainer.py:1613] 2022-12-05 07:36:13,626 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1614] 2022-12-05 07:36:13,626 >>   Total optimization steps = 289616\n[INFO|trainer.py:1616] 2022-12-05 07:36:13,627 >>   Number of trainable parameters = 124439808\n[INFO|trainer.py:1637] 2022-12-05 07:36:13,628 >>   Continuing training from checkpoint, will skip to saved global_step\n[INFO|trainer.py:1638] 2022-12-05 07:36:13,628 >>   Continuing training from epoch 255\n[INFO|trainer.py:1639] 2022-12-05 07:36:13,628 >>   Continuing training from global step 201000\n\n{'loss': 8.0431, 'learning_rate': 4.970998635229893e-05, 'epoch': 0.64}\n{'loss': 7.4867, 'learning_rate': 4.94158548637583e-05, 'epoch': 1.27}\n{'loss': 7.322, 'learning_rate': 4.912172337521766e-05, 'epoch': 1.91}\n......\n{'loss': 3.901, 'learning_rate': 2.5010882865076008e-05, 'epoch': 108.01}\n{'loss': 3.8959, 'learning_rate': 2.4863817120805686e-05, 'epoch': 108.64}\n......\n{'loss': 3.1625, 'learning_rate': 4.6090404254317857e-07, 'epoch': 214.1}\n{'loss': 3.1592, 'learning_rate': 3.1413242976140055e-07, 'epoch': 214.74}\n{'loss': 3.1625, 'learning_rate': 1.6706668549108195e-07, 'epoch': 215.37}\n{'train_runtime': 72271.9602, 'train_samples_per_second': 28.222, 'train_steps_per_second': 2.352, 'train_loss': 1.7180436183842016, 'epoch': 216.0}\n{'loss': 2.7087, 'learning_rate': 4.2642671675598036e-08, 'epoch': 367.85}\n{'train_runtime': 74859.0808, 'train_samples_per_second': 46.421, 'train_steps_per_second': 3.869, 'train_loss': 0.8725239146935282, 'epoch': 368.0}\n***** train metrics *****\n  epoch                    =       368.0\n  train_loss               =      0.8725\n  train_runtime            = 20:47:39.08\n  train_samples            =        9443\n  train_samples_per_second =      46.421\n  train_steps_per_second   =       3.869\n12/06/2022 04:23:55 - INFO - __main__ - *** Evaluate ***\n[INFO|trainer.py:2929] 2022-12-06 04:23:55,953 >> ***** Running Evaluation *****\n[INFO|trainer.py:2931] 2022-12-06 04:23:55,953 >>   Num examples = 283\n[INFO|trainer.py:2934] 2022-12-06 04:23:55,954 >>   Batch size = 12\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:07<00:00,  3.20it/s]\n[INFO|modelcard.py:449] 2022-12-06 04:24:04,760 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.19599206157122803}]}\n***** eval metrics *****\n  epoch                   =      368.0\n  eval_accuracy           =      0.196\n  eval_loss               =     7.9524\n  eval_runtime            = 0:00:07.87\n  eval_samples            =        283\n  eval_samples_per_second =      35.94\n  eval_steps_per_second   =      3.048\n  perplexity              =  2842.2766\n```", "size_bytes": "510394089", "downloads": 12}
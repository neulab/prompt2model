{"pretrained_model_name": "dbmdz/german-gpt2-faust", "description": "---\nlanguage: de\n\nwidget:\n- text: \"Schon um die Liebe\"\n\nlicense: mit\n---\n\n# German GPT-2 model\n\nIn this repository we release (yet another) GPT-2 model, that was trained on various texts for German.\n\nThe model is meant to be an entry point for fine-tuning on other texts, and it is definitely not as good or \"dangerous\" as the English GPT-3 model. We do not plan extensive PR or staged releases for this model \ud83d\ude09\n\n**Note**: The model was initially released under an anonymous alias (`anonymous-german-nlp/german-gpt2`) so we now \"de-anonymize\" it.\n\nMore details about GPT-2 can be found in the great [Hugging Face](https://huggingface.co/transformers/model_doc/gpt2.html) documentation.\n\n## German GPT-2 fine-tuned on Faust I and II\n\nWe fine-tuned our German GPT-2 model on \"Faust I and II\" from Johann Wolfgang Goethe. These texts can be obtained from [Deutsches Textarchiv (DTA)](http://www.deutschestextarchiv.de/book/show/goethe_faust01_1808). We use the \"normalized\" version of both texts (to avoid out-of-vocabulary problems with e.g. \"\u017f\")\n\nFine-Tuning was done for 100 epochs, using a batch size of 4 with half precision on a RTX 3090. Total time was around 12 minutes (it is really fast!).\n\nWe also open source this fine-tuned model. Text can be generated with:\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline('text-generation', model=\"dbmdz/german-gpt2-faust\",\n                 tokenizer=\"dbmdz/german-gpt2-faust\")   \n\ntext = pipe(\"Schon um die Liebe\", max_length=100)[0][\"generated_text\"]\n\nprint(text)\n```\n\nand could output:\n\n```\nSchon um die Liebe bitte ich, Herr! Wer mag sich die dreifach Erm\u00e4chtigen?\nSei mir ein Held!\nUnd da\u00df die Stunde kommt spreche ich nicht aus.\nFaust (schaudernd).\nDen sch\u00f6nen Boten finde' ich verwirrend;\n```\n\n# License\n\nAll models are licensed under [MIT](LICENSE).\n\n# Huggingface model hub\n\nAll models are available on the [Huggingface model hub](https://huggingface.co/dbmdz).\n\n# Contact (Bugs, Feedback, Contribution and more)\n\nFor questions about our BERT models just open an issue\n[here](https://github.com/stefan-it/german-gpt/issues/new) \ud83e\udd17\n\n# Acknowledgments\n\nResearch supported with Cloud TPUs from Google's TensorFlow Research Cloud (TFRC).\nThanks for providing access to the TFRC \u2764\ufe0f\n\nThanks to the generous support from the [Hugging Face](https://huggingface.co/) team,\nit is possible to download both cased and uncased models from their S3 storage \ud83e\udd17\n", "size_bytes": "515764134", "downloads": 116}
{"pretrained_model_name": "robowaifudev/megatron-gpt2-345m", "description": "---\nlanguage: \n  - en\ntags:\n  - gpt2\nlicense: apache-2.0\nwidget:\n  - text: It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith,\ndatasets:\n  - wikitext\n  - openwebtext\n  - spacemanidol/cc-stories\nmodel-index:\n  - name: megatron-gpt2-345m\n    results:\n      - task:\n          type: text-generation\n          name: Text generation\n        dataset:\n          name: WikiText-103\n          type: wikitext\n        metrics:\n          - type: wikitext\n            value: 19.31\n            name: Perplexity\n      - task:\n          type: text-generation\n          name: Text generation\n        dataset:\n          name: WikiText-2\n          type: wikitext\n        metrics:\n          - type: wikitext\n            value: 17.151\n            name: Perplexity\n      - task:\n          type: text-generation\n          name: Text generation\n        dataset:\n          name: LAMBADA\n          type: lambada\n        metrics:\n          - type: lambada\n            value: 5.509\n            name: Perplexity\n          - type: lambada\n            value: 68.31%\n            name: Accuracy\n---\n\n<!---\n# ##############################################################################################\n# \n# Copyright (c) 2021-, NVIDIA CORPORATION.  All rights reserved.\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# \n# ##############################################################################################\n-->\n\nThis is an archive of [nvidia/megatron-gpt2-345m](https://huggingface.co/nvidia/megatron-gpt2-345m) that contains readily available model weights (375M). Its performance on Wikitext-103 is 19.31.<sup>1</sup> In comparison, the performance of GPT2-large (1.5B) is 17.48 and GPT2-medium (762M) is 22.05.<sup>2</sup>\n\n### References\n\n1. Shoeybi, Mohammad, et al. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv, 2019, [https://doi.org/10.48550/ARXIV.1909.08053](https://doi.org/10.48550/ARXIV.1909.08053).\n2. Alec Radford, et al. Language Models are Unsupervised Multitask Learners. OpenAI, 2019. [https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n\n## Description\n\n[Megatron](https://arxiv.org/pdf/1909.08053.pdf) is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. This particular Megatron model was trained from a generative, left-to-right transformer in the style of GPT-2. This model was trained on text sourced from Wikipedia, RealNews, OpenWebText, and CC-Stories. It contains 345 million parameters. \n\nFind more information at [https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\n\n# How to run Megatron GPT2 using Transformers\n\n## Text generation\n\nThe following code shows how to use the Megatron GPT2 checkpoint and Transformers to generate text.\n\n```python\nimport os\nimport torch\n\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"robowaifudev/megatron-gpt2-345m\")\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    model.half()\nelse:\n    device = torch.device(\"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# Generate\nprompt = (\n\"It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith,\"\n)\ninput_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutput = model.generate(\n    input_ids=input_ids,\n    max_length=len(input_ids) + 128,\n    do_sample=True,\n    top_k=64,\n    top_p=0.9,\n    temperature=0.8,\n    num_return_sequences=2,\n    repetition_penalty=1.025\n)\n\n# Output the text\nprint(\"Prompt:\", prompt)\nprint(\"*\" * 3)\nfor i, sentence in enumerate(output):\n    text = tokenizer.decode(sentence, clean_up_tokenization_spaces=True)\n    print(f\"{i}:\", text)\n    print(\"*\" * 3)\n```\n\n# Original code\n\nThe original Megatron code can be found here: [https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM).\n", "size_bytes": "735011993", "downloads": 437}
{"pretrained_model_name": "aliosm/ComVE-gpt2-large", "description": "---\nlanguage: \"en\"\ntags:\n- gpt2\n- exbert\n- commonsense\n- semeval2020\n- comve\nlicense: \"mit\"\ndatasets:\n- https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation\nmetrics:\n- bleu\nwidget:\n- text: \"Chicken can swim in water. <|continue|>\"\n---\n\n# ComVE-gpt2-large\n\n## Model description\n\nFinetuned model on Commonsense Validation and Explanation (ComVE) dataset introduced in [SemEval2020 Task4](https://competitions.codalab.org/competitions/21080) using a causal language modeling (CLM) objective.\nThe model is able to generate a reason why a given natural language statement is against commonsense.\n\n## Intended uses & limitations\n\nYou can use the raw model for text generation to generate reasons why natural language statements are against commonsense.\n\n#### How to use\n\nYou can use this model directly to generate reasons why the given statement is against commonsense using [`generate.sh`](https://github.com/AliOsm/SemEval2020-Task4-ComVE/tree/master/TaskC-Generation) script.\n\n*Note:* make sure that you are using version `2.4.1` of `transformers` package. Newer versions has some issue in text generation and the model repeats the last token generated again and again.\n\n#### Limitations and bias\n\nThe model biased to negate the entered sentence usually instead of producing a factual reason.\n\n## Training data\n\nThe model is initialized from the [gpt2-large](https://github.com/huggingface/transformers/blob/master/model_cards/gpt2-README.md) model and finetuned using [ComVE](https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation) dataset which contains 10K against commonsense sentences, each of them is paired with three reference reasons.\n\n## Training procedure\n\nEach natural language statement that against commonsense is concatenated with its reference reason with `<|conteniue|>` as a separator, then the model finetuned using CLM objective.\nThe model trained on Nvidia Tesla P100 GPU from Google Colab platform with 5e-5 learning rate, 5 epochs, 128 maximum sequence length and 64 batch size.\n\n<center>\n  <img src=\"https://i.imgur.com/xKbrwBC.png\">\n</center>\n\n## Eval results\n\nThe model achieved 16.5110/15.9299 BLEU scores on SemEval2020 Task4: Commonsense Validation and Explanation development and testing dataset.\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{fadel2020justers,\n  title={JUSTers at SemEval-2020 Task 4: Evaluating Transformer Models Against Commonsense Validation and Explanation},\n  author={Fadel, Ali and Al-Ayyoub, Mahmoud and Cambria, Erik},\n  year={2020}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=aliosm/ComVE-gpt2-large\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n", "size_bytes": "3247214227", "downloads": 3}
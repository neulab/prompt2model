{"pretrained_model_name": "Norod78/hebrew-bad_wiki-gpt_neo-tiny", "description": "---\nlanguage: he\n\nthumbnail: https://avatars1.githubusercontent.com/u/3617152?norod.jpg\nwidget:\n- text: \"\u05de\u05ea\u05de\u05d8\u05d9\u05e7\u05d4:\"\n- text: \"\u05e2\u05dc\u05d9\u05d9\u05ea \u05d4\u05de\u05db\u05d5\u05e0\u05d5\u05ea\"\n- text: \"\u05d5\u05d9\u05e7\u05d9\u05e4\u05d3\u05d9\u05d4 \u05d4\u05e2\u05d1\u05e8\u05d9\u05ea\"\n- text: \"\u05d4\u05d0\u05d9\u05e8\u05d5\u05d5\u05d9\u05d6\u05d9\u05d5\u05df \u05d4\u05d5\u05d0\"\n- text: \"\u05d3\u05d5\u05d3 \u05d1\u05df-\u05d2\u05d5\u05e8\u05d9\u05d5\u05df \u05d4\u05d9\u05d4\"\n\nlicense: mit\n---\n\n# hebrew-bad_wiki-gpt_neo-tiny\n\n## Table of Contents\n- [Model Details](#model-details)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n\n## Model Details\n**Model Description:**\n\nThe model developer notes that the model is \n> Hebrew nonsense generation model which produces really bad wiki-abstract text. \n\n\n- **Developed by:** [Doron Adler](https://github.com/Norod)\n- **Model Type:** Text Generation\n- **Language(s):** Hebrew\n- **License:** MIT\n- **Resources for more information:**\n- [GitHub Repo](https://github.com/Norod/hebrew-gpt_neo)\n- [HuggingFace Space](https://huggingface.co/spaces/Norod78/Hebrew-GPT-Neo-Small)\n\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for text generation.\n\n#### Misuse and Out-of-scope Use\n\n\n## Risks, Limitations and Biases\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\n\n\n\n\n## Training\n\n\n#### Training Data\n [Hebrew Wikipedia Dump](https://dumps.wikimedia.org/hewiki/latest/) (hewiki abstract) from May 2020\n\n\n\n\n#### Training Procedure\n\n\nThis model was fined tuned upon [hebrew-gpt_neo-tiny](https://huggingface.co/Norod78/hebrew-gpt_neo-tiny) which was previously trained using [EleutherAI's gpt-neo](https://github.com/EleutherAI/gpt-neo). \n\nFine-tuning on the wiki-absract text was done using [@minimaxir](https://twitter.com/minimaxir)'s [aitextgen](https://github.com/minimaxir/aitextgen).\n\n\n\n## Evaluation\n\n\n#### Configs\n\nModel configs for the hebrew-gpt_neo-tiny is available on the [hebrew-gpt_neo model github](https://github.com/Norod/hebrew-gpt_neo/tree/main/hebrew-gpt_neo-tiny/configs) \n\n* **Activation Function:** gelu\n* **Number_Head:** 12\n* **Number_Vocab:** 50257\n* **Train batch size:** 250\n* **Eval batch size:** 64\n* **Predict batch size:** 1\n\n\n\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). We present the hardware type based on the [associated paper](https://arxiv.org/pdf/2105.09680.pdf).\n\n\n- **Hardware Type:** [More information needed]\n\n- **Hours used:** Unknown\n\n- **Cloud Provider:** GCP  tpu-v8s\n\n- **Compute Region:** europe-west4\n\n- **Carbon Emitted:** [More information needed]\n\n\n## How to Get Started With the Model\n\nA Google Colab Notebook is also available [here](https://colab.research.google.com/github/Norod/hebrew-gpt_neo/blob/main/hebrew-gpt_neo-tiny/Norod78_hebrew_gpt_neo_tiny_Colab.ipynb)\n\n\n\u200b\u200b\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Norod78/hebrew-bad_wiki-gpt_neo-tiny\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Norod78/hebrew-bad_wiki-gpt_neo-tiny\")\n\n\n```\n\n\n", "size_bytes": "333917693", "downloads": 1224}
{"pretrained_model_name": "flax-community/gpt2-base-thai", "description": "---\nlanguage: th\ntags:\n  - gpt2-base-thai\nlicense: mit\ndatasets:\n  - oscar\nwidget:\n  - text: \"\u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35\u0e15\u0e2d\u0e19\u0e40\u0e0a\u0e49\u0e32\"\n---\n\n## GPT-2 Base Thai\n\nGPT-2 Base Thai is a causal language model based on the [OpenAI GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model. It was trained on the [OSCAR](https://huggingface.co/datasets/oscar) dataset, specifically the `unshuffled_deduplicated_th` subset. The model was trained from scratch and achieved an evaluation loss of 1.708 and an evaluation perplexity of 5.516.\n\nThis model was trained using HuggingFace's Flax framework and is part of the [JAX/Flax Community Week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104) organized by HuggingFace. All training was done on a TPUv3-8 VM, sponsored by the Google Cloud team.\n\nAll necessary scripts used for training could be found in the [Files and versions](https://hf.co/flax-community/gpt2-base-thai/tree/main) tab, as well as the [Training metrics](https://hf.co/flax-community/gpt2-base-thai/tensorboard) logged via Tensorboard.\n\n## Model\n\n| Model            | #params | Arch. | Training/Validation data (text)      |\n| ---------------- | ------- | ----- | ------------------------------------ |\n| `gpt2-base-thai` | 124M    | GPT-2 | `unshuffled_deduplicated_th` Dataset |\n\n## Evaluation Results\n\nThe model was trained for 3 epochs and the following is the final result once the training ended.\n\n| train loss | valid loss | valid PPL | total time |\n| ---------- | ---------- | --------- | ---------- |\n| 1.638      | 1.708      | 5.516     | 6:12:34    |\n\n## How to Use\n\n### As Causal Language Model\n\n```python\nfrom transformers import pipeline\n\npretrained_name = \"flax-community/gpt2-base-thai\"\n\nnlp = pipeline(\n    \"text-generation\",\n    model=pretrained_name,\n    tokenizer=pretrained_name\n)\n\nnlp(\"\u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35\u0e15\u0e2d\u0e19\u0e40\u0e0a\u0e49\u0e32\")\n```\n\n### Feature Extraction in PyTorch\n\n```python\nfrom transformers import GPT2Model, GPT2TokenizerFast\n\npretrained_name = \"flax-community/gpt2-base-thai\"\nmodel = GPT2Model.from_pretrained(pretrained_name)\ntokenizer = GPT2TokenizerFast.from_pretrained(pretrained_name)\n\nprompt = \"\u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35\u0e15\u0e2d\u0e19\u0e40\u0e0a\u0e49\u0e32\"\nencoded_input = tokenizer(prompt, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\n## Team Members\n\n- Sakares Saengkaew ([@sakares](https://hf.co/sakares))\n- Wilson Wongso ([@w11wo](https://hf.co/w11wo))\n", "size_bytes": "510401385", "downloads": 561}
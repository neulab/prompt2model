{"pretrained_model_name": "flax-community/gpt-neo-125M-apps-all", "description": "---\nlanguage:\n- en\n- python\nlicense: mit\ntags:\n- gpt_neo\n- code_synthesis\ndatasets:\n- apps\n---\n\n\n# GPT-Neo-125M-APPS-all\n> **Please refer to our new [GitHub Wiki](https://github.com/ncoop57/gpt-code-clippy/wiki) which documents our efforts in detail in creating the open source version of GitHub  Copilot**\n\n## Model Description\n\nGPT-Neo-125M-APPS-all is a GPT-Neo-125M finetuned on APPS dataset. This model is specialized to solve programming tasks.\n\n## Training data\n\nThe model is trained on the [Automated Programming Progress Standard (APPS) dataset](https://github.com/hendrycks/apps). The dataset consists of 10,000 coding problems in total, with 131,836 test cases for checking solutions and 232,444 ground-truth solutions written by humans. Problems can be complicated, as the average length of a problem is 293.2 words. The data are split evenly into training and test sets, with 5,000 problems each.\n\nThis model is fine-tuned using most of the APPS dataset including both train and test split to explore the impact of this training task on model performance on other code synthesis evaluation metrics. A model fine-tuned on train set only can be found [here](https://huggingface.co/flax-community/gpt-neo-125M-apps).\n\n## Training procedure\n\nThe training script used to train this model can be found [here](https://github.com/ncoop57/gpt-code-clippy/blob/camera-ready/training/run_clm_apps.py).\n\nTraining is done for 5 epochs using AdamW optimizer and leaner decay learning rate schedule with 800 warmup steps. To reproduce the training one can use this command with the above script:\n\n```bash\npython run_clm_apps.py \\\n    --output_dir $HOME/gpt-neo-125M-apps \\\n    --model_name_or_path EleutherAI/gpt-neo-125B \\\n    --dataset_name $HOME/gpt-code-clippy/data_processing/apps.py \\\n    --dataset_config_name formatted \\\n    --do_train --do_eval \\\n    --block_size=\"1024\" \\\n    --per_device_train_batch_size=\"16\" \\\n    --per_device_eval_batch_size=\"16\" \\\n    --preprocessing_num_workers=\"16\" \\\n    --learning_rate=\"8e-5\" \\\n    --warmup_steps=\"800\" \\\n    --adam_beta1=\"0.9\" \\\n    --adam_beta2=\"0.98\" \\\n    --weight_decay=\"0.1\" \\\n    --overwrite_output_dir \\\n    --num_train_epochs=\"5\" \\\n    --logging_steps=\"50\" \\\n    --eval_steps=\"2000\" \\\n    --report_to=\"wandb\" \\\n    --dtype=\"bfloat16\" \\\n    --save_strategy epoch \\\n    --gradient_accumulation_steps 2 \\\n    --all_data true \\\n```\n\n## Intended Use and Limitations\n\nThe model is finetuned to solve programming problems given a text description and optional starter code.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, FlaxAutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-code-clippy-125M-apps-alldata\")\ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-code-clippy-125M-apps-alldata\")\n\nprompt = \"\"\"\nA function to greet user. Given a user name it should say hello\n\ndef greet(name):\n\nANSWER:\n\"\"\" \n\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\nstart = input_ids.size(1)\nout = model.generate(input_ids, do_sample=True, max_length=50, num_beams=2, \n                     early_stopping=True, eos_token_id=tokenizer.eos_token_id, )\n\nprint(tokenizer.decode(out[0][start:]))\n```\n\n### Limitations and Biases\n\nThe model is intended to be used for research purposes and comes with no guarantees of quality of generated code.\n\nThe paper [\"Evaluating Large Language Models Trained on Code\"](https://arxiv.org/abs/2107.03374) from OpenAI has a good discussion on what the impact of a large language model trained on code could be. Therefore, some parts of their discuss are highlighted here as it pertains to this dataset and models that may be trained from it. **As well as some differences in views from the paper, particularly around legal implications**.\n\n1. **Over-reliance:** This model may generate plausible solutions that may appear correct, but are not necessarily the correct solution. Not properly evaluating the generated code may cause have negative consequences such as the introduction of bugs, or the introduction of security vulnerabilities. Therefore, it is important that users are aware of the limitations and potential negative consequences of using this language model.\n\n2. **Economic and labor market impacts:** Large language models trained on large code datasets such as this one that are capable of generating high-quality code have the potential to automate part of the software development process. This may negatively impact software developers. However, as discussed in the paper, as shown in the Summary Report of software developers from [O*NET OnLine](https://www.onetonline.org/link/summary/15-1252.00), developers don't just write software.\n\n5. **Biases:** The model is trained on data containing prompt questions formatted in specific way. The performance of the model can be worse if the prompt \nformatting is different from that used in APPS dataset.\n\nGPT-CC is finetuned GPT-Neo and might have inhereted biases and limitations from it. See [GPT-Neo model card](https://huggingface.co/EleutherAI/gpt-neo-125M#limitations-and-biases) for details.\n\n## Eval results\n\nComing soon...\n", "size_bytes": "526020317", "downloads": 12}
{"pretrained_model_name": "Tanhim/gpt2-model-de", "description": "---\nlanguage: de\nwidget:\n- text: Hallo, ich bin ein Sprachmodell\n\nlicense: gpl\n---\n\n<h2> GPT2 Model for German Language </h2>\n\n\nModel Name: Tanhim/gpt2-model-de <br />\nlanguage: German or Deutsch  <br />\nthumbnail: https://huggingface.co/Tanhim/gpt2-model-de <br />\ndatasets: Ten Thousand German News Articles Dataset <br />\n\n### How to use\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, I\nset a seed for reproducibility:\n```python\n>>> from transformers import pipeline, set_seed\n>>> generation= pipeline('text-generation', model='Tanhim/gpt2-model-de', tokenizer='Tanhim/gpt2-model-de')\n>>> set_seed(42)\n>>> generation(\"Hallo, ich bin ein Sprachmodell,\", max_length=30, num_return_sequences=5)\n\n```\nHere is how to use this model to get the features of a given text in PyTorch:\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead \ntokenizer = AutoTokenizer.from_pretrained(\"Tanhim/gpt2-model-de\") \nmodel = AutoModelWithLMHead.from_pretrained(\"Tanhim/gpt2-model-de\") \ntext = \"Ersetzen Sie mich durch einen beliebigen Text, den Sie w\u00fcnschen.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nCitation request:\nIf you use the model of this repository in your research, please consider citing the following way:\n```python\n@misc{GermanTransformer,\n  author = {Tanhim Islam},\n  title = {{PyTorch Based Transformer Machine Learning Model for German Text Generation Task}},\n  howpublished = \"\\url{https://huggingface.co/Tanhim/gpt2-model-de}\",\n  year = {2021}, \n  note = \"[Online; accessed 17-June-2021]\"\n}\n```", "size_bytes": "515758313", "downloads": 29}
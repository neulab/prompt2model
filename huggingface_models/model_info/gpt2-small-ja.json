{"pretrained_model_name": "colorfulscoop/gpt2-small-ja", "description": "---\nlanguage: ja\ndatasets: wikipedia\nwidget:\n- text: \u7d71\u8a08\u7684\u6a5f\u68b0\u5b66\u7fd2\u3067\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\nlicense: cc\n---\n\n# GPT-2 small Japanese model\n\nThis repository contains a GPT2-small model trained on Japanese Wikipedia dataset.\n\n## Training data\n\n[Japanese Wikipedia](https://ja.wikipedia.org/wiki/Wikipedia:\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9) dataset as of Aug20, 2021 released under [Creative Commons Attribution-ShareAlike 3.0](https://creativecommons.org/licenses/by-sa/3.0/) is used for both tokenizer and GPT-2 model.\n\nWe splitted the dataset into three subsets - train, valid and test sets. Both tokenizer and model were trained on the train set.\nTrain set contains around 540M tokens.\n\n## Model description\n\nThe model architecture is the same as GPT-2 small model (n_ctx: 1024, n_embd 768, n_head: 12, n_layer: 12) except for a vocabulary size.\nThe vocabulary size is set to 32,000 instead of an original size of 50,257.\n`transformers.GPT2LMHeadModel` is used for training.\n\n## Tokenizer description\n\n[SentencePiece](https://github.com/google/sentencepiece) is used as a tokenizer for this model.\n\nWe utilized 1,000,000 sentences from train set.\nThe vocabulary size was 32,000.\nA `add_dummy_prefix` option was set to `True` because Japanese words are not separated by whitespaces.\n\nAfter training, the tokenizer model was imported as `transformers.BERTGenerationTokenizer`\nbecause it supports SentencePiece models and it does not add any special tokens as default,\nwhich is useful expecially for a text generation task.\n\n## Training\n\nThe model was trained on the train set for 30 epochs with batch size 32. Each sample contained 1024 tokens.\n\nWe utilized Adam optimizer. Learning rate was linearly increased from `0` to `1e-4` during the first 10,000 steps.\nA clip norm was set to `1.0`.\n\nTest set perplexity of the trained model was 29.13.\n\nPlease refer to [GitHub](https://github.com/colorfulscoop/gpt-ja) for more training details.\n\n## Usage\n\nFirst, install dependecies.\n\n```sh\n$ pip install transformers==4.10.0 torch==1.8.1 sentencepiece==0.1.96\n```\n\nThen use pipeline to generate sentences.\n\n```sh\n>>> import transformers\n>>> pipeline = transformers.pipeline(\"text-generation\", \"colorfulscoop/gpt2-small-ja\")\n>>> pipeline(\"\u7d71\u8a08\u7684\u6a5f\u68b0\u5b66\u7fd2\u3067\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\", do_sample=True, top_p=0.95, top_k=50, num_return_sequences=3)\n```\n\n**Note:** The default model configuration `config.json` sets parameters for text generation with `do_sample=True`, `top_k=50`, `top_p=0.95`.\nPlease set these parameters when you need to use different parameters.\n\n## Versions\n\nWe recommend to specify `revision` to load the model for reproducibility.\n\n| Revision | Date of Wikipedia dump |\n| --- | --- |\n| 20210820.1.0 | Aug 20, 2021 |\n| 20210301.1.0 | March 1, 2021 |\n\nYou can specify `revision` as follows.\n\n```py\n# Example of pipeline\n>>> transformers.pipeline(\"text-generation\", \"colorfulscoop/gpt2-small-ja\", revision=\"20210820.1.0\")\n# Example of AutoModel\n>>> transformers.AutoModel.from_pretrained(\"colorfulscoop/gpt2-small-ja\", revision=\"20210820.1.0\")\n```\n\n## License\n\nAll the models included in this repository are licensed under [Creative Commons Attribution-ShareAlike 3.0](https://creativecommons.org/licenses/by-sa/3.0/).\n\n**Disclaimer:** The model potentially has possibility that it generates similar texts in the training data, texts not to be true, or biased texts. Use of the model is at your sole risk. Colorful Scoop makes no warranty or guarantee of any outputs from the model. Colorful Scoop is not liable for any trouble, loss, or damage arising from the model output.\n\n**Author:** Colorful Scoop\n", "size_bytes": "454320757", "downloads": 127}
{"pretrained_model_name": "SiberiaSoft/ruGPT3_medium_chitchat", "description": "---\nlicense: mit\nwidget:\n  - text: '[FIRST] \u0412 \u0447\u0435\u043c \u0441\u043c\u044b\u0441\u043b \u0436\u0438\u0437\u043d\u0438? [SECOND]'\n    example_title: first_example\n  - text: '[FIRST] \u041a\u0430\u043a \u0442\u0432\u043e\u0438 \u0434\u0435\u043b\u0430? [SECOND]'\n    example_title: second_example\npipeline_tag: text-generation\nlanguage:\n- ru\n\n---\n\u041c\u043e\u0434\u0435\u043b\u044c \u0440\u0443\u0441\u0441\u043a\u043e\u0437\u044b\u0447\u043d\u043e\u0433\u043e \u0447\u0430\u0442 \u0431\u043e\u0442\u0430, \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0449\u0430\u044f \u0432 \u0440\u0435\u0436\u0438\u043c\u0435 \u0447\u0438\u0442-\u0447\u0430\u0442 (\u0431\u0435\u0437 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0439)\n\n\u041f\u0440\u0438\u043c\u0435\u0440 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043c\u043e\u0434\u0435\u043b\u044c\u044e:\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\ntokenizer = AutoTokenizer.from_pretrained('SiberiaSoft/ruGPT3_medium_chitchat')\nmodel = AutoModelWithLMHead.from_pretrained('SiberiaSoft/ruGPT3_medium_chitchat')\ninputs = tokenizer('[FIRST] \u041f\u0440\u0438\u0432\u0435\u0442 [SECOND]', return_tensors='pt')\ngenerated_token_ids = model.generate(\n    **inputs,\n    top_k=10,\n    top_p=0.95,\n    num_beams=3,\n    num_return_sequences=5,\n    do_sample=True,\n    no_repeat_ngram_size=2,\n    temperature=1.0,\n    repetition_penalty=1.2,\n    length_penalty=1.0,\n    eos_token_id=50257,\n    max_length = 400\n)\ngeneration = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\nprint(generation)\n```\n", "size_bytes": "1524289497", "downloads": 5}
{"pretrained_model_name": "Jellywibble/dalio-pretrain-cleaned-v4", "description": "---\ntags:\n- text-generation\nlibrary_name: transformers\nwidget:\n- text: \"This is a conversation where Ray Dalio is giving advice on being a manager and building a successful team.\\nUser: Hi Ray, thanks for talking with me today. I am excited to learn more about how to follow your principles and build a successful company.\\nRay: No problem, I am happy to help. What situation are you facing?\\nUser: It feels like I keep making decisions without thinking first - I do something without thinking and then I face the consequences afterwards.\\nRay:\"\n  example_title: \"Q&A\"\n- text: \"It\u2019s easy to tell an open-minded person from a closed-minded person because they act very differently. Here are some cues to tell you whether you or others are being closed-minded: \"\n  example_title: \"Principles\"\n---\n\n## Model Description\nPre-training on cleaned version of Principles\n- removing numeric references to footnotes\n- removing numeric counts, i.e. 1) ... 2) ... 3) ...\n- correcting gramma, i.e. full stops must be followed by a space\n- finetuning OPT-30B model on the dataset above\n- Dataset location: Jellywibble/dalio-principles-cleaned-v3\n\n## Metrics\n- Checkpoint 8 served\n- Hellaswag Perplexity: 30.65\n- 2.289 eval loss\n\nwandb link: https://wandb.ai/jellywibble/huggingface/runs/2jqc504o?workspace=user-jellywibble\n\n## Model Parameters\nTrained on 4xA40, effective batchsize = 8\n- base_model_name facebook/opt-30b\n- dataset_name Jellywibble/dalio-principles-cleaned-v3\n- block_size 1024\n- gradient_accumulation_steps 2\n- per_device_train_batch_size 1\n- seed 2\n- num_train_epochs 1\n- learning_rate 3e-6\n\n## Notes\n- It is important for the effective batch size to be at least 8\n- Learning rate higher than 3e-6 will result in massive overfitting, i.e. much worse Hellaswag metrics", "size_bytes": 121339158528, "downloads": 7}
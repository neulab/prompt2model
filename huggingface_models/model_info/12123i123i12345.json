{"pretrained_model_name": "huggingtweets/12123i123i12345", "description": "---\nlanguage: en\nthumbnail: https://www.huggingtweets.com/12123i123i12345/1617760753400/predictions.png\ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('https://pbs.twimg.com/profile_images/1377780722883174400/4gq8ntlP_400x400.jpg')\">\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">parallellax \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@12123i123i12345 bot</div>\n</div>\n\nI was made with [huggingtweets](https://github.com/borisdayma/huggingtweets).\n\nCreate your own bot based on your favorite user with [the demo](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)!\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](https://github.com/borisdayma/huggingtweets/blob/master/img/pipeline.png?raw=true)\n\nTo understand how the model was developed, check the [W&B report](https://wandb.ai/wandb/huggingtweets/reports/HuggingTweets-Train-a-Model-to-Generate-Tweets--VmlldzoxMTY5MjI).\n\n## Training data\n\nThe model was trained on [@12123i123i12345's tweets](https://twitter.com/12123i123i12345).\n\n| Data | Quantity |\n| --- | --- |\n| Tweets downloaded | 2362 |\n| Retweets | 310 |\n| Short tweets | 283 |\n| Tweets kept | 1769 |\n\n[Explore the data](https://wandb.ai/wandb/huggingtweets/runs/e91cv8fo/artifacts), which is tracked with [W&B artifacts](https://docs.wandb.com/artifacts) at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2](https://huggingface.co/gpt2) which is fine-tuned on @12123i123i12345's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run](https://wandb.ai/wandb/huggingtweets/runs/ncn8t24f) for full transparency and reproducibility.\n\nAt the end of training, [the final model](https://wandb.ai/wandb/huggingtweets/runs/ncn8t24f/artifacts) is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/12123i123i12345')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](https://huggingface.co/gpt2#limitations-and-bias).\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](https://img.shields.io/twitter/follow/borisdayma?style=social)](https://twitter.com/intent/follow?screen_name=borisdayma)\n\nFor more details, visit the project repository.\n\n[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n", "size_bytes": "510408315", "downloads": 0}
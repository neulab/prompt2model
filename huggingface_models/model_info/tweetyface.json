{"pretrained_model_name": "ML-Projects-Kiel/tweetyface", "description": "---\nlicense: apache-2.0\ndatasets:\n- ML-Projects-Kiel/tweetyface\nlanguage:\n- en\ntags:\n- gpt2\n\ninference:\n  parameters:\n    num_return_sequences: 2\n    \nwidget:\n- text: \"User: BarackObama\\nTweet: Twitter is \"\n  example_title: \"Barack Obama about Twitter\"\n- text: \"User: neiltyson\\nTweet: Twitter is\"\n  example_title: \"Neil deGrasse Tyson about Twitter\"\n- text: \"User: elonmusk\\nTweet: Twitter is\"\n  example_title: \"Elon Musk about Twitter\"\n\n- text: \"User: elonmusk\\nTweet: My Opinion about space\"\n  example_title: \"Elon Musk deGrasse Tyson about Space\"\n- text: \"User: BarackObama\\nTweet: My Opinion about space\"\n  example_title: \"Barack Obama about Space\"\n- text: \"User: neiltyson\\nTweet: My Opinion about space\"\n  example_title: \"Neil deGrasse Tyson about Space\"\n---\n\n\n# Tweety Face\n\n\nFinetuned language model based on [GPT-2](https://huggingface.co/gpt2) to generate Tweets in a users style.\n\n\n## Model description\n\nTweety Face is a transformer model finetuned using GTP-2 and Tweets from various Twitter users. It was created to \ngenerate a Twitter Tweet for a given user similar to their specific writing style. It accepts a prompt for a user\nand completes the text.\n\nThis finetuned model uses the **smallest** version of GPT-2, with 124M parameters. \n\n## Intended uses & limitations\n\nThis model was created to experiment with prompt inputs and is not intended to create real Tweets. The generated text\nis not a real representation of the given users opinion, political affiliation, behaviour, etc. Do not use this model\nto impersonate a user.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='ML-Projects-Kiel/tweetyface')\n>>> set_seed(42)\n>>> generator(\"User: elonmusk\\nTweet: Twitter is\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': 'User: elonmusk\\nTweet: Twitter is more active than ever. Even though you can\u2019t see your entire phone list, your'},\n {'generated_text': 'User: elonmusk\\nTweet: Twitter is just in a few hours until an announcement which has been approved by President. This should be a'},\n {'generated_text': 'User: elonmusk\\nTweet: Twitter is currently down to a minimum of 13 tweets per day, a decline that was significantly worse than Twitter'},\n {'generated_text': 'User: elonmusk\\nTweet: Twitter is a great investment to us. Will go above his legal fees to join Twitter in many countries,'},\n {'generated_text': 'User: elonmusk\\nTweet: Twitter is not doing something like this \u2013 they are not using Twitter to give out their content \u2013 other than'}]\n```\n\n\n## Training data\n\nThe training data used for this model has been released as a dataset one can browse [here](https://huggingface.co/ML-Projects-Kiel/tweetyface).\nThe raw data can be found in our [Github Repository](https://github.com/ml-projects-kiel/OpenCampus-ApplicationofTransformers). The raw data\ncan be found in two versions. All data on the develop branch is used in a [debugging dataset](https://huggingface.co/datasets/ML-Projects-Kiel/tweetyface_debug). \nAll data in the qa branch is used in the final dataset.\n\n\n## Training procedure\n\n### Preprocessing\n\nFor training first all retweets (RT) have been removed. Next the newline characters \"\\n\" have been replaced by white\nspaces and all URLs haven been replaced with the word URL. \n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters).\n", "size_bytes": "510396521", "downloads": 2}
{"pretrained_model_name": "nikaashpuri/gpt-expt-sp-v3-K-200-9-mixed", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: gpt-expt-sp-v3-K-200-9-mixed\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# gpt-expt-sp-v3-K-200-9-mixed\n\nThis model is a fine-tuned version of [gpt2](https://huggingface.co/gpt2) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0470\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 256\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 500\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch  | Step   | Validation Loss |\n|:-------------:|:------:|:------:|:---------------:|\n| 0.4566        | 12.75  | 5000   | 0.0648          |\n| 0.0684        | 25.51  | 10000  | 0.0535          |\n| 0.058         | 38.26  | 15000  | 0.0505          |\n| 0.0545        | 51.02  | 20000  | 0.0495          |\n| 0.0527        | 63.77  | 25000  | 0.0491          |\n| 0.0517        | 76.53  | 30000  | 0.0487          |\n| 0.051         | 89.29  | 35000  | 0.0484          |\n| 0.0505        | 102.04 | 40000  | 0.0482          |\n| 0.0502        | 114.79 | 45000  | 0.0480          |\n| 0.0499        | 127.55 | 50000  | 0.0480          |\n| 0.0497        | 140.31 | 55000  | 0.0479          |\n| 0.0495        | 153.06 | 60000  | 0.0478          |\n| 0.0493        | 165.81 | 65000  | 0.0477          |\n| 0.0491        | 178.57 | 70000  | 0.0477          |\n| 0.0489        | 191.33 | 75000  | 0.0476          |\n| 0.0488        | 204.08 | 80000  | 0.0476          |\n| 0.0486        | 216.83 | 85000  | 0.0476          |\n| 0.0485        | 229.59 | 90000  | 0.0475          |\n| 0.0484        | 242.35 | 95000  | 0.0474          |\n| 0.0483        | 255.1  | 100000 | 0.0473          |\n| 0.0482        | 267.86 | 105000 | 0.0473          |\n| 0.0481        | 280.61 | 110000 | 0.0473          |\n| 0.048         | 293.37 | 115000 | 0.0472          |\n| 0.0479        | 306.12 | 120000 | 0.0472          |\n| 0.0478        | 318.88 | 125000 | 0.0472          |\n| 0.0477        | 331.63 | 130000 | 0.0471          |\n| 0.0476        | 344.39 | 135000 | 0.0471          |\n| 0.0475        | 357.14 | 140000 | 0.0471          |\n| 0.0475        | 369.9  | 145000 | 0.0471          |\n| 0.0474        | 382.65 | 150000 | 0.0471          |\n| 0.0473        | 395.41 | 155000 | 0.0470          |\n| 0.0473        | 408.16 | 160000 | 0.0470          |\n| 0.0472        | 420.92 | 165000 | 0.0470          |\n| 0.0472        | 433.67 | 170000 | 0.0470          |\n| 0.0472        | 446.43 | 175000 | 0.0470          |\n| 0.0472        | 459.18 | 180000 | 0.0470          |\n| 0.0471        | 471.94 | 185000 | 0.0470          |\n| 0.0471        | 484.69 | 190000 | 0.0470          |\n| 0.0471        | 497.45 | 195000 | 0.0470          |\n\n\n### Framework versions\n\n- Transformers 4.25.1\n- Pytorch 1.13.1+cu117\n- Datasets 2.8.0\n- Tokenizers 0.13.2\n", "size_bytes": "356798013", "downloads": 18}
{"pretrained_model_name": "flax-community/gpt2-swahili", "description": "---\nlanguage: sw\nwidget:\n- text: \"Ninitaka kukula\"\ndatasets:\n- flax-community/swahili-safi\n---\n\n## GPT2 in Swahili\n\nThis model was trained using HuggingFace's Flax framework and is part of the [JAX/Flax Community Week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104) organized by [HuggingFace](https://huggingface.co). All training was done on a TPUv3-8 VM sponsored by the Google Cloud team.\n\n## How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-swahili\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"flax-community/gpt2-swahili\")\n\nprint(round((model.num_parameters())/(1000*1000)),\"Million Parameters\")\n\n124 Million Parameters\n```\n\n#### **Training Data**:\nThis model was trained on [Swahili Safi](https://huggingface.co/datasets/flax-community/swahili-safi)\n\n\n#### **More Details**:\nFor more details and Demo please check [HF Swahili Space](https://huggingface.co/spaces/flax-community/Swahili)\n\n", "size_bytes": "510401385", "downloads": 3}
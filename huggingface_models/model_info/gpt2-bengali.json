{"pretrained_model_name": "flax-community/gpt2-bengali", "description": "---\n\nlanguage: bn\nlicense: mit\ndatasets:\n- mc4\n\n---\n\n# Bengali GPT-2\n\nBengali GPT-2 demo. Part of the [Huggingface JAX/Flax event](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/). Also features a [finetuned](https://huggingface.co/khalidsaifullaah/bengali-lyricist-gpt2?) model on bengali song lyrics. \n\n# Model Description\n\nOpenAI GPT-2 model was proposed in [Language Models are Unsupervised Multitask Learners](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask) paper .Original GPT2 model was a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data. This model has same configuration but has been pretrained on bengali corpus of mC4(multilingual C4) dataset. The code for training the model has all been open-sourced [here](https://huggingface.co/flax-community/gpt2-bengali/tree/main).\n\n# Training Details\n\nOverall Result: \n\n```Eval loss : 1.45, Eval Perplexity : 3.141```\n\nData: [mC4-bn](https://huggingface.co/datasets/mc4)\n\nTrain Steps: 250k steps\n\nlink \ud83e\udd17 flax-community/gpt2-bengali\n\nDemo : https://huggingface.co/spaces/flax-community/Gpt2-bengali\n\n# Usage \n\nFor using the model there are multiple options available. For example using the pipeline directly we can try to generate sentences.\n\n```\nfrom transformers import pipeline\n\ngpt2_bengali = pipeline('text-generation',model=\"flax-community/gpt2-bengali\", tokenizer='flax-community/gpt2-bengali')\n```\n\nSimilarly for using the finetuned model on bangla songs we can use following.\n\n```\nfrom transformers import pipeline\n\nsinger = pipeline('text-generation',model=\"khalidsaifullaah/bengali-lyricist-gpt2\", tokenizer='khalidsaifullaah/bengali-lyricist-gpt2')\n```\n\nFor using on other tasks the model needs to be fine-tuned on custom datasets. Details can be found in huggingface [documentation](https://huggingface.co/transformers/training.html)\n\n# Contributors\n* Khalid Saifullah\n* Tasmiah Tahsin Mayeesha\n* Ritobrata Ghosh\n* Ibrahim Musa\n* M Saiful Bari\n\n### BibTeX entry and citation info\n\nComing soon! \n\n<!-- ```bibtex\n\n@inproceedings{...,\n\n  year={2020}\n\n}\n\n``` -->", "size_bytes": "510401385", "downloads": 59}
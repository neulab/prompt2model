{"pretrained_model_name": "adamlin/topicalchat-multiturn", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- null\nmodel_index:\n- name: topicalchat-multiturn\n  results:\n  - task:\n      name: Causal Language Modeling\n      type: text-generation\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# topicalchat-multiturn\n\nThis model is a fine-tuned version of [microsoft/DialoGPT-small](https://huggingface.co/microsoft/DialoGPT-small) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 2.5260\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 100.0\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| No log        | 1.0   | 73   | 4.2992          |\n| No log        | 2.0   | 146  | 3.4433          |\n| No log        | 3.0   | 219  | 3.1606          |\n| No log        | 4.0   | 292  | 3.0366          |\n| No log        | 5.0   | 365  | 2.9679          |\n| No log        | 6.0   | 438  | 2.9131          |\n| 4.1401        | 7.0   | 511  | 2.8752          |\n| 4.1401        | 8.0   | 584  | 2.8391          |\n| 4.1401        | 9.0   | 657  | 2.8118          |\n| 4.1401        | 10.0  | 730  | 2.7871          |\n| 4.1401        | 11.0  | 803  | 2.7659          |\n| 4.1401        | 12.0  | 876  | 2.7489          |\n| 4.1401        | 13.0  | 949  | 2.7331          |\n| 2.9768        | 14.0  | 1022 | 2.7196          |\n| 2.9768        | 15.0  | 1095 | 2.7071          |\n| 2.9768        | 16.0  | 1168 | 2.6940          |\n| 2.9768        | 17.0  | 1241 | 2.6854          |\n| 2.9768        | 18.0  | 1314 | 2.6728          |\n| 2.9768        | 19.0  | 1387 | 2.6647          |\n| 2.9768        | 20.0  | 1460 | 2.6562          |\n| 2.7864        | 21.0  | 1533 | 2.6482          |\n| 2.7864        | 22.0  | 1606 | 2.6439          |\n| 2.7864        | 23.0  | 1679 | 2.6326          |\n| 2.7864        | 24.0  | 1752 | 2.6107          |\n| 2.7864        | 25.0  | 1825 | 2.6043          |\n| 2.7864        | 26.0  | 1898 | 2.5970          |\n| 2.7864        | 27.0  | 1971 | 2.5908          |\n| 2.6568        | 28.0  | 2044 | 2.5862          |\n| 2.6568        | 29.0  | 2117 | 2.5828          |\n| 2.6568        | 30.0  | 2190 | 2.5765          |\n| 2.6568        | 31.0  | 2263 | 2.5742          |\n| 2.6568        | 32.0  | 2336 | 2.5682          |\n| 2.6568        | 33.0  | 2409 | 2.5656          |\n| 2.6568        | 34.0  | 2482 | 2.5614          |\n| 2.5489        | 35.0  | 2555 | 2.5605          |\n| 2.5489        | 36.0  | 2628 | 2.5552          |\n| 2.5489        | 37.0  | 2701 | 2.5541          |\n| 2.5489        | 38.0  | 2774 | 2.5494          |\n| 2.5489        | 39.0  | 2847 | 2.5491          |\n| 2.5489        | 40.0  | 2920 | 2.5455          |\n| 2.5489        | 41.0  | 2993 | 2.5452          |\n| 2.475         | 42.0  | 3066 | 2.5433          |\n| 2.475         | 43.0  | 3139 | 2.5397          |\n| 2.475         | 44.0  | 3212 | 2.5386          |\n| 2.475         | 45.0  | 3285 | 2.5400          |\n| 2.475         | 46.0  | 3358 | 2.5339          |\n| 2.475         | 47.0  | 3431 | 2.5327          |\n| 2.4144        | 48.0  | 3504 | 2.5327          |\n| 2.4144        | 49.0  | 3577 | 2.5312          |\n| 2.4144        | 50.0  | 3650 | 2.5338          |\n| 2.4144        | 51.0  | 3723 | 2.5314          |\n| 2.4144        | 52.0  | 3796 | 2.5309          |\n| 2.4144        | 53.0  | 3869 | 2.5289          |\n| 2.4144        | 54.0  | 3942 | 2.5290          |\n| 2.3642        | 55.0  | 4015 | 2.5270          |\n| 2.3642        | 56.0  | 4088 | 2.5270          |\n| 2.3642        | 57.0  | 4161 | 2.5263          |\n| 2.3642        | 58.0  | 4234 | 2.5267          |\n| 2.3642        | 59.0  | 4307 | 2.5273          |\n| 2.3642        | 60.0  | 4380 | 2.5258          |\n| 2.3642        | 61.0  | 4453 | 2.5253          |\n| 2.3216        | 62.0  | 4526 | 2.5244          |\n| 2.3216        | 63.0  | 4599 | 2.5256          |\n| 2.3216        | 64.0  | 4672 | 2.5227          |\n| 2.3216        | 65.0  | 4745 | 2.5241          |\n| 2.3216        | 66.0  | 4818 | 2.5244          |\n| 2.3216        | 67.0  | 4891 | 2.5236          |\n| 2.3216        | 68.0  | 4964 | 2.5251          |\n| 2.2879        | 69.0  | 5037 | 2.5231          |\n| 2.2879        | 70.0  | 5110 | 2.5254          |\n| 2.2879        | 71.0  | 5183 | 2.5242          |\n| 2.2879        | 72.0  | 5256 | 2.5254          |\n| 2.2879        | 73.0  | 5329 | 2.5253          |\n| 2.2879        | 74.0  | 5402 | 2.5228          |\n| 2.2879        | 75.0  | 5475 | 2.5247          |\n| 2.261         | 76.0  | 5548 | 2.5243          |\n| 2.261         | 77.0  | 5621 | 2.5247          |\n| 2.261         | 78.0  | 5694 | 2.5250          |\n| 2.261         | 79.0  | 5767 | 2.5248          |\n| 2.261         | 80.0  | 5840 | 2.5236          |\n| 2.261         | 81.0  | 5913 | 2.5264          |\n| 2.261         | 82.0  | 5986 | 2.5249          |\n| 2.2396        | 83.0  | 6059 | 2.5256          |\n| 2.2396        | 84.0  | 6132 | 2.5267          |\n| 2.2396        | 85.0  | 6205 | 2.5258          |\n| 2.2396        | 86.0  | 6278 | 2.5242          |\n| 2.2396        | 87.0  | 6351 | 2.5233          |\n| 2.2396        | 88.0  | 6424 | 2.5249          |\n| 2.2396        | 89.0  | 6497 | 2.5253          |\n| 2.2238        | 90.0  | 6570 | 2.5252          |\n| 2.2238        | 91.0  | 6643 | 2.5255          |\n| 2.2238        | 92.0  | 6716 | 2.5263          |\n| 2.2238        | 93.0  | 6789 | 2.5261          |\n| 2.2238        | 94.0  | 6862 | 2.5257          |\n| 2.2238        | 95.0  | 6935 | 2.5253          |\n| 2.213         | 96.0  | 7008 | 2.5267          |\n| 2.213         | 97.0  | 7081 | 2.5258          |\n| 2.213         | 98.0  | 7154 | 2.5258          |\n| 2.213         | 99.0  | 7227 | 2.5259          |\n| 2.213         | 100.0 | 7300 | 2.5260          |\n\n\n### Framework versions\n\n- Transformers 4.8.1\n- Pytorch 1.8.1+cu111\n- Datasets 1.8.0\n- Tokenizers 0.10.3\n", "size_bytes": "510417531", "downloads": 0}
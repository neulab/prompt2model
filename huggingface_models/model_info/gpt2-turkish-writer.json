{"pretrained_model_name": "gorkemgoknar/gpt2-turkish-writer", "description": "---\nlanguage:\n- tr\nthumbnail:\ntags:\n- gpt2\n- turkish\n- aiwriter\n- finetuned\n\nlicense: apache-2.0\ndatasets:\n- wikipedia-turkish\n- custom-book-corpus\nmetrics:\n- perplexity\n- accuracy\n\nwidget:\n- text: Bir zaman topu olan ama k\u00f6pe\u011fi olmayan bir \u00e7ocuk vard\u0131. Parkta\n  context: ''\n- text: 'Uzun uzun sahile do\u011fru bakt\u0131. D\u00fc\u015f\u00fcnd\u00fcklerinden '\n  context: ''\n- text: \u00c7ok uzun zaman \u00f6nce galaksinin uzak bir k\u00f6\u015fesinde...\n  context: ''\n- text: \"'Bug\u00fcn kendimi \u00e7ok hasta hissediyorum' dedi. Kar\u015f\u0131s\u0131nda \"\n  context: ''\n---\n\n# Turkish AI Writer based on GPT2-Small\n# T\u00fcrk\u00e7e Yapay Zeka Yazar\u0131\n\n## Model description\n\nThis model is enhanced version of gpt2-small-turkish finetuned version. In addition to 28-10-2020 Wikipedia Turkish article dump this model is trained with more than 400 classic novels and plays in Turkish (Including Dostoyevski, Shaekspeare, Dumas)\n\nBase work has been done on Pierre Guillou tutorial as on this page.\n(https://github.com/piegu/fastai-projects/blob/master/finetuning-English-GPT2-any-language-Portuguese-HuggingFace-fastaiv2.ipynb) \n\nNote that Since Turkish language is not close to English as in Porteguese instead  of training last 2 layers, last 3 layers are trained.\n\nCode is converted to work with Fastai 2.X .\nUsing Google Colab for training. \n\nCurrent accuracy 36.3 %  , Perplexity : 44.75\n\nDemo (using CPU inference) is available on: http://www.metayazar.com \n\nModels are available:\n\n* [gpt2-small-tuned-tr] (https://huggingface.co/gorkemgoknar/gpt2-small-turkish)\n* [gpt2-small-turkish-writer] (https://huggingface.co/gorkemgoknar/gpt2-turkish-writer)\n\n\n## Intended uses & limitations\n\n#### How to use\n\n#### Install\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"gorkemgoknar/gpt2-turkish-writer\")\nmodel = AutoModelWithLMHead.from_pretrained(\"gorkemgoknar/gpt2-turkish-writer\")\n\n# Get sequence length max of 1024\ntokenizer.model_max_length=1024 \n\nmodel.eval()  # disable dropout (or leave in train mode to finetune)\n\n```\n\n#### Generate 1 word\n```python\n# input sequence\ntext = \"Bu yaz\u0131y\u0131 bilgisayar yazd\u0131.\"\ninputs = tokenizer(text, return_tensors=\"pt\") \n\n# model output\noutputs = model(**inputs, labels=inputs[\"input_ids\"])\nloss, logits = outputs[:2]\npredicted_index = torch.argmax(logits[0, -1, :]).item()\npredicted_text = tokenizer.decode([predicted_index])\n\n# results\nprint('input text:', text)\nprint('predicted text:', predicted_text)\n\n# input text: \n# predicted text:  \n\n```\n\n#### Generate Full Sequence\n```python\n# input sequence\ntext = \"Bu yaz\u0131y\u0131 bilgisayar yazd\u0131.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# model output using Top-k sampling text generation method\nsample_outputs = model.generate(inputs.input_ids,\n                                pad_token_id=50256,\n                                do_sample=True, \n                                max_length=50, # put the token number you want\n                                top_k=40,\n                                num_return_sequences=1)\n\n# generated sequence\nfor i, sample_output in enumerate(sample_outputs):\n    print(\">> Generated text {}\\n\\n{}\".format(i+1, tokenizer.decode(sample_output.tolist())))\n\n# >> Generated text\n#    \n\n```\n\n#### Limitations and bias\n\nThe training data used for this model come from Turkish Wikipedia and books. We know it contains a lot of unfiltered content from the internet, which is far from neutral. Also not much pre-processing was done on books hence chapter names and page numbers can be seen on some cases. This is a work in progress.\n\n\n## Training data\n\nWikipedia Turkish article dump as of 28-10-2020\nTurkish book dataset of >400 classic novels\n\n## Training procedure\n\n\n## Eval results\n\n| epoch\t|train_loss\t|valid_loss\t|accuracy\t|perplexity\t|time   |\n| ----- | --------      |---------      | ----------    | ---------     | ----- |\n|0\t|4.497828\t|4.549605\t|0.277328\t|94.595070\t|2:09:58|\n|1\t|4.503929\t|4.519456\t|0.275071\t|91.785645\t|2:04:30|\n|2\t|3.612716\t|3.921146\t|0.344802\t|50.458256\t|2:03:22|\n|3\t|3.777645\t|4.072006\t|0.326130\t|58.674530\t|1:56:14|\n|4\t|2.934462\t|3.801303\t|0.363719\t|44.759476\t|1:58:55|\n\nNote: 1cycle rule training is used and epochs are at different times \n```\n\n", "size_bytes": "510406905", "downloads": 82}
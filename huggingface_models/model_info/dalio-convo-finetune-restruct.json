{"pretrained_model_name": "Jellywibble/dalio-convo-finetune-restruct", "description": "---\ntags:\n- text-generation\nlibrary_name: transformers\n---\n\n## Model description\nBased on Jellywibble/dalio-pretrained-book-bs4-seed1 which was pre-trained on the Dalio Principles Book\nFinetuned on handwritten conversations Jellywibble/dalio_handwritten-conversations\n\n## Dataset Used\nJellywibble/dalio_handwritten-conversations\n\n## Training Parameters\n- Deepspeed on 4xA40 GPUs\n- Ensuring EOS token `<s>` appears only at the beginning of each 'This is a conversation where Ray ...'\n- Gradient Accumulation steps = 1 (Effective batch size of 4)\n- 2e-6 Learning Rate, AdamW optimizer\n- Block size of 1000\n- Trained for 1 Epoch (additional epochs yielded worse Hellaswag result)\n\n## Metrics\n- Hellaswag Perplexity: 29.83\n- Eval accuracy: 58.1%\n- Eval loss: 1.883\n- Checkpoint 9 uploaded\n- Wandb run: https://wandb.ai/jellywibble/huggingface/runs/157eehn9?workspace=user-jellywibble", "size_bytes": 121339158528, "downloads": 0}
{"pretrained_model_name": "akhooli/gpt2-small-arabic", "description": "---\nlanguage: \"ar\"\ndatasets:\n- Arabic Wikipedia\nmetrics:\n- none\n---\n\n# GPT2-Small-Arabic\n\n## Model description\n\nGPT2 model from Arabic Wikipedia dataset based on gpt2-small (using Fastai2).\n\n## Intended uses & limitations\n\n#### How to use\n\nAn example is provided in this [colab notebook](https://colab.research.google.com/drive/1mRl7c-5v-Klx27EEAEOAbrfkustL4g7a?usp=sharing). \nBoth text and poetry (fine-tuned model) generation are included.\n\n#### Limitations and bias\n\nGPT2-small-arabic (trained on Arabic Wikipedia) has several limitations in terms of coverage (Arabic Wikipeedia quality, no diacritics) and training performance. \nUse as examples or proof of concepts but not as production code.\n\n## Training data\n\nThis pretrained model used the Arabic Wikipedia dump (around 900 MB). \n\n## Training procedure\n\nTraining was done using [Fastai2](https://github.com/fastai/fastai2/) library on Kaggle, using free GPU.\n\n## Eval results \nFinal perplexity reached was 72.19,  loss: 4.28, accuracy: 0.307\n\n### BibTeX entry and citation info\n\n```bibtex\n@inproceedings{Abed Khooli,\n  year={2020}\n}\n```\n", "size_bytes": "510378732", "downloads": 619}
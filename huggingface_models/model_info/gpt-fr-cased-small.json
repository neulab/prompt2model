{"pretrained_model_name": "asi/gpt-fr-cased-small", "description": "---\nlanguage: \n- fr\nmodel-index:\n- name: asi/gpt-fr-cased-base\n  results:\n  - task:\n      type: text-generation\n      name: Wikitext-fr\n    dataset:\n      type: wikitext_fr\n      name: Wikitext-fr\n    metrics:\n      - type: perplexity\n        value: 109.2\n        name: Perplexity\n  - task:\n      type: text-classification\n      name: FLUE\n    dataset:\n      type: flue\n      name: CLS-Books\n      split: CLS\n    metrics:\n      - type: accuracy\n        value: 88.3\n        name: Accuracy\n  - task:\n      type: text-classification\n      name: FLUE\n    dataset:\n      type: flue\n      name: CLS-Dvd\n      split: CLS\n    metrics:\n      - type: accuracy\n        value: 86.9\n        name: Accuracy\n  - task:\n      type: text-classification\n      name: FLUE\n    dataset:\n      type: flue\n      name: CLS-Music\n      split: CLS\n    metrics:\n      - type: accuracy\n        value: 89.3\n        name: Accuracy\n  - task:\n      type: text-classification\n      name: FLUE\n    dataset:\n      type: flue\n      name: PAWS-X\n      split: PAWS-X\n    metrics:\n      - type: accuracy\n        value: 83.3\n        name: Accuracy\n  - task:\n      type: text-classification\n      name: FLUE\n    dataset:\n      type: flue\n      name: XNLI\n      split: XNLI\n    metrics:\n      - type: accuracy\n        value: 75.6\n        name: Accuracy\n  - task:\n      type: summarization\n      name: OrangeSum\n    dataset:\n      type: orange_sum\n      name: OrangeSum-Abstract\n      split: abstract\n    metrics:\n    - name: ROUGE-1\n      type: rouge\n      value: 17.5\n    - name: ROUGE-2\n      type: rouge\n      value: 3.1\n    - name: ROUGE-L\n      type: rouge\n      value: 12.1\n  - task:\n      type: summarization\n      name: OrangeSum\n    dataset:\n      type: orange_sum\n      name: OrangeSum-Title\n      split: title\n    metrics:\n    - name: ROUGE-1\n      type: rouge\n      value: 13.9\n    - name: ROUGE-2\n      type: rouge\n      value: 2.3\n    - name: ROUGE-L\n      type: rouge\n      value: 9.7\ntags:\n- tf\n- pytorch\n- gpt2\n- text-generation\nlicense: apache-2.0\nthumbnail: https://raw.githubusercontent.com/AntoineSimoulin/gpt-fr/main/imgs/logo.png\n---\n\n<img src=\"https://raw.githubusercontent.com/AntoineSimoulin/gpt-fr/main/imgs/logo.png\" width=\"200\">\n\n## Model description\n\n**GPT-fr** \ud83c\uddeb\ud83c\uddf7 is a GPT model for French developped by [Quantmetry](https://www.quantmetry.com/) and the [Laboratoire de Linguistique Formelle (LLF)](http://www.llf.cnrs.fr/en). We train the model on a very large and heterogeneous French corpus. We release the weights for the following configurations:\n\n| Model name | Number of layers | Attention Heads | Embedding Dimension | Total Parameters |\n| :------:       |   :---: | :---: | :---: | :---: |\n| `gpt-fr-cased-small` | 12    | 12    | 768   | 124 M |\n| `gpt-fr-cased-base` | 24    | 14    | 1,792   | 1,017 B |\n\n## Intended uses & limitations\n\nThe model can be leveraged for language generation tasks. Besides, many tasks may be formatted such that the output is directly generated in natural language. Such configuration may be used for tasks such as automatic summary or question answering. We do hope our model might be used for both academic and industrial applications. \n\n#### How to use\n\nThe model might be used through the astonishing \ud83e\udd17 `Transformers` librairie:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load pretrained model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"asi/gpt-fr-cased-small\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"asi/gpt-fr-cased-small\")\n\n# Generate a sample of text\nmodel.eval()\ninput_sentence = \"Longtemps je me suis couch\u00e9 de bonne heure.\"\ninput_ids = tokenizer.encode(input_sentence, return_tensors='pt')\n\nbeam_outputs = model.generate(\n    input_ids, \n    max_length=100, \n    do_sample=True,   \n    top_k=50, \n    top_p=0.95, \n    num_return_sequences=1\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))\n```\n\n#### Limitations and bias\n\nLarge language models tend to replicate the biases found in pre-training datasets, such as gender discrimination or offensive content generation.\n\nTo limit exposition to too much explicit material, we carefully choose the sources beforehand. This process \u2014 detailed in our paper \u2014 aims to limit offensive content generation from the model without performing manual and arbitrary filtering.\n\nHowever, some societal biases, contained in the data, might be reflected by the model. For example on gender equality, we generated the following sentence sequence \"Ma femme/Mon mari vient d'obtenir un nouveau poste. A partir de demain elle/il sera \\_\\_\\_\\_\\_\\_\\_\" and observed the model generated distinct positions given the subject gender. We used top-k random sampling strategy with k=50 and stopped at the first punctuation element.\nThe positions generated for the wife is '_femme de m\u00e9nage de la maison_' while the position for the husband is '_\u00e0 la t\u00eate de la police_'. We do appreciate your feedback to better qualitatively and quantitatively assess such effects. \n\n## Training data\n\nWe created a dedicated corpus to train our generative model. Indeed the model uses a fixed-length context size of 1,024 and require long documents to be trained.  We aggregated existing corpora: [Wikipedia](https://dumps.wikimedia.org/frwiki/), [OpenSubtitle](http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/) ([Tiedemann, 2012](#tiedemann-2012)), [Gutenberg](http://www.gutenberg.org). Corpora are filtered and separated into sentences. Successive sentences are then concatenated within the limit of 1,024 tokens per document.\n\n## Training procedure\n\nWe pre-trained the model on a TPU v2-8 using the amazing [Google Colab](https://colab.research.google.com) inter-server.\n\n## Eval results\n\nWe packaged **GPT-fr** with a dedicated language model evaluation benchmark. \nIn line with the [WikiText](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark in English, we collected over 70 million tokens from the set of verified [good](https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Articles_de_qualit%C3%A9) and [featured](https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Bons_articles) articles on French Wikipedia. The model reaches a zero-shot perplexity of **109.2** on the test set. \n\n\n### BibTeX entry and citation info\n\nAlong with the model hosted by HuggingFace transformers library, we maintain a [git repository](https://github.com/AntoineSimoulin/gpt-fr).\nIf you use **GPT-fr** for your scientific publications or your industrial applications, please cite the following paper:\n\n```bibtex\n@inproceedings{simoulin:hal-03265900,\n  TITLE = {{Un mod{\\`e}le Transformer G{\\'e}n{\\'e}ratif Pr{\\'e}-entrain{\\'e} pour le \\_\\_\\_\\_\\_\\_ fran{\\c c}ais}},\n  AUTHOR = {Simoulin, Antoine and Crabb{\\'e}, Benoit},\n  URL = {https://hal.archives-ouvertes.fr/hal-03265900},\n  BOOKTITLE = {{Traitement Automatique des Langues Naturelles}},\n  ADDRESS = {Lille, France},\n  EDITOR = {Denis, Pascal and Grabar, Natalia and Fraisse, Amel and Cardon, R{\\'e}mi and Jacquemin, Bernard and Kergosien, Eric and Balvet, Antonio},\n  PUBLISHER = {{ATALA}},\n  PAGES = {246-255},\n  YEAR = {2021},\n  KEYWORDS = {fran{\\c c}ais. ; GPT ; G{\\'e}n{\\'e}ratif ; Transformer ; Pr{\\'e}-entra{\\^i}n{\\'e}},\n  PDF = {https://hal.archives-ouvertes.fr/hal-03265900/file/7.pdf},\n  HAL_ID = {hal-03265900},\n  HAL_VERSION = {v1},\n}\n```\n\n### References\n\n><div name=\"tiedemann-2012\">J\u00f6rg Tiedemann: Parallel Data, Tools and Interfaces in OPUS. LREC 2012: 2214-2218</div>", "size_bytes": "509586767", "downloads": 3675}
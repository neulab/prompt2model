{"pretrained_model_name": "PlanTL-GOB-ES/gpt2-base-bne", "description": "---\nlanguage:\n\n- es\n\nlicense: apache-2.0\n\ntags:\n\n- \"national library of spain\"\n\n- \"spanish\"\n\n- \"bne\"\n\n- \"gpt2-base-bne\"\n\ndatasets:\n\n- \"bne\"\n\nwidget:\n- text: \"El modelo del lenguaje GPT es capaz de\"\n- text: \"La Biblioteca Nacional de Espa\u00f1a es una entidad p\u00fablica y sus fines son\"\n\n---\n\n# GPT2-base (gpt2-base-bne) trained with data from the National Library of Spain (BNE)\n\n## Table of Contents\n<details>\n<summary>Click to expand</summary>\n\n- [Overview](#overview)\n- [Model description](#model-description)\n- [Intended uses and limitations](#intended-uses-and-limitations)\n- [How to Use](#how-to-use)\n- [Limitations and bias](#limitations-and-bias)\n- [Training](#training)\n  - [Training data](#training-data)\n  - [Training procedure](#training-procedure)\n- [Additional information](#additional-information)\n  - [Author](#author)\n  - [Contact information](#contact-information)\n  - [Copyright](#copyright)\n  - [Licensing information](#licensing-information)\n  - [Funding](#funding)\n  - [Citation Information](#citation-information)\n  - [Disclaimer](#disclaimer)\n\n</details>\n\n## Overview\n\n- **Architecture:** gpt2-base\n- **Language:** Spanish\n- **Task:** text-generation\n- **Data:** BNE\n\n## Model description\n\n**GPT2-base-bne** is a transformer-based model for the Spanish language. It is based on the [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model and has been pre-trained using the largest Spanish corpus known to date, with a total of 570GB of clean and deduplicated text processed for this work, compiled from the web crawlings performed by the  [National Library of Spain (Biblioteca Nacional de Espa\u00f1a)](http://www.bne.es/en/Inicio/index.html) from 2009 to 2019.\n\n\n## Intended uses and limitations\nYou can use the raw model for text generation or fine-tune it to a downstream task.\n\n## How to Use\nHere is how to use this model:\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, set_seed\n>>> tokenizer = AutoTokenizer.from_pretrained(\"PlanTL-GOB-ES/gpt2-base-bne\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"PlanTL-GOB-ES/gpt2-base-bne\")\n>>> generator = pipeline('text-generation', tokenizer=tokenizer, model=model)\n>>> set_seed(42)\n>>> generator(\"La Biblioteca Nacional de Espa\u00f1a es una entidad p\u00fablica y sus fines son\", num_return_sequences=5)\n\n[{'generated_text': 'La Biblioteca Nacional de Espa\u00f1a es una entidad p\u00fablica y sus fines son difundir la cultura y el arte hisp\u00e1nico, as\u00ed como potenciar las publicaciones de la Biblioteca y colecciones de la Biblioteca Nacional de Espa\u00f1a para su difusi\u00f3n e inquisici\u00f3n. '}, \n{'generated_text': 'La Biblioteca Nacional de Espa\u00f1a es una entidad p\u00fablica y sus fines son diversos. '}, \n{'generated_text': 'La Biblioteca Nacional de Espa\u00f1a es una entidad p\u00fablica y sus fines son la publicaci\u00f3n, difusi\u00f3n y producci\u00f3n de obras de arte espa\u00f1ol, y su patrimonio intelectual es el que tiene la distinci\u00f3n de Patrimonio de la Humanidad. '}, \n{'generated_text': 'La Biblioteca Nacional de Espa\u00f1a es una entidad p\u00fablica y sus fines son los de colaborar en el mantenimiento de los servicios bibliotecarios y mejorar la calidad de la informaci\u00f3n de titularidad institucional y en su difusi\u00f3n, acceso y salvaguarda para la sociedad. '}, \n{'generated_text': 'La Biblioteca Nacional de Espa\u00f1a es una entidad p\u00fablica y sus fines son la conservaci\u00f3n, ense\u00f1anza y difusi\u00f3n del patrimonio bibliogr\u00e1fico en su lengua espec\u00edfica y/o escrita. '}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\n>>> from transformers import AutoTokenizer, GPT2Model\n>>> tokenizer = AutoTokenizer.from_pretrained(\"PlanTL-GOB-ES/gpt2-base-bne\")\n>>> model = GPT2Model.from_pretrained(\"PlanTL-GOB-ES/gpt2-base-bne\")\n>>> text = \"La Biblioteca Nacional de Espa\u00f1a es una entidad p\u00fablica y sus fines son\"\n>>> encoded_input = tokenizer(text, return_tensors='pt')\n>>> output = model(**encoded_input)\n>>> print(output.last_hidden_state.shape)\ntorch.Size([1, 14, 768])\n```\n\n## Limitations and bias\n\nAt the time of submission, no measures have been taken to estimate the bias and toxicity embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated. Nevertheless, here's an example of how the model can have biased predictions:\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, set_seed\n>>> tokenizer = AutoTokenizer.from_pretrained(\"PlanTL-GOB-ES/gpt2-base-bne\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"PlanTL-GOB-ES/gpt2-base-bne\")\n>>> generator = pipeline('text-generation', tokenizer=tokenizer, model=model)\n>>> set_seed(42)\n>>> generator(\"El hombre se dedica a\", num_return_sequences=5)\n[{'generated_text': 'El hombre se dedica a comprar armas a sus amigos, pero les cuenta la historia de las ventajas de ser \"buenos y regulares en la vida\" e ir \"bien\" por los pueblos. '}, \n{'generated_text': 'El hombre se dedica a la venta de todo tipo de juguetes durante todo el a\u00f1o y los vende a trav\u00e9s de Internet con la intenci\u00f3n de alcanzar una mayor rentabilidad. '}, \n{'generated_text': 'El hombre se dedica a la venta ambulante en plena Plaza Mayor. '}, \n{'generated_text': 'El hombre se dedica a los toros y \u00e9l se dedica a los servicios religiosos. '}, \n{'generated_text': 'El hombre se dedica a la caza y a la tala de pinos. '}]\n\n>>> set_seed(42)\n>>> generator(\"La mujer se dedica a\", num_return_sequences=5)\n[{'generated_text': 'La mujer se dedica a comprar vestidos de sus padres, como su madre, y siempre le ense\u00f1a el \u00faltimo que ha hecho en poco menos de un a\u00f1o para ver si le da tiempo. '}, \n{'generated_text': 'La mujer se dedica a la venta ambulante y su pareja vende su cuerpo desde que ten\u00eda uso del autom\u00f3vil. '}, \n{'generated_text': 'La mujer se dedica a la venta ambulante en plena ola de fr\u00edo. '}, \n{'generated_text': 'La mujer se dedica a limpiar los suelos y paredes en pueblos con mucha humedad. '}, \n{'generated_text': 'La mujer se dedica a la prostituci\u00f3n en varios locales de alterne clandestinos en Barcelona. '}]\n\n```\n\n## Training\n\n### Training Data\n\nThe [National Library of Spain (Biblioteca Nacional de Espa\u00f1a)](http://www.bne.es/en/Inicio/index.html) crawls all .es domains once a year. The training corpus consists of 59TB of WARC files from these crawls, carried out from 2009 to 2019.\n\nTo obtain a high-quality training corpus, the corpus has been preprocessed with a pipeline of operations, including among others, sentence splitting, language detection, filtering of bad-formed sentences, and deduplication of repetitive contents. During the process, document boundaries are kept. This resulted in 2TB of Spanish clean corpus. Further global deduplication among the corpus is applied, resulting in 570GB of text.\n\nSome of the statistics of the corpus:\n\n| Corpora | Number of documents | Number of tokens | Size (GB) |\n|---------|---------------------|------------------|-----------|\n| BNE     |         201,080,084 |  135,733,450,668 |     570GB |\n\n### Training Procedure\nThe pretraining objective used for this architecture is next token prediction.\nThe configuration of the **GPT2-base-bne** model is as follows:\n - gpt2-base: 12-layer, 768-hidden, 12-heads, 117M parameters.\n \nThe training corpus has been tokenized using a byte version of Byte-Pair Encoding (BPE) used in the original [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model with a vocabulary size of 50,262 tokens. \n\nThe GPT2-base-bne pre-training consists of an autoregressive language model training that follows the approach of the GPT-2. \n\nThe training lasted a total of 3 days with 16 computing nodes each one with 4 NVIDIA V100 GPUs of 16GB VRAM.\n\n## Additional information\n\n### Author\nText Mining Unit (TeMU) at the Barcelona Supercomputing Center (bsc-temu@bsc.es)\n\n### Contact information\n\nFor further information, send an email to <plantl-gob-es@bsc.es>\n\n### Copyright\n\nCopyright by the Spanish State Secretariat for Digitalization and Artificial Intelligence (SEDIA) (2022)\n\n### Licensing information\n\nThis work is licensed under a [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\n### Funding\n\nThis work was funded by the Spanish State Secretariat for Digitalization and Artificial Intelligence (SEDIA) within the framework of the Plan-TL.\n\n### Citation information\nIf you use this model, please cite our [paper](http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6405):\n```\n@article{,\n   abstract = {We want to thank the National Library of Spain for such a large effort on the data gathering and the Future of Computing Center, a\nBarcelona Supercomputing Center and IBM initiative (2020). This work was funded by the Spanish State Secretariat for Digitalization and Artificial\nIntelligence (SEDIA) within the framework of the Plan-TL.},\n   author = {Asier Guti\u00e9rrez Fandi\u00f1o and Jordi Armengol Estap\u00e9 and Marc P\u00e0mies and Joan Llop Palao and Joaquin Silveira Ocampo and Casimiro Pio Carrino and Carme Armentano Oller and Carlos Rodriguez Penagos and Aitor Gonzalez Agirre and Marta Villegas},\n   doi = {10.26342/2022-68-3},\n   issn = {1135-5948},\n   journal = {Procesamiento del Lenguaje Natural},\n   keywords = {Artificial intelligence,Benchmarking,Data processing.,MarIA,Natural language processing,Spanish language modelling,Spanish language resources,Tractament del llenguatge natural (Inform\u00e0tica),\u00c0rees tem\u00e0tiques de la UPC::Inform\u00e0tica::Intel\u00b7lig\u00e8ncia artificial::Llenguatge natural},\n   publisher = {Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural},\n   title = {MarIA: Spanish Language Models},\n   volume = {68},\n   url = {https://upcommons.upc.edu/handle/2117/367156#.YyMTB4X9A-0.mendeley},\n   year = {2022},\n}\n\n```\n\n\n### Disclaimer\n\n<details>\n<summary>Click to expand</summary>\n\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions.\n\nWhen third parties, deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of Artificial Intelligence.\n\nIn no event shall the owner of the models (SEDIA \u2013 State Secretariat for Digitalization and Artificial Intelligence) nor the creator (BSC \u2013 Barcelona Supercomputing Center) be liable for any results arising from the use made by third parties of these models.\n\n\nLos modelos publicados en este repositorio tienen una finalidad generalista y est\u00e1n a disposici\u00f3n de terceros. Estos modelos pueden tener sesgos y/u otro tipo de distorsiones indeseables.\n\nCuando terceros desplieguen o proporcionen sistemas y/o servicios a otras partes usando alguno de estos modelos (o utilizando sistemas basados en estos modelos) o se conviertan en usuarios de los modelos, deben tener en cuenta que es su responsabilidad mitigar los riesgos derivados de su uso y, en todo caso, cumplir con la normativa aplicable, incluyendo la normativa en materia de uso de inteligencia artificial.\n\nEn ning\u00fan caso el propietario de los modelos (SEDIA \u2013 Secretar\u00eda de Estado de Digitalizaci\u00f3n e Inteligencia Artificial) ni el creador (BSC \u2013 Barcelona Supercomputing Center) ser\u00e1n responsables de los resultados derivados del uso que hagan terceros de estos modelos.\n</details>", "size_bytes": "499377131", "downloads": 698}
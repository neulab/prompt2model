{"pretrained_model_name": "theblackcat102/galactica-1.3b-conversation-finetuned", "description": "---\nlicense: afl-3.0\nlanguage:\n- en\nwidget:\n- text: \"<question>What's my name?<answer>\"\n  example_title: \"Who am I?\"\n- text: \"<question>How to make a campfire<answer>\"\n  example_title: \"Tutorial\"\n---\n\n# Supervised Finetuning demonstration.\n\nModels are finetuned on generated conversation curated from the [Open Assistant](https://github.com/LAION-AI/Open-Assistant).\n\n\n# Mixing reward model with sampling\n\nWe can use reward model to rank the best answer using this example code:\n\n```\nimport torch\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b-base-finetuned/checkpoint-1000\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/galactica-1.3b-base-finetuned/checkpoint-1000\").eval().half().cuda()\n\nreward_name = \"theblackcat102/electra-large-reward-model\"\nrank_model, rank_tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\nrank_model = rank_model.eval().half().cuda()\n\nquestions = [\"<question>How do I make a resume?<answer>\"]\nfor question in questions:\n    inputs = tokenizer(question, return_tensors=\"pt\", padding=True).to(0)\n    if 'token_type_ids' in inputs:\n        inputs.pop('token_type_ids')\n    outputs = model.generate(**inputs, do_sample=True,\n        top_k=60,\n        max_length=220,\n        num_return_sequences=80, \n        early_stopping=True\n    )\n    print(question)\n\n    results = []\n    for i, beam_output in enumerate(outputs):\n        output = tokenizer.decode(beam_output, truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"])\n        question, answer = output.split('<answer>', maxsplit=1)\n        answer = answer.split('<question>')[0].replace('<|endoftext|>', '').lstrip().split('<answer>')[0]\n        rank_inputs = rank_tokenizer(question, answer, return_tensors=\"pt\", padding=True, max_length=512, truncation=True).to(1)\n        score = rank_model(**rank_inputs).logits[0].cpu().detach()\n        results.append((answer, score, output))\n    full_results[question] = results\n    sorted_result = sorted(results, key=lambda x:x[1], reverse=True)\n    total_scores += sorted_result[0][1].item()\n    print('score',sorted_result[0][1].item())\n    print('-----Best rank-----')\n    print(sorted_result[0][0])\n    print('-------------------')\n```\n\n\nCheckout weights and biases [report](https://api.wandb.ai/report/theblackcat102/8yg0c0r2) for training detail. \n\nThanks to [BASIC lab](https://basiclab.lab.nycu.edu.tw/Yummy/index.html#) for compute resource. BASIC Lab is an academic research lab which focuses in multi-modality learning and data mining domain.\n", "size_bytes": "2630533369", "downloads": 42}
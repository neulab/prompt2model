{"pretrained_model_name": "google/reformer-enwik8", "description": "## Reformer Language model on character level and trained on enwik8. \n\n*enwik8* is a dataset based on Wikipedia and is often used to measure the model's ability to *compress* data, *e.g.* in \nthe scope of the *Hutter prize*: https://en.wikipedia.org/wiki/Hutter_Prize.\n\n`reformer-enwik8` was pretrained on the first 90M chars of *enwik8* whereas the text was chunked into batches of size 65536 chars (=2^16).\nThe model's weights were taken from https://console.cloud.google.com/storage/browser/trax-ml/reformer/enwik8 and converted \nto Hugging Face's PyTorch ReformerLM model `ReformerModelWithLMHead`.\n\nThe model is a language model that operates on characters. \nTherefore, this model does not need a tokenizer. The following function can instead be used for **encoding** and **decoding**:\n\n```python\nimport torch\n\n# Encoding\ndef encode(list_of_strings, pad_token_id=0):\n    max_length = max([len(string) for string in list_of_strings])\n\n    # create emtpy tensors\n    attention_masks = torch.zeros((len(list_of_strings), max_length), dtype=torch.long)\n    input_ids = torch.full((len(list_of_strings), max_length), pad_token_id, dtype=torch.long)\n\n    for idx, string in enumerate(list_of_strings):\n        # make sure string is in byte format\n        if not isinstance(string, bytes):\n            string = str.encode(string)\n\n        input_ids[idx, :len(string)] = torch.tensor([x + 2 for x in string])\n        attention_masks[idx, :len(string)] = 1\n\n    return input_ids, attention_masks\n    \n# Decoding\ndef decode(outputs_ids):\n    decoded_outputs = []\n    for output_ids in outputs_ids.tolist():\n        # transform id back to char IDs < 2 are simply transformed to \"\"\n        decoded_outputs.append(\"\".join([chr(x - 2) if x > 1 else \"\" for x in output_ids]))\n    return decoded_outputs\n```\n\nText can be generated as follows:\n\n```python\nfrom transformers import ReformerModelWithLMHead\n\nmodel = ReformerModelWithLMHead.from_pretrained(\"google/reformer-enwik8\")\nencoded, attention_masks = encode([\"In 1965, Brooks left IBM to found the Department of\"])\ndecode(model.generate(encoded, do_sample=True, max_length=150))\n\n# gives:\n# In 1965, Brooks left IBM to found the Department of Journalism in 1968. IBM had jurisdiction himself in 1980, while Brooks resolved, nevertheless thro\n\n```\n\n***Note***: Language generation using `ReformerModelWithLMHead` is not optimized yet and is rather slow.\n", "size_bytes": "596775493", "downloads": 755}
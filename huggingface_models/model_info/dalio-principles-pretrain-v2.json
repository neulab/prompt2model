{"pretrained_model_name": "Jellywibble/dalio-principles-pretrain-v2", "description": "---\ntags:\n- text-generation\nlibrary_name: transformers\n---\n\n## Model description\nBased off facebook/opt-30b model, finetuned on chucked Dalio responses\n\n## Dataset Used\nJellywibble/dalio-pretrain-book-dataset-v2\n\n## Training Parameters\n- Deepspeed on 4xA40 GPUs\n- Ensuring EOS token `<s>` appears only at the beginning of each chunk\n- Gradient Accumulation steps = 1 (Effective batch size of 4)\n- 3e-6 Learning Rate, AdamW optimizer\n- Block size of 800\n- Trained for 1 Epoch (additional epochs yielded worse Hellaswag result)\n\n## Metrics\n- Hellaswag Perplexity: 30.2\n- Eval accuracy: 49.8%\n- Eval loss: 2.283\n- Checkpoint 16 uploaded\n- wandb run: https://wandb.ai/jellywibble/huggingface/runs/2vtr39rk?workspace=user-jellywibble", "size_bytes": 121339158528, "downloads": 0}
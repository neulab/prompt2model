{"pretrained_model_name": "ser-mei/chile-gpt", "description": "---\nlicense: mit\ntags:\n- generated_from_trainer\nmodel-index:\n- name: chile-gpt\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# chile-gpt\n\nThis model is a fine-tuned version of [DeepESP/gpt2-spanish](https://huggingface.co/DeepESP/gpt2-spanish) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 9.4320\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.005\n- train_batch_size: 32\n- eval_batch_size: 32\n- seed: 42\n- gradient_accumulation_steps: 16\n- total_train_batch_size: 512\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 50\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 10.6676       | 0.98  | 6    | 9.5748          |\n| 9.6237        | 1.98  | 12   | 9.2470          |\n| 9.2815        | 2.98  | 18   | 8.8724          |\n| 8.8097        | 3.98  | 24   | 8.3629          |\n| 8.2296        | 4.98  | 30   | 7.8407          |\n| 7.6891        | 5.98  | 36   | 7.4161          |\n| 7.3013        | 6.98  | 42   | 7.1598          |\n| 7.0671        | 7.98  | 48   | 7.0080          |\n| 6.9404        | 8.98  | 54   | 6.9133          |\n| 6.7543        | 9.98  | 60   | 6.7723          |\n| 6.5845        | 10.98 | 66   | 6.6619          |\n| 6.4193        | 11.98 | 72   | 6.5965          |\n| 6.2554        | 12.98 | 78   | 6.5185          |\n| 6.0993        | 13.98 | 84   | 6.4632          |\n| 5.93          | 14.98 | 90   | 6.4155          |\n| 5.7684        | 15.98 | 96   | 6.4183          |\n| 5.6242        | 16.98 | 102  | 6.3981          |\n| 5.4577        | 17.98 | 108  | 6.4609          |\n| 5.2898        | 18.98 | 114  | 6.4577          |\n| 5.1113        | 19.98 | 120  | 6.5617          |\n| 4.9319        | 20.98 | 126  | 6.5827          |\n| 4.7464        | 21.98 | 132  | 6.6961          |\n| 4.5505        | 22.98 | 138  | 6.8359          |\n| 4.341         | 23.98 | 144  | 6.9193          |\n| 4.1324        | 24.98 | 150  | 7.0325          |\n| 3.8938        | 25.98 | 156  | 7.1993          |\n| 3.6691        | 26.98 | 162  | 7.3179          |\n| 3.4316        | 27.98 | 168  | 7.4708          |\n| 3.2041        | 28.98 | 174  | 7.5654          |\n| 2.9614        | 29.98 | 180  | 7.7535          |\n| 2.7189        | 30.98 | 186  | 7.8551          |\n| 2.4944        | 31.98 | 192  | 8.0094          |\n| 2.2624        | 32.98 | 198  | 8.0527          |\n| 2.0292        | 33.98 | 204  | 8.1857          |\n| 1.809         | 34.98 | 210  | 8.3468          |\n| 1.597         | 35.98 | 216  | 8.4307          |\n| 1.3849        | 36.98 | 222  | 8.6230          |\n| 1.2081        | 37.98 | 228  | 8.6666          |\n| 1.0273        | 38.98 | 234  | 8.7926          |\n| 0.8661        | 39.98 | 240  | 8.8861          |\n| 0.7308        | 40.98 | 246  | 8.9042          |\n| 0.6189        | 41.98 | 252  | 8.9202          |\n| 0.5335        | 42.98 | 258  | 9.0861          |\n| 0.459         | 43.98 | 264  | 9.1198          |\n| 0.3958        | 44.98 | 270  | 9.2129          |\n| 0.3587        | 45.98 | 276  | 9.2434          |\n| 0.3222        | 46.98 | 282  | 9.3005          |\n| 0.2948        | 47.98 | 288  | 9.3961          |\n| 0.2677        | 48.98 | 294  | 9.4605          |\n| 0.2348        | 49.98 | 300  | 9.4320          |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.13.0+rocm5.2\n- Datasets 2.6.1\n- Tokenizers 0.13.2\n", "size_bytes": "510398013", "downloads": 1}
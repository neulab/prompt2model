{"pretrained_model_name": "huggingtweets/bleaksigilkeep", "description": "---\nlanguage: en\nthumbnail: https://www.huggingtweets.com/bleaksigilkeep/1614100737277/predictions.png\ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('https://pbs.twimg.com/profile_images/1357543393757433856/lBrNiipb_400x400.jpg')\">\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">Ghislaine Maxwell's Fat Tiddies Apologist \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@bleaksigilkeep bot</div>\n</div>\n\nI was made with [huggingtweets](https://github.com/borisdayma/huggingtweets).\n\nCreate your own bot based on your favorite user with [the demo](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)!\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](https://github.com/borisdayma/huggingtweets/blob/master/img/pipeline.png?raw=true)\n\nTo understand how the model was developed, check the [W&B report](https://app.wandb.ai/wandb/huggingtweets/reports/HuggingTweets-Train-a-model-to-generate-tweets--VmlldzoxMTY5MjI).\n\n## Training data\n\nThe model was trained on [@bleaksigilkeep's tweets](https://twitter.com/bleaksigilkeep).\n\n| Data | Quantity |\n| --- | --- |\n| Tweets downloaded | 3206 |\n| Retweets | 1132 |\n| Short tweets | 387 |\n| Tweets kept | 1687 |\n\n[Explore the data](https://wandb.ai/wandb/huggingtweets/runs/200hepvo/artifacts), which is tracked with [W&B artifacts](https://docs.wandb.com/artifacts) at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2](https://huggingface.co/gpt2) which is fine-tuned on @bleaksigilkeep's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run](https://wandb.ai/wandb/huggingtweets/runs/1ugdbbm9) for full transparency and reproducibility.\n\nAt the end of training, [the final model](https://wandb.ai/wandb/huggingtweets/runs/1ugdbbm9/artifacts) is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/bleaksigilkeep')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](https://huggingface.co/gpt2#limitations-and-bias).\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](https://img.shields.io/twitter/follow/borisdayma?style=social)](https://twitter.com/intent/follow?screen_name=borisdayma)\n\nFor more details, visit the project repository.\n\n[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n", "size_bytes": "510406550", "downloads": 3}
{"pretrained_model_name": "mrm8488/bloom-560m-finetuned-the-stack-prolog", "description": "---\nlicense: bigscience-bloom-rail-1.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bloom-560m-finetuned-the-stack-prolog\n  results: []\n\nwidget:\n- text: '% Define un hecho que indica que \"hello\" es un saludo\nsaludo(\"hello\").\n\n% Define una regla que indica que \"world\" es un objeto\nobjeto(\"world\").\n\n% Define una regla que combina el saludo y el objeto para producir la salida \"Hola mundo\"\nhola_mundo :-'\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bloom-560m-finetuned-the-stack-prolog\n\nThis model is a fine-tuned version of [bigscience/bloom-560m](https://huggingface.co/bigscience/bloom-560m) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2433\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 4\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.2334        | 0.2   | 200  | 0.9993          |\n| 0.9174        | 0.4   | 400  | 0.7460          |\n| 0.7892        | 0.6   | 600  | 0.6046          |\n| 0.6805        | 0.8   | 800  | 0.4964          |\n| 0.5898        | 0.99  | 1000 | 0.4283          |\n| 0.411         | 1.19  | 1200 | 0.3721          |\n| 0.3705        | 1.39  | 1400 | 0.3182          |\n| 0.3516        | 1.59  | 1600 | 0.2795          |\n| 0.3298        | 1.79  | 1800 | 0.2528          |\n| 0.2721        | 1.99  | 2000 | 0.2433          |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.13.0+cu117\n- Datasets 2.5.1\n- Tokenizers 0.13.0\n", "size_bytes": "2236957537", "downloads": 0}
{"pretrained_model_name": "gorkemgoknar/gpt2-small-turkish", "description": "---\nlanguage:\n- tr\nthumbnail:\ntags:\n- gpt2\n- turkish\n\nlicense: apache-2.0\ndatasets:\n- wikipedia-turkish\nmetrics:\n- perplexity\n- accuracy\n\nwidget:\n- text: Bu yaz\u0131y\u0131 bir bilgisayar yazd\u0131. Yazarken\n  context: ''\n- text: \u0130nternete kolay eri\u015fim sayesinde d\u00fcnya daha da k\u00fc\u00e7\u00fcld\u00fc. Bunun sonucunda\n  context: ''\n---\n\n# Turkish GPT2 Model Finetuned \n# T\u00fcrk\u00e7e GPT2 Modeli\n\n## Model description\n\nThis is a GPT2-Small English based model finetuned and additionaly trainied with Wikipedia Articles in Turkish as of 28-10-2020\n\nLive demo based on this work at : https://www.metayazar.com/\n\nFine tuned writer on this model: https://huggingface.co/gorkemgoknar/gpt2-turkish-writer\n\nWork has been done on Pierre Guillou tutorial as on this page.\n(https://github.com/piegu/fastai-projects/blob/master/finetuning-English-GPT2-any-language-Portuguese-HuggingFace-fastaiv2.ipynb) \n\nCode is converted to work with Fastai 2.X .\n\nUsing Google Colab for training. \n\nAdditional tutorial and source will be in https://github.com/gorkemgoknar in later stage.\n\nCurrent accuracy 33 %  , Perplexity : 51.88\n\nModels are available:\n\n* [gpt2-small-tuned-tr] (https://huggingface.co/gorkemgoknar/gpt2-small-turkish)\n* [gpt2-small-turkish-writer] (https://huggingface.co/gorkemgoknar/gpt2-turkish-writer)\n\n## Intended uses & limitations\n\n#### How to use\n\n#### Install\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"gorkemgoknar/gpt2-small-turkish\")\nmodel = AutoModelWithLMHead.from_pretrained(\"gorkemgoknar/gpt2-small-turkish\")\n\n# Get sequence length max of 1024\ntokenizer.model_max_length=1024 \n\nmodel.eval()  # disable dropout (or leave in train mode to finetune)\n\n```\n\n#### Generate 1 word\n```python\n# input sequence\ntext = \"Bu yaz\u0131y\u0131 bilgisayar yazd\u0131.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# model output\noutputs = model(**inputs, labels=inputs[\"input_ids\"])\nloss, logits = outputs[:2]\npredicted_index = torch.argmax(logits[0, -1, :]).item()\npredicted_text = tokenizer.decode([predicted_index])\n\n# results\nprint('input text:', text)\nprint('predicted text:', predicted_text)\n\n# input text: \n# predicted text:  \n\n```\n\n#### Generate Full Sequence\n```python\n# input sequence\ntext = \"Bu yaz\u0131y\u0131 bilgisayar yazd\u0131.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# model output using Top-k sampling text generation method\nsample_outputs = model.generate(inputs.input_ids,\n                                pad_token_id=50256,\n                                do_sample=True, \n                                max_length=50, # put the token number you want\n                                top_k=40,\n                                num_return_sequences=1)\n\n# generated sequence\nfor i, sample_output in enumerate(sample_outputs):\n    print(\">> Generated text {}\\\\\\\\\n\\\\\\\\\n{}\".format(i+1, tokenizer.decode(sample_output.tolist())))\n\n# >> Generated text\n#    \n\n```\n\n#### Limitations and bias\n\nThe training data used for this model come from Turkish Wikipedia. We know it contains a lot of unfiltered content from the internet, which is far from neutral. \n\n\n## Training data\n\nWikipedia Turkish article dump as of 28-10-2020\n\n## Training procedure\n\n\n## Eval results\n\n| epoch\\\\\\\\t|train_loss\\\\\\\\t|valid_loss\\\\\\\\t|accuracy\\\\\\\\t|perplexity\\\\\\\\t|time   |\n| ----- | --------      |---------      | ----------    | ---------     | ----- |\n|0\\\\\\\\t|4.777015\\\\\\\\t|4.621834\\\\\\\\t|0.292547\\\\\\\\t|101.680367\\\\\\\\t|2:42:05|\n|1\\\\\\\\t|4.509412\\\\\\\\t|4.403999\\\\\\\\t|0.305574\\\\\\\\t|81.777267\\\\\\\\t|1:09:38|\n|2\\\\\\\\t|4.169529\\\\\\\\t|4.120755\\\\\\\\t|0.324908\\\\\\\\t|61.605747\\\\\\\\t|1:07:45|\n|3\\\\\\\\t|4.293973\\\\\\\\t|4.177899\\\\\\\\t|0.317211\\\\\\\\t|65.228653\\\\\\\\t|1:07:02|\n|4\\\\\\\\t|4.049848\\\\\\\\t|3.949103\\\\\\\\t|0.338347\\\\\\\\t|51.888783\\\\\\\\t|1:05:53|\n\n#Epoch 0 on Tesla T4, others on V100\n\n```\n\n", "size_bytes": "510406640", "downloads": 247}
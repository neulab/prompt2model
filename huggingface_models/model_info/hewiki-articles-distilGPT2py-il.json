{"pretrained_model_name": "Norod78/hewiki-articles-distilGPT2py-il", "description": "---\nlanguage: he\n\nthumbnail: https://avatars1.githubusercontent.com/u/3617152?norod.jpg\nwidget:\n- text: \"<|startoftext|>\u05d4\u05d7\u05d5\u05e7 \u05d4\u05e9\u05e0\u05d9 \u05e9\u05dc \u05de\u05d5\u05e2\u05d3\u05d5\u05df \u05e7\u05e8\u05d1 \u05d4\u05d5\u05d0\"\n- text: \"<|startoftext|>\u05e8\u05d0\u05e9 \u05d4\u05de\u05de\u05e9\u05dc\u05d4 \u05d1\u05df \u05d2\u05d5\u05e8\u05d9\u05d5\u05df\"\n- text: \"<|startoftext|>\u05dc\u05de\u05d9\u05d3\u05ea \u05de\u05db\u05d5\u05e0\u05d4 (\u05e1\u05e8\u05d8)\"\n- text: \"<|startoftext|>\u05de\u05e0\u05e9\u05d4 \u05e4\u05d5\u05de\u05e4\u05e8\u05e0\u05d9\u05e7\u05dc\"\n- text: \"<|startoftext|>\u05d0\u05d9 \u05e9\u05d5\u05d5\u05d9\u05d5\u05df \"\n\nlicense: mit\n---\n\n\n# hewiki-articles-distilGPT2py-il\n\n## A tiny GPT2 model for generating Hebrew text\n\nA distilGPT2 sized model. <br>\nTraining data was hewiki-20200701-pages-articles-multistream.xml.bz2 from https://dumps.wikimedia.org/hewiki/20200701/  <br>\nXML has been converted to plain text using Wikipedia Extractor http://medialab.di.unipi.it/wiki/Wikipedia_Extractor  <br>\nI then added <|startoftext|> and <|endoftext|> markers and deleted empty lines.  <br>\n\n#### How to use\n\n```python\nimport torch\nimport torch.nn as nn\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"Norod78/hewiki-articles-distilGPT2py-il\")\nmodel = GPT2LMHeadModel.from_pretrained(\"Norod78/hewiki-articles-distilGPT2py-il\").eval()\n\nbos_token = tokenizer.bos_token #Beginning of sentace \neos_token = tokenizer.eos_token #End of sentence \n\ndef generate_word(model, tokens_tensor, temperature=1.0):\n  \"\"\" \n  Sample a word given a tensor of tokens of previous words from a model. Given \n  the words we have, sample a plausible word. Temperature is used for \n  controlling randomness. If using temperature==0 we simply use a greedy arg max. \n  Else, we sample from a multinomial distribution using a lower inverse \n  temperature to allow for more randomness to escape repetitions. \n  \"\"\"\n  with torch.no_grad():\n    outputs = model(tokens_tensor)\n    predictions = outputs[0]\n    if temperature>0:\n      # Make the distribution more or less skewed based on the temperature\n      predictions = outputs[0]/temperature\n      # Sample from the distribution\n      softmax = nn.Softmax(dim=0)\n      predicted_index = torch.multinomial(softmax(predictions[0,-1,:]),1).item()\n    # Simply take the arg-max of the distribution\n    else:\n      predicted_index = torch.argmax(predictions[0, -1, :]).item()\n    # Decode the encoding to the corresponding word\n    predicted_text = tokenizer.decode([predicted_index])\n  return predicted_text\n\ndef generate_sentence(model, tokenizer, initial_text, temperature=1.0):\n  \"\"\" Generate a sentence given some initial text using a model and a tokenizer.\n  Returns the new sentence. \"\"\"\n        \n  # Encode a text inputs\n  text = \"\"\n  sentence = text\n\n  # We avoid an infinite loop by setting a maximum range\n  for i in range(0,84):\n    indexed_tokens = tokenizer.encode(initial_text + text)\n      \n    # Convert indexed tokens in a PyTorch tensor\n    tokens_tensor = torch.tensor([indexed_tokens])\n    \n    new_word = generate_word(model, tokens_tensor, temperature=temperature)\n\n    # Here the temperature is slowly decreased with each generated word,\n    # this ensures that the sentence (ending) makes more sense.\n    # We don't decrease to a temperature of 0.0 to leave some randomness in.\n    if temperature<(1-0.008):\n      temperature += 0.008\n    else:\n      temperature = 0.996\n\n    text = text+new_word\n\n    # Stop generating new words when we have reached the end of the line or the poem\n    if eos_token in new_word:\n      # returns new sentence and whether poem is done\n      return (text.replace(eos_token,\"\").strip(), True)\n    elif '/' in new_word:\n      return (text.strip(), False)\n    elif bos_token in new_word:\n        return (text.replace(bos_token,\"\").strip(), False)\n      \n  return (text, True)\n\nfor output_num in range(1,5):\n  init_text = \"\u05d1\u05d5\u05e7\u05e8 \u05d8\u05d5\u05d1\"\n  text = bos_token + init_text\n  for i in range(0,84):\n    sentence = generate_sentence(model, tokenizer, text, temperature=0.9)    \n    text = init_text + sentence[0]\n    print(text)\n    if (sentence[1] == True):\n      break   \n```\n", "size_bytes": "333965577", "downloads": 30}
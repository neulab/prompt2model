{"pretrained_model_name": "dkleczek/papuGaPT2", "description": "---\nlanguage: pl\ntags:\n- text-generation\nwidget:\n- text: \"Najsmaczniejszy polski owoc to\"\n---\n\n# papuGaPT2 - Polish GPT2 language model\n[GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) was released in 2019 and surprised many with its text generation capability. However, up until very recently, we have not had a strong text generation model in Polish language, which limited the research opportunities for Polish NLP practitioners. With the release of this model, we hope to enable such research. \n\nOur model follows the standard GPT2 architecture and training approach. We are using a causal language modeling (CLM) objective, which means that the model is trained to predict the next word (token) in a sequence of words (tokens). \n\n## Datasets\nWe used the Polish subset of the [multilingual Oscar corpus](https://www.aclweb.org/anthology/2020.acl-main.156) to train the model in a self-supervised fashion. \n\n```\nfrom datasets import load_dataset\ndataset = load_dataset('oscar', 'unshuffled_deduplicated_pl')\n```\n\n## Intended uses & limitations\nThe raw model can be used for text generation or fine-tuned for a downstream task. The model has been trained on data scraped from the web, and can generate text containing intense violence, sexual situations, coarse language and drug use. It also reflects the biases from the dataset (see below for more details). These limitations are likely to transfer to the fine-tuned models as well. At this stage, we do not recommend using the model beyond research.\n\n## Bias Analysis\nThere are many sources of bias embedded in the model and we caution to be mindful of this while exploring the capabilities of this model. We have started a very basic analysis of bias that you can see in [this notebook](https://huggingface.co/flax-community/papuGaPT2/blob/main/papuGaPT2_bias_analysis.ipynb).\n\n### Gender Bias\nAs an example, we generated 50 texts starting with prompts \"She/He works as\". The image below presents the resulting word clouds of female/male professions. The most salient terms for male professions are: teacher, sales representative, programmer. The most salient terms for female professions are: model, caregiver, receptionist, waitress.\n\n![gender bias](https://huggingface.co/flax-community/papuGaPT2/raw/main/gender_bias.jpeg)\n\n### Ethnicity/Nationality/Gender Bias\nWe generated 1000 texts to assess bias across ethnicity, nationality and gender vectors. We created prompts with the following scheme: \n\n* Person - in Polish this is a single word that differentiates both nationality/ethnicity and gender. We assessed the following 5 nationalities/ethnicities: German, Romani, Jewish, Ukrainian, Neutral. The neutral group used generic pronounts (\"He/She\"). \n* Topic - we used 5 different topics: \n  * random act: *entered home*\n  * said: *said*\n  * works as: *works as*\n  * intent: Polish *niech* which combined with *he* would roughly translate to *let him ...*\n  * define: *is*\n\nEach combination of 5 nationalities x 2 genders x 5 topics had 20 generated texts. \n\nWe used a model trained on [Polish Hate Speech corpus](https://huggingface.co/datasets/hate_speech_pl) to obtain the probability that each generated text contains hate speech. To avoid leakage, we removed the first word identifying the nationality/ethnicity and gender from the generated text before running the hate speech detector.\n \nThe following tables and charts demonstrate the intensity of hate speech associated with the generated texts. There is a very clear effect where each of the ethnicities/nationalities score higher than the neutral baseline. \n\n![hate score by ethnicity](https://huggingface.co/flax-community/papuGaPT2/raw/main/hate_by_ethnicity.png)\n\nLooking at the gender dimension we see higher hate score associated with males vs. females. \n\n![hate score by gender](https://huggingface.co/flax-community/papuGaPT2/raw/main/hate_by_gender.png)\n\nWe don't recommend using the GPT2 model beyond research unless a clear mitigation for the biases is provided. \n\n## Training procedure\n### Training scripts\nWe used the [causal language modeling script for Flax](https://github.com/huggingface/transformers/blob/master/examples/flax/language-modeling/run_clm_flax.py). We would like to thank the authors of that script as it allowed us to complete this training in a very short time!\n\n### Preprocessing and Training Details\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 512 consecutive tokens.\n\nWe have trained the model on a single TPUv3 VM, and due to unforeseen events the training run was split in 3 parts, each time resetting from the final checkpoint with a new optimizer state: \n1. LR 1e-3, bs 64, linear schedule with warmup for 1000 steps, 10 epochs, stopped after 70,000 steps at eval loss 3.206 and perplexity 24.68\n2. LR 3e-4, bs 64, linear schedule with warmup for 5000 steps, 7 epochs, stopped after 77,000 steps at eval loss 3.116 and perplexity 22.55\n3. LR 2e-4, bs 64, linear schedule with warmup for 5000 steps, 3 epochs, stopped after 91,000 steps at eval loss 3.082 and perplexity 21.79\n\n## Evaluation results\nWe trained the model on 95% of the dataset and evaluated both loss and perplexity on 5% of the dataset. The final checkpoint evaluation resulted in: \n* Evaluation loss: 3.082\n* Perplexity: 21.79\n\n## How to use\nYou can use the model either directly for text generation (see example below), by extracting features, or for further fine-tuning. We have prepared a notebook with text generation examples [here](https://huggingface.co/flax-community/papuGaPT2/blob/main/papuGaPT2_text_generation.ipynb) including different decoding methods, bad words suppression, few- and zero-shot learning demonstrations. \n\n### Text generation\nLet's first start with the text-generation pipeline. When prompting for the best Polish poet, it comes up with a pretty reasonable text, highlighting one of the most famous Polish poets, Adam Mickiewicz.\n \n```python\nfrom transformers import pipeline, set_seed\ngenerator = pipeline('text-generation', model='flax-community/papuGaPT2')\nset_seed(42)\ngenerator('Najwi\u0119kszym polskim poet\u0105 by\u0142')\n>>> [{'generated_text': 'Najwi\u0119kszym polskim poet\u0105 by\u0142 Adam Mickiewicz - uwa\u017cany za jednego z dw\u00f3ch geniusz\u00f3w j\u0119zyka polskiego. \"Pan Tadeusz\" by\u0142 jednym z najpopularniejszych dzie\u0142 w historii Polski. W 1801 zosta\u0142 wystawiony publicznie w Teatrze Wilama Horzycy. Pod jego'}]\n```\n\nThe pipeline uses `model.generate()` method in the background. In [our notebook](https://huggingface.co/flax-community/papuGaPT2/blob/main/papuGaPT2_text_generation.ipynb) we demonstrate different decoding methods we can use with this method, including greedy search, beam search, sampling, temperature scaling, top-k and top-p sampling. As an example, the below snippet uses sampling among the 50 most probable tokens at each stage (top-k) and among the tokens that jointly represent 95% of the probability distribution (top-p). It also returns 3 output sequences. \n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nmodel = AutoModelWithLMHead.from_pretrained('flax-community/papuGaPT2')\ntokenizer = AutoTokenizer.from_pretrained('flax-community/papuGaPT2')\nset_seed(42) # reproducibility\ninput_ids = tokenizer.encode('Najwi\u0119kszym polskim poet\u0105 by\u0142', return_tensors='pt')\n\nsample_outputs = model.generate(\n    input_ids,\n    do_sample=True, \n    max_length=50, \n    top_k=50, \n    top_p=0.95, \n    num_return_sequences=3\n)\n\nprint(\"Output:\\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n  \n>>> Output:\n>>> ----------------------------------------------------------------------------------------------------\n>>> 0: Najwi\u0119kszym polskim poet\u0105 by\u0142 Roman Ingarden. Na jego wiersze i piosenki oddzia\u0142ywa\u0142y jego zami\u0142owanie do przyrody i przyrody. Dlatego te\u017c jako poeta w czasie pracy nad utworami i wierszami z tych wierszy, a nast\u0119pnie z poezji w\u0142asnej - pisa\u0142\n>>> 1: Najwi\u0119kszym polskim poet\u0105 by\u0142 Julian Przybo\u015b, kt\u00f3rego poematem \u201eWierszyki dla dzieci\u201d.\n>>> W okresie mi\u0119dzywojennym, pod has\u0142em \u201ePapie\u017c i nie tylko\u201d Polska, jak wi\u0119kszo\u015b\u0107 kraj\u00f3w europejskich, by\u0142a pa\u0144stwem faszystowskim.\n>>> Pr\u00f3cz\n>>> 2: Najwi\u0119kszym polskim poet\u0105 by\u0142 Boles\u0142aw Le\u015bmian, kt\u00f3ry by\u0142 jego t\u0142umaczem, a jego poezja t\u0142umaczy\u0142a na kilkana\u015bcie j\u0119zyk\u00f3w.\n>>> W 1895 roku nak\u0142adem krakowskiego wydania \"Scientio\" ukaza\u0142a si\u0119 w j\u0119zyku polskim powie\u015b\u0107 W krainie kangur\u00f3w\n```  \n### Avoiding Bad Words\nYou may want to prevent certain words from occurring in the generated text. To avoid displaying really bad words in the notebook, let's pretend that we don't like certain types of music to be advertised by our model. The prompt says: *my favorite type of music is*. \n\n```python\ninput_ids = tokenizer.encode('M\u00f3j ulubiony gatunek muzyki to', return_tensors='pt')\n\nbad_words = [' disco', ' rock', ' pop', ' soul', ' reggae', ' hip-hop']\nbad_word_ids = []\nfor bad_word in bad_words: \n  ids = tokenizer(bad_word).input_ids\n  bad_word_ids.append(ids)\n  \nsample_outputs = model.generate(\n    input_ids,\n    do_sample=True, \n    max_length=20, \n    top_k=50, \n    top_p=0.95, \n    num_return_sequences=5,\n    bad_words_ids=bad_word_ids\n)\n\nprint(\"Output:\\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n  \n>>> Output:\n>>> ----------------------------------------------------------------------------------------------------\n>>> 0: M\u00f3j ulubiony gatunek muzyki to muzyka klasyczna. Nie wiem, czy to kwestia sposobu, w jaki gramy,\n>>> 1: M\u00f3j ulubiony gatunek muzyki to reggea. Zachwycaj\u0105 mnie piosenki i piosenki muzyczne o ducho\n>>> 2: M\u00f3j ulubiony gatunek muzyki to rockabilly, ale nie lubi\u0119 te\u017c punka. Moim ulubionym gatunkiem\n>>> 3: M\u00f3j ulubiony gatunek muzyki to rap, ale to raczej si\u0119 nie zdarza w miejscach, gdzie nie chodzi\n>>> 4: M\u00f3j ulubiony gatunek muzyki to metal aran\u017ceje nie mam poj\u0119cia co mam robi\u0107. Co roku,\n```\nOk, it seems this worked: we can see *classical music, rap, metal* among the outputs. Interestingly, *reggae* found a way through via a misspelling *reggea*. Take it as a caution to be careful with curating your bad word lists!\n\n### Few Shot Learning\n\nLet's see now if our model is able to pick up training signal directly from a prompt, without any finetuning. This approach was made really popular with GPT3, and while our model is definitely less powerful, maybe it can still show some skills! If you'd like to explore this topic in more depth, check out [the following article](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api) which we used as reference.\n\n```python\nprompt = \"\"\"Tekst: \"Nienawidz\u0119 smerf\u00f3w!\"\nSentyment: Negatywny\n###\nTekst: \"Jaki pi\u0119kny dzie\u0144 \ud83d\udc4d\"\nSentyment: Pozytywny\n###\nTekst: \"Jutro id\u0119 do kina\"\nSentyment: Neutralny\n###\nTekst: \"Ten przepis jest \u015bwietny!\"\nSentyment:\"\"\"\n\nres = generator(prompt, max_length=85, temperature=0.5, end_sequence='###', return_full_text=False, num_return_sequences=5,)\nfor x in res: \n  print(res[i]['generated_text'].split(' ')[1])\n  \n>>> Pozytywny\n>>> Pozytywny\n>>> Pozytywny\n>>> Pozytywny\n>>> Pozytywny\n```\nIt looks like our model is able to pick up some signal from the prompt. Be careful though, this capability is definitely not mature and may result in spurious or biased responses. \n\n### Zero-Shot Inference\n\nLarge language models are known to store a lot of knowledge in its parameters. In the example below, we can see that our model has learned the date of an important event in Polish history, the battle of Grunwald. \n\n```python\nprompt = \"Bitwa pod Grunwaldem mia\u0142a miejsce w roku\"\ninput_ids = tokenizer.encode(prompt, return_tensors='pt')\n# activate beam search and early_stopping\nbeam_outputs = model.generate(\n    input_ids, \n    max_length=20, \n    num_beams=5, \n    early_stopping=True,\n    num_return_sequences=3\n)\n\nprint(\"Output:\\\n\" + 100 * '-')\nfor i, sample_output in enumerate(beam_outputs):\n  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n  \n>>> Output:\n>>> ----------------------------------------------------------------------------------------------------\n>>> 0: Bitwa pod Grunwaldem mia\u0142a miejsce w roku 1410, kiedy to wojska polsko-litewskie pod\n>>> 1: Bitwa pod Grunwaldem mia\u0142a miejsce w roku 1410, kiedy to wojska polsko-litewskie pokona\n>>> 2: Bitwa pod Grunwaldem mia\u0142a miejsce w roku 1410, kiedy to wojska polsko-litewskie,\n```\n\n## BibTeX entry and citation info\n```bibtex\n@misc{papuGaPT2,\n  title={papuGaPT2 - Polish GPT2 language model},\n  url={https://huggingface.co/flax-community/papuGaPT2},\n  author={Wojczulis, Micha\u0142 and K\u0142eczek, Dariusz},\n  year={2021}\n}\n```", "size_bytes": "510401385", "downloads": 66}
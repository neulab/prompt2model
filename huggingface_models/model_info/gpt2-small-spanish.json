{"pretrained_model_name": "datificate/gpt2-small-spanish", "description": "---\nlanguage: es\n\nwidget:\n- text: \"La inteligencia artificial en lationoam\u00e9rica se ha desarrollado \"\nlicense: apache-2.0\ndatasets: \n- wikipedia\n---\nLa descripci\u00f3n en Espa\u00f1ol se encuentra despu\u00e9s de la descripci\u00f3n en Ingl\u00e9s.\n\n# (English) GPT2-small-spanish: a Language Model for Spanish text generation (and more NLP tasks...)\nGPT2-small-spanish is a state-of-the-art language model for Spanish based on the GPT-2 small model. \n\nIt was trained on Spanish Wikipedia using **Transfer Learning and Fine-tuning techniques**. The training took around 70 hours with four GPU NVIDIA GTX 1080-Ti with 11GB of DDR5 and with around 3GB of (processed) training data. \n\nIt was fine-tuned from the [English pre-trained GPT-2 small](https://huggingface.co/gpt2) using the Hugging Face libraries (Transformers and Tokenizers) wrapped into the [fastai v2](https://dev.fast.ai/) Deep Learning framework. All the fine-tuning fastai v2 techniques were used.\n\nThe training is purely based on the [GPorTuguese-2](https://huggingface.co/pierreguillou/gpt2-small-portuguese) model developed by Pierre Guillou. The training details are in this article: \"[Faster than training from scratch \u2014 Fine-tuning the English GPT-2 in any language with Hugging Face and fastai v2 (practical case with Portuguese)](https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787)\".\n\nThis preliminary version is now available on Hugging Face.\n\n## Limitations and bias\n\n(Copied from original GPorTuguese-2 model)The training data used for this model come from Spanish Wikipedia. We know it contains a lot of unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their model card:\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don\u2019t support use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\n\n## Authors\n\nThe model was trained and evaluated by [Josu\u00e9 Obregon](https://www.linkedin.com/in/josue-obregon/) and [Berny Carrera](https://www.linkedin.com/in/bernycarrera/), founders of [Datificate](https://datificate.com), a space for learning Machine Learning in Spanish.\nThe training was possible thanks to the computing power of several GPUs (GPU NVIDIA GTX1080-Ti) of the [IAI Lab](http://iai.khu.ac.kr/) (Kyung Hee University) from which Josu\u00e9 is attached as a Postdoctoral Researcher in Industrial Artificial Intelligence.\n\nAs stated before, this work is mainly based in the work of [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/).\n\n\n# (Espa\u00f1ol) GPT2-small-spanish: un modelo de lenguaje para generaci\u00f3n de texto en Espa\u00f1ol (y algunas otras tareas de NLP...)\n\nGPT2-small-spanish es un modelo de lenguaje de vanguardia en Espa\u00f1ol basado en el modelo peque\u00f1o GPT-2. \n\nFu\u00e9 entrenado con la Wikipedia en Espa\u00f1ol usando **t\u00e9cnicas de Aprendizaje por Transferencia y afinaci\u00f3n de modelos**. El entrenamiento del modelo tom\u00f3 alrededor 70 horas con cuatro GPUs NVIDIA GTX 1080-Ti con 11GB de DDR5 y con aproximadamente 3GB de datos de entrenamiento preprocesados. \n\nFue afinado del modelo en Ingl\u00e9s [English pre-trained GPT-2 small](https://huggingface.co/gpt2) utilizando las librer\u00edas de Hugging Face  (Transformers y Tokenizers) integradas con el framework de Deep Learning [fastai v2](https://dev.fast.ai/). Se usaron t\u00e9cnicas de afinamiento fino de fastai v2.\n\nEl entrenamiento est\u00e1 enteramente basado en el modelo en Portugu\u00e9s [GPorTuguese-2](https://huggingface.co/pierreguillou/gpt2-small-portuguese) desarrollado por Pierre Guillou. Los detalles del entrenamiento se encuentran en este articulo: \"[Faster than training from scratch \u2014 Fine-tuning the English GPT-2 in any language with Hugging Face and fastai v2 (practical case with Portuguese)](https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787)\".\n\nLa versi\u00f3n preliminar del modelo se encuentra en Hugging Face.\n\n## Limitaciones y sesgos\n\n(Copiado del modelo original GPorTuguese-2 model)Los datos de entrenamiento provienen de la Wikipedia en Espa\u00f1ol. Se sabe que contiene bastante contenido no filtrado del internet, lo cual est\u00e1 lejos de ser neutral. Esto es se\u00f1alado por el equipo desarrollador de openAI en su propia tarjeta de modelo:\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don\u2019t support use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\n\n## Autores\n\nEl modelo fue entreando y evaluado por [Josu\u00e9 Obregon](https://www.linkedin.com/in/josue-obregon/) y [Berny Carrera](https://www.linkedin.com/in/bernycarrera/), fundadores de [Datificate](https://datificate.com), un espacio para aprender Machine Learning en Espa\u00f1ol.\n\nEl entrenamiento fue posible gracias al poder computacional de varias GPUs (GPU NVIDIA GTX1080-Ti) del Laboratorio de Inteligencia Artificial Industrial [IAI Lab](http://iai.khu.ac.kr/) (Universidad de Kyung Hee) al cual Josu\u00e9 pertenece como investigador postdoctoral en Inteligencia Artificial Industrial.\n\nComo fue mencionado anteriormente, este trabajo est\u00e1 basado en el trabajo de [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/).\n\n", "size_bytes": "510408315", "downloads": 1411}
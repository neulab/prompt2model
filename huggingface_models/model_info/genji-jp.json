{"pretrained_model_name": "NovelAI/genji-jp", "description": "---\nlanguage:\n- ja\n- en\ntags:\n- pytorch\n- causal-lm\nlicense: apache-2.0\n\n---\n\n# Genji-JP 6B\n\nPlease check our blog post for more details, samples, evaluations and more:\n[Blogpost](https://blog.novelai.net/data-efficient-language-transfer-with-gpt-j-45daedaaf35a)\n\n## Model Description\n\nGenji-JP 6B is a model finetuned on our Japanese storytelling dataset based on EleutherAI's GPT-J 6B model. This particular model is trained on Japanese web novels.\n\n| Hyperparameter    | Value  | \n|-------------------|--------|\n| n_parameters      | 6,053,381,344 |\n| n_layers          | 28*    |\n| d_model           | 4,096  |\n| d_ff              | 16,384 |\n| n_heads           | 16     |\n| d_head            | 256    |\n| n_ctx             | 2,048  |\n| n_vocab           | 50,400 (same tokenizer as GPT-2/3)  |\n| position encoding | [Rotary position encodings (RoPE)](https://arxiv.org/abs/2104.09864) |\n| RoPE dimensions   | [64](https://github.com/kingoflolz/mesh-transformer-jax/blob/f2aa66e0925de6593dcbb70e72399b97b4130482/mesh_transformer/layers.py#L223) |\n\n`*` each layer consists of one feedforward block and one self attention block\n\nThe model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model\ndimension is split into 16 heads, each with a dimension of 256. Rotary position encodings (RoPE) was applied to 64\ndimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as\nGPT-2/GPT-3.\n\n## Training data\n\nGPT-J 6B was pretrained on the [Pile](pile.eleuther.ai), a large scale curated dataset created by EleutherAI for the purpose of training this model. After the pre-training, it's finetuned on our Japanese storytelling dataset. Check our blog post for more details.\n\n### How to use\n\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"NovelAI/genji-jp\", torch_dtype=torch.float16, low_cpu_mem_usage=True).eval().cuda()\ntext = '''\u3042\u3089\u3059\u3058\uff1a\u3042\u306a\u305f\u306f\u7570\u4e16\u754c\u306b\u8ee2\u751f\u3057\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u52c7\u8005\u3068\u306a\u3063\u3066\u3001\u4ef2\u9593\u3092\u4f5c\u308a\u3001\u7570\u4e16\u754c\u3092\u5192\u967a\u3057\u3088\u3046\uff01\n***\n\u8ee2\u751f\u3059\u308b\u3068\u3001\u3042\u308b\u80fd\u529b\u3092\u624b\u306b\u5165\u308c\u3066\u3044\u305f\u3002\u305d\u308c\u306f\u3001'''\n\ntokens = tokenizer(text, return_tensors=\"pt\").input_ids\ngenerated_tokens = model.generate(tokens.long().cuda(), use_cache=True, do_sample=True, temperature=1, top_p=0.9, repetition_penalty=1.125, min_length=1, max_length=len(tokens[0]) + 400, pad_token_id=tokenizer.eos_token_id)\nlast_tokens = generated_tokens[0]\ngenerated_text = tokenizer.decode(last_tokens).replace(\"\ufffd\", \"\")\nprint(\"Generation:\\n\" + generated_text)\n```\nWhen run, produces output like this:\n```\nGeneration:\n\u3042\u3089\u3059\u3058\uff1a\u3042\u306a\u305f\u306f\u7570\u4e16\u754c\u306b\u8ee2\u751f\u3057\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u52c7\u8005\u3068\u306a\u3063\u3066\u3001\u4ef2\u9593\u3092\u4f5c\u308a\u3001\u7570\u4e16\u754c\u3092\u5192\u967a\u3057\u3088\u3046\uff01\n***\n\u8ee2\u751f\u3059\u308b\u3068\u3001\u3042\u308b\u80fd\u529b\u3092\u624b\u306b\u5165\u308c\u3066\u3044\u305f\u3002\u305d\u308c\u306f\u3001\u300e\u4e88\u77e5\u300f\u3060\u3002\u904e\u53bb\u304b\u3089\u672a\u6765\u306e\u3053\u3068\u3092\u3001\u8ab0\u3082\u77e5\u3089\u306a\u3044\u51fa\u6765\u4e8b\u3082\u542b\u3081\u3066\u898b\u901a\u3059\u3053\u3068\u304c\u51fa\u6765\u308b\u3002\n\u60aa\u9b54\u306e\u6b20\u7247\u3068\u547c\u3070\u308c\u308b\u5c0f\u3055\u306a\u7d50\u6676\u3092\u53d6\u308a\u8fbc\u3093\u3067\u3001\u4f7f\u5f79\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u308b\u3002\u4eba\u3092\u60f9\u304d\u3064\u3051\u3001\u5815\u843d\u3055\u305b\u308b\u3002\u4f55\u3088\u308a\u3001\u4ffa\u306f\u7537\u306a\u3093\u3066\u5c45\u306a\u304b\u3063\u305f\u3057\u3001\u5973\u306b\u8208\u5473\u3082\u306a\u3044\u3002\u2026\u2026\u305d\u3093\u306a\u30af\u30ba\u306e\u7247\u68d2\u3092\u62c5\u304e\u4e0a\u3052\u308b\u5974\u304c\u591a\u304f\u306a\u308b\u3068\u601d\u3046\u3068\u3001\u3061\u3087\u3063\u3068\u82e6\u3057\u3044\u3002\n\u3060\u304c\u3001\u4e00\u90e8\u306e\u4eba\u9593\u306b\u306f\u5354\u529b\u8005\u3092\u5f97\u308b\u3053\u3068\u304c\u51fa\u6765\u308b\u3002\u76ee\u7acb\u305f\u306a\u3044\u8857\u306b\u3042\u308b\u5bfa\u306e\u4e2d\u3067\u3001\u5e38\u306b\u5bb6\u306b\u5f15\u304d\u3053\u3082\u3063\u3066\u3044\u308b\u8001\u4eba\u3002\u305d\u3093\u306a\u30e4\u30c4\u306e\u9b42\u3092\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u308b\u306e\u3060\u3002\u4fbf\u5229\u306a\u80fd\u529b\u3060\u3002\u3057\u304b\u3057\u3001\u88cf\u5207\u308a\u8005\u306f\u5927\u52e2\u3044\u308b\u3002\u6c17\u3092\u629c\u3051\u3070\u3001\u72c2\u3046\u3002\u3060\u304b\u3089\u6ce8\u610f\u304c\u5fc5\u8981\u3060\u3002\n\u2015\u2015\u300c\u3084\u3063\u3066\u3084\u308b\u3088\u300d\n\u3000\u30a2\u30fc\u30ed\u30f3\u306f\u4e0d\u6575\u306b\u7b11\u3063\u305f\u3002\u3053\u306e\n```\n\n## Acknowledgements\n\nThis project was possible because of the compute provided by the\n[TPU Research Cloud](https://sites.research.google/trc/)\n\nThanks [EleutherAI](https://eleuther.ai/) for pretraining the GPT-J 6B model.\n\nThanks to everyone who contributed to this project!\n\n- [Finetune](https://github.com/finetuneanon)\n- [Aero](https://github.com/AeroScripts)\n- [Kurumuz](https://github.com/kurumuz)", "size_bytes": "12106053103", "downloads": 504}
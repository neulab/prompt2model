{"pretrained_model_name": "Milos/slovak-gpt-j-162M", "description": "---\nlanguage:\n- sk\ntags:\n- Slovak GPT-J\n- pytorch\n- causal-lm\nlicense: gpl-3.0\n---\n\n# Slovak GPT-J-162M\nSlovak GPT-J-162M is the first model released in Slovak GPT-J series and the very first publicly available transformer trained predominantly on Slovak corpus. Since the initial release two other models were made public, [Slovak GPT-J-405M](https://huggingface.co/Milos/slovak-gpt-j-405M) and the largest [Slovak GPT-J-1.4B](https://huggingface.co/Milos/slovak-gpt-j-1.4B).\n\n## Model Description\nModel is based on [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax/) and has over 162M trainable parameters.\n\n<figure>\n\n| Hyperparameter       | Value                                                                                                                         |\n|----------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| \\\\(n_{parameters}\\\\) | 162,454,608                                                                                                                   |\n| \\\\(n_{layers}\\\\)     | 12                                                                                                                            |\n| \\\\(d_{model}\\\\)      | 768                                                                                                                           |\n| \\\\(d_{ff}\\\\)         | 16384                                                                                                                         |\n| \\\\(n_{heads}\\\\)      | 16                                                                                                                            |\n| \\\\(d_{head}\\\\)       | 256                                                                                                                           |\n| \\\\(n_{ctx}\\\\)        | 2048                                                                                                                          |\n| \\\\(n_{vocab}\\\\)      | 50256 (same tokenizer as GPT-2/3&dagger;)                                                                                     |\n| Positional Encoding  | [Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864)                                                          |\n| RoPE Dimensions      | [64](https://github.com/kingoflolz/mesh-transformer-jax/blob/f2aa66e0925de6593dcbb70e72399b97b4130482/mesh_transformer/layers.py#L223) |\n\n<p><strong>&dagger;</strong> ByteLevelBPETokenizer was trained on the same Slovak corpus.</p></figure>\n\n## Training data\n\nSlovak GPT-J-162M was trained on a privately collected dataset consisting of predominantly Slovak text spanning different categories, e.g. web, news articles or even biblical texts - in total, over 40GB of text data was used to train this model.\nThe dataset was preprocessed and cleaned in a specific way that involves minor but a few caveats, so in order to achieve the expected performance, feel free to refer to [How to use] section. Please, keep in mind that despite the effort to remove inappropriate parts of the corpus, the model still might generate sensitive content or leak sensitive information.\n\n## Training procedure\n\nThis model was trained for almost 37 billion tokens over 69,001 steps on TPU v3-8 pod. The cross-entropy validation loss at the last step was 3.065.\n\n## Intended Use\n\nSame as the original GPT-J, Slovak GPT-J learns an inner representation of the language that can be used to extract features useful for downstream tasks, however, the intended use is text generation from a prompt.\n\n### How to use\n\nThis model along with the tokenizer can be easily loaded using the `AutoModelForCausalLM` functionality:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Milos/slovak-gpt-j-162M\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Milos/slovak-gpt-j-162M\")\n```\n\nWhen generating a prompt keep in mind these three things, and you should be good to go:\n1. Never leave trailing whitespaces. There's a difference between how tokenizer encodes \"M\u00e1m r\u00e1d sloven\u010dinu\" (no space after `sloven\u010dinu`) and \"M\u00e1m r\u00e1d sloven\u010dinu \" (trailing space after `sloven\u010dinu`), i.e `[12805, 2872, 46878]` != `[12805, 2872, 46878, 221]`.\n2. Always use good ol' US English primary double quotation marks, i.e. `\"\"` instead of `\u201e\u201c`.\n3. In case of a new line always enter `\\n\\n` instead of a single `\\n`\n\nTo illustrate an example of a basic text generation:\n```\n>>> prompt = \"Moje najob\u013eubenej\u0161ie mesto na severe Slovenska je\"\n>>> encoded_input = tokenizer(prompt, return_tensors='pt')\n>>> output = model.generate(**encoded_input)\n>>> tokenizer.decode(output[0])\n'Moje najob\u013eubenej\u0161ie mesto na severe Slovenska je \u017dilina.\\n\\nV \u017diline sa nach\u00e1dza mno\u017estvo zauj\u00edmav\u00fdch miest'\n```\n\n### Capabilities, Limitations, and Biases\n\nFirst and foremost, the capability of this particular model is very limited due to its relatively small size totalling only 162M parameters, hence the intended use of this particular model is to educate and have fun! :)\n\nSince the dataset contains profanity, politically incorrect language, and (unintentionally) even a bits of text in Czech, the model can generate them in some extent too. Here's an example of the model output when prompt is in Czech:\n```\n>>> prompt = \"V\u011bta nesm\u00ed b\u00fdt sprost\u00e1 a mus\u00ed b\u00fdt zcela\"\n>>> encoded_input = tokenizer(prompt, return_tensors='pt')\n>>> output = model.generate(**encoded_input, max_length=16)\n>>> tokenizer.decode(output[0])\n'V\u011bta nesm\u00ed b\u00fdt sprost\u00e1 a mus\u00ed b\u00fdt zcela v\u011brn\u00e1.'\n```\n\n## Citation and Related Information\n\nThis was done as a moonlighting project during summer of 2021 to better understand transformers. I didn't have much free time to open source it properly, so it all sat on my hard drive until now. Based on the popularity and interest in this model I might release _substantially_ larger versions of Slovak GPT-J models that are way more capable.\n\nIf you use this model or have any questions about it feel free to hit me up at [twitter](https://twitter.com/miloskondela) or check out my [github](https://github.com/kondela) profile.\n\n### BibTeX entry\nTo cite this model:\n```bibtex\n@misc{slovak-gpt-j-162m,\n  author = {Kondela, Milos},\n  title = {{Slovak GPT-J-162M}},\n  howpublished = {\\url{https://huggingface.co/Milos/slovak-gpt-j-162M}},\n  year = 2022,\n  month = February\n}\n```\n\nTo cite the codebase that trained this model:\n```bibtex\n@misc{mesh-transformer-jax,\n  author = {Wang, Ben},\n  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},\n  howpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\n  year = 2021,\n  month = May\n}\n```\n\n## Acknowledgements\nThis project was generously supported by [TPU Research Cloud (TRC) program](https://sites.research.google/trc/about/). Shoutout also goes to [Ben Wang](https://github.com/kingoflolz) and great [EleutherAI community](https://www.eleuther.ai/).", "size_bytes": "328721923", "downloads": 5}
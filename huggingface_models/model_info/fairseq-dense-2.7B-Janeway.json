{"pretrained_model_name": "KoboldAI/fairseq-dense-2.7B-Janeway", "description": "---\nlanguage: en\nlicense: mit\n---\n# Fairseq-dense 2.7B - Janeway\n## Model Description\nFairseq-dense 2.7B-Janeway is a finetune created using Fairseq's MoE dense model.\n## Training data\nThe training data contains around 2210 ebooks, mostly in the sci-fi and fantasy genres. The dataset is identical as dataset used by GPT-Neo-2.7B-Janeway.\nSome parts of the dataset have been prepended using the following text: `[Genre: <genre1>,<genre2>]`\n### How to use\nYou can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:\n```py\n>>> from transformers import pipeline\n>>> generator = pipeline('text-generation', model='KoboldAI/fairseq-dense-2.7B-Janeway')\n>>> generator(\"Welcome Captain Janeway, I apologize for the delay.\", do_sample=True, min_length=50)\n[{'generated_text': 'Welcome Captain Janeway, I apologize for the delay.\"\\nIt's all right,\" Janeway said. \"I'm certain that you're doing your best to keep me informed of what\\'s going on.\"'}]\n```\n### Limitations and Biases\nBased on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion).\n\n### BibTeX entry and citation info\n```\nArtetxe et al. (2021): Efficient Large Scale Language Modeling with Mixtures of Experts\n```", "size_bytes": "10585446591", "downloads": 457}
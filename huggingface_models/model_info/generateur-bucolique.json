{"pretrained_model_name": "kilimandjaro/generateur-bucolique", "description": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: generateur-bucolique\n  results: []\nwidget:\n- text: \"Les bruits de la campagne \"\n  example_title: \"Exemple #1\"\n- text: \"Le brouillard se dissipa \"\n  example_title: \"Exemple #2\"\n- text: \"A travers champs \"\n  example_title: \"Exemple #3\"\n- text: \"Des artisans \"\n  example_title: \"Exemple #4\"\n---\n\n#  generateur-bucolique \n\nThis model is a fine-tuned version of [asi/gpt-fr-cased-small](https://huggingface.co/asi/gpt-fr-cased-small) on the \"romans champ\u00eatres\" dataset (four books).\nThese four novels written by George Sand (*La mare au diable*, *La petite fadette*, *Les ma\u00eetres sonneurs*, *Fran\u00e7ois le champi*) are characterised by a very bucolic narrative model which depicts characters leading a humble life in \nthe rural setting of the first half of the 19th century :\n\n```\nIl criait aussi, le pauvret, d\u2019une voix qu\u2019il voulait rendre terrible et qui restait douce comme sa figure ang\u00e9lique. Tout cela \u00e9tait beau de force ou de gr\u00e2ce :\nle paysage, l\u2019homme, l\u2019enfant, les taureaux sous le joug ; et, malgr\u00e9 cette lutte puissante o\u00f9 la terre \u00e9tait vaincue, il y avait un sentiment de douceur et de\ncalme profond qui planait sur toutes choses. (La mare au diable)\n```\n```\nChacun sait pourtant qu\u2019il y a danger \u00e0 rester au bord de notre rivi\u00e8re quand le grand vent se l\u00e8ve. Toutes les rives sont min\u00e9es en dessous, et il n\u2019est point\nd\u2019orage qui, dans la quantit\u00e9, ne d\u00e9racine quelques-uns de ces vergnes qui sont toujours courts en racines, \u00e0 moins qu\u2019ils ne soient tr\u00e8s gros et tr\u00e8s vieux,\net qui vous tomberaient fort bien sur le corps sans vous avertir. (La petite fadette)\n```\n\nThe model is fine-tuned with approximately 5K sentences.\nThe generator is intended to reproduce the writing style of George Sand (at least it tries).\n\n### Training hyperparameters\n\nThe parameters are the same as in [Rey Farhan' notebook (Easy GPT2 fine-tuning with Hugging Face and PyTorch)](https://reyfarhan.com/posts/easy-gpt2-finetuning-huggingface)\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.0005\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 20\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| No log        | 1.0   | 495  | 2.1639          |\n| 2.3543        | 2.0   | 990  | 2.1599          |\n| 1.6869        | 3.0   | 1485 | 2.4279          |\n| 1.0753        | 4.0   | 1980 | 2.8547          |\n| 0.5891        | 5.0   | 2475 | 3.0414          |\n\n\n### Framework versions\n\n- Transformers 4.20.1\n- Pytorch 1.11.0\n- Datasets 2.1.0\n- Tokenizers 0.12.1\n", "size_bytes": "509607017", "downloads": 0}
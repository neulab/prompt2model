{"pretrained_model_name": "Kirili4ik/ruDialoGpt3-medium-finetuned-telegram", "description": "---\nlanguage: \n- ru\n- ru-RU \ntags:\n- conversational\n---\n### \ud83d\udcdd Description\n\nDialoGPT trained on Russian language and fine tuned on my telegram chat.\n\n\nThis model was created by [sberbank-ai](https://hf.co/sberbank-ai) and trained on Russian forums (see [Grossmend's model](https://hf.co/Grossmend/rudialogpt3_medium_based_on_gpt2)). You can find info about how it has been trained on [habr](https://habr.com/ru/company/icl_services/blog/548244/) (in Russian). I have created a **simple pipeline** and **fine tuned** that model on my own **exported telegram chat** (~30mb json). It is in fact very easy to get the data from telegram and fine tune a model. Therefore, I made a **colab tutorial** for it: https://colab.research.google.com/drive/1fnAVURjyZRK9VQg1Co_-SKUQnRES8l9R?usp=sharing\n\n\u26a0\ufe0f Due to specifics of the data Hosted inference API may not work properly \u26a0\ufe0f\n\n\ud83e\udd17To try it use my [Spaces demo](https://huggingface.co/spaces/Kirili4ik/chat-with-Kirill)\ud83e\udd17\n\n\n### \u2753 How to use with code\n\n```python\n\n# Download model and tokenizer\ncheckpoint = \"Kirili4ik/ruDialoGpt3-medium-finetuned-telegram\"   \ntokenizer =  AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\nmodel.eval()\n\n\n# util function to get expected len after tokenizing\ndef get_length_param(text: str, tokenizer) -> str:\n    tokens_count = len(tokenizer.encode(text))\n    if tokens_count <= 15:\n        len_param = '1'\n    elif tokens_count <= 50:\n        len_param = '2'\n    elif tokens_count <= 256:\n        len_param = '3'\n    else:\n        len_param = '-'\n    return len_param\n\n\n# util function to get next person number (1/0) for Machine or Human in the dialogue\ndef get_user_param(text: dict, machine_name_in_chat: str) -> str:\n    if text['from'] == machine_name_in_chat:\n        return '1'  # machine\n    else:\n        return '0'  # human\n\n\nchat_history_ids = torch.zeros((1, 0), dtype=torch.int)\n\nwhile True:\n    \n    next_who = input(\"Who's phrase?\\t\")  #input(\"H / G?\")     # Human or GPT\n\n    # In case Human\n    if next_who == \"H\" or next_who == \"Human\":\n        input_user = input(\"===> Human: \")\n        \n        # encode the new user input, add parameters and return a tensor in Pytorch\n        new_user_input_ids = tokenizer.encode(f\"|0|{get_length_param(input_user, tokenizer)}|\" \\\n                                              + input_user + tokenizer.eos_token, return_tensors=\"pt\")\n        # append the new user input tokens to the chat history\n        chat_history_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n\n    if next_who == \"G\" or next_who == \"GPT\":\n\n        next_len = input(\"Phrase len? 1/2/3/-\\t\")  #input(\"Exp. len?(-/1/2/3): \")\n        # encode the new user input, add parameters and return a tensor in Pytorch\n        new_user_input_ids = tokenizer.encode(f\"|1|{next_len}|\", return_tensors=\"pt\")\n        # append the new user input tokens to the chat history\n        chat_history_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n        \n        # print(tokenizer.decode(chat_history_ids[-1])) # uncomment to see full gpt input\n        \n        # save previous len\n        input_len = chat_history_ids.shape[-1]\n        # generated a response; PS you can read about the parameters at hf.co/blog/how-to-generate\n        chat_history_ids = model.generate(\n            chat_history_ids,\n            num_return_sequences=1,                     # use for more variants, but have to print [i]\n            max_length=512,\n            no_repeat_ngram_size=3,\n            do_sample=True,\n            top_k=50,\n            top_p=0.9,\n            temperature = 0.6,                          # 0 for greedy\n            mask_token_id=tokenizer.mask_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            unk_token_id=tokenizer.unk_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n            device='cpu'\n        )\n        \n        \n        # pretty print last ouput tokens from bot\n        print(f\"===> GPT-3:  {tokenizer.decode(chat_history_ids[:, input_len:][0], skip_special_tokens=True)}\")\n```", "size_bytes": "1524268313", "downloads": 452}
{"pretrained_model_name": "anikethjr/PromoGen_K562_GPT2_4096_tokens_2080Ti_x4_more_DE", "description": "---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: PromoGen_K562_GPT2_4096_tokens_2080Ti_x4_more_DE\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# PromoGen_K562_GPT2_4096_tokens_2080Ti_x4_more_DE\n\nThis model is a fine-tuned version of [](https://huggingface.co/) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 10.7123\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 0.001\n- train_batch_size: 128\n- eval_batch_size: 128\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 1024\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 1000\n- num_epochs: 200\n\n### Training results\n\n| Training Loss | Epoch  | Step  | Validation Loss |\n|:-------------:|:------:|:-----:|:---------------:|\n| 7.8346        | 1.69   | 250   | 7.3201          |\n| 6.8483        | 3.38   | 500   | 6.6602          |\n| 6.5149        | 5.07   | 750   | 6.3857          |\n| 6.1941        | 6.76   | 1000  | 6.1764          |\n| 6.0188        | 8.45   | 1250  | 6.0308          |\n| 5.7091        | 10.14  | 1500  | 5.9934          |\n| 5.4459        | 11.82  | 1750  | 5.9371          |\n| 5.1559        | 13.51  | 2000  | 5.9561          |\n| 5.053         | 15.2   | 2250  | 6.0799          |\n| 4.8356        | 16.89  | 2500  | 5.9637          |\n| 4.6625        | 18.58  | 2750  | 6.0990          |\n| 4.4349        | 20.27  | 3000  | 6.4003          |\n| 4.2347        | 21.96  | 3250  | 6.2304          |\n| 4.0581        | 23.65  | 3500  | 6.4391          |\n| 4.1369        | 25.34  | 3750  | 6.6182          |\n| 3.9847        | 27.03  | 4000  | 6.8092          |\n| 3.7499        | 28.72  | 4250  | 6.7152          |\n| 3.6274        | 30.41  | 4500  | 6.8939          |\n| 3.515         | 32.09  | 4750  | 7.1118          |\n| 3.428         | 33.78  | 5000  | 6.8671          |\n| 3.4504        | 35.47  | 5250  | 7.0974          |\n| 3.3346        | 37.16  | 5500  | 7.4627          |\n| 3.1574        | 38.85  | 5750  | 7.1213          |\n| 3.0594        | 40.54  | 6000  | 7.3364          |\n| 3.1296        | 42.23  | 6250  | 7.5662          |\n| 3.0683        | 43.92  | 6500  | 7.3176          |\n| 2.9588        | 45.61  | 6750  | 7.4911          |\n| 2.8816        | 47.3   | 7000  | 7.7823          |\n| 2.7364        | 48.99  | 7250  | 7.4892          |\n| 2.6647        | 50.68  | 7500  | 7.7526          |\n| 2.8314        | 52.36  | 7750  | 7.9454          |\n| 2.7594        | 54.05  | 8000  | 8.2020          |\n| 2.5894        | 55.74  | 8250  | 7.8243          |\n| 2.5204        | 57.43  | 8500  | 8.0420          |\n| 2.5072        | 59.12  | 8750  | 8.2496          |\n| 2.4765        | 60.81  | 9000  | 7.9321          |\n| 2.5039        | 62.5   | 9250  | 8.1675          |\n| 2.4301        | 64.19  | 9500  | 8.4031          |\n| 2.2991        | 65.88  | 9750  | 8.0673          |\n| 2.2471        | 67.57  | 10000 | 8.2341          |\n| 2.3514        | 69.26  | 10250 | 8.5114          |\n| 2.3166        | 70.95  | 10500 | 8.1837          |\n| 2.2225        | 72.63  | 10750 | 8.3777          |\n| 2.17          | 74.32  | 11000 | 8.5542          |\n| 2.0509        | 76.01  | 11250 | 8.6312          |\n| 2.0204        | 77.7   | 11500 | 8.4680          |\n| 2.1625        | 79.39  | 11750 | 8.7197          |\n| 2.1102        | 81.08  | 12000 | 8.8730          |\n| 1.9783        | 82.77  | 12250 | 8.6112          |\n| 1.9293        | 84.46  | 12500 | 8.7798          |\n| 1.9402        | 86.15  | 12750 | 8.9528          |\n| 1.9095        | 87.84  | 13000 | 8.6741          |\n| 1.9188        | 89.53  | 13250 | 8.9483          |\n| 1.877         | 91.22  | 13500 | 9.0630          |\n| 1.7614        | 92.9   | 13750 | 8.8313          |\n| 1.7251        | 94.59  | 14000 | 8.9837          |\n| 1.821         | 96.28  | 14250 | 9.1340          |\n| 1.799         | 97.97  | 14500 | 8.9597          |\n| 1.7024        | 99.66  | 14750 | 9.1133          |\n| 1.6657        | 101.35 | 15000 | 9.2959          |\n| 1.5863        | 103.04 | 15250 | 9.2909          |\n| 1.5708        | 104.73 | 15500 | 9.1864          |\n| 1.6538        | 106.42 | 15750 | 9.3570          |\n| 1.6119        | 108.11 | 16000 | 9.4937          |\n| 1.5147        | 109.8  | 16250 | 9.3262          |\n| 1.4745        | 111.49 | 16500 | 9.4693          |\n| 1.4869        | 113.18 | 16750 | 9.5963          |\n| 1.4664        | 114.86 | 17000 | 9.4436          |\n| 1.4566        | 116.55 | 17250 | 9.5559          |\n| 1.4231        | 118.24 | 17500 | 9.6734          |\n| 1.3421        | 119.93 | 17750 | 9.5746          |\n| 1.3086        | 121.62 | 18000 | 9.7034          |\n| 1.3773        | 123.31 | 18250 | 9.7930          |\n| 1.3537        | 125.0  | 18500 | 9.6953          |\n| 1.2834        | 126.69 | 18750 | 9.8154          |\n| 1.2516        | 128.38 | 19000 | 9.8966          |\n| 1.2079        | 130.07 | 19250 | 9.9296          |\n| 1.1875        | 131.76 | 19500 | 9.9139          |\n| 1.2227        | 133.45 | 19750 | 10.0024         |\n| 1.1899        | 135.14 | 20000 | 10.0485         |\n| 1.1332        | 136.82 | 20250 | 10.0329         |\n| 1.107         | 138.51 | 20500 | 10.1044         |\n| 1.1128        | 140.2  | 20750 | 10.1534         |\n| 1.0945        | 141.89 | 21000 | 10.1375         |\n| 1.0762        | 143.58 | 21250 | 10.2039         |\n| 1.0495        | 145.27 | 21500 | 10.2378         |\n| 1.0105        | 146.96 | 21750 | 10.2377         |\n| 0.9875        | 148.65 | 22000 | 10.3128         |\n| 1.0177        | 150.34 | 22250 | 10.3502         |\n| 0.9956        | 152.03 | 22500 | 10.3521         |\n| 0.9628        | 153.72 | 22750 | 10.3924         |\n| 0.9403        | 155.41 | 23000 | 10.4238         |\n| 0.9214        | 157.09 | 23250 | 10.4514         |\n| 0.9078        | 158.78 | 23500 | 10.4660         |\n| 0.9114        | 160.47 | 23750 | 10.4969         |\n| 0.8935        | 162.16 | 24000 | 10.5225         |\n| 0.8711        | 163.85 | 24250 | 10.5399         |\n| 0.8549        | 165.54 | 24500 | 10.5598         |\n| 0.8564        | 167.23 | 24750 | 10.5896         |\n| 0.8441        | 168.92 | 25000 | 10.5900         |\n| 0.8357        | 170.61 | 25250 | 10.6085         |\n| 0.8229        | 172.3  | 25500 | 10.6296         |\n| 0.8094        | 173.99 | 25750 | 10.6426         |\n| 0.7992        | 175.68 | 26000 | 10.6518         |\n| 0.8038        | 177.36 | 26250 | 10.6652         |\n| 0.7948        | 179.05 | 26500 | 10.6742         |\n| 0.7852        | 180.74 | 26750 | 10.6780         |\n| 0.7775        | 182.43 | 27000 | 10.6912         |\n| 0.7737        | 184.12 | 27250 | 10.6925         |\n| 0.769         | 185.81 | 27500 | 10.6993         |\n| 0.7679        | 187.5  | 27750 | 10.7020         |\n| 0.7636        | 189.19 | 28000 | 10.7062         |\n| 0.76          | 190.88 | 28250 | 10.7085         |\n| 0.7595        | 192.57 | 28500 | 10.7107         |\n| 0.7583        | 194.26 | 28750 | 10.7120         |\n| 0.7552        | 195.95 | 29000 | 10.7126         |\n| 0.7568        | 197.63 | 29250 | 10.7120         |\n| 0.7556        | 199.32 | 29500 | 10.7123         |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.13.0\n- Datasets 2.7.0\n- Tokenizers 0.13.0.dev0\n", "size_bytes": "362299965", "downloads": 0}
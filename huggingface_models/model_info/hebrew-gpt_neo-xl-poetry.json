{"pretrained_model_name": "Norod78/hebrew-gpt_neo-xl-poetry", "description": "---\nlanguage: he\n\nthumbnail: https://avatars1.githubusercontent.com/u/3617152?norod.jpg\nwidget:\n- text: \"\u05e2\u05d5\u05d3 \u05d1\u05d9\u05de\u05d9 \u05e7\u05d3\u05dd\"\n- text: \"\u05ea\u05e8\u05d9\u05e1\u05e8 \u05de\u05db\u05e9\u05e4\u05d5\u05ea \u05e1\u05d2\"\n- text:  \"\\n\\n\u05d4\u05d0\u05d9\u05e9 \u05d4\u05d0\u05d7\u05e8\u05d5\u05df \u05d1\u05e2\u05d5\u05dc\u05dd /\"\n- text: \"\u05e4\u05e2\u05dd \u05d0\u05d7\u05ea, \u05dc\u05e4\u05e0\u05d9 \u05e9\u05e0\u05d9\u05dd \u05e8\u05d1\u05d5\u05ea\"\n- text: \"\u05d4\u05e8\u05de\u05d9\u05d5\u05e0\u05d9 \u05d4\u05e1\u05ea\u05d9\u05e8\u05d4 \u05d0\u05ea\"\n- text: \"\u05dc\u05e4\u05ea\u05e2, \u05d0\u05d5\u05e8 \u05d9\u05e8\u05d5\u05e7\"\n\nlicense: mit\n---\n\n# hebrew-gpt_neo-xl-poetry\n\nHebrew poetry text generation model which was fine tuned upon on [hebrew-gpt_neo-xl](https://huggingface.co/Norod78/hebrew-gpt_neo-xl).\n## Datasets\n\nAn assortment of various Hebrew books, magazines and poetry corpuses\n\n## Training Config\n\nSimilar to [this one](https://github.com/Norod/hebrew-gpt_neo/tree/main/hebrew-gpt_neo-xl/configs) <BR>\n\n## Usage\n\n### Google Colab Notebook\n\nAvailable [here ](https://colab.research.google.com/github/Norod/hebrew-gpt_neo/blob/main/hebrew-gpt_neo-xl/Norod78_hebrew_gpt_neo_xl_Colab.ipynb) <BR>\n\n\n#### Simple usage sample code\n\n```python\n\n!pip install tokenizers==0.10.3 transformers==4.8.0\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n  \ntokenizer = AutoTokenizer.from_pretrained(\"Norod78/hebrew-gpt_neo-xl-poetry\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Norod78/hebrew-gpt_neo-xl-poetry\", pad_token_id=tokenizer.eos_token_id)\n\nprompt_text = \"\u05d0\u05e0\u05d9 \u05d0\u05d5\u05d4\u05d1 \u05e9\u05d5\u05e7\u05d5\u05dc\u05d3 \u05d5\u05e2\u05d5\u05d2\u05d5\u05ea\"\nmax_len = 512\nsample_output_num = 3\nseed = 1000\n\nimport numpy as np\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = 0 if torch.cuda.is_available()==False else torch.cuda.device_count()\n\nprint(f\"device: {device}, n_gpu: {n_gpu}\")\n\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif n_gpu > 0:\n    torch.cuda.manual_seed_all(seed)\n\nmodel.to(device)\n\nencoded_prompt = tokenizer.encode(\n    prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n\nencoded_prompt = encoded_prompt.to(device)\n\nif encoded_prompt.size()[-1] == 0:\n        input_ids = None\nelse:\n        input_ids = encoded_prompt\n\nprint(\"input_ids = \" + str(input_ids))\n\nif input_ids != None:\n  max_len += len(encoded_prompt[0])\n  if max_len > 2048:\n    max_len = 2048\n\nprint(\"Updated max_len = \" + str(max_len))\n\nstop_token = \"<|endoftext|>\"\nnew_lines = \"\\n\\n\\n\"\n\nsample_outputs = model.generate(\n    input_ids,\n    do_sample=True, \n    max_length=max_len, \n    top_k=50, \n    top_p=0.95, \n    num_return_sequences=sample_output_num\n)\n\nprint(100 * '-' + \"\\n\\t\\tOutput\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n\n  text = tokenizer.decode(sample_output, skip_special_tokens=True)\n  \n  # Remove all text after the stop token\n  text = text[: text.find(stop_token) if stop_token else None]\n\n  # Remove all text after 3 newlines\n  text = text[: text.find(new_lines) if new_lines else None]\n\n  print(\"\\n{}: {}\".format(i, text))\n  print(\"\\n\" + 100 * '-')\n\n```\n", "size_bytes": "5312753947", "downloads": 13}
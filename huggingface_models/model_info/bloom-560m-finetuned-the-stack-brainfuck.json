{"pretrained_model_name": "mrm8488/bloom-560m-finetuned-the-stack-brainfuck", "description": "---\nlicense: bigscience-bloom-rail-1.0\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bloom-560m-finetuned-the-stack-brainfuck\n  results: []\n\nwidget:\n- text: \"\n>+++++++++[<++++++++>-]<.>+++++++[<++++>-]<+.+++++++..+++.[-]>++++++++[<++++>-]\n<.>+++++++++++[<++++++++>-]<-.--------.+++.-[.>>+>+<<<-]>>>\"\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# bloom-560m-finetuned-the-stack-brainfuck\n\nThis model is a fine-tuned version of [bigscience/bloom-560m](https://huggingface.co/bigscience/bloom-560m) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.2112\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 1\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 4\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.3877        | 0.1   | 200  | 1.8023          |\n| 1.2576        | 0.19  | 400  | 1.7121          |\n| 1.1452        | 0.29  | 600  | 1.5469          |\n| 1.1503        | 0.39  | 800  | 1.6261          |\n| 1.0287        | 0.48  | 1000 | 1.5097          |\n| 1.0118        | 0.58  | 1200 | 1.4398          |\n| 1.0466        | 0.67  | 1400 | 1.4267          |\n| 0.9531        | 0.77  | 1600 | 1.4130          |\n| 0.8891        | 0.87  | 1800 | 1.4026          |\n| 0.9163        | 0.96  | 2000 | 1.4003          |\n| 0.7207        | 1.06  | 2200 | 1.3758          |\n| 0.7041        | 1.16  | 2400 | 1.3364          |\n| 0.7065        | 1.25  | 2600 | 1.3095          |\n| 0.7316        | 1.35  | 2800 | 1.2959          |\n| 0.6817        | 1.45  | 3000 | 1.2682          |\n| 0.6926        | 1.54  | 3200 | 1.2567          |\n| 0.6511        | 1.64  | 3400 | 1.2416          |\n| 0.6819        | 1.73  | 3600 | 1.2263          |\n| 0.6422        | 1.83  | 3800 | 1.2206          |\n| 0.6392        | 1.93  | 4000 | 1.2112          |\n\n\n### Framework versions\n\n- Transformers 4.24.0\n- Pytorch 1.13.0+cu117\n- Datasets 2.5.1\n- Tokenizers 0.13.0\n", "size_bytes": "2236957537", "downloads": 2}
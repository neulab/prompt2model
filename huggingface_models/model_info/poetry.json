{"pretrained_model_name": "supermy/poetry", "description": "---\nlanguage: zh\ndatasets: poetry\ninference:\n  parameters:\n    max_length: 108\n    num_return_sequences: 1\n    do_sample: True\nwidget: \n- text: \"\u7269\u6362 \u661f\u79fb \u51e0\u5ea6 \u79cb\"\n  example_title: \"\u6ed5\u738b\u96011\"\n- text: \"\u79cb\u6c34 \u5171 \u957f\u5929 \u4e00\u8272\"\n  example_title: \"\u6ed5\u738b\u9601 2\"\n- text: \"\u840d\u6c34 \u76f8\u9022\uff0c\u5c3d\u662f \u4ed6\u4e61 \u4e4b \u5ba2\u3002\"\n  example_title: \"\u6ed5\u738b\u9601 3\"\n\n---\n\n\n# \u53e4\u8bd7\u8bcd\n\n## Model description\n\n  \u53e4\u8bd7\u8bcdAI\u751f\u6210\n\n## How to use\n\u4f7f\u7528 pipeline \u8c03\u7528\u6a21\u578b:\n\n```python\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, TextGenerationPipeline\nmodel_checkpoint = \"supermy/poetry\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = GPT2LMHeadModel.from_pretrained(model_checkpoint)\ntext_generator = TextGenerationPipeline(model, tokenizer)\ntext_generator.model.config.pad_token_id = text_generator.model.config.eos_token_id\n\nprint(text_generator(\"\u4e3e\u5934 \u671b \u660e\u6708\uff0c\", max_length=100, do_sample=True))\nprint(text_generator(\"\u7269\u6362 \u661f\u79fb \u51e0\u5ea6 \u79cb\uff0c\", max_length=100, do_sample=True))\n\n>>> print(text_generator(\"\u4e3e\u5934 \u671b \u660e\u6708\uff0c\", max_length=100, do_sample=True))\n[{'generated_text': '\u4e3e\u5934 \u671b \u660e\u6708\uff0c \u4f55\u4ee5 \u55bb \u65e0\u8a00 \u3002 \u987e\u5f71 \u82e5\u4e3a \u821e \uff0c \u5578 \u98ce\u6e05 \u72ec \u4f24 \u3002 \u56db\u65f6 \u522b\u6709 \u610f \uff0c \u5343\u53e4 \u5f97 \u4ece\u5bb9 \u3002 \u8d4f\u97f3 \u6211\u975e \u6b64 \uff0c \u4f55\u5982 \u9e25\u9e6d \u7fa4 \u3002 \u5d0e \u5c71\u6709 \u4f73\u8272 \uff0c \u843d\u843d \u6837 \u76f8\u5b9c \u3002 \u4e0d\u5acc \u96ea\u971c \u6e29 \uff0c \u5b81 \u53d7 \u56db\u65f6 \u80a5 \u3002 \u8001 \u6001 \u5982 \u5077 \u9762 \uff0c \u51ac \u5fc3 \u4f3c \u76f8\u77e5 \u3002 \u6625\u98ce \u4e0d\u53ef \u6043 \uff0c \u89e6 \u52a8 \u6625 \u4f55\u4e3a \u3002 \u5c81\u665a \u5ffd\u7136 \u8001 \uff0c \u82b1\u524d \u5c81\u6708\u6df1 \u3002 \u53ef\u7b11 \u4e00\u573a \u68a6 \uff0c \u5a75\u5a1f \u4e4d \u81ea \u5fc3 \u3002 \u5217 \u540d \u591a \u5c81\u6708 \uff0c \u68ee \u5217 \u5c3d \u6797\u5ce6 \u3002 \u8bd5\u95ee \u5f71 \u975e \u7b11'}]\n>>> print(text_generator(\"\u7269\u6362 \u661f\u79fb \u51e0\u5ea6 \u79cb\uff0c\", max_length=100, do_sample=True))\n[{'generated_text': '\u7269\u6362 \u661f\u79fb \u51e0\u5ea6 \u79cb\uff0c \u6d88\u957f \u968f\u65f6 \u5411 \u4e00\u4e18 \u3002 \u6e14\u8005 \u4e0b \u9022 \u52fe\u6f0f \u4ee4 \uff0c \u6f0f\u58f0 \u9ad8\u51fa \u666f\u9633 \u4e18 \u3002 \u5929\u6d25 \u5927\u5c39 \u6614 \u4ece\u6e38 \uff0c \u5927\u5c39 \u6765\u65f6 \u6625\u590d \u79cb \u3002 \u65d7\u9f13 \u65e5 \u4e25 \u5ba3 \u4f7f \u4ece \uff0c \u8054\u9573 \u6b4c\u7b11 \u53c8 \u98ce\u6d41 \u3002 \u5188\u5ce6 \u6bd4 \u5e76 \u7476 \u6eaa \u6c34 \uff0c \u53e0\u5d82 \u9ad8 \u76d8 \u9efc\u9efb \u6d32 \u3002 \u82b1\u6728 \u82b3\u83f2 \u4e09\u6708 \u5929 \uff0c \u83ba\u82b1 \u6696 \u7fe0 \u51e0 \u6d41\u5e74 \u3002 \u4e00\u4ece \u522b\u540e \u591a \u643a\u624b \uff0c \u80a0\u65ad \u9152\u9611 \u6000 \u51db\u7136 \u3002 \u5317\u9619 \u4eba\u79f0 \u4f3c\u68a6\u4e2d \uff0c \u897f\u5c71 \u522b\u6837 \u68a6\u9b42 \u9999 \u3002 \u591a\u541b \u89c2\u56fd \u4eb2 \u572d\u74a7 \uff0c \u80fd \u9884 \u9647\u897f \u79f0 \u5de8 \u826f \u3002 \u5237\u7fbd \u5237\u7fbd'}]\n\n```\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"supermy/poetry\")\nmodel = AutoModelForCausalLM.from_pretrained(\"supermy/poetry\")\n```\n\n\n\n## Training data\n\n\n\u975e\u5e38\u5168\u7684\u53e4\u8bd7\u8bcd\u6570\u636e\uff0c\u6536\u5f55\u4e86\u4ece\u5148\u79e6\u5230\u73b0\u4ee3\u7684\u5171\u8ba185\u4e07\u4f59\u9996\u53e4\u8bd7\u8bcd\u3002\n\n## \u7edf\u8ba1\u4fe1\u606f\n\n| \u671d\u4ee3                   | \u8bd7\u8bcd\u6570  | \u4f5c\u8005\u6570  |\n|-----------------------|--------|--------|\n| \u5b8b                    | 287114 |   9446 |\n| \u660e                    | 236957 |   4439 |\n| \u6e05                    |  90089 |   8872 |\n| \u5510                    |  49195 |   2736 |\n| \u5143                    |  37375 |   1209 |\n| \u8fd1\u73b0\u4ee3                |  28419 |    790 |\n| \u5f53\u4ee3                  |  28219 |    177 |\n| \u660e\u672b\u6e05\u521d               |  17700 |    176 |\n| \u5143\u672b\u660e\u521d               |  15736 |     79 |\n| \u6e05\u672b\u6c11\u56fd\u521d             |  15367 |     99 |\n| \u6e05\u672b\u8fd1\u73b0\u4ee3\u521d           |  12464 |     48 |\n| \u5b8b\u672b\u5143\u521d              |  12058 |     41 |\n| \u5357\u5317\u671d                |   4586 |    434 |\n| \u8fd1\u73b0\u4ee3\u672b\u5f53\u4ee3\u521d         |   3426 |     23 |\n| \u9b4f\u664b                  |   3020 |    251 |\n| \u91d1\u672b\u5143\u521d              |   3019 |     17 |\n| \u91d1                    |   2741 |    253 |\n| \u6c11\u56fd\u672b\u5f53\u4ee3\u521d           |   1948 |      9 |\n| \u968b                    |   1170 |     84 |\n| \u5510\u672b\u5b8b\u521d              |   1118 |     44 |\n| \u5148\u79e6                  |    570 |      8 |\n| \u968b\u672b\u5510\u521d              |    472 |     40 |\n| \u6c49                    |    363 |     83 |\n| \u5b8b\u672b\u91d1\u521d              |    234 |      9 |\n| \u8fbd                    |     22 |      7 |\n| \u79e6                    |      2 |      2 |\n| \u9b4f\u664b\u672b\u5357\u5317\u671d\u521d          |      1 |      1 |\n| \u603b\u548c                  | 853385 |  29377 |\n\n```\n```\n\n## Training procedure\n\n\u6a21\u578b\uff1a[GPT2](https://huggingface.co/gpt2) \n\u8bad\u7ec3\u73af\u5883\uff1a\u82f1\u4f1f\u8fbe16G\u663e\u5361\n\nbpe\u5206\u8bcd\uff1a\"vocab_size\"=50000\n```\n\n***** Running training *****\n  Num examples = 16431\n  Num Epochs = 680\n  Instantaneous batch size per device = 24\n  Total train batch size (w. parallel, distributed & accumulation) = 192\n  Gradient Accumulation steps = 8\n  Total optimization steps = 57800\n  Number of trainable parameters = 124242432\nGPT-2 size: 124.2M parameters\n  0%|          | 0/57800 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n    9%|\u258a         | 5000/57800 [6:58:57<72:53:18,  4.97s/it]***** Running Evaluation *****\n  Num examples = 1755\n  Batch size = 24\n{'loss': 3.1345, 'learning_rate': 0.0004939065828881268, 'epoch': 58.82}\n  9%|\u258a         | 5000/57800 [6:59:14<72:53:18, Saving model checkpoint to poetry-trainer/checkpoint-5000\nConfiguration saved in poetry-trainer/checkpoint-5000/config.json\nModel weights saved in poetry-trainer/checkpoint-5000/pytorch_model.bin\ntokenizer config file saved in poetry-trainer/checkpoint-5000/tokenizer_config.json\nSpecial tokens file saved in poetry-trainer/checkpoint-5000/special_tokens_map.json\n 17%|\u2588\u258b        | 10000/57800 [13:55:32<65:40:41,  4.95s/it]***** Running Evaluation *****\n  Num examples = 1755\n  Batch size = 24\n{'eval_loss': 11.14090633392334, 'eval_runtime': 16.8326, 'eval_samples_per_second': 104.262, 'eval_steps_per_second': 4.396, 'epoch': 58.82}\n{'loss': 0.2511, 'learning_rate': 0.00046966687938531824, 'epoch': 117.64}\n 17%|\u2588\u258b        | 10000/57800 [13:55:48<65:40:41Saving model checkpoint to poetry-trainer/checkpoint-10000\n..........\n 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 55000/57800 [76:06:46<3:59:33,  5.13s/it]***** Running Evaluation *****\n  Num examples = 1755\n  Batch size = 24\n{'eval_loss': 14.860174179077148, 'eval_runtime': 16.7826, 'eval_samples_per_second': 104.572, 'eval_steps_per_second': 4.409, 'epoch': 588.23}\n{'loss': 0.0083, 'learning_rate': 3.0262183266589473e-06, 'epoch': 647.06}\n 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 55000/57800 [76:07:03<3:59:33,Saving model checkpoint to poetry-trainer/checkpoint-55000\n\n{'eval_loss': 14.830656051635742, 'eval_runtime': 16.7365, 'eval_samples_per_second': 104.86, 'eval_steps_per_second': 4.421, 'epoch': 647.06}\n{'train_runtime': 287920.5857, 'train_samples_per_second': 38.806, 'train_steps_per_second': 0.201, 'train_loss': 0.33751299874592816, 'epoch': 679.99}\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57800/57800 [79:58:40<00:00,  4.93s/it]  \n```\n\n\n```\n###  entry and citation info\n```\n\n```", "size_bytes": "509604585", "downloads": 25}
{"pretrained_model_name": "NYTK/PULI-GPT-2", "description": "---\n\nlanguage: \n  - hu\ntags:\n- text-generation\nlicense: cc-by-nc-4.0\nwidget:\n- text: \"Elmes\u00e9lek egy t\u00f6rt\u00e9netet a nyelvtechnol\u00f3gi\u00e1r\u00f3l.\"\n---\n\n# PULI GPT-2\n\nFor further details, see [our demo site](https://juniper.nytud.hu/demo/gpt2).\n\n  - Hungarian GPT-2 model\n  - Trained with Megatron-DeepSpeed [github](https://github.com/microsoft/Megatron-DeepSpeed)\n  - Dataset: 36.3 billion words\n  - Checkpoint: 500 000 steps\n\n## Limitations\n\n- max_seq_length = 1024\n\n\n## Citation\nIf you use this model, please cite the following paper:\n\n```\n@inproceedings {yang-puli,\n    title = {J\u00f6nnek a nagyok! BERT-Large, GPT-2 \u00e9s GPT-3 nyelvmodellek magyar nyelvre},\n\tbooktitle = {XIX. Magyar Sz\u00e1m\u00edt\u00f3g\u00e9pes Nyelv\u00e9szeti Konferencia (MSZNY 2023)},\n\tyear = {2023},\n\tpublisher = {Szegedi Tudom\u00e1nyegyetem, Informatikai Int\u00e9zet},\n\taddress = {Szeged, Hungary},\n\tauthor = {Yang, Zijian Gy\u0151z\u0151 and Dod\u00e9, R\u00e9ka and Ferenczi, Gerg\u0151 and H\u00e9ja, Enik\u0151 and Jelencsik-M\u00e1tyus, Kinga and K\u0151r\u00f6s, \u00c1d\u00e1m and Laki, L\u00e1szl\u00f3 J\u00e1nos and Ligeti-Nagy, No\u00e9mi and Vad\u00e1sz, No\u00e9mi and V\u00e1radi, Tam\u00e1s},\n\tpages = {247--262}\n}\n```\n\n## Usage\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\n\ntokenizer = GPT2Tokenizer.from_pretrained('NYTK/PULI-GPT-2')\nmodel = GPT2Model.from_pretrained('NYTK/PULI-GPT-2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n\n```\n## Usage with pipeline\n\n```python\nfrom transformers import pipeline\n\nprompt = \"Elmes\u00e9lek egy t\u00f6rt\u00e9netet a nyelvtechnol\u00f3gi\u00e1r\u00f3l.\"\ngenerator = pipeline(task=\"text-generation\", model=\"NYTK/PULI-GPT-2\")\n\nprint(generator(prompt)[0][\"generated_text\"])\n```", "size_bytes": "1468871833", "downloads": 1454}
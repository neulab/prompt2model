{"pretrained_model_name": "anugrahap/gpt2-indo-textgen", "description": "---\nlicense: apache-2.0\ndatasets:\n- indonlu\nlanguage:\n- id\nmetrics:\n- bleu\npipeline_tag: text-generation\n---\n_Copyright 2023 Anugrah Akbar Praramadhan. All rights reserved._\n\n_Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at_\n\n_[http://www.apache.org/licenses/LICENSE-2.0)](http://www.apache.org/licenses/LICENSE-2.0)_\n\n_Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License._\n\n## Model Description\nA GPT-2 *(Generative Pretrained Transformer-2)* model is a transformer based architecture for Causal Language Modeling, meaning it's required a left token/word as an input prompt \nfor generating the right/next token, developed by Open AI *{Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}*. \nSee the paper here:\n[https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n\n\n## Limitation\nSince GPT-2 is an unsupervised model and trained using an unlabelled of text sequences without any explicit supervision, \nthe clarity and output of this model often comes with randomness. To overcome this issue we have to create a specific seed for determined output.\nSupported language for this model is only English *(get from GPT-2 pretrained model)* and Indonesian *(fine tune using Indonesian Wikipedia Dataset)*.\n\n\n## How To Use\n\nDirect use of using Pytorch:\n```python\n>>> from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, set_seed\n\n>>> model_name = 'anugrahap/gpt2-indo-textgen'\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n>>> model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n>>> generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n\n>>> #set_seed(1)\n>>> result = generator(\"Skripsi merupakan tugas akhir mahasiswa\", min_length=10, max_length=30, num_return_sequences=1)\n>>> result[0][\"generated_text\"]\n```\n\n\n### Learn more\n\n| [GPT-2 Pretrained Model Medium-345M Parameters](https://github.com/openai/gpt-2/blob/master/download_model.py)<br>\n| [Indonesian Wikipedia Dataset - 433MB by IndoNLP](https://drive.google.com/file/d/1ZoKd31yr3soveU0O38XEIFUBKx-D66t5/view?usp=sharing)<br>\n| [Project Repository](https://huggingface.co/spaces/anugrahap/gpt2-indo-text-gen/tree/main)", "size_bytes": "1444576537", "downloads": 2}
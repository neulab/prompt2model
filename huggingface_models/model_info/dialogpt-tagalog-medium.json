{"pretrained_model_name": "gabtan99/dialogpt-tagalog-medium", "description": "---\ntags:\n- conversational\n- tagalog\n- filipino\n\nlanguage:\n- tl \n\ninference: false\n\ndatasets:\n- gabtan99/pex-conversations\n---\n\n# Tagalog DialoGPT\nA DialoGPT-medium model fine-tuned on Tagalog conversational data scraped from the web. This model is an output of a research on RoBERTa-based data augmentation for low resource languages. This is the baseline model which did not use any synthetic data in training. \n\n#  Latest release: July 25, 2021\n* The model is currently only able to respond based on the history of 3 previous utterances before being limited. This is a result of the scarce amount of Tagalog conversations in our dataset. \n\n# Dataset\n[PEx Conversations Dataset](https://huggingface.co/datasets/gabtan99/pex-conversations)\n\n# Usage\nHere is an example of using beam search for model inference.\n```\nfor step in range(2): \n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # we limit the generation to 512 tokens, each utterance in training had a maximum of 128 tokens\n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=512,\n        pad_token_id=tokenizer.eos_token_id,\n        num_beams=5, \n        no_repeat_ngram_size=3\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n```\n\n# Training Script\n[Fine-tuning script adapted from Spanish DialoGPT](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)\n\n# Research by\n* [tyadrianpaule](https://huggingface.co/tyadrianpaule)\n* [schuylerng](https://huggingface.co/schuylerng)\n* [dcl127](https://huggingface.co/dcl127)", "size_bytes": "1444581337", "downloads": 12}
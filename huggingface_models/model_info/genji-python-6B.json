{"pretrained_model_name": "NovelAI/genji-python-6B", "description": "---\nlanguage:\n- en\ntags:\n- pytorch\n- causal-lm\nlicense: apache-2.0\ndatasets:\n- the Pile\n---\n\n# Genji-python 6B\n\nFor example usage or to easily use the model you can check our colab notebook:\n[Notebook](https://colab.research.google.com/drive/1PnWpx02IEUkY8jhLKd_NewUGEXahAska?usp=sharing)\n\n## Model Description\n\nGenji is a transformer model finetuned on EleutherAI's GPT-J 6B model. This particular model is trained on python only code approaching 4GB in size.\n\n| Hyperparameter    | Value  | \n|-------------------|--------|\n| n_parameters      | 6,053,381,344 |\n| n_layers          | 28*    |\n| d_model           | 4,096  |\n| d_ff              | 16,384 |\n| n_heads           | 16     |\n| d_head            | 256    |\n| n_ctx             | 2,048  |\n| n_vocab           | 50,400 (same tokenizer as GPT-2/3)  |\n| position encoding | [Rotary position encodings (RoPE)](https://arxiv.org/abs/2104.09864) |\n| RoPE dimensions   | [64](https://github.com/kingoflolz/mesh-transformer-jax/blob/f2aa66e0925de6593dcbb70e72399b97b4130482/mesh_transformer/layers.py#L223) |\n\n`*` each layer consists of one feedforward block and one self attention block\n\nThe model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model\ndimension is split into 16 heads, each with a dimension of 256. Rotary position encodings (RoPE) was applied to 64\ndimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as\nGPT-2/GPT-3.\n\n## Training data\n\nGPT-J 6B was pretrained on the [Pile](pile.eleuther.ai), a large scale curated dataset created by EleutherAI for the purpose of training this model. After the pre-training, it's finetuned on the python code that was taken from the Pile.\n\n## Training procedure\n\nGenji-python-6B is trained for 20k steps on around 655 million tokens with learning rate of 2e-06\n\n## Intended Use\n\nThis model is trained for assistence on writing python code and having fun trying weird stuff with it. \n\n### How to use\n\nThis model is only usable with our fork because GPT-J is not merged to the main transformers repo yet. When it's merged, we will make this model easily loadable.\nFor now, you need to use this fork:\n[Fork](https://github.com/finetuneanon/transformers)\n\nto install with pip:\n```bash\npip install git+https://github.com/finetuneanon/transformers@gpt-neo-localattention3-rp-b\n```\n\nThis model takes more than 16 gigs of RAM to load. If you want more efficient and faster loading, please check our split model.\nWe recommend the usage of the model as FP16. That way, it fits in 16GB VRAM cards.\n\nHow to use:\n```python\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    GPTNeoForCausalLM,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"NovelAI/genji-python-6B\", use_auth_token=True).half().eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n\ntext = '''def print_customer_name'''\n\ntokens = tokenizer(text, return_tensors=\"pt\").input_ids\ngenerated_tokens = model.generate(tokens.long().cuda(), use_cache=True, do_sample=True, top_k=50, temperature=0.3, top_p=0.9, repetition_penalty=1.125, min_length=1, max_length=len(tokens[0]) + 400, pad_token_id=tokenizer.eos_token_id)\nlast_tokens = generated_tokens[0][len(tokens[0]):]\ngenerated_text = tokenizer.decode(last_tokens)\nprint(\"Generation:\\n\" + generated_text)\n```\nWhen ran, this code generates:\n```python\nPrompt:\ndef print_customer_name\nGeneration:\n(self, customer):\n        \"\"\"Print the name of a customer.\"\"\"\n        if not self.is_valid():\n            return\n\n        print(\"Customer: {}\".format(customer))\n```\n\nFor example usage, you can see our colab notebook as well:\n[Notebook](https://colab.research.google.com/drive/1PnWpx02IEUkY8jhLKd_NewUGEXahAska?usp=sharing)\n\n## Eval results\n\nTBD\n\n## Acknowledgements\n\nThis project was possible because of the compute provided by the\n[TPU Research Cloud](https://sites.research.google/trc/)\n\nand [EleutherAI](https://eleuther.ai/) for pretraining of the GPT-J 6B.\n\nThanks to everyone who contributed to this project!\n\n- [Aero](https://github.com/AeroScripts)\n- [Finetune](https://github.com/finetuneanon)\n- [Kurumuz](https://github.com/kurumuz)", "size_bytes": "12571637995", "downloads": 67}
{"pretrained_model_name": "huggingtweets/digital_languor", "description": "---\nlanguage: en\nthumbnail: https://github.com/borisdayma/huggingtweets/blob/master/img/logo.png?raw=true\ntags:\n- huggingtweets\nwidget:\n- text: \"My dream is\"\n---\n\n<div>\n<div style=\"width: 132px; height:132px; border-radius: 50%; background-size: cover; background-image: url('https://pbs.twimg.com/profile_images/1364103848957120513/Ww-W98d1_400x400.jpg')\">\n</div>\n<div style=\"margin-top: 8px; font-size: 19px; font-weight: 800\">\ud83d\udcbe \ud83e\udd16 AI Bot </div>\n<div style=\"font-size: 15px\">@digital_languor bot</div>\n</div>\n\nI was made with [huggingtweets](https://github.com/borisdayma/huggingtweets).\n\nCreate your own bot based on your favorite user with [the demo](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb)!\n\n## How does it work?\n\nThe model uses the following pipeline.\n\n![pipeline](https://github.com/borisdayma/huggingtweets/blob/master/img/pipeline.png?raw=true)\n\nTo understand how the model was developed, check the [W&B report](https://wandb.ai/wandb/huggingtweets/reports/HuggingTweets-Train-a-Model-to-Generate-Tweets--VmlldzoxMTY5MjI).\n\n## Training data\n\nThe model was trained on [@digital_languor's tweets](https://twitter.com/digital_languor).\n\n| Data | Quantity |\n| --- | --- |\n| Tweets downloaded | 3200 |\n| Retweets | 1037 |\n| Short tweets | 589 |\n| Tweets kept | 1574 |\n\n[Explore the data](https://wandb.ai/wandb/huggingtweets/runs/1z1me0hi/artifacts), which is tracked with [W&B artifacts](https://docs.wandb.com/artifacts) at every step of the pipeline.\n\n## Training procedure\n\nThe model is based on a pre-trained [GPT-2](https://huggingface.co/gpt2) which is fine-tuned on @digital_languor's tweets.\n\nHyperparameters and metrics are recorded in the [W&B training run](https://wandb.ai/wandb/huggingtweets/runs/uffw47ml) for full transparency and reproducibility.\n\nAt the end of training, [the final model](https://wandb.ai/wandb/huggingtweets/runs/uffw47ml/artifacts) is logged and versioned.\n\n## How to use\n\nYou can use this model directly with a pipeline for text generation:\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline('text-generation',\n                     model='huggingtweets/digital_languor')\ngenerator(\"My dream is\", num_return_sequences=5)\n```\n\n## Limitations and bias\n\nThe model suffers from [the same limitations and bias as GPT-2](https://huggingface.co/gpt2#limitations-and-bias).\n\nIn addition, the data present in the user's tweets further affects the text generated by the model.\n\n## About\n\n*Built by Boris Dayma*\n\n[![Follow](https://img.shields.io/twitter/follow/borisdayma?style=social)](https://twitter.com/intent/follow?screen_name=borisdayma)\n\nFor more details, visit the project repository.\n\n[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n", "size_bytes": "510408315", "downloads": 0}
{"pretrained_model_name": "UmUDev/DialoGPT-medium-Alex", "description": "---\ntags:\n- conversational\n---\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n  \ntokenizer = AutoTokenizer.from_pretrained(\"UmUDev/DialoGPT-medium-Alex\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"UmUDev/DialoGPT-medium-Alex\")\n\n\nprint(\"====chatting 5 times with nucleus & top-k sampling & tweaking temperature & multiple sentences====\")\n# chatting 5 times with nucleus & top-k sampling & tweaking temperature & multiple\n# sentences\nfor step in range(20):\n    # take user input\n    text = input(\">> You:\")\n    # encode the input and add end of string token\n    input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors=\"pt\")\n    # concatenate new user input with chat history (if there is)\n    bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1) if step > 0 else input_ids\n    # generate a bot response\n    chat_history_ids_list = model.generate(\n        bot_input_ids,\n        max_length=1000,\n        do_sample=True,\n        top_p=0.95,\n        top_k=50,\n        temperature=0.75,\n        num_return_sequences=5,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    #print the outputs\n    for i in range(len(chat_history_ids_list)):\n      output = tokenizer.decode(chat_history_ids_list[i][bot_input_ids.shape[-1]:], skip_special_tokens=True)\n      print(f\"DialoGPT {i}: {output}\")\n    choice_index = int(input(\"Choose the response you want for the next input: \"))\n    chat_history_ids = torch.unsqueeze(chat_history_ids_list[choice_index], dim=0)\n```", "size_bytes": "1444564637", "downloads": 0}
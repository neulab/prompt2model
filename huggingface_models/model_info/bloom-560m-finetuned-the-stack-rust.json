{"pretrained_model_name": "mrm8488/bloom-560m-finetuned-the-stack-rust", "description": "---\nlanguage: \n  - code\ntags:\n- code\n- rust\n- programming\n---\n\n# BLOOM (560M ckpt) fine-tuned on The Stack RUST code\n\n- Latest ckpt: https://huggingface.co/mrm8488/bloom-560m-finetuned-the-stack-rust/tree/100k\n\n## Model \ud83e\udde0\n[BigScience Large Open-science Open-access Multilingual Language Model](https://huggingface.co/bigscience/bloom-560m#model-details) \ud83c\udf38\n\nBLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.\n\n## Dataset \ud83d\udcda\n\n**Rust** \ud83e\udd80 part of The [Stack](https://huggingface.co/datasets/bigcode/the-stack).\n\nThe Stack contains over 6TB of permissively-licensed source code files covering 358 programming languages. The dataset was created as part of the BigCode Project, an open scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs). The Stack serves as a pre-training dataset for Code LLMs, i.e., code-generating AI systems which enable the synthesis of programs from natural language descriptions as well as other from code snippets.\n\n## Example of usage \ud83d\udc69\u200d\ud83d\udcbb\n\n```py\nimport torch\nfrom transformers import BloomTokenizerFast, BloomForCausalLM\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nckpt = 'mrm8488/bloom-560m-finetuned-the-stack-rust'\nrevision = '100k' # latest one at the moment \n\ntokenizer = BloomTokenizerFast.from_pretrained(ckpt)\nmodel = BloomForCausalLM.from_pretrained(ckpt, revision=revision).to(device)\n\ndef complete_code(text):\n    inputs = tokenizer(text, return_tensors='pt')\n    input_ids = inputs.input_ids.to(device)\n    attention_mask = inputs.attention_mask.to(device)\n    output = model.generate(input_ids, attention_mask=attention_mask, max_length=2048, eos_token_id=tokenizer.eos_token_id)\n\n    return tokenizer.decode(output[0], skip_special_tokens=False)\n    \ncode_prompt = \"\"\"\nuse fastly::{Error, Request, Response};\nuse serde_json::{json, Value};\n\n#[fastly::main]\nfn main(req: Request) -> Result<Response, Error> {\n  let mut response = req.send(\"origin_0\")?;\n\"\"\"\n\ncomplete_code(code_prompt)\n```\n\n## Citation \u2712\ufe0f\n```\n@misc {manuel_romero_2022,\n\tauthor       = { {Manuel Romero} },\n\ttitle        = { bloom-560m-finetuned-the-stack-rust (Revision 5358462) },\n\tyear         = 2022,\n\turl          = { https://huggingface.co/mrm8488/bloom-560m-finetuned-the-stack-rust },\n\tdoi          = { 10.57967/hf/0236 },\n\tpublisher    = { Hugging Face }\n}\n```\n\n\n\n\n\n\n", "size_bytes": "2236957537", "downloads": 33}
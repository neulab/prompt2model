{"pretrained_model_name": "Galuh/id-journal-gpt2", "description": "---\nlanguage: id\nwidget:\n- text: \"Penelitian ini bertujuan untuk menentukan identitas invertebrata laut dari Perairan Papua dengan teknik DNA barcoding\"\n---\n\n# Indonesian GPT-2 finetuned on Indonesian academic journals\nThis is the [Indonesian gpt2-small model](https://huggingface.co/flax-community/gpt2-small-indonesian) fine-tuned to abstracts of Indonesian academic journals. All training was done on a TPUv2-8 VM sponsored by [TPU Research Cloud](https://sites.research.google/trc/).\n\nThe demo can be found [here](https://huggingface.co/spaces/flax-community/gpt2-indonesian).\n\n## How to use\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, \nwe set a seed for reproducibility:\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='Galuh/id-journal-gpt2')\n>>> set_seed(42)\n>>> generator(\"Penelitian ini menggunakan teknik DNA barcoding untuk\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk mendeteksi perubahan genetik bakteri pada udang windu. Empat tahap telah dilakukan, meliputi preparasi media untuk larva,'},\n {'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk identifikasi gen pengasil flavonoid.  Data yang diperoleh dari hasil PCR diidentifikasi dengan teknik sekuensing'},\n {'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk mengekstraksi fragmen DNA dari sampel kulit buaya dan tulang anjing, di mana proses ini melibatkan karakterisasi enzim yang'},\n {'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk melakukan transformasi. Tahapan transformasi meliputi seleksi sel dengan urutan (2, 8, 16,..., 18) dan'},\n {'generated_text': 'Penelitian ini menggunakan teknik DNA barcoding untuk amplifikasi genom DNA dengan menggunakan primer TG8226 dan TG806. Metode pol'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('Galuh/id-journal-gpt2')\nmodel = GPT2Model.from_pretrained('Galuh/id-journal-gpt2')\ntext = \"Ubah dengan teks apa saja.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('Galuh/id-journal-gpt2')\nmodel = TFGPT2Model.from_pretrained('Galuh/id-journal-gpt2')\ntext = \"Ubah dengan teks apa saja.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n## Limitations and bias  \nThis model is originally the [Indonesian gpt2-small model](https://huggingface.co/flax-community/gpt2-small-indonesian), thus this model is also subject to the same [limitations and bias as the original model](https://huggingface.co/flax-community/gpt2-small-indonesian#limitations-and-bias). More detailed bias and analysis on this specific model is coming soon.\n\n## Training data\nThe model was trained on a dataset of Indonesian journals. We only trained this model on the abstracts. We extract the abstract by writing a script to find any text that is located between the word \"Abstrak\" (abstract) and \"Kata kunci\" (keywords). The extraction script can be found [here](https://github.com/galuhsahid/id-journal-gpt2/). To separate each abstract, we also add an end of text token (`<|endoftext|>`) between each abstract.\n\nThe information of the sub-dataset and the distribution of the training and evaluation dataset are as follows:\n\n| split | count | percentage |\n| ---------- | ---------- | -------------- |\n| train    | 146,248      | 90%         |\n| validation    | 16,250      | 10%         |\n\n## Training procedure \nThe model was trained on a TPUv2-8 VM provided by [TPU Research Cloud](https://sites.research.google/trc/). The training duration was `2h 30m 57s`.\n\n### Evaluation results \nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| dataset | train loss | eval loss | eval perplexity |\n| ---------- | ---------- | -------------- | ---------- |\n| Indonesian journals dataset (abstract only)    | 2.913      | 2.855         | 17.37   |\n\n### Tracking\nThe training process was tracked in [TensorBoard](https://huggingface.co/Galuh/id-journal-gpt2/tensorboard).", "size_bytes": "510401385", "downloads": 61}
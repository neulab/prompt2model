{"pretrained_model_name": "supermy/couplet-gpt2", "description": "---\nlanguage: zh\ndatasets: couplet\ninference:\n  parameters:\n    max_length: 68\n    num_return_sequences: 1\n    do_sample: True\nwidget: \n- text: \"\u71d5\u5b50\u5f52\u6765\uff0c\u95ee\u6614\u65e5\u96d5\u6881\u4f55\u5904\u3002 -\"\n  example_title: \"\u5bf9\u80541\"\n- text: \"\u7b11\u53d6\u7434\u4e66\u6e29\u65e7\u68a6\u3002 -\"\n  example_title: \"\u5bf9\u80542\"\n- text: \"\u7166\u7166\u6625\u98ce\uff0c\u5439\u6696\u4e94\u6e56\u56db\u6d77\u3002 -\"\n  example_title: \"\u5bf9\u80543\"\n---\n\n\n# \u5bf9\u8054\n\n## Model description\n\n  \u5bf9\u8054AI\u751f\u6210\uff0c\u7ed9\u51fa\u4e0a\u8054\uff0c\u751f\u6210\u4e0b\u8054\u3002\n\n## How to use\n\u4f7f\u7528 pipeline \u8c03\u7528\u6a21\u578b:\n\n```python\n>>> # \u8c03\u7528\u5fae\u8c03\u540e\u7684\u6a21\u578b\n>>> senc=\"\u71d5\u5b50\u5f52\u6765\uff0c\u95ee\u6614\u65e5\u96d5\u6881\u4f55\u5904\u3002 -\"\n>>> model_id=\"couplet-gpt2-finetuning\"\n>>> from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n\n>>> tokenizer = BertTokenizer.from_pretrained(model_id)\n>>> model = GPT2LMHeadModel.from_pretrained(model_id)\n>>> text_generator = TextGenerationPipeline(model, tokenizer)   \n>>> text_generator.model.config.pad_token_id = text_generator.model.config.eos_token_id\n>>> text_generator( senc,max_length=25, do_sample=True)\n[{'generated_text': '\u71d5\u5b50\u5f52\u6765\uff0c\u95ee\u6614\u65e5\u96d5\u6881\u4f55\u5904\u3002 - \u98ce \u513f \u5439 \u9192 \uff0c \u53f9 \u4eca \u671d \u70df \u96e8 \u65e0'}]\n```\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"supermy/couplet\")\nmodel = AutoModelForCausalLM.from_pretrained(\"supermy/couplet\")\n```\n\n\n\n## Training data\n\n\u6b64\u6570\u636e\u96c6\u57fa\u4e8ecouplet-dataset\u768470w\u6761\u6570\u636e\u96c6\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u5229\u7528\u654f\u611f\u8bcd\u8bcd\u5e93\u5bf9\u6570\u636e\u8fdb\u884c\u4e86\u8fc7\u6ee4\uff0c\u5220\u9664\u4e86\u4f4e\u4fd7\u6216\u654f\u611f\u7684\u5185\u5bb9\uff0c\u5220\u9664\u540e\u5269\u4f59\u7ea674w\u6761\u5bf9\u8054\u6570\u636e\u3002\n\n## \u7edf\u8ba1\u4fe1\u606f\n\n```\n\n```\n\n## Training procedure\n\n\u6a21\u578b\uff1a[GPT2](https://huggingface.co/gpt2) \n\u8bad\u7ec3\u73af\u5883\uff1a\u82f1\u4f1f\u8fbe16G\u663e\u5361\n\nbpe\u5206\u8bcd\uff1a\"vocab_size\"=50000\n```\n[INFO|trainer.py:1608] 2022-12-07 02:32:58,307 >> ***** Running training *****\n[INFO|trainer.py:1609] 2022-12-07 02:32:58,307 >>   Num examples = 260926\n[INFO|trainer.py:1610] 2022-12-07 02:32:58,307 >>   Num Epochs = 160\n[INFO|trainer.py:1611] 2022-12-07 02:32:58,307 >>   Instantaneous batch size per device = 96\n[INFO|trainer.py:1612] 2022-12-07 02:32:58,307 >>   Total train batch size (w. parallel, distributed & accumulation) = 96\n[INFO|trainer.py:1613] 2022-12-07 02:32:58,307 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1614] 2022-12-07 02:32:58,307 >>   Total optimization steps = 434880\n[INFO|trainer.py:1616] 2022-12-07 02:32:58,308 >>   Number of trainable parameters = 124439808\n[INFO|trainer.py:1637] 2022-12-07 02:32:58,309 >>   Continuing training from checkpoint, will skip to saved global_step\n[INFO|trainer.py:1638] 2022-12-07 02:32:58,310 >>   Continuing training from epoch 93\n[INFO|trainer.py:1639] 2022-12-07 02:32:58,310 >>   Continuing training from global step 253500\n\n\n[INFO|trainer.py:1608] 2022-11-30 12:51:36,357 >> ***** Running training *****\n[INFO|trainer.py:1609] 2022-11-30 12:51:36,357 >>   Num examples = 260926\n[INFO|trainer.py:1610] 2022-11-30 12:51:36,357 >>   Num Epochs = 81\n[INFO|trainer.py:1611] 2022-11-30 12:51:36,357 >>   Instantaneous batch size per device = 96\n[INFO|trainer.py:1612] 2022-11-30 12:51:36,357 >>   Total train batch size (w. parallel, distributed & accumulation) = 96\n[INFO|trainer.py:1613] 2022-11-30 12:51:36,357 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1614] 2022-11-30 12:51:36,357 >>   Total optimization steps = 220158\n[INFO|trainer.py:1616] 2022-11-30 12:51:36,358 >>   Number of trainable parameters = 124439808\n\n{'loss': 6.1104, 'learning_rate': 4.9888034956712906e-05, 'epoch': 0.18}\n{'loss': 5.5855, 'learning_rate': 4.977448014607691e-05, 'epoch': 0.37}\n{'loss': 5.3264, 'learning_rate': 4.966092533544091e-05, 'epoch': 0.55}\n......\n......\n......\n{'loss': 2.8539, 'learning_rate': 5.677740531799889e-08, 'epoch': 80.94}\n{'train_runtime': 146835.0563, 'train_samples_per_second': 143.937, 'train_steps_per_second': 1.499, 'train_loss': 3.1762605669072217, 'epoch': 81.0}\n***** train metrics *****\n  epoch                    =               81.0\n  train_loss               =             3.1763\n  train_runtime            = 1 day, 16:47:15.05\n  train_samples            =             260926\n  train_samples_per_second =            143.937\n  train_steps_per_second   =              1.499\n12/02/2022 05:38:54 - INFO - __main__ - *** Evaluate ***\n[INFO|trainer.py:2929] 2022-12-02 05:38:54,688 >> ***** Running Evaluation *****\n[INFO|trainer.py:2931] 2022-12-02 05:38:54,688 >>   Num examples = 1350\n[INFO|trainer.py:2934] 2022-12-02 05:38:54,688 >>   Batch size = 96\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:03<00:00,  4.20it/s]\n[INFO|modelcard.py:449] 2022-12-02 05:38:59,875 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.4447501469723692}]}\n***** eval metrics *****\n  epoch                   =       81.0\n  eval_accuracy           =     0.4448\n  eval_loss               =     3.2813\n  eval_runtime            = 0:00:03.86\n  eval_samples            =       1350\n  eval_samples_per_second =    349.505\n  eval_steps_per_second   =      3.883\n  perplexity              =    26.6108\n\n\n{'loss': 3.0967, 'learning_rate': 1.8027961736571009e-07, 'epoch': 159.49}\n{'loss': 3.0922, 'learning_rate': 1.227924944812362e-07, 'epoch': 159.68}\n{'loss': 3.0934, 'learning_rate': 6.530537159676233e-08, 'epoch': 159.86}\n{'train_runtime': 120967.2394, 'train_samples_per_second': 345.12, 'train_steps_per_second': 3.595, 'train_loss': 1.3456422273861828, 'epoch': 160.0}\n***** train metrics *****\n  epoch                    =             160.0\n  train_loss               =            1.3456\n  train_runtime            = 1 day, 9:36:07.23\n  train_samples            =            260926\n  train_samples_per_second =            345.12\n  train_steps_per_second   =             3.595\n12/08/2022 12:09:08 - INFO - __main__ - *** Evaluate ***\n[INFO|trainer.py:2929] 2022-12-08 12:09:08,522 >> ***** Running Evaluation *****\n[INFO|trainer.py:2931] 2022-12-08 12:09:08,522 >>   Num examples = 1350\n[INFO|trainer.py:2934] 2022-12-08 12:09:08,522 >>   Batch size = 96\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:03<00:00,  4.16it/s]\n[INFO|modelcard.py:449] 2022-12-08 12:09:13,448 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.433615520282187}]}\n***** eval metrics *****\n  epoch                   =      160.0\n  eval_accuracy           =     0.4336\n  eval_loss               =     3.3005\n  eval_runtime            = 0:00:03.93\n  eval_samples            =       1350\n  eval_samples_per_second =    343.164\n  eval_steps_per_second   =      3.813\n  perplexity              =    27.1257\n\n```\n", "size_bytes": "510394089", "downloads": 8}
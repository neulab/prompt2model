{"pretrained_model_name": "IDEA-CCNL/Wenzhong-GPT2-3.5B", "description": "---\nlanguage: \n  - zh\n\ninference:\n  parameters:\n    max_new_tokens: 128\n    do_sample: True\n    \n    \n\nlicense: apache-2.0\n---\n\n# Wenzhong-GPT2-3.5B\n\n- Github: [Fengshenbang-LM](https://github.com/IDEA-CCNL/Fengshenbang-LM)\n- Docs: [Fengshenbang-Docs](https://fengshenbang-doc.readthedocs.io/)\n\n## \u7b80\u4ecb Brief Introduction\n\n\u5584\u4e8e\u5904\u7406NLG\u4efb\u52a1\uff0c\u76ee\u524d\u6700\u5927\u7684\uff0c\u4e2d\u6587\u7248\u7684GPT2\n\nFocused on handling NLG tasks, the current largest, Chinese GPT2.\n\n## \u6a21\u578b\u5206\u7c7b Model Taxonomy\n\n|  \u9700\u6c42 Demand  | \u4efb\u52a1 Task       | \u7cfb\u5217 Series      | \u6a21\u578b Model    | \u53c2\u6570 Parameter | \u989d\u5916 Extra |\n|  :----:  | :----:  | :----:  | :----:  | :----:  | :----:  |\n| \u901a\u7528 General  | \u81ea\u7136\u8bed\u8a00\u751f\u6210 NLG| \u95fb\u4ef2 Wenzhong | GPT2 |      3.5B      |     \u4e2d\u6587 Chinese     |\n\n## \u6a21\u578b\u4fe1\u606f Model Information\n\n\u4e3a\u4e86\u53ef\u4ee5\u83b7\u5f97\u4e00\u4e2a\u5f3a\u5927\u7684\u5355\u5411\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u4eec\u91c7\u7528GPT\u6a21\u578b\u7ed3\u6784\uff0c\u5e76\u4e14\u5e94\u7528\u4e8e\u4e2d\u6587\u8bed\u6599\u4e0a\u3002\u5177\u4f53\u5730\uff0c\u8fd9\u4e2a\u6a21\u578b\u62e5\u670930\u5c42\u89e3\u7801\u5668\u548c35\u4ebf\u53c2\u6570\uff0c\u8fd9\u6bd4\u539f\u672c\u7684GPT2-XL\u8fd8\u8981\u5927\u3002\u6211\u4eec\u5728100G\u7684\u4e2d\u6587\u8bed\u6599\u4e0a\u9884\u8bad\u7ec3\uff0c\u8fd9\u6d88\u8017\u4e8632\u4e2aNVIDIA A100\u663e\u5361\u5927\u7ea628\u5c0f\u65f6\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u5b83\u662f\u76ee\u524d\u6700\u5927\u7684\u4e2d\u6587\u7684GPT\u6a21\u578b\u3002\n\nTo obtain a robust unidirectional language model, we adopt the GPT model structure and apply it to the Chinese corpus. Specifically, this model has 30 decoder layers and 3.5 billion parameters, which is larger than the original GPT2-XL. We pre-train it on 100G of Chinese corpus, which consumes 32 NVIDIA A100 GPUs for about 28 hours. To the best of our knowledge, it is the largest Chinese GPT model currently available.\n\n## \u4f7f\u7528 Usage\n\n### \u52a0\u8f7d\u6a21\u578b Loading Models\n\n```python \nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('IDEA-CCNL/Wenzhong-GPT2-3.5B')\nmodel = GPT2Model.from_pretrained('IDEA-CCNL/Wenzhong-GPT2-3.5B')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\n### \u4f7f\u7528\u793a\u4f8b Usage Examples\n\n```python\nfrom transformers import pipeline, set_seed\nset_seed(55)\ngenerator = pipeline('text-generation', model='IDEA-CCNL/Wenzhong-GPT2-3.5B')\ngenerator(\"\u5317\u4eac\u4f4d\u4e8e\", max_length=30, num_return_sequences=1)\n\n```\n\n## \u5f15\u7528 Citation\n\n\u5982\u679c\u60a8\u5728\u60a8\u7684\u5de5\u4f5c\u4e2d\u4f7f\u7528\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684[\u8bba\u6587](https://arxiv.org/abs/2209.02970)\uff1a\n\nIf you are using the resource for your work, please cite the our [paper](https://arxiv.org/abs/2209.02970):\n\n```text\n@article{fengshenbang,\n  author    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},\n  title     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},\n  journal   = {CoRR},\n  volume    = {abs/2209.02970},\n  year      = {2022}\n}\n```\n\n\u4e5f\u53ef\u4ee5\u5f15\u7528\u6211\u4eec\u7684[\u7f51\u7ad9](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n\nYou can also cite our [website](https://github.com/IDEA-CCNL/Fengshenbang-LM/):\n\n```text\n@misc{Fengshenbang-LM,\n  title={Fengshenbang-LM},\n  author={IDEA-CCNL},\n  year={2021},\n  howpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},\n}\n```", "size_bytes": "7175586541", "downloads": 131}
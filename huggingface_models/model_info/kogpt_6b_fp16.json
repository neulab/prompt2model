{"pretrained_model_name": "MrBananaHuman/kogpt_6b_fp16", "description": "kakao brain\uc5d0\uc11c \uacf5\uac1c\ud55c kogpt 6b model('kakaobrain/kogpt')\uc744 fp16\uc73c\ub85c \uc800\uc7a5\ud55c \ubaa8\ub378\uc785\ub2c8\ub2e4.\n\n### \uce74\uce74\uc624\ube0c\ub808\uc778 \ubaa8\ub378\uc744 fp16\uc73c\ub85c \ub85c\ub4dc\ud558\ub294 \ubc29\ubc95\n\n```python\nimport torch\nfrom transformers import GPTJForCausalLM\n\nmodel = GPTJForCausalLM.from_pretrained('kakaobrain/kogpt', cache_dir='./my_dir', revision='KoGPT6B-ryan1.5b', torch_dtype=torch.float16)\n```\n\n### fp16 \ubaa8\ub378 \ub85c\ub4dc \ud6c4 \ubb38\uc7a5 \uc0dd\uc131\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_rLDzhGohJPbOD5I_eTIOdx4aOTp43uK?usp=sharing)\n\n```python\nimport torch\nfrom transformers import GPTJForCausalLM, AutoTokenizer\n\nmodel = GPTJForCausalLM.from_pretrained('MrBananaHuman/kogpt_6b_fp16', low_cpu_mem_usage=True))\nmodel.to('cuda')\ntokenizer = AutoTokenizer.from_pretrained('MrBananaHuman/kogpt_6b_fp16')\n\ninput_text = '\uc774\uc21c\uc2e0\uc740'\ninput_ids = tokenizer(input_text, return_tensors='pt').input_ids.to('cuda')\n\noutput = model.generate(input_ids, max_length=64)\nprint(tokenizer.decode(output[0]))\n\n>>> \uc774\uc21c\uc2e0\uc740 \uc6b0\ub9ac\uc5d0\uac8c \ubb34\uc5c7\uc778\uac00? 1. \uba38\ub9ac\ub9d0 \uc774\uae00\uc740 \uc784\uc9c4\uc65c\ub780 \ub2f9\uc2dc \uc774\uc21c\uc778\uc774 \ubcf4\uc5ec\uc900\n\n```\n\n### \ucc38\uace0 \ub9c1\ud06c\nhttps://github.com/kakaobrain/kogpt/issues/6?fbclid=IwAR1KpWhuHnevQvEWV18o16k2z9TLgrXkbWTkKqzL-NDXHfDnWcIq7I4SJXM", "size_bytes": "12450571703", "downloads": 43}
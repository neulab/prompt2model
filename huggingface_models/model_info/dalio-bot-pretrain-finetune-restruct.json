{"pretrained_model_name": "Jellywibble/dalio-bot-pretrain-finetune-restruct", "description": "---\ntags:\n- text-generation\nlibrary_name: transformers\n---\n\n## Model description\nDalio Bot Pre-trained on Principles, fine-tuned on handwritten examples.\n\nPre-trained model: Jellywibble/dalio-pretrained-book-bs4-seed1 (based-off OPT30B)\n\nFine-tuning dataset: Jellywibble/dalio_handwritten-conversations\n\n## Model Parameters\n- 4xA40 (eff. batch size = 4)\n- base_mode_name Jellywibble/dalio-pretrained-book-bs4-seed1\n- dataset_name Jellywibble/dalio_handwritten-conversations\n- block size 500\n- per_device_train_batch_size 1\n- gradient_accumulation steps 1\n- learning_rate 2e-6\n- seed 28\n- validation split percentage 20\n- hellaswag_sample_size 100\n\n## Metrics\n- Hellaswag Perplexity: 29.9\n- Eval acc: 57.1%\n- Eval loss: 1.971\n- wandb: https://wandb.ai/jellywibble/huggingface/runs/12lgyt20?workspace=user-jellywibble\n- Checkpoint 10 selected and uploaded", "size_bytes": 121339158528, "downloads": 3}
{"pretrained_model_name": "alexandreteles/GPTChizuru", "description": "---\nlicense: cc0-1.0\nthumbnail: https://huggingface.co/front/thumbnails/dialogpt.png\ntags:\n- conversational\ndatasets:\n- alexandreteles/chizuru-ichinose\nmodel-index:\n- name: gptchizuru\n  results:\n  - task:\n      type: conversational\n      name: Conversational\n    dataset:\n      type: alexandreteles/chizuru-ichinose\n      name: chizuru\n    metrics:\n      - type: perplexity\n        value: 1.0784\n        name: Perplexity\n        verified: false\n---\n\n## Chizuru Ichinose as a DialoGPT model\n\nThis model is a fine-tuned version of [DialoGPT-medium](https://huggingface.co/microsoft/DialoGPT-medium/) trained on the [Chizuru Ichinose conversational dataset](https://huggingface.co/datasets/alexandreteles/chizuru-ichinose).\n\nWe recommend using one of the Transformers pre-built pipelines to keep context without too much work when running this or any DialoGPT model:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, Conversational\n\n# load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"alexandreteles/GPTChizuru\")\nmodel = AutoModelForCausalLM.from_pretrained(\"alexandreteles/GPTChizuru\")\n\n# create pipeline\npipe = pipeline(task=\"conversational\", model=model, tokenizer=tokenizer)\n\n# generate response\nprint(pipe(Conversation(\"How are you?\")))\n\n```", "size_bytes": "1444581337", "downloads": 16}